{"paragraphs": ["FNet: Mixing Tokens with Fourier Transforms. We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.1 FNet: Mixing Tokens with Fourier Transforms. We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).", "Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.1\n1 Introduction. The Transformer architecture (Vaswani et al., 2017) has achieved rapid and widespread dominance in NLP. At its heart is a attention mechanism \u2013 an inductive bias that connects each token in the input through a relevance weighted basis of every other token. Many papers have prodded and probed the Transformer, and in particular the attention sublayers, in an effort to better understand the architecture; see, for example, Tenney et al. (2019); Vig and Belinkov (2019); Clark et al. (2019); Voita et al. (2019). Although potentially limited in their effectiveness (Hewitt and Liang, 2019), these probes generally back the intuition that, by allowing higher order units to form out of compositions of the input,\n1Code is available at https://github.com/ google-research/google-research/tree/ master/f_net. Transformer models can flexibly capture diverse syntactic and semantic relationships. In this work, we investigate whether simpler token mixing mechanisms can wholly replace the relatively complex self-attention layers in Transformer encoder architectures. We first replace the attention sublayer with two parameterized matrix multiplications \u2013 one mixing the sequence dimension and one mixing the hidden dimension. Seeing promising results in this simple linear mixing scheme, we further investigate the efficacy of faster, structured linear transformations. Surprisingly, we find that the Fourier Transform, despite having no parameters at all, achieves nearly the same performance as dense linear mixing and scales very efficiently to long inputs, especially on GPUs (owing to the O(N logN) Fast Fourier Transform (FFT) algorithm). We call the resulting model FNet.", "While Fourier Transforms have previously been used to approximate or speed up computations in Convolutional Neural Networks (El-Bakry and Zhao, 2004; Mathieu et al., 2014; Highlander and Rodriguez, 2015; Pratt et al., 2017; Lin et al., 2018; Chitsaz et al., 2020; Goldberg et al., 2020), Recurrent Neural Networks (Koplon and Sontag, 1997; Zhang and Chan, 2000; Zhang et al., 2018), Transformers (Choromanski et al., 2020; Tamkin et al., 2020), and MLP layers more generally (Cheng et al., 2015; Moczulski et al., 2016; Sindhwani et al., 2015), we believe our work is the first to wholly replace particular neural network sublayers with a Fourier Transform. This approach of viewing the Fourier Transform as a first class mixing mechanism is reminiscent of the MLP-Mixer (Tolstikhin et al., 2021) for vision, which replaces attention with MLPs; although in contrast to MLPMixer, FNet has no learnable parameters that mix along the spatial dimension. Given the favorable asymptotic complexity of the FFT, our work also connects with the literature on \u201clong sequence\u201d or \u201cefficient\u201d Transformers,\nar X\niv :2\n10 5.\n03 82\n4v 4\n[ cs\n.C L\n] 2\n6 M\nay 2\n02 2\nwhich aim to make the attention mechanism scale better via sparsity patterns (Child et al., 2019; Qiu et al., 2020; Parmar et al., 2018; Beltagy et al., 2020; Ainslie et al., 2020; Zaheer et al., 2020; Wang et al., 2020; Tay et al., 2020b,a; Kitaev et al., 2020; Roy et al., 2021; Vyas et al., 2020; Liu et al., 2018) or via linearization of the attention matrix (Katharopoulos et al., 2020; Choromanski et al., 2021; Peng et al., 2021). As we will show in our experiments, while some of those works achieve O(N) scaling of attention, this complexity often hides large constants, which make them less scalable in practice than FNet.", "The contributions of our paper are:\n\u2022 We show that simple linear transformations, including even (parameter-free) Fourier Transforms, along with standard MLPs in feedforward layers, are competent at modeling diverse relationships in text. That such a simple linear transformation works at all is surprising, and suggests that, for at least some NLP problems, attention may not be the principal component driving the performance of Transformers. \u2022 We introduce a new model, FNet, that uses the Fourier Transform as a mixing mechanism. FNet offers an excellent compromise between speed, memory footprint, and accuracy, achieving 92% and 97%, respectively, of the accuracy of BERT-Base and BERT-Large (Devlin et al., 2019) on the GLUE benchmark (Wang et al., 2018), while training 80% faster on GPUs and 70% faster on TPUs. \u2022 We find that FNet hybrid models containing only two self-attention sublayers achieve 97\u2212 99% of their BERT counterparts\u2019 accuracy on GLUE, while still running 40\u2212 70% faster. This indicates that, while attention can improve accuracy, it may not be necessary to use in every layer. \u2022 We demonstrate FNet scales very well to long inputs and offers a better compromise between speed and accuracy than the efficient Transformers evaluated on the Long-Range Arena (LRA) benchmark (Tay et al., 2021a). Specifically, FNet achieves accuracy comparable to the most accurate efficient Transformer architectures but is significantly faster at both\ntraining and inference than all of the evaluated Transformer architectures across all sequence lengths on GPUs. On TPUs, FNet is faster for relatively shorter sequence lengths; for longer sequences, the only efficient Transformers that are faster than FNet on TPUs are less accurate on the LRA benchmark. Based on this, we argue that rather than seeking more efficient approximations of the attention, there may be more value in seeking out completely new mixing mechanisms. 2 Related work. 2.1 Fourier Transforms in neural networks.", "Fourier analysis features heavily in studies of the universal approximation properties of neural networks; see, for example, (Cybenko, 1989; Barron, 1993). In terms of practical applications, discrete Fourier Transforms (DFT), and in particular the Fast Fourier Transform (FFT), have been used to tackle signal processing problems such as fitting neural networks to FFTs of electrocardiogram signals (Minami et al., 1999; Gothwal et al., 2011; Mironovova and B\u00edla, 2015) and vibration signals (Zhang et al., 2013), or to evolve solutions of Partial Differential Equations (Li et al., 2021). Because ordinary multiplication in the frequency domain corresponds to a convolution in the time domain, FFTs have been deployed in Convolutional Neural Networks to speed up computations, in Recurrent Neural Networks to speed up training and reduce exploding and vanishing gradients, and generally to approximate dense, linear layers to reduce computational complexity; see references cited in Section 1. DFTs have also been used indirectly in several Transformer works. The Performer (Choromanski et al., 2020) linearizes the Transformer selfattention mechanism by leveraging random Fourier features to approximate a Gaussian representation of the softmax kernel. In our work, rather than approximating attention, we replace attention with the Fourier Transform, which acts as an alternate hidden representation mixing mechanism. Tamkin et al. (2020) use spectral filters to generate hierarchical features, showing that the filtered embeddings perform well in different tasks (word-level, sentence-level or document-level), depending on which frequency scales are filtered. In contrast to FNet, they separate Fourier frequencies, rather than using the transform to combine features. Finally, through personal communication, we were alerted\nto concurrent, unpublished work (Backurs et al., 2021) that describes an FFT based neural model that is very similar to FNet. 2.2 Modeling semantic relations via attention.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "Several \u201cefficient Transformers\u201d achieve O(N \u221a N) or even O(N) theoretical complexity. However, the constants hidden by this notation can be large. For example, in models such as Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020), attention is O(N) as a\nfunction of the input length, but quadratic in the number of \u201cglobal tokens\u201d; the latter must be sufficiently large to ensure good performance. The Long-Range Arena benchmark (Tay et al., 2021a) attempts to compare many of the efficient Transformers in a series of tasks requiring long range dependencies, finding that the Performer (Choromanski et al., 2021), Linear Transformer (Katharopoulos et al., 2020), Linformer (Wang et al., 2020), and Image Transformer (Local Attention) (Parmar et al., 2018) were the fastest on TPUs and had the lowest peak memory usages per device.2 Instead, in this paper we completely replace self-attention with a different mixing, namely the Fourier Transform, which offers: (1) performance, (2) reduced model size (no learnable parameters), and (3) simplicity. Finally, we note that, in an effort to investigate different token mixing mechanisms, we compare a vanilla BERT model (Devlin et al., 2019) with a vanilla FNet, ignoring more recent Transformer optimizations, which we consider orthogonal to this work; see, for example, (Narang et al., 2021; Kim and Hassan, 2020; Shleifer and Rush, 2020). 3 Model.\n3.1 Discrete Fourier Transform. The Fourier Transform decomposes a function into its constituent frequencies. Given a sequence {xn} with n \u2208 [0, N\u22121], the discrete Fourier Transform (DFT) is defined by the formula: Xk = N\u22121\u2211 n=0 xne \u2212 2\u03c0i N nk, 0 \u2264 k \u2264 N \u2212 1. (1)\nFor each k, the DFT generates a new representation Xk as a sum of all of the original input tokens xn, with so-called \u201ctwiddle factors\u201d. There are two primary approaches to computing the DFT: the Fast Fourier Transform (FFT) and matrix multiplication.", "The standard FFT algorithm is the Cooley\u2013Tukey algorithm (Cooley and Tukey, 1965; Frigo and Johnson, 2005), which recursively re-expresses the DFT of a sequence of length N = N1N2 in terms of N1 smaller DFTs of sizes N2 to reduce the computation time to O(N logN). An alternative approach is to simply apply the DFT matrix to the input sequence. The DFT matrix,\n2Memory usage is often overlooked, but empirical studies have shown that Transformer architectures are often memorybound (Ivanov et al., 2020; Shazeer, 2019). Embeddings\nFeed Forward\nAdd & Normalize\nFourier\nAdd & Normalize\nWord Position Type\nDense\nOutput Projection\nInput\nOutput\n+ +\nN x\nEmbeddings\nFeed Forward\nAdd & Normalize\nFourier\nAdd & Normalize\nWord Position Type\nDense\nOutput Projection\nInput\nOutput\n+ +\nN x\nEmbeddings\nFeed ForwardFourier Add / Norm\nWord\nPosition\nType\nDense Output ProjectionInput Output +\n+\nN x\nAdd / Norm\nFigure 1: FNet architecture with N encoder blocks. W , is a Vandermonde matrix for the roots of unity up to a normalization factor: Wnk = ( e\u2212 2\u03c0i N nk/ \u221a N ) , (2)\nwhere n, k = 0, . . . , N \u2212 1. This matrix multiplication is an O(N2) operation, which has higher asymptotic complexity than the FFT, but turns out to be faster for relatively shorter sequences on TPUs.\n3.2 FNet architecture. FNet is an attention-free Transformer architecture, wherein each layer consists of a Fourier mixing sublayer followed by a feed-forward sublayer. The architecture is shown in Figure 1. Essentially, we replace the self-attention sublayer of each Transformer encoder layer with a Fourier sublayer, which applies a 2D DFT to its (sequence length, hidden dimension) embedding input \u2013 one 1D DFT along the sequence dimension, Fseq, and one 1D DFT along the hidden dimension, Fh:3\ny = < ( Fseq (Fh(x)) ) . (3)\nAs indicated by Equation (3), we only keep the real part of the result; hence, we do not need to modify the (nonlinear) feed-forward sublayers or output layers to handle complex numbers.", "We found that FNet obtained the best results when the real part of the total transformation was only extracted at\n3The relative ordering of Fseq and Fh in Equation (3) is immaterial because the two 1D DFTs commute. the end of the Fourier sublayer; that is, after applying both Fseq and Fh. We also experimented with the Hadamard, Hartley and Discrete Cosine Transforms. Of these three, the Hartley Transform was the strongest alternative, obtaining comparable accuracy to Equation (3); see Appendix A.3 for details. The simplest interpretation for the Fourier Transform is as a particularly effective mechanism for mixing tokens, which provides the feed-forward sublayers sufficient access to all tokens. Because of the duality of the Fourier Transform, we can also view each alternating encoder block as applying alternating Fourier and inverse Fourier Transforms, transforming the input back and forth between the \u201ctime\u201d and frequency domain. Because multiplying by the feed-forward sublayer coefficients in the frequency domain is equivalent to convolving (with a related set of coefficients) in the time domain, FNet can be thought of as alternating between multiplications and convolutions.4\nWe use the same embedding layers as in Devlin et al. (2019); namely, we combine the word embeddings, absolute position embeddings of the tokens and type embeddings of the sentences. Because of the positional information encoded by the Fourier Transform in Equation (1) (see n, k indices), FNet performs just as well without position embeddings. Nevertheless, we include the position embeddings to allow for a cleaner comparison with BERT. 3.3 Implementation.", "Empirically, we found that on GPUs: the FFT is faster than matrix multiplications for all sequence lengths we consider (512\u2212 8192 tokens), whereas on TPUs: for relatively shorter sequences (\u2264 4096 tokens), it is faster to cache the DFT matrix and then compute the DFT through matrix multiplications than using the FFT; for longer sequences, the FFT is faster. As a result, our GPU FNet implementation always uses the FFT, while our TPU implementation computes the 2D DFT using matrix multiplications for sequences up to lengths of 4096 and the FFT for longer lengths. Presumably the GPU vs TPU difference is primarily a result of two factors: (1) TPUs are even more highly optimized for matrix multiplications than GPUs, and (2) GPUs offer a more efficient FFT implementa-\n4This is merely an intuition; the reality is more complicated due to the presence of residual connections and since the transformation in Equation (3) is no longer invertible if we only use the real component.\ntion than TPUs. We suspect that FNet will only become more performant on TPUs as the TPU implementation of the FFT improves. Our model uses JAX and, in particular, the Flax framework5. Core model code is given in Appendix A.7 and the full source core is available online.6\n4 Results. 4.1 Transfer learning. We compare FNet and Transformer architectures in a common transfer learning setting. For a fuller picture, we compare multiple models (see Table 1 for parameter counts in \u201cBase\u201d configuration):\n\u2022 BERT-Base: a Transformer encoder model. \u2022 FNet encoder: we replace every self-attention sublayer with a Fourier sublayer. \u2022 Linear encoder: we replace each self-attention sublayer with a two learnable, dense, linear sublayers, one applied to the hidden dimension and one to the sequence dimension. \u2022 Random encoder: we replace each selfattention sublayer with a two constant random matrices, one applied to the hidden dimension and one applied to the sequence dimension. \u2022", "Feed Forward-only (FF-only) encoder: we completely remove the self-attention sublayer; so that this model has no token mixing. 5https://github.com/google/flax 6https://github.com/google-research/\ngoogle-research/tree/master/f_net\nDespite its simplicity, the Linear baseline turns out to be surprisingly accurate and fast. Our Linear model is similar to the MLP-Mixer (Tolstikhin et al., 2021) (for vision) and also the Random Synthesizer (Tay et al., 2020a), but simplifies the latter model further by removing the multiple heads and softmax projections, resulting in just two matrix multiplications in the mixing sublayer. It is reasonable to expect that the Linear encoder, which uses densely parameterized mixing layers, will learn more flexibly than FNet, which uses parameter-free mixing layers. As we will show, although the Linear-Base model outperforms FNetBase slightly on GLUE (0.3 points), it has several efficiency drawbacks relative to FNet: it has a much larger memory footprint (see Table 4b), it is slower to train on regular 512 sequence lengths (see Table 3), and scales significantly worse on long sequence lengths (see Tables 4b-4c).7 We also found that Linear-Large was more difficult to train due to gradient blow up (see \u201cLarge\u201d scores in Table 2). We adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and training configurations as for the original BERT (Devlin et al., 2019), except that we pretrain on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) (see Appendix A.1 for full pre-training details). For fine-tuning on the GLUE benchmark (Wang et al., 2018), we found that different BERT runs with the same base learning rate could yield slightly different results. Consequently, for the Base (Large) models, we performed 3 (6) trials, respectively, for each base learning rate and reported the best result across all experiments.", "This reflects our observation that BERT-Large was less stable than BERT-Base, as noted in Devlin et al. (2019). We report the results for the best base learning rate (no early stopping) on the GLUE Validation split in Table 2.8 For Base models, results mirror the pre-training metrics (see Appendix A.1): BERT performs best. FNet and the Linear model both underperform BERT by 7.5 \u2212 8%. Referring to Table 3, we see that although less accurate, FNet trains significantly faster than BERT \u2013 80% faster on GPUs and 70% faster on TPUs \u2013 and performs\n7On the other hand, the smaller sized Linear models do generally perform well on 512 sequence lengths; see Figure 2. 8WNLI is excluded in Devlin et al. (2019). BERT\u2019s accuracy on WNLI is below baseline, unless a special training recipe is used. See also (12) in https:// gluebenchmark.com/faq. 63% of BERT\u2019s FLOPS. Measured in isolation, the Fourier sublayers perform forward and backward passes an order of magnitude faster than the self-attention sublayers (see Appendix A.4), but FNet\u2019s overall training speed is impeded by the feed-forward sublayers that all models share. Returning to Table 2: the FF-only model severely underperforms all other models: as expected, token mixing is critical to the expressivity of the model. For example, 50% accuracy scores on the binary classification tasks (QNLI, SST-2, RTE), indicate that the model fails to learn the tasks. The weak accuracy of the Random model suggests that not just any mixing will do; rather, a structured mixing is required. We also include metrics from a hybrid FNet attention model. In the hybrid model, we replace the final two Fourier sublayers of FNet with self-attention sublayers \u2013 other configurations\nare possible, but we generally found that replacing the final layers worked best; see Appendix A.5.", "With the addition of just two self-attention sublayers, the hybrid FNet models achieve 97% and 99% of their respective BERT counterpart\u2019s accuracies with only limited speed degradations (see Table 3). Interestingly, the gap between BERT and FNet shrinks to just 3% for Large models; this is likely due to FNet-Large being more stable during training than BERT-Large.9 The Linear-Large model severely underperforms its Base counterpart on GLUE benchmark due to training instabilities. We generally found that the Linear model and BERT were less stable than the models with no param-\n9Devlin et al. (2019) obtain a roughly 2.5 average point boost on the Test split going from BERT-Base to BERT-Large. We only see a roughly 1.5 boost on the Validation split, which may be due to reduced headroom.\neters in their mixing sublayers, namely the FNet, Random and FF-only models. The speed vs MLM accuracy curve for GPU (8 V100 chips) pre-training is shown in Figure 2 (see Appendix A.2 for TPU results). Both TPU and GPU models are trained for 1 million steps as in Devlin et al. (2019). Motivated by the models considered in Turc et al. (2019), we evaluated several model sizes; see Table 6 in Appendix A.1. We found that the smaller model architectures benefited from larger learning rates, so we select the best result using 10\u22123 and 10\u22124 for all models.10\nThe GPU (Figure 2), and TPU (Figure 3 in Appendix A.2) results display the same trends. For larger, slower models, BERT and FNet-Hybrid define the Pareto speed-accuracy efficiency frontier. For smaller, faster models, FNet and the Linear model define the efficiency frontier. 4.2 Long-Range Arena (LRA) benchmark. Of the efficient Transformers evaluated on LRA benchmark by Tay et al. (2021a), their results suggest that (1) the vanilla Transformer is (by a small margin) the second most accurate model, and (2) the Performer (Choromanski et al., 2021) is the fastest model.", "We benchmark FNet\u2019s accuracy against both of these models using Tay et al.\n10We have opted to compare FNet with Transformer models as the latter are the most commonly used models in NLP transfer learning settings. It would also be interesting to compare FNet with convolutional-based models, although, to our knowledge, such models have only recently found limited success in pre-training NLP setups (Tay et al., 2021b); and even there, the authors did not consider the small model regime. (2021a)\u2019s codebase and running on the same hardware (4\u00d7 4 TPU v3 chips); the results are shown in Table 4a.11 To ensure a fair comparison, we also report the results of our own experiments for the vanilla Transformer (see Appendix A.6 for details). Table 4a suggests that, in aggregate, the (vanilla) Transformer and FNet obtain comparable results. Given that the Transformer is the second most accurate model evaluated by Tay et al. (2021a) and that the relative differences in the average accuracy scores within Table 4a are small, our results suggest that FNet is competitive with the most accurate of the efficient Transformers on LRA. Turning to efficiency, in Table 4b, we provide training speed and memory usage statistics from our experiments on GPUs (8 V100 chips); see Appendix A.2 for results on TPUs. We perform a sweep over sequence lengths {512, 1024, 2048, 4096, 8192}. On GPUs, FNet is much faster than all other models across all sequence lengths, due to the highly efficient FFT implementation on GPUs. Table 4b also indicates that FNet has a lighter memory footprint (this holds for both GPUs and TPUs; see extended results in Appendix A.2). This is partly because FNet has no learnable parameters in its mixing sublayer, but also due to the FFT\u2019s efficiency, especially at longer sequence lengths. Lastly, Table 4c shows that training speed gains generally carry over to inference gains (see Appendix A.2 for detailed TPU results).", "11The \u201cLinear\u201d model in Table 4 is the baseline model introduced in Section 4.1.\n(a) Accuracy results obtained on TPUs as in Tay et al. (2021a). Asterisked results quoted from Tay et al. (2021a). Average does not include the Path-X task, which all models fail (Transformer due to memory limits; others perform no better than chance). Training Speed (steps/s) Peak Memory Usage (GB) Seq. length 512 1024 2048 4096 8192 512 1024 2048 4096 8192 Transformer 21 10 4 OOM OOM 1.6 4.0 12.2 OOM OOM Linear 34 (1.6x) 19 (1.8x) 9 (2.0x) 4 OOM 0.9 1.6 2.8 6.9 OOM FNet (FFT) 43 (2.0x) 24 (2.3x) 14 (3.2x) 7 4 0.8 1.3 2.2 3.9 7.4 Performer 28 (1.3x) 15 (1.5x) 9 (1.9x) 4 2 1.1 1.9 3.1 5.5 10.4\n(b) GPU training for sequence lengths up to 8192. Only the fastest efficient Transformer, namely Performer, from Tay et al. (2021a) is shown. Left: training speeds (in steps per second; larger is better), with speed-up multipliers relative to the Transformer given in parentheses. Right: peak memory usage (in GB; smaller is better). Seq. length 512 1024 2048 4096 8192 16384 Transformer 12 28 76 244 OOM OOM Linear 9 (1.4x) 14 (2.0x) 30 (2.6x) 72 (3.4x) 208 OOM FNet (FFT) 8 (1.5x) 12 (2.3x) 23 (3.4x) 43 (5.7x) 83 164 Performer 11 (1.2x) 17 (1.6x) 32 (2.4x) 60 (4.0x) 116 238\n(c) GPU inference speeds on the LRA Text classification task (in milliseconds per batch; smaller is better). Only the fastest efficient Transformer, Performer, from Tay et al. (2021a) is shown. Speed up relative to the Transformer is given in parentheses. 5 Conclusions. In this work, we studied simplified token mixing modules for Transformer-like encoder architectures, making several contributions. First, we showed that simple, linear mixing transformations, along with the nonlinearities in feed-forward layers, can competently model diverse semantic relationships in text. Second, we introduced FNet, a Transformer-like model wherein the self-attention sublayer is replaced by an unparameterized Fourier Transform.", "FNets achieve 92 and 97% of their respective BERT-Base and BERT-Large counterparts\u2019 accuracy on the GLUE benchmark, but train 70\u2212 80% faster on GPUs/TPUs. Third, because of\nits favorable scaling properties, FNet is very competitive with the \u201cefficient Transformers\u201d evaluated on the Long-Range Arena benchmark, matching the accuracy of the most accurate models while being much faster and lighter on memory. Our work highlights the potential of linear units as a drop-in replacement for the attention mechanism in text classification tasks. We found the Fourier Transform to be a particularly efficient and effective mixing mechanism, due to the speed of the FFT. However, we only performed a cursory survey of other linear transformations (see also Appendix A.3), and additional fast alternatives are worth exploring. Given the speed and accuracy advantages of\nsmaller FNet models relative to Transformers, we suspect that FNet will be effective as a lightweight, distilled student model deployed in resourceconstrained settings such as production services or on edge devices. The need for such lightweight serving models is only forecast to grow given the interest in giant models (Raffel et al., 2020; Brown et al., 2020; Lepikhin et al., 2021). A natural avenue to explore in this regard is knowledge distillation of small FNet models from larger Transformer teacher models, following, for example, Sanh et al. (2019) ; Jiao et al. (2020); Turc et al. (2019). Another aspect of interest and worthy of further study is hybrid FNet-attention models. We found that adding only a few self-attention sublayers to FNet offers a simple way to trade speed for accuracy. Specifically, replacing the final two Fourier sublayers with self-attention provided 97\u2212 99% of BERT\u2019s accuracy with limited speed penalties. Throughout this work we have restricted our focus to encoders.", "FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "Indeed, Table 5 shows that BERT-Base is actually more accurate than FNet-Large, which contains more than twice as many parameters. BERT is presumably more expressive because the mixing (attention) weights are both task specific and token dependent, determined\n768 12 111 93 83 88.\n512 12 55 49 42 44.\n512 8 42 38 34 36.\n256 8 15 15 13 13.\n512 4 30 28 26 28.\n256 4 12 12 11 11.\n256 2 10 10 10 -.\nby token-token (query-key) dot products; see also Tay et al. (2020a). FNet\u2019s mixing weights, on the other hand, are neither task specific nor token dependent. Finally, Table 6 shows the model sizes that were used to construct Figure 2 (main text) and Figure 3 (Appendix A.2). A.2 TPU results In this section, we report FNet efficiency results for TPUs; the main text focuses on GPUs. Figure 3 shows the speed vs MLM pre-training accuracy curve when training on TPU (4\u00d7 4 v3 chips). As on GPUs, FNet and the Linear model define the Pareto efficiency frontier for smaller, faster models, while BERT defines the frontier for larger, slower models. Table 7 shows Long Range Arena Text classification efficiency results on TPUs (4\u00d7 4 v3 chips). The Linear model and FNet train faster than all the efficient Transformers for sequence lengths\u2264 2048 and 512, respectively. For longer sequences, FNet is slower than the Performer and, based on results in Tay et al. (2021a), likely also slower than the other efficient Transformers that linearize attention, namely Local Attention (Parmar et al., 2018), Linformer (Wang et al., 2020) and Linear Transformer (Katharopoulos et al., 2020). However, it is worth noting that Table 4a suggests that FNet is more accurate than all of the aforementioned models. Moreover, we expect that the GPU speed gains will\ntransfer to TPUs as the TPU FFT implementation improves. A.3 Additional configurations that we experimented with\nWe experimented with a number of additional ideas to improve FNet. Fourier Transform algorithm.", "On GPUs, the FFT was the fastest algorithm for computing the DFT across all sequence lengths that we experimented with (512 \u2212 8192). On TPUs, it is faster to compute the DFT directly using matrix multiplications for relatively shorter sequence lengths (up to lengths of 4096; see Table 7). This efficiency boundary between matrix multiplication and FFT on TPUs will change depending on the XLA precision for the matrix multiplications. We found that, although (slower) HIGHEST XLA precision was required to very accurately reproduce FFT in computing the DFT, (faster) DEFAULT XLA precision was sufficient to facilitate accurate model convergence. Modifying the Fourier Transform computation. To keep the entire FNet architecture simple, the Fourier sublayer accepts real input and returns real output. The standard Fourier sublayer in FNet simply extracts the real part after computing the 2D DFT. We found that FNet was less accurate and less stable during training if only the real part of the DFT was used throughout the computation. Simply extracting the absolute value (instead of the real part) also led to a significantly less accurate model. Because the feed-forward sublayer mixes the hidden dimension, we experimented with applying a 1D DFT along the token dimension only in the Fourier sublayer (i.e. no hidden dimension mixing in the Fourier sublayer). This yielded some training speed gains but hurt accuracy. The 1D (token mixing only) DFT model still significantly outperformed the (no token mixing) FF-only model, indicating that token mixing is most important mechanism in the Fourier sublayer. Other transforms. We experimented with three natural alternatives to the Fourier Transform:\n\u2022 Discrete Cosine Transform (DCT). The DCT is closely related to the DFT but transforms real input to real output. However, we found that the DCT model underperformed FNet (\u223c 4% accuracy degradation). \u2022 Hadamard Transform12.", "Although the Hadamard Transform was slightly faster than the DFT, it yielded less accurate results (\u223c 2% accuracy degradation). \u2022 Hartley Transform. The Hartley Transform, which transforms real input to real output, can be described in terms of the Fourier Transform: H = <{F} \u2212 ={F}. We found that the Hartley Transform matched the Fourier Transform on GLUE (76.7 vs. 76.7). Introducing learnable parameters to the Fourier sublayer. Our attempts to introduce learnable parameters into the Fourier sublayer were either detrimental or inconsequential, and generally slightly slowed the model. For the (sequence length, hidden dimension) input in each Fourier sublayer, we tried two approaches to introduce learnable parameters: (1) element wise multiplication with a (sequence length, hidden dimension) matrix, and (2) regular matrix multiplication with (sequence length, sequence length) and (hidden dimension, hidden dimension) matrices. We experimented with these approaches in various configurations: preceding and/or following the DFT, and also in combination with inverse DFT (e.g. transform to frequency domain, apply element wise multiplication, transform back to time domain), but most setups degraded accuracy and reduced training stability, while a few did not change accuracy but lead to small speed decreases. In a slightly different set of experiments and in an effort to provide more flexibility to the model, we added (complex) learnable weights to the 2D DFT matrix. This model was stable but did not yield any accuracy gains, suggesting that the DFT is locally optimal in some sense. FNet block modifications. The standard FNet encoder block structure follows that of the Transformer: a Fourier sublayer followed by a feedforward sublayer, with residual connections and layer norms after each sublayer; see Figure 1. We tried several modifications to this structure, based on the intuition of moving in and out of the frequency domain between multiplications.", "For example, the sandwiching of Fourier, feed-forward, Fourier (or inverse Fourier) sublayers and only applying the residual connections and layer norms to the final result, yields a structure that more closely\n12Whereas the DFT matrix in Equation (2) contains N roots of unity, the Hadamard Transform simply contains two roots of unity: {\u00b11}; see also Kunz (1979). mimics convolutions. However, these setups degraded accuracy and lead to a more unstable model during training. Adding extra feed-forward sublayers to this layering, or swapping out the feedforward sublayers for simpler dense sublayers, did not help either. A.4 Mixing layer speeds Table 8 summarizes the inference and training speeds for the different mixing layers. For each of the Base and Large configurations, we have removed all other sublayers and transformations and then calculated the speed per batch of input examples. The FNet training speeds are particularly fast because no parameters are updated. The Linear model has faster inference than FNet on TPUs because it is performing real matrix multiplications, whereas FNet performs complex matrix multiplications; see Equation (2). Although the Fourier mixing sublayer itself performs forward and backward passes significantly faster than the self-attention sublayer, FNet is overall 70-80% faster than BERT because the overall training and inference speeds are bottle-necked by the feed-forward sublayers that all models share. A.5 FNet-Hybrid ablations Table 9 shows the effects of varying the number of attention sublayers and the attention layout in the FNet-Hybrid model. For the \u201cBOTTOM\u201d layout, all attention sublayers are placed in the first few encoder layers, where they replace the Fourier mixing sublayers. For the \u201cTOP\u201d layout, attention sublayers are placed in the final encoder layers; for the \u201cMIDDLE\u201d layout they are placed in the middle layers; and for the \u201cMIXED\u201d layout, they are distributed through the model.", "From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i", "n i s t i c ) : 18 x = nn . Dense ( s e l f . d_ f f , 19 k e r n e l _ i n i t =nn . i n i t i a l i z e r s . normal (2e\u22122) , 20 b i a s _ i n i t =nn . i n i t i a l i z e r s . normal (2e\u22122) , 21 name=\" in te rmed ia te \" ) ( x ) 22 x = nn . gelu ( x ) 23 x = nn . Dense ( x . shape [ \u22121 ] , 24 k e r n e l _ i n i t =nn . i n i t i a l i z e r s . normal (2e\u22122) , 25 name=\" output \" ) ( x ) 26 r e t u r n nn . Dropout ( s e l f . d ropout_ra te ) ( x , d e t e r m i n i s t i c ) 27 28 29 class FNetEncoderBlock ( nn . Module ) : 30 f o u r i e r _ l a y e r : Four ierTransformLayer 31 f f _ l a y e r : FeedForwardLayer 32 33 @nn. compact 34 def __ca l l__ ( s e l f , x , d e t e r m i n i s t i c ) : 35 mix ing_output = s e l f . f o u r i e r _ l a y e r ( x ) 36 x = nn . LayerNorm (1e\u221212 , name=\" mixing_layer_norm \" ) ( x + mix ing_output ) 37 feed_forward_output = s e l f . f f _ l a y e r ( x , d e t e r m i n i s t i c ) 38 r e t u r n nn . LayerNorm ( 39 1e\u221212 , name=\" output_layer_norm \" ) ( x + feed_forward_output ) 40 41 42 class FNetEncoder ( nn . Module ) : 43 num_layers : i n t 44 d_model : i n t 45 d _ f f : i n t 46 dropout_ra te : f l o a t 47 48 def setup ( s e l f ) : 49 encoder_blocks = [ ] 50 f o r l aye r i n range ( s e l f . num_layers ) : 51 encoder_blocks . append ( FNetEncoderBlock ( 52 Four ierTransformerLayer ( ) , 53 FeedForwardLayer ( s e l f . d_ f f , s e l f . d ropout_ra te ) , 54 name= f \" encoder_ { l aye r } \" ) ) 55 s e l f . encoder_blocks = encoder_blocks 56 s e l f . poo ler = nn . Dense ( 57 s e l f . d_model , 58 k e r n e l _ i n i t =nn . i n i t i a l i z e r s . normal (2e\u22122) , 59 name=\" poo ler \" ) 60 61 def __ca l l__ ( s e l f , x , d e t e r m i n i s t i c ) : 62 f o r encoder_block i n s e l f . encoder_blocks : 63 x = encoder_block ( x , d e t e r m i n i s t i c ) 64 pooled_output = s e l f . poo ler ( x [ : , 0 ] ) 65 pooled_output = jnp . tanh ( pooled_output ) 66 r e t u r n x , pooled_output\nListing 1: FNet code written in JAX/Flax. Embedding and output projection layers are omitted for simplicity."]}