{"paragraphs": ["MuRIL: Multilingual Representations for Indian Languages. India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011). India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both MuRIL: Multilingual Representations for Indian Languages. India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011).", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\nMuRIL: Multilingual Representations for Indian Languages\nSimran Khanuja1 Diksha Bansal* 2 Sarvesh Mehtani* 3 Savya Khosla* 4 Atreyee Dey1\nBalaji Gopalan1 Dilip Kumar Margam1 Pooja Aggarwal1 Rajiv Teja Nagipogu1 Shachi Dave1\nShruti Gupta1 Subhash Chandra Bose Gali1 Vish Subramanian1 Partha Talukdar1\n1Google 2Indian Institute of Technology, Patna 3Indian Institute of Technology, Bombay 4Delhi Technological University\n1 Why MuRIL?. India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011).", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\n* Work done during a summer internship at Google India. Correspondence to the MuRIL Team (muril-contact@google.com)\ntranslated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native \u2192 Latin) test sets of the chosen datasets, and demonstrate the efficacy of MuRIL in handling transliterated data. 2 Model and Data.\nMuRIL currently supports 17 languages for which monolingual data is publicly available. These are further grouped into 16 IN languages and English (en).", "The IN languages include: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Kashmiri (ks), Malayalam (ml), Marathi (mr), Nepali (ne), Oriya (or), Punjabi (pa), Sanskrit (sa), Sindhi (sd), Tamil (ta), Telugu (te) and Urdu (ur). ar X\niv :2\n10 3.\n10 73\n0v 2\n[ cs\n.C L\n] 2\nA pr\n2 02\n1\nWe train our model with two language modeling objectives. The first is the conventional Masked Language Modeling (MLM) objective (Taylor, 1953) that leverages monolingual text data only (unsupervised). The second is the Translation Language Modeling (TLM) objective (Lample and Conneau, 2019) that leverages parallel data (supervised). We use monolingual documents to train the model with MLM, and both translated and transliterated document pairs to train the model with TLM. Monolingual Data: We collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus1 and Wikipedia2. Translated Data: We have two sources of translated data. First, we use the PMINDIA (Haddow and Kirefu, 2020) parallel corpus containing sentence pairs for 8 IN languages (bn, gu, hi, kn, ml, mr, ta, te). Each pair comprises of a sentence in a native language and its English translation. Second, we translate the aforementioned monolingual corpora (both Common Crawl and Wikipedia) to English, using an in-house translation system. The source and translated documents are used as parallel instances to train the model. Note that we translate corpora of all IN languages excluding as, ks and sa, for which the current translation system lacks support. Transliterated Data: We have two sources of 1https://oscar-corpus.com 2https://www.tensorflow.org/datasets/ catalog/wikipedia\ntransliterated data as well. First, we use the Dakshina Dataset (Roark et al., 2020) that contains 10,000 sentence pairs for 12 IN languages (bn, gu, hi, kn, ml, mr, pa, ta, te, ur). Each pair is a native script sentence and its manually romanized transliteration.", "Second, we use the indic-trans library (Bhat et al., 2015) to transliterate Wikipedia corpora of all IN languages to Latin (except ks, sa and sd, for which the library doesn\u2019t have support). The source document and its Latin transliteration are used as parallel instances to train the model. Upsampling: In the corpora collected above, the percentage of tokens per language is highly uneven in its distribution. Hence, data smoothing is essential so that all languages have their representation reflect their usage in the real world. To achieve this, we upsample monolingual Wikipedia corpora of each language according to its multiplier value given by:\nmi =\n(max j\u2208L nj\nni\n)(1\u2212\u03b1) (1) In the above equation, mi represents the multiplier value for language i, ni is its original token count, L represents the set of all 17 languages and \u03b1 is a hyperparameter whose value is set to 0.3, following Conneau et al. (2020). Hence, the upsampled token count for language i is mi \u2217 ni. The final data distribution after upsampling is shown in Figure 2. The upsampled token counts for each language and corpus are reported in Appendix A.\nVocabulary: We learn a cased WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016) vocabulary from the upsampled pre-training data using the wordpiece vocabulary generation\nlibrary from Tensorflow Text3. Since our data is upsampled, we set the language smoothing exponent from the vocabulary generation tool to 1, and the rest of the parameters are set to their default value. The final vocabulary size is 197,285. Figure 3 shows a few common IN language words tokenized using mBERT and MuRIL vocabularies. We also plot the fertility ratio (average number of sub-words/word) of mBERT and MuRIL tokenizers on a random sample of text from our training data in Figure 5. Here, a higher fertility ratio equates to a larger number of sub-words per word, eventually leading to a loss in preservation of semantic meaning.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We would also like to thank Hyung Won Chung, Anosh Raj, Yinfei Yang and Fangxiaoyu Feng for contributing to our discussions around MuRIL. Finally, we would like to thank Nick Doiron for his feedback on the HuggingFace implementation of the model. A Pre-training Data Statistics. The upsampled token counts for each language and corpus are reported in Table 4.\nB Detailed Results. We report per language results for each XTREME (IN) dataset in Tables 5 (PANX), 6 (UDPOS), 7 (XNLI), 8 (Tatoeba), 9 (XQuAD, MLQA) and 10 (TyDiQA-GoldP). The detailed results for transliterated test sets are shown in Tables 11 (PANX), 12 (UDPOS), 13 (XNLI), 14 (Tatoeba). C Analysis. In this section, we analyse the predictions of mBERT and MuRIL on a random sample of test examples. Named Entity Recognition (NER): NER is the task of locating and classifying entities in unstructured text into pre-defined categories such as person, location, organization etc. In Figure 6, we present entity predictions of mBERT and MuRIL on a random sample of test examples. In the first example, we observe that Atlanta Falcons, a football team, is predicted as ORG (Organisation) by MuRIL but LOC (Location) by mBERT, probably looking at the word Atlanta without context. In the second example, MuRIL correctly takes the context into account and predicts Shirdi\u2019s Sai Baba as PER (Person), whereas mBERT resorts to predicting LOC taking a cue from the word Shirdi. A similar pattern is observed in other examples like Nepal\u2019s Prime Minister, the President of America etc. In the third example, the Bajirao Mastani movie is being spoken about, specified with the word (film) in parentheses, which MuRIL correctly captures. In the last example, we observe that MuRIL can correctly classify misspelled words (vimbledon)\nutilising the context. Sentiment Analysis: Sentiment analysis is a sentence classification task wherein each sentence is labeled to be expressing a positive, negative or neutral sentiment.", "We present the sentiment predictions on a sample set of sentences in Figure 7. In the first example, \u201cIt\u2019s good that the account hasn\u2019t closed\u201d, we observe that the original Hindi sentence borrows an English word (account) and also contains a negation (not) word, but MuRIL correctly predicts it as expressing a positive statement. A similar observation can be made in the second example where MuRIL correctly predicts the sentiment of the transliterated sentence, \u201cRamu didn\u2019t let the film\u2019s pace slow down\u201d. Question Answering (QA): QA is the task of answering a question based on the given context or world knowledge. We show two context-question pairs, with their answers and predicted answers in Figure 8. In the first example, despite the fact that the word Greek is referred to by its Hindi translation in the context and its transliteration in the question (as highlighted), MuRIL correctly infers the answer from the context. In the second example, MuRIL understands that \u201cbank ki paribhasha\u201d (the definition of a bank), as a whole entity, is what differs across countries and not banks."]}