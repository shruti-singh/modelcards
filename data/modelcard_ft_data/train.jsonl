{"pkey": "longformer_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length.", "title": "Longformer: The Long-Document Transformer", "context": ["In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.1\n1 Introduction. Transformers (Vaswani et al., 2017) have achieved state-of-the-art results in a wide range of natural language tasks including generative language modeling (Dai et al., 2019; Radford et al., 2019) and discriminative language understanding (Devlin et al., 2019). This success is partly due to the self-attention component which enables the network to capture contextual information from the entire sequence. While powerful, the memory and computational requirements of self-attention grow\n\u2217 Equal contribution. 1https://github.com/allenai/longformer\nquadratically with sequence length, making it infeasible (or very expensive) to process long sequences. To address this limitation, we present Longformer, a modified Transformer architecture with a self-attention operation that scales linearly with the sequence length, making it versatile for processing long documents (Fig 1). This is an advantage for natural language tasks such as long document classification, question answering (QA), and coreference resolution, where existing approaches partition or shorten the long context into smaller sequences that fall within the typical 512 token limit of BERT-style pretrained models. Such partitioning could potentially result in loss of important cross-partition information, and to mitigate this problem, existing methods often rely on complex architectures to address such interactions.", "Continued MLM Pretraining We pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.\nTab. 5 shows the BPC on the development set of our training corpus. The first row shows a 1.846\n6Adding dilation on a few heads as in \u00a74.1 hurt performance, likely because it is not compatible with the pretrained RoBERTa weights. Retraining such model from scratch might be needed to improve performance. BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Traininig for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large. Frozen RoBERTa Weights We also pretrained Longformer while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this configuration is to perfectly preserve the RoBERTa performance on short documents.", "Main Result Tab. 7 summarizes the results of all our finetuning experiments. We observe that Longformer consistently outperforms the RoBERTa baseline. Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we\n8For Hyperpartisan we split the training data into 80/10/10 train/dev/test sets, and report mean F1 across five seeds. found that the distance between any two mentions is typically quite small so that a baseline that processes smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions. Longformer-large for QA We also evaluate the performance of Longformer-large on long context QA tasks. Tab. 8 shows that our Longformer-large achieves new state-of-the-art results9 on WikiHop and TriviaQA by large margins (3.6 and 4 points respectively), and for HotpotQA, it underperforms the current state-of-the-art (Fang et al., 2020) by a point. Tab. 9 shows the detailed results of HotpotQA compared with published and unpublished concurrent models. Longformer places second on the published leaderboard, outperforming all other published results except for HGN (Fang et al., 2020)."]}
{"pkey": "longformer_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Recent work has addressed the computational in-efficiency of Transformers on long sequences (see Tab. 1). However, they primarily focus on autoregressive language modeling (LM), while the application of long document transformers to document-level NLP tasks in the transfer learning setting (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) has remained largely unexplored. The paper authors address this gap and show that Longformer\u2019s attention mechanism can act as a drop-in replacement for the self-attention mechanism in pretrained Transformers, and leads to gains across a suite of document NLP tasks.", "title": "Longformer: The Long-Document Transformer", "context": ["On the other hand, our proposed Longformer is able to build contextual representations of the entire context using multiple layers of attention, reducing the ar X iv :2\n00 4.\n05 15\n0v 2\n[ cs\n.C L\n] 2\nD ec\n2 02\n0\nneed for task-specific architectures. Recent work has addressed the computational inefficiency of Transformers on long sequences (see Tab. 1). However, they primarily focus on autoregressive language modeling (LM), while the application of long document transformers to documentlevel NLP tasks in the transfer learning setting (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) has remained largely unexplored. We address this gap and show that Longformer\u2019s attention mechanism can act as a drop-in replacement for the self-attention mechanism in pretrained Transformers, and leads to gains across a suite of document NLP tasks. Longformer\u2019s attention mechanism is a combination of a windowed local-context self-attention and an end task motivated global attention that encodes inductive bias about the task. Through ablations and controlled trials we show both attention types are essential \u2013 the local attention is primarily used to build contextual representations, while the global attention allows Longformer to build full sequence representations for prediction. We first evaluate Longformer on autoregressive character-level language modeling using a combination of windowed and a new dilated attention pattern, allowing the model to process sequences of up to 32K characters on modern GPUs. We achieve state-of-the-art results on text8 and enwik8 benchmark datasets, demonstrating the effectiveness of Longformer in long document modeling. Then, to evaluate Longformer\u2019s ability to replace the full self-attention operation of existing pretrained models, we pretrain it with the masked language modeling (MLM) objective, continuing from the RoBERTa (Liu et al., 2019) released checkpoint.", "Continued MLM Pretraining We pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.\nTab. 5 shows the BPC on the development set of our training corpus. The first row shows a 1.846\n6Adding dilation on a few heads as in \u00a74.1 hurt performance, likely because it is not compatible with the pretrained RoBERTa weights. Retraining such model from scratch might be needed to improve performance. BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Traininig for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large. Frozen RoBERTa Weights We also pretrained Longformer while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this configuration is to perfectly preserve the RoBERTa performance on short documents.", "Longformer: The Long-Document Transformer. Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer\u2019s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.1 Longformer: The Long-Document Transformer. Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer\u2019s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8."]}
{"pkey": "longformer_3", "question": "What are the main contributions of the paper?", "answer": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer\u2019s attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, the paper authors evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, the paper authors also pretrain Longformer and finetune it on a variety of downstream tasks.\nOur pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. The paper authors finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.", "title": "Longformer: The Long-Document Transformer", "context": ["Continued MLM Pretraining We pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.\nTab. 5 shows the BPC on the development set of our training corpus. The first row shows a 1.846\n6Adding dilation on a few heads as in \u00a74.1 hurt performance, likely because it is not compatible with the pretrained RoBERTa weights. Retraining such model from scratch might be needed to improve performance. BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Traininig for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large. Frozen RoBERTa Weights We also pretrained Longformer while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this configuration is to perfectly preserve the RoBERTa performance on short documents.", "We achieve a new state-of-the-art on both text8 and enwik8 using the small models with BPC of 1.10 and 1.00 on text8 and enwik8 respectively, demonstrating the effectiveness of our model. For large models, given how expensive these experiments are, and following recent work (Kitaev et al., 2020; Rae et al., 2020), we are only evaluating on enwik8. Tab. 3 shows that Longformer outperforms the comparable TransformerXL model, matches the performance of the comparable Sparse Transformer (Child et al., 2019), and matches or slightly underperforms recent models that have more than twice the number of parameters. It is worth noting that Adaptive Span (Sukhbaatar et al., 2019) and Compressive Transformer (Rae et al., 2020) are not good fit for the pretrainingfinetuning paradigm as discussed in \u00a72.\n4.2.2 Ablation Study. To show the importance of the design choices of our attention patterns, we tried different variants and report their controlled experiment results. To make the ablation study more manageable, we train each configuration for 150K steps4 with phase 1 configuration on a small model on text8, then report the BPC performance on the dev set. The top of Tab. 4 demonstrates the impact of different ways of configuring the window sizes per layer. We observe that increasing the window size from the bottom to the top layer leads to the best performance, arranging them in the reverse way leads to worse performance, and using a fixed window size (the average of window sizes of the other configuration) leads to a performance that it is in between. The bottom of Tab. 4 shows the impact of adding dilation. Adding some dilation to two heads leads to some improvement compared with no dilation at all. 5 Pretraining and Finetuning. Current state-of-the-art systems for many NLP tasks finetune a pretrained model with task supervision (e.g. BERT). One of our main motivations is to develop such a model suitable for long document tasks.", "Main Result Tab. 7 summarizes the results of all our finetuning experiments. We observe that Longformer consistently outperforms the RoBERTa baseline. Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we\n8For Hyperpartisan we split the training data into 80/10/10 train/dev/test sets, and report mean F1 across five seeds. found that the distance between any two mentions is typically quite small so that a baseline that processes smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions. Longformer-large for QA We also evaluate the performance of Longformer-large on long context QA tasks. Tab. 8 shows that our Longformer-large achieves new state-of-the-art results9 on WikiHop and TriviaQA by large margins (3.6 and 4 points respectively), and for HotpotQA, it underperforms the current state-of-the-art (Fang et al., 2020) by a point. Tab. 9 shows the detailed results of HotpotQA compared with published and unpublished concurrent models. Longformer places second on the published leaderboard, outperforming all other published results except for HGN (Fang et al., 2020)."]}
{"pkey": "longformer_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "To address this limitation, the paper authors introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer.", "title": "Longformer: The Long-Document Transformer", "context": ["We also introduce additional task motivated global attention patterns suitable for common NLP tasks (\u00a73) and show they are essential for good performance in the transfer learning setting. A few models tried tasks other than autoregressive language modeling, which is a step forward because arguably focusing on language modeling as the primary evaluation has led to the development of models with limited applicability. BPTransformer (Ye et al., 2019) evaluated on machine\ntranslation (MT), but didn\u2019t explore the pretrainfinetune setting. Blockwise attention (Qiu et al., 2019) pretrained their models and evaluated on question answering (QA). However, the evaluation is limited as it doesn\u2019t include language modeling, and the QA datasets are of relatively short documents,2 therefore the effectiveness of this model on long document tasks remains unexplored. Task-specific Models for Long Documents Many task-specific approaches have been developed to workaround the 512 limit of pretrained transformer models like BERT. The simplest approach just truncates the document, commonly used for classification (Xie et al., 2019). Another approach chunks the document into chunks of length 512 (could be overlapping), processes each chunk separately, then combines the activations with a task specific model (Joshi et al., 2019). A third approach popular for multihop and open domain QA tasks uses a two-stage model where the first stage retrieves relevant documents that are passed onto the second stage for answer extraction (Clark and Gardner, 2017; Chen et al., 2017). All of these approaches suffer from information loss due to truncation or cascading errors from the two stage approach. In contrast, Longformer can process long sequences without truncating or chunking, allowing us to adopt a much simpler approach that concatenates the available context and processes it in a single pass.", "While encoder-only Transformers are effective on a variety of NLP tasks, pre-trained encoderdecoder Transformer models (e.g. BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) have achieved strong results on tasks like summarization. Yet, such models can\u2019t efficiently scale to seq2seq tasks with longer inputs. To facilitate modeling long sequences for seq2seq learning, we propose a Longformer variant that has both the encoder and decoder Transformer stacks but instead of the full self-attention in the encoder, it uses the efficient local+global attention pattern of the Longformer. The decoder uses the full self-attention to the entire encoded tokens and to previously decoded locations. We call this model Longformer-Encoder-Decoder (LED) which scales linearly with the input. Since pre-training LED is expensive, we initialize LED parameters from the BART, and follow BART\u2019s exact architecture in terms of number of layers and hidden sizes. The only difference is that to process longer inputs, we extend position embedding to 16K tokens (up from BART\u2019s 1K tokens) and we initialize the new position embedding matrix by repeatedly copying BART\u2019s 1K position embeddings 16 times as in Section 5 for RoBERTa. Following BART, we release two model sizes, LED-base and LED-large, which respectively have 6 and 12 layers in both encoder and decoder stacks. We evaluate LED on the summarization task using the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summarization in the scientific domain. The 90th percentile of document lengths is 14.5K tokens, making it an appropriate testbed for evaluating LED. LED\u2019s encoder reads the document and its decoder generates the output summary. The encoder uses local attention with window size 1,024 tokens and global attention on the first <s> token. The decoder uses full attention to the entire encoder and previously decoded locations.", "A few contemporaneous works3 have explored similar ideas to Longformer using local + global attention in Transformers, and pre-training it for long document natural language tasks. In particular, ETC (Ainslie et al., 2020) uses a similar local + global attention instead of full self-attention to scale Transformers to long documents. Different from Longformer, ETC uses relative position em-\n2SQuAD contexts typically fit within the 512 limit, and MRQA is constructed by dropping long-document examples. 3All were published on arXiv after Longformer.\nbeddings (which we only used for the Autoregressive LM setting), introduces an additional training objective (CPC loss) for pre-training, and configures global attention in a slightly different way. It shows strong results on several tasks including reading comprehension and classification. GMAT (Gupta and Berant, 2020) uses a similar idea of few global locations in the input serving as global memory. BigBird (Zaheer et al., 2020) is an extension over ETC with evaluation on additional tasks, including summarization. Importantly, through theoretical analysis, BigBird shows that sparse Transformers are universal approximators of sequence functions and preserve these properties of the full self-attention. 3 Longformer. The original Transformer model has a self-attention component with O(n2) time and memory complexity where n is the input sequence length. To address this challenge, we sparsify the full self-attention matrix according to an \u201cattention pattern\u201d specifying pairs of input locations attending to one another. Unlike the full self-attention, our proposed attention pattern scales linearly with the input sequence, making it efficient for longer sequences. This section discusses the design and implementation of this attention pattern.\n3.1 Attention Pattern. Sliding Window Given the importance of local context (Kovaleva et al., 2019), our attention pattern employs a fixed-size window attention surrounding each token."]}
{"pkey": "longformer_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "1. To compare to prior work the paper authors focus on character-level LM (text8 and enwik8; Mahoney, 2009) (This is for language modelling)\n2. Table 7 (For finetuned tasks:  WikiHop, TriviaQA, HotpotQA,  OntoNotes, IMDB, Hyperpartisan)", "title": "Longformer: The Long-Document Transformer", "context": ["As standard in seq2seq models, LED is trained using teacher forcing on gold training summaries and uses beam search at inference. Tab. 11 demonstrates the results of LED-large 16K on the arXiv summarization task. This model is merely initialized from BART, with no additional\npre-training. We observe that LED achieves stateof-the-art results on arXiv, slightly outperforming BigBird (Zaheer et al., 2020). Note that the BigBird summarization model supports sequence length of 4K tokens but starts from and continues pre-training Pegasus (Zhang et al., 2020), a model specifically designed and pre-trained for summarization. With no pre-training or task-specific initialization, but with ability to process longer inputs, LED can slightly outperform BigBird. Further improvements should be possible through pre-training of LED. Fig. 3 further illustrates the importance of sequence length showing the ablility to process longer input significantly improves the results. 8 Conclusion and Future Work. We present Longformer, a transformer-based model that is scalable for processing long documents and that makes it easy to perform a wide range of document-level NLP tasks without chunking/shortening the long input and without complex architecture to combine information across these chunks. Longformer employs an attention pattern that combines local and global information while also scaling linearly with the sequence length. Longformer achieves state-of-the-art results on the character-level language modeling tasks of text8\nand enwik8. When pretrained, Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We further present LED, an encoder-decoder variant of Longformer for modeling sequence-to-sequence tasks, and achieve stateof-the-art results on the arXiv long document summarization task.", "On the other hand, our proposed Longformer is able to build contextual representations of the entire context using multiple layers of attention, reducing the ar X iv :2\n00 4.\n05 15\n0v 2\n[ cs\n.C L\n] 2\nD ec\n2 02\n0\nneed for task-specific architectures. Recent work has addressed the computational inefficiency of Transformers on long sequences (see Tab. 1). However, they primarily focus on autoregressive language modeling (LM), while the application of long document transformers to documentlevel NLP tasks in the transfer learning setting (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) has remained largely unexplored. We address this gap and show that Longformer\u2019s attention mechanism can act as a drop-in replacement for the self-attention mechanism in pretrained Transformers, and leads to gains across a suite of document NLP tasks. Longformer\u2019s attention mechanism is a combination of a windowed local-context self-attention and an end task motivated global attention that encodes inductive bias about the task. Through ablations and controlled trials we show both attention types are essential \u2013 the local attention is primarily used to build contextual representations, while the global attention allows Longformer to build full sequence representations for prediction. We first evaluate Longformer on autoregressive character-level language modeling using a combination of windowed and a new dilated attention pattern, allowing the model to process sequences of up to 32K characters on modern GPUs. We achieve state-of-the-art results on text8 and enwik8 benchmark datasets, demonstrating the effectiveness of Longformer in long document modeling. Then, to evaluate Longformer\u2019s ability to replace the full self-attention operation of existing pretrained models, we pretrain it with the masked language modeling (MLM) objective, continuing from the RoBERTa (Liu et al., 2019) released checkpoint.", "While encoder-only Transformers are effective on a variety of NLP tasks, pre-trained encoderdecoder Transformer models (e.g. BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) have achieved strong results on tasks like summarization. Yet, such models can\u2019t efficiently scale to seq2seq tasks with longer inputs. To facilitate modeling long sequences for seq2seq learning, we propose a Longformer variant that has both the encoder and decoder Transformer stacks but instead of the full self-attention in the encoder, it uses the efficient local+global attention pattern of the Longformer. The decoder uses the full self-attention to the entire encoded tokens and to previously decoded locations. We call this model Longformer-Encoder-Decoder (LED) which scales linearly with the input. Since pre-training LED is expensive, we initialize LED parameters from the BART, and follow BART\u2019s exact architecture in terms of number of layers and hidden sizes. The only difference is that to process longer inputs, we extend position embedding to 16K tokens (up from BART\u2019s 1K tokens) and we initialize the new position embedding matrix by repeatedly copying BART\u2019s 1K position embeddings 16 times as in Section 5 for RoBERTa. Following BART, we release two model sizes, LED-base and LED-large, which respectively have 6 and 12 layers in both encoder and decoder stacks. We evaluate LED on the summarization task using the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summarization in the scientific domain. The 90th percentile of document lengths is 14.5K tokens, making it an appropriate testbed for evaluating LED. LED\u2019s encoder reads the document and its decoder generates the output summary. The encoder uses local attention with window size 1,024 tokens and global attention on the first <s> token. The decoder uses full attention to the entire encoder and previously decoded locations."]}
{"pkey": "longformer_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Longformer\u2019s attention mechanism is a combination of a windowed local-context self-attention and an end task motivated global attention that encodes inductive bias about the task", "title": "Longformer: The Long-Document Transformer", "context": ["Main Result Tab. 7 summarizes the results of all our finetuning experiments. We observe that Longformer consistently outperforms the RoBERTa baseline. Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we\n8For Hyperpartisan we split the training data into 80/10/10 train/dev/test sets, and report mean F1 across five seeds. found that the distance between any two mentions is typically quite small so that a baseline that processes smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions. Longformer-large for QA We also evaluate the performance of Longformer-large on long context QA tasks. Tab. 8 shows that our Longformer-large achieves new state-of-the-art results9 on WikiHop and TriviaQA by large margins (3.6 and 4 points respectively), and for HotpotQA, it underperforms the current state-of-the-art (Fang et al., 2020) by a point. Tab. 9 shows the detailed results of HotpotQA compared with published and unpublished concurrent models. Longformer places second on the published leaderboard, outperforming all other published results except for HGN (Fang et al., 2020).", "To do so, we pretrained Longformer on a document corpus and finetune it for six tasks, including classification, QA and coreference resolution. The resulting model can process sequences up to 4,096 tokens long (8 times longer than BERT)5. We pretrain Longformer with masked language modeling (MLM), where the goal is to recover randomly masked tokens in a sequence. Since MLM pretraining is expensive, we continue pretraining from the RoBERTa (Liu et al., 2019) released checkpoint, while only making the minimal\n4One caveat is that the ordering of end performance will not agree with that at step 150K. However, this approximation saves the huge cost of running every experiment to completion. 5Sequences up to 16K are possible on current GPUs. changes necessary to support Longformer\u2019s attention mechanism. Note that our attention pattern can be plugged into any pretrained transformer model without the need to change the model architecture. Attention Pattern We use sliding window attention with window size of 512, therefore using the same amount of computation as RoBERTa.6\nPosition Embeddings RoBERTa uses learned absolute position embeddings with the maximum position being 512. To support longer documents, we add extra position embeddings to support up to position 4,096. To leverage RoBERTa\u2019s pretrained weights, instead of randomly initializing the new position embeddings, we initialize them by copying the 512 position embeddings from RoBERTa multiple times as analysis of BERT\u2019s attention heads shows a strong learned bias to attending to local context, including the previous or next token (Clark et al., 2019). Using the copy initialization preserves this local structure everywhere except at the partition boundaries. Despite its simplicity, we found this to be a very effective (see Tab. 5), allowing Longformer pretraining to rapidly converge with a small number of gradient updates.", "Continued MLM Pretraining We pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.\nTab. 5 shows the BPC on the development set of our training corpus. The first row shows a 1.846\n6Adding dilation on a few heads as in \u00a74.1 hurt performance, likely because it is not compatible with the pretrained RoBERTa weights. Retraining such model from scratch might be needed to improve performance. BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Traininig for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large. Frozen RoBERTa Weights We also pretrained Longformer while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this configuration is to perfectly preserve the RoBERTa performance on short documents."]}
{"pkey": "longformer_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Not mentioned in the paper.", "title": "Longformer: The Long-Document Transformer", "context": ["We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining. In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs. TriviaQA TriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s]\ndocument [/s]. We truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens. For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them. Hyperparameters of the best configuration are listed in Tab. 14.", "For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.\nAcknowledgment. We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. B Character LM Hyperparameters. We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8.", "Continued MLM Pretraining We pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.\nTab. 5 shows the BPC on the development set of our training corpus. The first row shows a 1.846\n6Adding dilation on a few heads as in \u00a74.1 hurt performance, likely because it is not compatible with the pretrained RoBERTa weights. Retraining such model from scratch might be needed to improve performance. BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Traininig for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large. Frozen RoBERTa Weights We also pretrained Longformer while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this configuration is to perfectly preserve the RoBERTa performance on short documents."]}
{"pkey": "longformer_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "1. The paper authors evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. \n2. Table 13 mentions the details of Pretraining data. Books corpus (Zhu et al., 2015), English Wikipedia, similar to RoBERTa model, alongwith additionally inclusion of one third of a subset of the Realnews dataset (Zellers et al., 2019) with documents longer than 1,200 tokens as well as one third of the Stories (Trinh and Le, 2018) corpus. The goal was to include a mix of llong and short documents to both allow the model to learn longer dependencies while not to forget information from the original RoBERTa pretraining.", "title": "Longformer: The Long-Document Transformer", "context": ["We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining. In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs. TriviaQA TriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s]\ndocument [/s]. We truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens. For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them. Hyperparameters of the best configuration are listed in Tab. 14.", "Coreference model details The coreference model is a straightforward adaptation of the coarseto-fine BERT based model from Joshi et al. (2019). After preprocessing each document with the RoBERTa wordpiece tokenizer, it splits each\ndocument into non-overlapping segments up to the maximum sequence length, then concatenates the activations for the coarse-to-fine clustering stage that forms coreference clusters. The maximum sequence length was 384 for RoBERTa-base, chosen after three trials from [256, 384, 512] using the default hyperparameters in the original implementation.16 For Longformer-base the sequence length was 4,096. Similar to the original implementation, different learning rates were used for the pretrained RoBERTa parameters and the randomly initialized task parameters. Using a larger learning rate in the task parameters allows the optimizer to adjust them farther from their randomly initialized values without destroying the information in the pretrained RoBERTa parameters. Hyperparameter searches were minimal and consisted of grid searches of RoBERTa LR in [1e-5, 2e-5, 3e-5] and task LR in [1e-4, 2e-4, 3e-4] for both RoBERTa and Longformer for a fair comparison. The best configuration for Longformer-base was RoBERTa lr=1e-5, task lr=1e-4. All other hyperparameters were the same as in the original implementation. Training takes about 10 hours on a single GPU. Our implementation is a superhack that involves PyTorch and Tensorflow sharing a single process and GPU. To avoid re-implementing the complicated coarse-to-fine logic from Tensorflow in PyTorch (that involves a highly optimized custom GPU kernel originally released by Lee et al. (2018)) , we devised a system where the lower transformer portion of the model passes activations and gradients back and forth between PyTorch and Tensorflow.", "While encoder-only Transformers are effective on a variety of NLP tasks, pre-trained encoderdecoder Transformer models (e.g. BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) have achieved strong results on tasks like summarization. Yet, such models can\u2019t efficiently scale to seq2seq tasks with longer inputs. To facilitate modeling long sequences for seq2seq learning, we propose a Longformer variant that has both the encoder and decoder Transformer stacks but instead of the full self-attention in the encoder, it uses the efficient local+global attention pattern of the Longformer. The decoder uses the full self-attention to the entire encoded tokens and to previously decoded locations. We call this model Longformer-Encoder-Decoder (LED) which scales linearly with the input. Since pre-training LED is expensive, we initialize LED parameters from the BART, and follow BART\u2019s exact architecture in terms of number of layers and hidden sizes. The only difference is that to process longer inputs, we extend position embedding to 16K tokens (up from BART\u2019s 1K tokens) and we initialize the new position embedding matrix by repeatedly copying BART\u2019s 1K position embeddings 16 times as in Section 5 for RoBERTa. Following BART, we release two model sizes, LED-base and LED-large, which respectively have 6 and 12 layers in both encoder and decoder stacks. We evaluate LED on the summarization task using the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summarization in the scientific domain. The 90th percentile of document lengths is 14.5K tokens, making it an appropriate testbed for evaluating LED. LED\u2019s encoder reads the document and its decoder generates the output summary. The encoder uses local attention with window size 1,024 tokens and global attention on the first <s> token. The decoder uses full attention to the entire encoder and previously decoded locations."]}
{"pkey": "longformer_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "1. To prepare the data for input to Longformer and RoBERTa, the paper authors first tokenize the question, answer candidates, and support contexts using RoBERTa\u2019s wordpiece tokenizer.\n2.  The special tokens [q], [/q], [ent], [/ent] were added to the RoBERTa vocabulary and randomly initialized before task finetuning.\n\nNOTE: Similar strategy was performed for all tasks. And vocabulary size is similar to RoBERTa's vocabulary", "title": "Longformer: The Long-Document Transformer", "context": ["We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining. In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs. TriviaQA TriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s]\ndocument [/s]. We truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens. For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them. Hyperparameters of the best configuration are listed in Tab. 14.", "The dataset does not provide any intermediate annotation for the multihop reasoning chains, requiring models to instead infer them from the indirect answer supervision. To prepare the data for input to Longformer and RoBERTa, we first tokenize the question, answer candidates, and support contexts using RoBERTa\u2019s wordpiece tokenizer. Then we concatenate the question and answer candidates with special tokens as [q] question [/q] [ent] candidate1 [/ent] ... [ent] candidateN [/ent]. The contexts are also concatenated using RoBERTa\u2019s document delimiter tokens as separators: </s> context1 </s> ... </s > contextM </s>. The special tokens [q], [/q], [ent], [/ent] were added to the RoBERTa vocabulary and randomly initialized before task finetuning. 14https://github.com/PyTorchLightning/ pytorch-lightning\nAfter preparing the input data, we compute activations from the top layer of each model as follows. We take the question and answer candidates and concatenate them to as much context as possible up to the model sequence length (512 for RoBERTa, 4,096 for Longformer), run the sequence through the model, collect the output activations, and repeat until all of the context is exhausted (for all models except Longformer-large, where we just include the first 4,096 length sequence due to memory requirements). Then all activations for all chunks are concatenated into one long sequence. In the case of Longformer, we use global attention to the entire question and answer candidate sequence. For prediction, we attach a linear layer to each [ent] that outputs a single logit, average over all logits for each candidate across the chunks, apply a softmax and use the cross entropy loss with the correct answer candidate. Training used the Adam optimizer with linear warmup over 200 gradient updates to a maximum LR, and linear decay over the remainder of training.", "For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.\nAcknowledgment. We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. B Character LM Hyperparameters. We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8."]}
{"pkey": "longformer_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "For WikiHop:\nTo prepare the data for input to Longformer and RoBERTa, the paper authors first tokenize the question, answer candidates, and support contexts using RoBERTa\u2019s wordpiece tokenizer. Then we concatenate the question and answer candidates with special tokens as [q] question [/q] [ent] candidate1 [/ent] ... [ent] candidateN [/ent]. The contexts are also concatenated using RoBERTa\u2019s document delimiter tokens as separators: </s> context1 </s> ... </s> contextM </s>. The special tokens [q], [/q],[ent], [/ent] were added to the RoBERTa vocabulary and randomly initialized before task finetuning.\n\nFor TriviaQA:\n\nSimilar to WikiHop, the paper authors tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s] document [/s]. The paper authors truncate the document at 4,096 wordpiece to avoid it being very slow.\nFor HotpotQA:\n\nSimilar to Wikihop and TriviaQA, to prepare the data for input to Longformer, the paper authors concatenate question and then all the 10 paragraphs in one long context. The paper authors particularly use the following input format with special tokens: \u201c[CLS] [q] question [/q] <t> title1 </t> sent1,1 [s] sent1,2 [s] ... <t> title2 </t> sent2,1 [s] sent2,2 [s] ...\u201d where [q], [/q], <t>, </t>, [s], [p] are special tokens representing, question start and end, paragraph title start and end, and sentence, respectively. The special tokens were added to the Longformer vocabulary and randomly initialized before task finetuning.", "title": "Longformer: The Long-Document Transformer", "context": ["Coreference model details The coreference model is a straightforward adaptation of the coarseto-fine BERT based model from Joshi et al. (2019). After preprocessing each document with the RoBERTa wordpiece tokenizer, it splits each\ndocument into non-overlapping segments up to the maximum sequence length, then concatenates the activations for the coarse-to-fine clustering stage that forms coreference clusters. The maximum sequence length was 384 for RoBERTa-base, chosen after three trials from [256, 384, 512] using the default hyperparameters in the original implementation.16 For Longformer-base the sequence length was 4,096. Similar to the original implementation, different learning rates were used for the pretrained RoBERTa parameters and the randomly initialized task parameters. Using a larger learning rate in the task parameters allows the optimizer to adjust them farther from their randomly initialized values without destroying the information in the pretrained RoBERTa parameters. Hyperparameter searches were minimal and consisted of grid searches of RoBERTa LR in [1e-5, 2e-5, 3e-5] and task LR in [1e-4, 2e-4, 3e-4] for both RoBERTa and Longformer for a fair comparison. The best configuration for Longformer-base was RoBERTa lr=1e-5, task lr=1e-4. All other hyperparameters were the same as in the original implementation. Training takes about 10 hours on a single GPU. Our implementation is a superhack that involves PyTorch and Tensorflow sharing a single process and GPU. To avoid re-implementing the complicated coarse-to-fine logic from Tensorflow in PyTorch (that involves a highly optimized custom GPU kernel originally released by Lee et al. (2018)) , we devised a system where the lower transformer portion of the model passes activations and gradients back and forth between PyTorch and Tensorflow.", "For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.\nAcknowledgment. We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. B Character LM Hyperparameters. We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8.", "We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining. In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs. TriviaQA TriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s]\ndocument [/s]. We truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens. For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them. Hyperparameters of the best configuration are listed in Tab. 14."]}
{"pkey": "longformer_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "1. \na) Then, to evaluate Longformer\u2019s ability to replace the full self-attention operation of existing pretrained models, the paper authors pretrain it with the masked language modeling (MLM) objective, continuing from the RoBERTa (Liu et al., 2019) released checkpoint.\nb) The rest of the hyperparameters are the same as RoBERTa. [Implying encoder-only variant and architecture similar to that of RoBERTa]\n\nTable 12, Row2, Row3 [For number of layers, number of heads, embedding dimension of encoder-only variant of character-level language modeling]\n\nTable 2, Column 2, Last entries [For number of parameters in encoder-only base variant for character-level language modeling]\n\nTable 3, Column 2, Last entry [For number of parameters in encoder-only large variant for character-level language modeling]\n\nTo facilitate modeling long sequences for seq2seq learning, the paper authors propose a Longformer variant that has both the encoder and decoder Transformer stacks but instead of the full self-attention in the encoder, it uses the efficient local+global attention pattern of the Longformer. The decoder uses the full self-attention to the entire encoded tokens and to previously decoded locations. The paper authors call this model Longformer-Encoder-Decoder (LED) [Implying encoder-decoder variant]\n\nSince pre-training LED is expensive, the paper authors initialize LED parameters from the BART, and follow BART\u2019s exact architecture in terms of number of layers and hidden sizes. The only difference is that to process longer inputs, we extend position embedding to 16K tokens (up from BART\u2019s 1K tokens) and the paper authors initialize the new position embedding matrix by repeatedly copying BART\u2019s 1K position embeddings 16 times as in Section 5 for RoBERTa. Following BART, the paper authors release two model sizes, LED-base and LED-large, which respectively have 6 and 12 layers in both encoder and decoder stacks. [Implying the number of parameters and architecture of encoder-decoder variant]", "title": "Longformer: The Long-Document Transformer", "context": ["While encoder-only Transformers are effective on a variety of NLP tasks, pre-trained encoderdecoder Transformer models (e.g. BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) have achieved strong results on tasks like summarization. Yet, such models can\u2019t efficiently scale to seq2seq tasks with longer inputs. To facilitate modeling long sequences for seq2seq learning, we propose a Longformer variant that has both the encoder and decoder Transformer stacks but instead of the full self-attention in the encoder, it uses the efficient local+global attention pattern of the Longformer. The decoder uses the full self-attention to the entire encoded tokens and to previously decoded locations. We call this model Longformer-Encoder-Decoder (LED) which scales linearly with the input. Since pre-training LED is expensive, we initialize LED parameters from the BART, and follow BART\u2019s exact architecture in terms of number of layers and hidden sizes. The only difference is that to process longer inputs, we extend position embedding to 16K tokens (up from BART\u2019s 1K tokens) and we initialize the new position embedding matrix by repeatedly copying BART\u2019s 1K position embeddings 16 times as in Section 5 for RoBERTa. Following BART, we release two model sizes, LED-base and LED-large, which respectively have 6 and 12 layers in both encoder and decoder stacks. We evaluate LED on the summarization task using the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summarization in the scientific domain. The 90th percentile of document lengths is 14.5K tokens, making it an appropriate testbed for evaluating LED. LED\u2019s encoder reads the document and its decoder generates the output summary. The encoder uses local attention with window size 1,024 tokens and global attention on the first <s> token. The decoder uses full attention to the entire encoder and previously decoded locations.", "In addition, it provides balance between efficiency (smaller window sizes are less computationally expensive due to fewer nonzero values) and performance (larger window sizes have richer representation power and often result in performance improvements). We do not use dilated sliding windows for lower layers to maximize their capacity to learn and utilize the immediate local context. For the higher layers, we use a small amount of increasing dilation only on 2 heads. This gives the model the ability to directly attend to distant tokens without sacrificing local context. 4.2 Experiment Setup. To compare to prior work we focus on characterlevel LM (text8 and enwik8; Mahoney, 2009). Training Ideally, we would like to train our model on the largest window size and sequence length we can fit in a modern GPU memory. However, we found that the model needs a large number of gradient updates to learn the local context first, before learning to utilize longer context. To accommodate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phases. In particular, in the first phase we start with a short sequence length and window size, then on each subsequent phase, we double the window size and the sequence length, and halve the learning rate. This makes training fast, while keeping the slow part (longest sequences and window sizes) to the end. We train the model over 5 total phases with starting sequence length of 2,048 and ending sequence length of 23,040 on the last phase (see Appendix B for detailed configurations of each phase, and for all other hyperparameters). Evaluation We evaluate with sequences of length 32,256. Following Dai et al. (2019), we split the dataset into overlapping sequences of size 32,256 with a step of size 512, and report the performance on the last 512 tokens on the sequence. 4.2.1 Results. Tab. 2 and 3 summarize evaluation results on text8 and enwik8 datasets.", "We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining. In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs. TriviaQA TriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s]\ndocument [/s]. We truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens. For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them. Hyperparameters of the best configuration are listed in Tab. 14."]}
{"pkey": "longformer_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "1.  The paper authors train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.[For MLM Pretraining]\n\n2. Table 12: Hyperparameters for the best performing model for character-level language modeling\n\n3. Table 14: Hyperparameters of the QA models. All models use a similar scheduler with linear warmup and decay.\n\n3.  [For coreference resolution] The maximum sequence length was 384 for RoBERTa-base, chosen after three trials from [256, 384, 512] using the default hyperparameters in the original implementation.16 For Longformer-base the sequence length was 4,096.....\n\n4. [For coreference resolution] .... Hyperparameter searches were minimal and consisted of grid searches of RoBERTa LR in [1e-5, 2e-5, 3e-5] and task LR in [1e-4, 2e-4, 3e-4] for both RoBERTa and Longformer for a fair comparison. The best configuration for Longformer-base was RoBERTa lr=1e-5, task lr=1e-4. All other hyperparameters were the same as in the original implementation.\n\n5. [For text classification] The paper authors used Adam optimizer with batch sizes of 32 and linear warmup and decay with warmup steps equal to 0.1 of the total training steps. For both IMDB and Hyperpartisan news the paper authors did grid search of LRs [3e-5, 5e-5] and epochs [10, 15, 20] and found the model with [3e-5] and epochs 15 to work best.", "title": "Longformer: The Long-Document Transformer", "context": ["For Longformer, we use global attention to question tokens, paragraph title start tokens as well as sentence tokens. The model includes additional feedforward layers on top of paragraph title start tokens for prediction of relevant paragraphs, as well as sentence tokens for predicting evidence sentences. After training the first stage model, we predict relevant paragraph scores for both training and development set. We then keep up to 5 paragraphs whose raw score is higher than a pre-specified threshold (-3.0), and remove the other paragraphs from the context. We then train the second stage model on the resulting shortened context. For answer span extraction we use BERT\u2019s QA model (Devlin et al., 2019) with addition of a question type (yes/no/span) classification head over the first special token ([CLS]). For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model. At inference time for evidence extraction, we use a constrained decoding strategy similar to Groeneveld et al. (2020) that ensures that the evidence sentences come from exactly two paragraphs which is the setup of this dataset. We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses. Our experiments are done on RTX8000 GPUs and training each epoch takes approximately half a day on 4 GPUs. We trained the model using Adam optimizer with linear warmup (1000 steps) and linear decay. We used minimal hyperparameter tuning using LRs of 3e-5 and 5e-5 and epochs of 3 to 7 and found the model with LR of 3e-5 and 5 epochs to work best. We conduct the same hyperparameter search for the RoBERTa baseline as well. The rest of hyperparameters are reported in Tab 14.", "In addition, it provides balance between efficiency (smaller window sizes are less computationally expensive due to fewer nonzero values) and performance (larger window sizes have richer representation power and often result in performance improvements). We do not use dilated sliding windows for lower layers to maximize their capacity to learn and utilize the immediate local context. For the higher layers, we use a small amount of increasing dilation only on 2 heads. This gives the model the ability to directly attend to distant tokens without sacrificing local context. 4.2 Experiment Setup. To compare to prior work we focus on characterlevel LM (text8 and enwik8; Mahoney, 2009). Training Ideally, we would like to train our model on the largest window size and sequence length we can fit in a modern GPU memory. However, we found that the model needs a large number of gradient updates to learn the local context first, before learning to utilize longer context. To accommodate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phases. In particular, in the first phase we start with a short sequence length and window size, then on each subsequent phase, we double the window size and the sequence length, and halve the learning rate. This makes training fast, while keeping the slow part (longest sequences and window sizes) to the end. We train the model over 5 total phases with starting sequence length of 2,048 and ending sequence length of 23,040 on the last phase (see Appendix B for detailed configurations of each phase, and for all other hyperparameters). Evaluation We evaluate with sequences of length 32,256. Following Dai et al. (2019), we split the dataset into overlapping sequences of size 32,256 with a step of size 512, and report the performance on the last 512 tokens on the sequence. 4.2.1 Results. Tab. 2 and 3 summarize evaluation results on text8 and enwik8 datasets.", "A few contemporaneous works3 have explored similar ideas to Longformer using local + global attention in Transformers, and pre-training it for long document natural language tasks. In particular, ETC (Ainslie et al., 2020) uses a similar local + global attention instead of full self-attention to scale Transformers to long documents. Different from Longformer, ETC uses relative position em-\n2SQuAD contexts typically fit within the 512 limit, and MRQA is constructed by dropping long-document examples. 3All were published on arXiv after Longformer.\nbeddings (which we only used for the Autoregressive LM setting), introduces an additional training objective (CPC loss) for pre-training, and configures global attention in a slightly different way. It shows strong results on several tasks including reading comprehension and classification. GMAT (Gupta and Berant, 2020) uses a similar idea of few global locations in the input serving as global memory. BigBird (Zaheer et al., 2020) is an extension over ETC with evaluation on additional tasks, including summarization. Importantly, through theoretical analysis, BigBird shows that sparse Transformers are universal approximators of sequence functions and preserve these properties of the full self-attention. 3 Longformer. The original Transformer model has a self-attention component with O(n2) time and memory complexity where n is the input sequence length. To address this challenge, we sparsify the full self-attention matrix according to an \u201cattention pattern\u201d specifying pairs of input locations attending to one another. Unlike the full self-attention, our proposed attention pattern scales linearly with the input sequence, making it efficient for longer sequences. This section discusses the design and implementation of this attention pattern.\n3.1 Attention Pattern. Sliding Window Given the importance of local context (Kovaleva et al., 2019), our attention pattern employs a fixed-size window attention surrounding each token."]}
{"pkey": "longformer_13", "question": "Describe the computational resources used to train the model.", "answer": "1. [Character Level Language Modelling]\n\nThe paper authors ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. \n\n2. [For wikihop]\n\nAll models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs.\n\n3. [For TriviaQA]\n\nThe paper authors ran our experiments on 32GB V100 GPUs. Small model takes 1 day to train on 4 GPUs, while large model takes 1 day on 8 GPUs.\n\n4. [For Hotpot QA]\n\n Our experiments are done on RTX8000 GPUs and training each epoch takes approximately half a day on 4 GPUs.\n\n5. [Text Classification]\n\n Experiments were done on a single RTX8000 GPU.", "title": "Longformer: The Long-Document Transformer", "context": ["For Longformer, we use global attention to question tokens, paragraph title start tokens as well as sentence tokens. The model includes additional feedforward layers on top of paragraph title start tokens for prediction of relevant paragraphs, as well as sentence tokens for predicting evidence sentences. After training the first stage model, we predict relevant paragraph scores for both training and development set. We then keep up to 5 paragraphs whose raw score is higher than a pre-specified threshold (-3.0), and remove the other paragraphs from the context. We then train the second stage model on the resulting shortened context. For answer span extraction we use BERT\u2019s QA model (Devlin et al., 2019) with addition of a question type (yes/no/span) classification head over the first special token ([CLS]). For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model. At inference time for evidence extraction, we use a constrained decoding strategy similar to Groeneveld et al. (2020) that ensures that the evidence sentences come from exactly two paragraphs which is the setup of this dataset. We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses. Our experiments are done on RTX8000 GPUs and training each epoch takes approximately half a day on 4 GPUs. We trained the model using Adam optimizer with linear warmup (1000 steps) and linear decay. We used minimal hyperparameter tuning using LRs of 3e-5 and 5e-5 and epochs of 3 to 7 and found the model with LR of 3e-5 and 5 epochs to work best. We conduct the same hyperparameter search for the RoBERTa baseline as well. The rest of hyperparameters are reported in Tab 14.", "For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.\nAcknowledgment. We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. B Character LM Hyperparameters. We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8.", "The dataset does not provide any intermediate annotation for the multihop reasoning chains, requiring models to instead infer them from the indirect answer supervision. To prepare the data for input to Longformer and RoBERTa, we first tokenize the question, answer candidates, and support contexts using RoBERTa\u2019s wordpiece tokenizer. Then we concatenate the question and answer candidates with special tokens as [q] question [/q] [ent] candidate1 [/ent] ... [ent] candidateN [/ent]. The contexts are also concatenated using RoBERTa\u2019s document delimiter tokens as separators: </s> context1 </s> ... </s > contextM </s>. The special tokens [q], [/q], [ent], [/ent] were added to the RoBERTa vocabulary and randomly initialized before task finetuning. 14https://github.com/PyTorchLightning/ pytorch-lightning\nAfter preparing the input data, we compute activations from the top layer of each model as follows. We take the question and answer candidates and concatenate them to as much context as possible up to the model sequence length (512 for RoBERTa, 4,096 for Longformer), run the sequence through the model, collect the output activations, and repeat until all of the context is exhausted (for all models except Longformer-large, where we just include the first 4,096 length sequence due to memory requirements). Then all activations for all chunks are concatenated into one long sequence. In the case of Longformer, we use global attention to the entire question and answer candidate sequence. For prediction, we attach a linear layer to each [ent] that outputs a single logit, average over all logits for each candidate across the chunks, apply a softmax and use the cross entropy loss with the correct answer candidate. Training used the Adam optimizer with linear warmup over 200 gradient updates to a maximum LR, and linear decay over the remainder of training."]}
{"pkey": "longformer_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "Not mentioned in the paper.", "title": "Longformer: The Long-Document Transformer", "context": ["Continued MLM Pretraining We pretrain Longformer using fairseq (Ott et al., 2019) on a corpus of long documents that we compiled (see Appendix C for corpus details). We train two model sizes, a base model and a large model. Both models are trained for 65K gradient updates with sequences length 4,096, batch size 64 (218 tokens), maximum learning rate of 3e-5, linear warmup of 500 steps, followed by a power 3 polynomial decay. The rest of the hyperparameters are the same as RoBERTa.\nTab. 5 shows the BPC on the development set of our training corpus. The first row shows a 1.846\n6Adding dilation on a few heads as in \u00a74.1 hurt performance, likely because it is not compatible with the pretrained RoBERTa weights. Retraining such model from scratch might be needed to improve performance. BPC using RoBERTa-base, which is comparable to the 1.880 BPC reported on the RoBERTa paper on their corpus. This indicates our training corpus is from a distribution close to that used to train RoBERTa. The following two rows show the performance of Longformer before pretraining with randomly initialized position embeddings and with copied position embeddings. The significant difference indicates the importance of the copy initialization, and the relative small difference between the RoBERTa BPC and the initialized BPC indicates that our sliding window attention is working well with the RoBERTa weights. The following two rows show the impact of continuing pretraining. Traininig for 2K steps improves BPC from 1.957 to 1.753, which further decreases to 1.705 after 65K steps, demonstrating the model is learning to better utilize the sliding window attention and longer context. Similar patterns are observed with RoBERTa-large and Longformer-large. Frozen RoBERTa Weights We also pretrained Longformer while freezing all RoBERTa weights, and only training the new position embeddings. The motivation for this configuration is to perfectly preserve the RoBERTa performance on short documents.", "In our case, the windowed and dilated attention are not flexible enough to learn task-specific representations. Accordingly, we add \u201cglobal attention\u201d on few pre-selected input locations. Importantly, we make this attention operation symmetric: that is, a token with a global attention attends to all tokens across the sequence, and all tokens in the sequence attend to it. Fig. 2d shows an example of a sliding window attention with global attention at a few tokens at custom locations. For example for classification, global attention is used for the [CLS] token while in QA global attention is provided on all question tokens. Since the number of such tokens is small relative to and independent of n the complexity of the combined local and global attention is still O(n). While specifying global attention is task specific, it is a easy way to add inductive bias to the model\u2019s attention, and it is much\nsimpler than existing task specific approaches that use complex architecture to combine information across smaller input chunks. Linear Projections for Global Attention Recall that given the linear projections Q, K, V , the Transformer model (Vaswani et al., 2017) computes attention scores as follows:\nAttention(Q,K, V ) = softmax ( QKT\u221a\ndk\n) V (1) We use two sets of projections, Qs, Ks, Vs to compute attention scores of sliding window attention, and Qg, Kg, Vg to compute attention scores for the global attention. The additional projections provide flexibility to model the different types of attention, which we show is critical for best performance on downstream tasks. Qg, Kg, Vg are all initialized with values that match Qs, Ks, Vs.\n3.2 Implementation. In regular transformers, attention scores are computed as in Eqn. 1. The expensive operation is the matrix multiplication QKT because both Q and K have n (sequence length) projections. For Longformer, the dilated sliding window attention computes only a fixed number of the diagonals of QKT .", "In addition, it provides balance between efficiency (smaller window sizes are less computationally expensive due to fewer nonzero values) and performance (larger window sizes have richer representation power and often result in performance improvements). We do not use dilated sliding windows for lower layers to maximize their capacity to learn and utilize the immediate local context. For the higher layers, we use a small amount of increasing dilation only on 2 heads. This gives the model the ability to directly attend to distant tokens without sacrificing local context. 4.2 Experiment Setup. To compare to prior work we focus on characterlevel LM (text8 and enwik8; Mahoney, 2009). Training Ideally, we would like to train our model on the largest window size and sequence length we can fit in a modern GPU memory. However, we found that the model needs a large number of gradient updates to learn the local context first, before learning to utilize longer context. To accommodate this, we adopt a staged training procedure where we increase the attention window size and sequence length across multiple training phases. In particular, in the first phase we start with a short sequence length and window size, then on each subsequent phase, we double the window size and the sequence length, and halve the learning rate. This makes training fast, while keeping the slow part (longest sequences and window sizes) to the end. We train the model over 5 total phases with starting sequence length of 2,048 and ending sequence length of 23,040 on the last phase (see Appendix B for detailed configurations of each phase, and for all other hyperparameters). Evaluation We evaluate with sequences of length 32,256. Following Dai et al. (2019), we split the dataset into overlapping sequences of size 32,256 with a step of size 512, and report the performance on the last 512 tokens on the sequence. 4.2.1 Results. Tab. 2 and 3 summarize evaluation results on text8 and enwik8 datasets."]}
{"pkey": "longformer_15", "question": "What is the pretraining objective of the model? ", "answer": "The paper authors pretrain Longformer with masked language modeling (MLM), where the goal is to recover randomly masked tokens in a sequence.", "title": "Longformer: The Long-Document Transformer", "context": ["On the other hand, our proposed Longformer is able to build contextual representations of the entire context using multiple layers of attention, reducing the ar X iv :2\n00 4.\n05 15\n0v 2\n[ cs\n.C L\n] 2\nD ec\n2 02\n0\nneed for task-specific architectures. Recent work has addressed the computational inefficiency of Transformers on long sequences (see Tab. 1). However, they primarily focus on autoregressive language modeling (LM), while the application of long document transformers to documentlevel NLP tasks in the transfer learning setting (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018; Devlin et al., 2019) has remained largely unexplored. We address this gap and show that Longformer\u2019s attention mechanism can act as a drop-in replacement for the self-attention mechanism in pretrained Transformers, and leads to gains across a suite of document NLP tasks. Longformer\u2019s attention mechanism is a combination of a windowed local-context self-attention and an end task motivated global attention that encodes inductive bias about the task. Through ablations and controlled trials we show both attention types are essential \u2013 the local attention is primarily used to build contextual representations, while the global attention allows Longformer to build full sequence representations for prediction. We first evaluate Longformer on autoregressive character-level language modeling using a combination of windowed and a new dilated attention pattern, allowing the model to process sequences of up to 32K characters on modern GPUs. We achieve state-of-the-art results on text8 and enwik8 benchmark datasets, demonstrating the effectiveness of Longformer in long document modeling. Then, to evaluate Longformer\u2019s ability to replace the full self-attention operation of existing pretrained models, we pretrain it with the masked language modeling (MLM) objective, continuing from the RoBERTa (Liu et al., 2019) released checkpoint.", "For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.\nAcknowledgment. We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. B Character LM Hyperparameters. We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8.", "A few contemporaneous works3 have explored similar ideas to Longformer using local + global attention in Transformers, and pre-training it for long document natural language tasks. In particular, ETC (Ainslie et al., 2020) uses a similar local + global attention instead of full self-attention to scale Transformers to long documents. Different from Longformer, ETC uses relative position em-\n2SQuAD contexts typically fit within the 512 limit, and MRQA is constructed by dropping long-document examples. 3All were published on arXiv after Longformer.\nbeddings (which we only used for the Autoregressive LM setting), introduces an additional training objective (CPC loss) for pre-training, and configures global attention in a slightly different way. It shows strong results on several tasks including reading comprehension and classification. GMAT (Gupta and Berant, 2020) uses a similar idea of few global locations in the input serving as global memory. BigBird (Zaheer et al., 2020) is an extension over ETC with evaluation on additional tasks, including summarization. Importantly, through theoretical analysis, BigBird shows that sparse Transformers are universal approximators of sequence functions and preserve these properties of the full self-attention. 3 Longformer. The original Transformer model has a self-attention component with O(n2) time and memory complexity where n is the input sequence length. To address this challenge, we sparsify the full self-attention matrix according to an \u201cattention pattern\u201d specifying pairs of input locations attending to one another. Unlike the full self-attention, our proposed attention pattern scales linearly with the input sequence, making it efficient for longer sequences. This section discusses the design and implementation of this attention pattern.\n3.1 Attention Pattern. Sliding Window Given the importance of local context (Kovaleva et al., 2019), our attention pattern employs a fixed-size window attention surrounding each token."]}
{"pkey": "longformer_16", "question": "What is the loss function that is used to train the model?", "answer": "1. [WikiHop]\n\nFor prediction, the paper authors attach a linear layer to each [ent] that outputs a single logit, average over all logits for each candidate across the chunks, apply a softmax and use the cross entropy loss with the correct answer candidate.\n\n2. [TriviaQA]\n\nFor prediction, the paper authors add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), the paper authors use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them.\n\n3. [HotpotQA]\n\nFor evidence extraction the paper authors apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n\n4. [Coreference Resolution]\n\nFor evidence extraction the paper authors apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model.\n\n5. [Text classification]\n\nFor classification, following BERT, the paper authors used a simple binary cross entropy loss on top of a first [CLS] token with addition of global attention to [CLS].", "title": "Longformer: The Long-Document Transformer", "context": ["We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining. In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs. TriviaQA TriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s]\ndocument [/s]. We truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens. For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them. Hyperparameters of the best configuration are listed in Tab. 14.", "For Longformer, we use global attention to question tokens, paragraph title start tokens as well as sentence tokens. The model includes additional feedforward layers on top of paragraph title start tokens for prediction of relevant paragraphs, as well as sentence tokens for predicting evidence sentences. After training the first stage model, we predict relevant paragraph scores for both training and development set. We then keep up to 5 paragraphs whose raw score is higher than a pre-specified threshold (-3.0), and remove the other paragraphs from the context. We then train the second stage model on the resulting shortened context. For answer span extraction we use BERT\u2019s QA model (Devlin et al., 2019) with addition of a question type (yes/no/span) classification head over the first special token ([CLS]). For evidence extraction we apply 2 layer feedforward networks on top of the representations corresponding to sentence and paragraph tokens to get the corresponding evidence prediction scores and use binary cross entropy loss to train the model. At inference time for evidence extraction, we use a constrained decoding strategy similar to Groeneveld et al. (2020) that ensures that the evidence sentences come from exactly two paragraphs which is the setup of this dataset. We combine span, question classification, sentence, and paragraphs losses and train the model in a multitask way using linear combination of losses. Our experiments are done on RTX8000 GPUs and training each epoch takes approximately half a day on 4 GPUs. We trained the model using Adam optimizer with linear warmup (1000 steps) and linear decay. We used minimal hyperparameter tuning using LRs of 3e-5 and 5e-5 and epochs of 3 to 7 and found the model with LR of 3e-5 and 5 epochs to work best. We conduct the same hyperparameter search for the RoBERTa baseline as well. The rest of hyperparameters are reported in Tab 14.", "This configuration has a BPC of 1.850 (down from 1.957 at initialization), but higher than 1.705 where all the weights are trainable. 6 Tasks. We apply Longformer to multiple long document tasks, including QA, coreference resolution and classification. Tab. 6 shows the evaluation datasets have contexts significantly longer than 512 wordpieces. Our primary goal is to evaluate whether our attention mechanism can act as a replacement for the standard self-attention mechanism in BERT style models, and to perform controlled trials against a strong baseline. We are also interested in evaluating whether we can replace complicated task specific models necessitated by BERT\u2019s limited context with simpler models that just concate-\nnate all available context into a single sequence. Our baseline is a RoBERTa based model that breaks the context into the longest possible segment, passes each individually through RoBERTa, and concatenates the activations for further processing. For QA tasks, we also concatenate the question to each segment so that RoBERTa can condition it\u2019s contextual representations of the context on the question. The Longformer variant replaces the RoBERTa self-attention mechanism with our windowed attention used during pretraining, plus a task motivated global attention. The global attention uses additional linear projections (\u00a73.1). 6.1 Question answering. We used three datasets: WikiHop (Welbl et al., 2018), TriviaQA (Joshi et al., 2017, Wikipedia setting), and HotpotQA, (Yang et al., 2018, distractor setting).7\nFor WikiHop and TriviaQA we follow the simple QA model of BERT (Devlin et al., 2019), and concatenate question and documents into one long sequence, run it through Longformer, then have a dataset-specific prediction layer. WikiHop uses a classification layer for the candidate while TriviaQA uses the loss function of Clark and Gardner (2017) to predict answer span."]}
{"pkey": "longformer_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer.", "title": "Longformer: The Long-Document Transformer", "context": ["As shown in Fig. 1, this results in a linear increase in memory usage compared to quadratic increase for full self-attention. However, implementing it requires a form of banded matrix multiplication that is not supported in existing deep learning libraries like PyTorch/Tensorflow. Fig. 1 compares the performance of three different ways of implementing it: loop is a memory efficient PyTorch implementation that supports dilation but is unusably slow and only used for testing; chunks only supports the non-dilated case and is used for the pretraining/finetuning setting; and cuda is our fully functioning highly optimized custom CUDA kernel implemented using TVM (Chen et al., 2018) and used for the language modeling experiments (see Appendix A for more details). 4 Autoregressive Language Modeling. Autoregressive or left-to-right language modeling is loosely defined as estimating the probability distribution of an existing token/character given its previous tokens/characters in an input sequence. This task is considered one of the fundamental tasks in natural language and recent prior work on modeling long sequences using transformers has relied\non this task as their primary evaluation (Dai et al., 2019; Rae et al., 2020; Sukhbaatar et al., 2019). Similarly, we develop and evaluate our model on autoregressive language modeling. 4.1 Attention Pattern. For autoregressive language modeling we use our dilated sliding window attention. Following Sukhbaatar et al. (2019) we use differing window sizes across the layers. In particular, we use small window sizes for the lower layers and increase window sizes as we move to higher layers. This allows the top layers to learn higher-level representation of the entire sequence while having the lower layers capture local information.", "For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.\nAcknowledgment. We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. B Character LM Hyperparameters. We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8.", "After pretraining, we apply it to downstream language tasks through finetuning and demonstrate that Longformer consistently outperforms RoBERTa on a wide range of document-level natural language tasks including text classification, QA, and coreference resolution, achieving state-ofthe-art results on two of these datasets. We finally introduce a variant of Longformer which instead of an encoder-only Transformer architecture, it follows an encoder-decoder architecture similar to the original Transformer model (Vaswani et al., 2017), and it is intended for sequence-to-sequence (seq2seq) learning (Sutskever et al., 2014). We call this model Longformer-Encoder-Decoder (LED) that uses\nLongformer\u2019s efficient attention pattern on the encoder network, allowing it to address long document seq2seq tasks such as summarization. We demonstrate the effectiveness of LED on the arXiv summarization dataset (Cohan et al., 2018). 2 Related Work. Long-Document Transformers Tab. 1 summarizes recent prior work on long documents. Two types of self-attention approaches have been explored. The first is a left-to-right (ltr) approach that processes the document in chunks moving from left-to-right. While such models have been successful in autoregressive language modeling, they are unsuitable for transfer learning approaches with tasks that benefit from bidirectional context. Our work falls within the other general approach that defines some form of sparse attention pattern and avoids computing the full quadratic attention matrix multiplication. The model with the most similar attention pattern to ours is Sparse Transformer (Child et al., 2019), which uses a form of dilated sliding window of blocks of size 8x8 provided by BlockSparse (Gray et al., 2017). Our implementation (\u00a73) also includes a custom CUDA kernel, but it is more flexible and maintainable than BlockSparse which is implemented in C++, and designed for a specific version of TensorFlow."]}
{"pkey": "longformer_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "1. [Character level langyage modeling]\n\na) To compare to prior work the paper authors focus on character-level LM (text8 and enwik8; Mahoney, 2009).\n\nb) Tab. 2 and 3 summarize evaluation results on text8 and enwik8 datasets. The paper authors achieve a new state-of-the-art on both text8 and enwik8 using the small models with BPC of 1.10 and 1.00 on text8 and enwik8 respectively, demonstrating the effectiveness of our model.\n\n2. [Pretraining]\n\na) The paper authors pretrain Longformer with masked language modeling (MLM), where the goal is to recover randomly masked tokens in a sequence. \n\nb) Table 5: MLM BPC for RoBERTa and various pretrained Longformer configurations.\n\n3. [WikiHop]\n\nInstances in WikiHop consist of: a question, answer candidates (ranging from two candidates to 79 candidates), supporting contexts (ranging from three paragraphs to 63 paragraphs),\nand the correct answer. The dataset does not provide any intermediate annotation for the multihop reasoning chains, requiring models to instead infer them from the indirect answer supervision.\n\n4. [TriviaQA]\n\nTriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching.\n\n5. [HotpotQA]\n\nHotpotQA dataset involves answering questions from a set of 10 paragraphs from 10 different Wikipedia articles where 2 paragraphs are relevant to the question and the rest are distractors. It includes 2 tasks of answer span extraction and evidence sentence identification. Our model for HotpotQA combines both answer span extraction and evidence extraction in one joint model.\n\n6. [Coreference model]\n\nThe coreference model is a straightforward adaptation of the coarse-to-fine BERT based model from Joshi et al. (2019). \n\n7.  [Text classification]\n\nFor classification, following BERT, the paper authors used a simple binary cross entropy loss on top of a first [CLS] token with addition of global attention to [CLS].\n\n8. [Evaluation metric for finetuned tasks]\n\nTable 7: Summary of finetuning results on QA, coreference resolution, and document classification. Results are on the development sets comparing our Longformer-base with RoBERTa-base. TriviaQA, Hyperpartisan metrics are F1, WikiHop and IMDB use accuracy, HotpotQA is joint F1, OntoNotes is average F1.\n\n9. [Summarization]\n\na) The paper authors evaluate LED on the summarization task using the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summarization in the scientific domain. \n\nb) Table 11: Summarization results of Longformer-Encoder-Decoder (LED) on the arXiv dataset. Metrics from left to right are ROUGE-1, ROUGE-2 and ROUGE-L.", "title": "Longformer: The Long-Document Transformer", "context": ["All other hyperparameters are similar to RoBERTa\u2019s. For hyperparameter search, we only tuned LR for the RoBERTa baseline and tried rates [3e-5, 5e-5, 1e-4], then used the best, which is 3e-5, for all subsequent experiments with no further tuning. We trained the Longformer-large with the best configuration once and submitted its output to the leaderboard. We ran our experiments\non 32GB V100 GPUs. Small model takes 1 day to train on 4 GPUs, while large model takes 1 day on 8 GPUs. HotpotQA HotpotQA dataset involves answering questions from a set of 10 paragraphs from 10 different Wikipedia articles where 2 paragraphs are relevant to the question and the rest are distractors. It includes 2 tasks of answer span extraction and evidence sentence identification. Our model for HotpotQA combines both answer span extraction and evidence extraction in one joint model. We found a higher performance using a two-stage Longformer model with similar setup that first identifies relevant paragraphs and then does find the final answer span and evidence.15 This is largely because removing the distracting paragraphs first reduces the noise for the final evidence and span detection as also found to be important by recent state-of-the-art methods in this dataset (Fang et al., 2020). Similar to Wikihop and TriviaQA, to prepare the data for input to Longformer, we concatenate question and then all the 10 paragraphs in one long context. We particularly use the following input format with special tokens: \u201c[CLS] [q] question [/q] \u3008t\u3009 title1 \u3008/t\u3009 sent1,1 [s] sent1,2 [s] ...\n15The final dev performance of the two stage model improves over a single stage model by about 4.2 points on jointF1 metric\n\u3008t\u3009 title2 \u3008/t\u3009 sent2,1 [s] sent2,2 [s] ...\u201d where [q], [/q], \u3008t\u3009, \u3008/t\u3009, [s], [p] are special tokens representing, question start and end, paragraph title start and end, and sentence, respectively. The special tokens were added to the Longformer vocabulary and randomly initialized before task finetuning.", "While encoder-only Transformers are effective on a variety of NLP tasks, pre-trained encoderdecoder Transformer models (e.g. BART (Lewis et al., 2020) and T5 (Raffel et al., 2020)) have achieved strong results on tasks like summarization. Yet, such models can\u2019t efficiently scale to seq2seq tasks with longer inputs. To facilitate modeling long sequences for seq2seq learning, we propose a Longformer variant that has both the encoder and decoder Transformer stacks but instead of the full self-attention in the encoder, it uses the efficient local+global attention pattern of the Longformer. The decoder uses the full self-attention to the entire encoded tokens and to previously decoded locations. We call this model Longformer-Encoder-Decoder (LED) which scales linearly with the input. Since pre-training LED is expensive, we initialize LED parameters from the BART, and follow BART\u2019s exact architecture in terms of number of layers and hidden sizes. The only difference is that to process longer inputs, we extend position embedding to 16K tokens (up from BART\u2019s 1K tokens) and we initialize the new position embedding matrix by repeatedly copying BART\u2019s 1K position embeddings 16 times as in Section 5 for RoBERTa. Following BART, we release two model sizes, LED-base and LED-large, which respectively have 6 and 12 layers in both encoder and decoder stacks. We evaluate LED on the summarization task using the arXiv summarization dataset (Cohan et al., 2018) which focuses on long document summarization in the scientific domain. The 90th percentile of document lengths is 14.5K tokens, making it an appropriate testbed for evaluating LED. LED\u2019s encoder reads the document and its decoder generates the output summary. The encoder uses local attention with window size 1,024 tokens and global attention on the first <s> token. The decoder uses full attention to the entire encoder and previously decoded locations.", "The dataset does not provide any intermediate annotation for the multihop reasoning chains, requiring models to instead infer them from the indirect answer supervision. To prepare the data for input to Longformer and RoBERTa, we first tokenize the question, answer candidates, and support contexts using RoBERTa\u2019s wordpiece tokenizer. Then we concatenate the question and answer candidates with special tokens as [q] question [/q] [ent] candidate1 [/ent] ... [ent] candidateN [/ent]. The contexts are also concatenated using RoBERTa\u2019s document delimiter tokens as separators: </s> context1 </s> ... </s > contextM </s>. The special tokens [q], [/q], [ent], [/ent] were added to the RoBERTa vocabulary and randomly initialized before task finetuning. 14https://github.com/PyTorchLightning/ pytorch-lightning\nAfter preparing the input data, we compute activations from the top layer of each model as follows. We take the question and answer candidates and concatenate them to as much context as possible up to the model sequence length (512 for RoBERTa, 4,096 for Longformer), run the sequence through the model, collect the output activations, and repeat until all of the context is exhausted (for all models except Longformer-large, where we just include the first 4,096 length sequence due to memory requirements). Then all activations for all chunks are concatenated into one long sequence. In the case of Longformer, we use global attention to the entire question and answer candidate sequence. For prediction, we attach a linear layer to each [ent] that outputs a single logit, average over all logits for each candidate across the chunks, apply a softmax and use the cross entropy loss with the correct answer candidate. Training used the Adam optimizer with linear warmup over 200 gradient updates to a maximum LR, and linear decay over the remainder of training."]}
{"pkey": "longformer_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "1. Tab. 10 presents an ablation study for WikiHop on the development set. All results use Longformer-base, fine-tuned for five epochs with identical hyperparameters except where noted. Longformer benefits from longer sequences, global attention, separate projection matrices for global attention, MLM pretraining, and longer training. In addition, when configured as in RoBERTa-base (seqlen: 512, and n2 attention) Longformer performs slightly worse then RoBERTa-base, confirming that per- formance gains are not due to additional pretraining. Performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings, showing that Longformer can learn to use long range context in task specific fine-tuning with large training datasets such as WikiHop.\n\n2. Table 10: WikiHop development set ablations", "title": "Longformer: The Long-Document Transformer", "context": ["We achieve a new state-of-the-art on both text8 and enwik8 using the small models with BPC of 1.10 and 1.00 on text8 and enwik8 respectively, demonstrating the effectiveness of our model. For large models, given how expensive these experiments are, and following recent work (Kitaev et al., 2020; Rae et al., 2020), we are only evaluating on enwik8. Tab. 3 shows that Longformer outperforms the comparable TransformerXL model, matches the performance of the comparable Sparse Transformer (Child et al., 2019), and matches or slightly underperforms recent models that have more than twice the number of parameters. It is worth noting that Adaptive Span (Sukhbaatar et al., 2019) and Compressive Transformer (Rae et al., 2020) are not good fit for the pretrainingfinetuning paradigm as discussed in \u00a72.\n4.2.2 Ablation Study. To show the importance of the design choices of our attention patterns, we tried different variants and report their controlled experiment results. To make the ablation study more manageable, we train each configuration for 150K steps4 with phase 1 configuration on a small model on text8, then report the BPC performance on the dev set. The top of Tab. 4 demonstrates the impact of different ways of configuring the window sizes per layer. We observe that increasing the window size from the bottom to the top layer leads to the best performance, arranging them in the reverse way leads to worse performance, and using a fixed window size (the average of window sizes of the other configuration) leads to a performance that it is in between. The bottom of Tab. 4 shows the impact of adding dilation. Adding some dilation to two heads leads to some improvement compared with no dilation at all. 5 Pretraining and Finetuning. Current state-of-the-art systems for many NLP tasks finetune a pretrained model with task supervision (e.g. BERT). One of our main motivations is to develop such a model suitable for long document tasks.", "All published top performing models in this task (Tu et al., 2019; Fang et al., 2020; Shao et al., 2020) use GNNs (Kipf and Welling, 2017) or graph network of entities, which seem to encode an important inductive bias for the task and can potentially improve our results further. Nevertheless, Longformer performs strongly outperforming all other methods including the recent non-GNN methods (Gla\u00df et al., 2019; Shao et al., 2020; Groeneveld et al., 2020). 6.5 Ablations on WikiHop. Tab. 10 presents an ablation study for WikiHop on the development set. All results use Longformerbase, fine-tuned for five epochs with identical hyperparameters except where noted. Longformer benefits from longer sequences, global attention, separate projection matrices for global attention, MLM pretraining, and longer training. In addition, when configured as in RoBERTa-base (seqlen: 512, and n2 attention) Longformer performs slightly worse then RoBERTa-base, confirming that performance gains are not due to additional pretraining. Performance drops slightly when using the RoBERTa model pretrained when only unfreezing the additional position embeddings, showing that Longformer can learn to use long range context in task specific fine-tuning with large training datasets such as WikiHop. 9At submission time, May 2020. Later, BigBird (Zaheer et al., 2020) improved leaderboard results on these datasets. There are confounding factors such as using 16X more compute in BigBird\u2019s pretraining compared with Longformer, potentially affecting the performance. 7 Longformer-Encoder-Decoder (LED). The original Transformer (Vaswani et al., 2017) consisted of an encoder-decoder architecture, intended for sequence-to-sequence tasks (Sutskever et al., 2014), such as summarization and translation.", "For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.\nAcknowledgment. We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. B Character LM Hyperparameters. We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8."]}
{"pkey": "longformer_20", "question": "List the future work mentioned in the paper.", "answer": "For future work, the paper authors would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.", "title": "Longformer: The Long-Document Transformer", "context": ["We used gradient accumulation to effective batch size of 32 instances, checking the development accuracy every 250 gradient updates and reported the maximum development accuracy. Other hyperparameters (dropout, weight decay) were identical to RoBERTa pretraining. In general, we ran minimal hyperparameter trials, but for fair comparison between Longformer and RoBERTa ran an identical hyperparameter search with Longformer-base and RoBERTa-base. This consisted of a grid search of LR in [2e-5, 3e-5, 5e-5] and number epochs in [5, 10, 15]. The best Longformer-base configuration used lr=3e-5, 15 epochs. We ran two hyperparameter trials for Longformer-large, lr=3e-5 and number epochs in [5, 15] (the 5 epoch model had higher dev accuracy of 77.6, and was the single model submitted to the public leaderboard for test set evaluation). All models were trained on a single RTX8000 GPU, with Longformer-base taking about a day for 5 epochs. TriviaQA TriviaQA has more than 100K question, answer, document triplets for training. Documents are Wikipedia articles, and answers are named entities mentioned in the article. The span that answers the question is not annotated, but it is found using simple text matching. Similar to WikiHop, we tokenize the question and the document using RoBERTa\u2019s tokenizer, then form the input as [s] question [/s]\ndocument [/s]. We truncate the document at 4,096 wordpiece to avoid it being very slow. Afterwards, we get the activations from RoBERTa and Longformer similar to WikiHop (discussed above). We use global attention on all question tokens. For prediction, we add one layer that predicts the beginning and end of the answer span. Because of the distant supervision nature of the training data (no gold answer spans), we use the loss function of Clark and Gardner (2017) which works like an OR that the model only needs to get one answer span right, not all of them. Hyperparameters of the best configuration are listed in Tab. 14.", "For future work, we would like to study other pretraining objectives, especially for LED, increase the sequence length, and explore other tasks that might benefit from our model.\nAcknowledgment. We would like to thank Noah Smith, Dan Weld, Dirk Groeneveld, Kyle Lo, Daniel King and Doug Downey for helpful discussions and feedback, and the AI2 infrastructure team for technical support. B Character LM Hyperparameters. We evaluate on text8 and enwik8, both contain 100M characters from Wikipedia split into 90M, 5M, 5M for train, dev, test. Our model only specifies how the self-attention component works, and it is agnostic to the other design choices for the transformer model. Our implementation is based on the Transformer-XL (Dai et al., 2019) code11 with the memory mechanism disabled. We use relative position embeddings with sinusoidal weights as in Dai et al. (2019). We use two different model sizes, a small (12 layers, 512 hidden size) model as in Dai et al. (2019), and a large (30 layers, 512 hidden size) model as in Child et al. (2019). We employed mixed precision training (floating points 16 and 32) using apex12 to reduce memory consumption and speed-up training. However, we kept the attention computation in fp32 to avoid numerical instability issues.13 We used gradient checkpointing (Chen et al., 2016) to reduce memory usage, and ran our experiments on 48GB RTX8000 GPUs. All hyperparameters and stage configurations are listed in Tab. 12. Our CUDA kernel supports the autoregressive mode where each token attends to a window of previous tokens only. Our implementation also includes a version of the relative position embedding that is compatible with our dilated sliding window attention. We ran the small model experiments on 4 RTX8000 GPUs for 16 days. For the large model, we ran experiments on 8 RTX8000 GPUs for 13 days. Most of our hyperparameter search is similar to the ablation in Tab. 4 where we run the configuration for 150K steps on text8.", "Main Result Tab. 7 summarizes the results of all our finetuning experiments. We observe that Longformer consistently outperforms the RoBERTa baseline. Its performance gain is especially obvious for tasks that require long context such as WikiHop and Hyperpartisan. For TriviaQA, the improvement is more modest as the local context is often sufficient to answer the question. In the case of HotpotQA, the supporting fact auxiliary supervision allows models to easily find relevant contexts and then focus on local context, leading to smaller gains. This is contrasted with WikiHop that only includes distant supervision of intermediate reasoning chains, where our approach excels by reasoning over the entire context. On the IMDB and OntoNotes datasets the performance gains are smaller. For IMDB, the majority of the dataset consists of short documents and thus it is expected to see smaller improvements. For OntoNotes, we\n8For Hyperpartisan we split the training data into 80/10/10 train/dev/test sets, and report mean F1 across five seeds. found that the distance between any two mentions is typically quite small so that a baseline that processes smaller chunks separately is able to stitch together mentions into coreference chains without considering cross chunk interactions. Longformer-large for QA We also evaluate the performance of Longformer-large on long context QA tasks. Tab. 8 shows that our Longformer-large achieves new state-of-the-art results9 on WikiHop and TriviaQA by large margins (3.6 and 4 points respectively), and for HotpotQA, it underperforms the current state-of-the-art (Fang et al., 2020) by a point. Tab. 9 shows the detailed results of HotpotQA compared with published and unpublished concurrent models. Longformer places second on the published leaderboard, outperforming all other published results except for HGN (Fang et al., 2020)."]}
{"pkey": "t5_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "In this paper, the paper authors explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. By combining the insights from our exploration with scale and our new \u201cColossal Clean Crawled Corpus\u201d, the paper authors achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation.", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["This finding mostly relies on basic observations like the fact that SQuAD was created using data from Wikipedia. It would be useful to formulate a more rigorous notion of the \u201csimilarity\u201d between the pre-training and downstream tasks, so that we could make more principled choices about what source of unlabeled data to use. There is some early empirical work along these lines in the field of computer vision (Huh et al., 2016; Kornblith et al., 2018; He et al., 2018). A better notion of the relatedness of tasks could also help choose supervised pre-training tasks, which has been shown to be helpful for the GLUE benchmark (Phang et al., 2018). Language-agnostic models We were disappointed to find that English-only pre-training did not achieve state-of-the-art results on the translation tasks we studied. We also are interested in avoiding the logistical difficulty of needing to specify which languages a vocabulary can encode ahead of time. To address these issues, we are interested in further investigating language-agnostic models, i.e. models that can perform a given NLP task with good performance regardless of the text\u2019s language. This is an especially\npertinent issue given that English is not the native language for the majority of the world\u2019s population. The motivation for this paper was the flurry of recent work on transfer learning for NLP. Before we began this work, these advances had already enabled breakthroughs in settings where learning-based methods had not yet been shown to be effective. We are happy to be able to continue this trend, for example by nearly matching human-level performance on the SuperGLUE benchmark, a task specifically designed to be difficult for modern transfer-learning pipelines. Our results stem from the combination of a straightforward and unified text-to-text framework, our new C4 data set, and insights from our systematic study. Additionally, we provided an empirical overview of the field and a perspective on where it stands.", "In order to perform experiments at this scale, we introduce the \u201cColossal Clean Crawled Corpus\u201d (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.1\nThe remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.\n2. Setup. Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our \u201cColossal Clean Crawled Corpus\u201d (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the \u201cText-to-Text Transfer Transformer\u201d (T5). 2.1 Model. Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the \u201cTransformer\u201d architecture (Vaswani et al., 2017).", "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.1 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks."]}
{"pkey": "t5_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "The basic idea underlying our work is to treat every text processing problem as a \u201ctext-to-text\u201d problem, i.e. taking text as input and producing new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al. (2019b) tasks.\" (Introduction,Page 2). For WMT English to German, the paper authors use the same training data as (Vaswani et al., 2017) (i.e. News Commentary v13, Common Crawl, Europarl v7), WMT 2016 (Bojar et al., 2016) (Section 2.3, Page 8).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["This finding mostly relies on basic observations like the fact that SQuAD was created using data from Wikipedia. It would be useful to formulate a more rigorous notion of the \u201csimilarity\u201d between the pre-training and downstream tasks, so that we could make more principled choices about what source of unlabeled data to use. There is some early empirical work along these lines in the field of computer vision (Huh et al., 2016; Kornblith et al., 2018; He et al., 2018). A better notion of the relatedness of tasks could also help choose supervised pre-training tasks, which has been shown to be helpful for the GLUE benchmark (Phang et al., 2018). Language-agnostic models We were disappointed to find that English-only pre-training did not achieve state-of-the-art results on the translation tasks we studied. We also are interested in avoiding the logistical difficulty of needing to specify which languages a vocabulary can encode ahead of time. To address these issues, we are interested in further investigating language-agnostic models, i.e. models that can perform a given NLP task with good performance regardless of the text\u2019s language. This is an especially\npertinent issue given that English is not the native language for the majority of the world\u2019s population. The motivation for this paper was the flurry of recent work on transfer learning for NLP. Before we began this work, these advances had already enabled breakthroughs in settings where learning-based methods had not yet been shown to be effective. We are happy to be able to continue this trend, for example by nearly matching human-level performance on the SuperGLUE benchmark, a task specifically designed to be difficult for modern transfer-learning pipelines. Our results stem from the combination of a straightforward and unified text-to-text framework, our new C4 data set, and insights from our systematic study. Additionally, we provided an empirical overview of the field and a perspective on where it stands.", "Much of the previous work on transfer learning for NLP makes use of large unlabeled data sets for unsupervised learning. In this paper, we are interested in measuring the effect of the quality, characteristics, and size of this unlabeled data. To generate data sets that satisfy our needs, we leverage Common Crawl as a source of text scraped from the web. Common\n5. https://cloud.google.com/tpu/\nCrawl has previously been used as a source of text data for NLP, for example to train an n-gram language model (Buck et al., 2014), as training data for commonsense reasoning (Trinh and Le, 2018), for mining parallel texts for machine translation (Smith et al., 2013), as a pre-training data set (Grave et al., 2018; Zellers et al., 2019; Liu et al., 2019c), and even simply as a giant text corpus for testing optimizers (Anil et al., 2019). Common Crawl is a publicly-available web archive that provides \u201cweb extracted text\u201d by removing markup and other non-text content from the scraped HTML files. This process produces around 20TB of scraped text data each month. Unfortunately, the majority of the resulting text is not natural language. Instead, it largely comprises gibberish or boiler-plate text like menus, error messages, or duplicate text. Furthermore, a good deal of the scraped text contains content that is unlikely to be helpful for any of the tasks we consider (offensive language, placeholder text, source code, etc.). To address these issues, we used the following heuristics for cleaning up Common Crawl\u2019s web extracted text:\n\u2022 We only retained lines that ended in a terminal punctuation mark (i.e. a period, exclamation mark, question mark, or end quotation mark). \u2022 We discarded any page with fewer than 3 sentences and only retained lines that contained at least 5 words. \u2022", "Interestingly, the performance of \u201cleave-one-out\u201d training was only slightly worse, suggesting that a model that was trained on a variety of tasks can still adapt to new tasks (i.e. multi-task pretraining might not result in a dramatic task interference). Finally, supervised multi-task pre-training performed significantly worse in every case except for the translation tasks. This\ncould suggest that the translation tasks benefit less from (English) pre-training, whereas unsupervised pre-training is an important factor in the other tasks. 3.6 Scaling. The \u201cbitter lesson\u201d of machine learning research argues that general methods that can leverage additional computation ultimately win out against methods that rely on human expertise (Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b; Keskar et al., 2019a). Recent results suggest that this may hold true for transfer learning in NLP (Liu et al., 2019c; Radford et al., 2019; Yang et al., 2019; Lan et al., 2019), i.e. it has repeatedly been shown that scaling up produces improved performance compared to more carefully-engineered methods. However, there are a variety of possible ways to scale, including using a bigger model, training the model for more steps, and ensembling. In this section, we compare these different approaches by addressing the following premise: \u201cYou were just given 4\u00d7 more compute. How should you use it?\u201d\nWe start with our baseline model, which has 220M parameters and is pre-trained and fine-tuned for 219 and 218 steps respectively. The encoder and decoder are both sized similarly to \u201cBERTBASE\u201d. To experiment with increased model size, we follow the guidelines of \u201cBERTLARGE\u201d Devlin et al. (2018) and use dff = 4096, dmodel = 1024, dkv = 64 and 16-head attention mechanisms."]}
{"pkey": "t5_3", "question": "What are the main contributions of the paper?", "answer": "Introducing a unified framework that converts all text-based language problems into a text-to-text format. (Abstract, Page 1) Specifically, the paper authors measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation. (Section 2.3, Page 8) However, unlike objectives and benchmarks, new pre-training datasets are usually not treated as significant contributions on their own and are often not released alongside pre-trained models and code. The paper authors release all of the C4 dataset variants the paper authors consider as part of TensorFlow Datasets. (Section 3.4, Page 25)", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. Original target: 0. Processed target: entailment\nD.6 QQP. Original input:.\nQuestion 1: What attributes would have made you highly desirable in ancient Rome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nProcessed input: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nOriginal target: 0.\nProcessed target: not_duplicate\nD.7 SST2. Original input:. Sentence: it confirms fincher \u2019s status as a film maker who artfully bends technical know-how to the service of psychological insight . Processed input: sst2 sentence: it confirms fincher \u2019s status as a film maker who artfully bends technical know-how to the service of psychological insight . Original target: 1.\nProcessed target: positive\nD.8 STSB. Original input:. Sentence 1: Representatives for Puretunes could not immediately be reached for comment Wednesday. Sentence 2: Puretunes representatives could not be located Thursday to comment on the suit. Processed input: stsb sentence1: Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2: Puretunes representatives could not be located Thursday to comment on the suit. Original target: 3.25\nProcessed target: 3.2\nD.9 CB. Original input:. Hypothesis: Valence was helping Premise: Valence the void-brain, Valence the virtuous valet. Why couldn\u2019t\nthe figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet. Why couldn\u2019t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Original target: 1.\nProcessed target: contradiction\nD.10 COPA. Original input:. Question: effect Premise: Political violence broke out in the nation.", "louis van gaal\u2019s side currently sit two points clear of liverpool in fourth . D.15 SQuAD. Original input:. Question: What does increased oxygen concentrations in the patient\u2019s lungs displace? Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Processed input: question: What does increased oxygen concentrations in the patient\u2019s lungs displace? context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Original target: carbon monoxide\nProcessed target: carbon monoxide\nD.16 WMT English to German.", "We are excited to see continued work using transfer learning towards the goal of general language understanding. Acknowledgments. We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito, Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on this manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for his guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff Klingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil Houlsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield, Yi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and Pierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our download pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on SentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and many other members of the Google Brain team for their discussion and insight. Appendix A. Contributions. Colin designed the scope of this project and wrote this paper, ran all the experiments in Sections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed many of the ideas, including the text-to-text framework, unsupervised objectives, and data set mixing strategies; implemented our base Transformer model and its architectural variants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects for this project, created the C4 data set, implemented our data set pipeline, and added various benchmark data sets. Katherine coordinated experiments, wrote and updated documentation, ran experiments to help design our baseline, and contributed to many parts of our codebase. Sharan contributed some of the required data sets and preprocessors, and ran assorted preliminary experiments, in addition to co-leading the open-sourcing of our codebase."]}
{"pkey": "t5_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "It has become increasingly common to pre-train the entire model on a data-rich task. Ideally, this pre-training causes the model to develop general-purpose abilities and knowledge that can then be transferred to downstream tasks (Introduction, Page 2). The paper authors instead allow for separately fine-tuning the model on each individual task (Section 2.4 , Page 8).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["This is unsurprising but also unsatisfying if our goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary domains. Liu et al. (2019c) also observed that pre-training on a more diverse data set yielded improvements on downstream tasks. This observation also motivates the parallel line of research on domain adaptation for natural language processing; for surveys of this field see e.g. Ruder (2019); Li (2012). A drawback to only pre-training on a single domain is that the resulting data sets are often substantially smaller. Similarly, while the WebText-like variant performed as well or better than the C4 data set in our baseline setting, the Reddit-based filtering produced a data set that was about 40\u00d7 smaller than C4 despite being based on 12\u00d7 more data from Common Crawl. Note, however, that in our baseline setup we only pre-train on 235 \u2248 34B tokens, which is only about 8 times larger than the smallest pre-training data set we consider. We investigate at what point using a smaller pre-training data sets poses an issue in the following section. 3.4.2 Pre-training Data set Size. The pipeline we use to create C4 was designed to be able to create extremely large pretraining data sets. The access to so much data allows us to pre-train our models without repeating examples. It is not clear whether repeating examples during pre-training would be helpful or harmful to downstream performance because our pre-training objective is itself stochastic and can help prevent the model from seeing the same exact data multiple times. To test the effect of limited unlabeled data set sizes, we pre-trained our baseline model on artificially truncated versions of C4. Recall that we pre-train our baseline model on 235 \u2248 34B tokens (a small fraction of the total size of C4). We consider training on truncated variants of C4 consisting of 229, 227, 225 and 223 tokens.", "We also showed that sharing the parameters in the encoder and decoder did not result in a substantial performance drop while halving the total parameter count. Unsupervised objectives Overall, we found that most \u201cdenoising\u201d objectives, which train the model to reconstruct randomly corrupted text, performed similarly in the text-totext setup. As a result, we suggest using objectives that produce short target sequences so that unsupervised pre-training is more computationally efficient. Data sets We introduced the \u201cColossal Clean Crawled Corpus\u201d (C4), which comprises heuristically-cleaned text from the Common Crawl web dump. When comparing C4 to data sets that use additional filtering, we found that training on in-domain unlabeled data could boost performance in a few downstream tasks. However, constraining to a single domain typically results in a smaller data set. We separately showed that performance can degrade when an unlabeled data set is small enough that it is repeated many times over the course of pre-training. This motivates the use of a large and diverse data set like C4 for generic language understanding tasks. Training strategies We found that the basic approach of updating all of a pre-trained model\u2019s parameters during fine-tuning outperformed methods that are designed to update fewer parameters, although updating all parameters is most expensive. We also experimented with various approaches for training the model on multiple tasks at once, which in our text-to-text setting simply corresponds to mixing examples from different data sets when constructing batches. The primary concern in multi-task learning is setting the proportion of each task to train on. We ultimately did not find a strategy for setting mixing proportions that matched the performance of the basic approach of unsupervised pre-training followed by supervised fine-tuning.", "To mitigate this, BERT (Devlin et al., 2018) combined data from Wikipedia with the Toronto Books Corpus (TBC) (Zhu et al., 2015). TBC contains text extracted from eBooks, which represents a different domain of natural language. BERT\u2019s popularity has led to the Wikipedia + TBC combination being used in many subsequent works. The results achieved after pre-training on each of these data sets is shown in Table 8. A first obvious takeaway is that removing the heuristic filtering from C4 uniformly degrades performance and makes the unfiltered variant perform the worst in every task. Beyond this, we found that in some cases a pre-training data set with a more constrained domain outperformed the diverse C4 data set. For example, using the Wikipedia + TBC corpus\n12. https://github.com/jcpeterson/openwebtext 13. https://www.tensorflow.org/datasets/catalog/wikipedia\nproduced a SuperGLUE score of 73.24, beating our baseline\u2019s score (using C4) of 71.36. This is almost entirely attributable to a boost in performance from 25.78 (baseline, C4) to 50.93 (Wikipedia + TBC) on the Exact Match score for MultiRC (see Table 16). MultiRC is a reading comprehension data set whose largest source of data comes from fiction books, which is exactly the domain covered by TBC. Similarly, using the RealNews-like data set for pre-training conferred an increase from 68.16 to 73.72 on the Exact Match score for ReCoRD, a data set that measures reading comprehension on news articles. As a final example, using data from Wikipedia produced significant (but less dramatic) gains on SQuAD, which is a question-answering data set with passages sourced from Wikipedia. Similar observations have been made in prior work, e.g. Beltagy et al. (2019) found that pre-training BERT on text from research papers improved its performance on scientific tasks. The main lesson behind these findings is that pre-training on in-domain unlabeled data can improve performance on downstream tasks."]}
{"pkey": "t5_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "Specifically, the paper authors measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation, Natural language inference (MNLI (Williams et al., 2017) (Section 2.3, Page 7).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["For example, our large batch size of 211 length-512 sequences would result in the entire data set appearing multiple times in each batch for many of the low-resource GLUE and SuperGLUE tasks. We therefore use a smaller batch size of 8 length-512 sequences during fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints every 1,000 steps rather than every 5,000 steps to ensure we have access to the model\u2019s parameters before it overfits. Beam search All of our previous results were reported using greedy decoding. For tasks with long output sequences, we found improved performance from using beam search (Sutskever et al., 2014). Specifically, we use a beam width of 4 and a length penalty of \u03b1 = 0.6 (Wu et al., 2016) for the WMT translation and CNN/DM summarization tasks. Test set Since this is our final set of experiments, we report results on the test set rather than the validation set. For CNN/Daily Mail, we use the standard test set distributed with the data set. For the WMT tasks, this corresponds to using newstest2014 for English-German, newstest2015 for English-French, and newstest2016 for EnglishRomanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to compute official test set scores.15,16 For SQuAD, evaluating on the test set requires running inference on a benchmark server. Unfortunately, the computational resources on this server are insufficient for obtaining predictions from our largest models. As a result, we instead continue to report performance on the SQuAD validation set. Fortunately, the model with the highest performance on the SQuAD test set also reported results on the validation set, so we can still compare to what is ostensibly the state-of-the-art. Apart from those changes mentioned above, we use the same training procedure and hyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate schedule for pre-training, constant learning rate for fine-tuning, dropout regularization, vocabulary, etc.).", "The rapid rate of progress and diversity of techniques in this burgeoning field can make it difficult to compare different algorithms, tease apart the effects of new contributions, and understand the space of existing methods for transfer learning. Motivated by a need for more rigorous understanding, we leverage a unified approach to transfer learning that allows us to systematically study different approaches and push the current limits of the field. The basic idea underlying our work is to treat every text processing problem as a \u201ctext-to-text\u201d problem, i.e. taking text as input and producing new text as output. This approach is inspired by previous unifying frameworks for NLP tasks, including casting all text problems as question answering (McCann et al., 2018), language modeling (Radford et al., 2019), or span extraction Keskar et al. (2019b) tasks. Crucially, the text-to-text framework allows us to directly apply the same model, objective, training procedure, and decoding process to every task we consider. We leverage this flexibility by evaluating performance on a wide variety of English-based NLP problems, including question answering, document\n2. http://commoncrawl.org\nsummarization, and sentiment classification, to name a few. With this unified approach, we can compare the effectiveness of different transfer learning objectives, unlabeled data sets, and other factors, while exploring the limits of transfer learning for NLP by scaling up models and data sets beyond what has previously been considered. We emphasize that our goal is not to propose new methods but instead to provide a comprehensive perspective on where the field stands. As such, our work primarily comprises a survey, exploration, and empirical comparison of existing techniques. We also explore the limits of current approaches by scaling up the insights from our systematic study (training models up to 11 billion parameters) to obtain state-of-the-art results in many of the tasks we consider.", "sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. Original target: 0. Processed target: entailment\nD.6 QQP. Original input:.\nQuestion 1: What attributes would have made you highly desirable in ancient Rome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nProcessed input: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nOriginal target: 0.\nProcessed target: not_duplicate\nD.7 SST2. Original input:. Sentence: it confirms fincher \u2019s status as a film maker who artfully bends technical know-how to the service of psychological insight . Processed input: sst2 sentence: it confirms fincher \u2019s status as a film maker who artfully bends technical know-how to the service of psychological insight . Original target: 1.\nProcessed target: positive\nD.8 STSB. Original input:. Sentence 1: Representatives for Puretunes could not immediately be reached for comment Wednesday. Sentence 2: Puretunes representatives could not be located Thursday to comment on the suit. Processed input: stsb sentence1: Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2: Puretunes representatives could not be located Thursday to comment on the suit. Original target: 3.25\nProcessed target: 3.2\nD.9 CB. Original input:. Hypothesis: Valence was helping Premise: Valence the void-brain, Valence the virtuous valet. Why couldn\u2019t\nthe figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet. Why couldn\u2019t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Original target: 1.\nProcessed target: contradiction\nD.10 COPA. Original input:. Question: effect Premise: Political violence broke out in the nation."]}
{"pkey": "t5_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "The paper authors were disappointed to find that English-only pre-training did not achieve state-of-the-art results on the translation tasks the paper authors studied. The paper authors also are interested in avoiding the logistical difficulty of needing to specify which languages a vocabulary can encode ahead of time (Section 4.2, Page 43).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["louis van gaal\u2019s side currently sit two points clear of liverpool in fourth . D.15 SQuAD. Original input:. Question: What does increased oxygen concentrations in the patient\u2019s lungs displace? Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Processed input: question: What does increased oxygen concentrations in the patient\u2019s lungs displace? context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Original target: carbon monoxide\nProcessed target: carbon monoxide\nD.16 WMT English to German.", "Note that we only pre-train on English data, so in order to learn to translate a given model will need to learn to generate text in a new language. 2.4 Input and Output Format. In order to train a single model on the diverse set of tasks described above, we cast all of the tasks we consider into a \u201ctext-to-text\u201d format\u2014that is, a task where the model is fed some text for context or conditioning and is then asked to produce some output text. This framework provides a consistent training objective both for pre-training and fine-tuning. Specifically, the model is trained with a maximum likelihood objective (using \u201cteacher forcing\u201d (Williams and Zipser, 1989)) regardless of the task. To specify which task the model should perform, we add a task-specific (text) prefix to the original input sequence before feeding it to the model. As an example, to ask the model to translate the sentence \u201cThat is good.\u201d from English to German, the model would be fed the sequence \u201ctranslate English to German: That is good.\u201d and would be trained to output \u201cDas ist gut.\u201d For text classification tasks, the model simply predicts a single word corresponding to the target label. For example, on the MNLI benchmark (Williams et al., 2017) the goal is to predict whether a premise implies (\u201centailment\u201d), contradicts (\u201ccontradiction\u201d), or neither (\u201cneutral\u201d) a hypothesis. With our preprocessing, the input sequence becomes \u201cmnli premise: I hate pigeons. hypothesis: My feelings towards pigeons are filled with animosity.\u201d with the corresponding target word \u201centailment\u201d. Note that an issue arises if our model outputs text on a text classification task that does not correspond to any of the possible labels (for example if the model outputs \u201chamburger\u201d when the only possible labels for a task were \u201centailment\u201d, \u201cneutral\u201d, or \u201ccontradiction\u201d). In this case, we always count the model\u2019s output as wrong, though we never observed this behavior in any of our trained models.", "A fundamental and frequently cited drawback of using a language model in the textto-text setting is that causal masking forces the model\u2019s representation of the ith entry of the input sequence to only depend on the entries up until i. To see why this is potentially disadvantageous, consider the text-to-text framework where the model is provided with a prefix/context before being asked to make predictions (e.g., the prefix is an English sentence and the model is asked to predict the German translation). With fully causal masking, the model\u2019s representation of a prefix state can only depend on prior entries of the prefix. So, when predicting an entry of the output, the model will attend to a representation of the prefix that is unnecessarily limited. Similar arguments have been made against using a unidirectional recurrent neural network encoder in sequence-to-sequence models (Bahdanau et al., 2015). This issue can be avoided in a Transformer-based language model simply by changing the masking pattern. Instead of using a causal mask, we use fully-visible masking during the prefix portion of the sequence. This masking pattern and a schematic of the resulting \u201cprefix LM\u201d (the third model structure we consider) are illustrated in the rightmost panels of Figures 3 and 4, respectively. In the English to German translation example mentioned above, fully-visible masking would be applied to the prefix \u201ctranslate English to German: That is good. target:\u201d and causal masking would be used during training for predicting the target \u201cDas ist gut.\u201d Using a prefix LM in the text-to-text framework was originally proposed by Liu et al. (2018). More recently, Dong et al. (2019) showed that this architecture is effective on a wide variety of text-to-text tasks. This architecture is similar to an encoder-decoder model with parameters shared across the encoder and decoder and with the encoder-decoder attention replaced with full attention across the input and target sequence."]}
{"pkey": "t5_7", "question": "List the limitations of the model discussed in the paper.", "answer": "We introduce the \"Colossal Clean Crawled Corpus\" (C4), a dataset consisting of hundreds of gigabytes of clean English text scraped from the web (Introduction, Page 3).\nA bigger model may be more prone to overfitting to a smaller pre-training dataset (Section 3.4.2, Page 28).\nUsing a smaller dataset size results in smaller training loss values, which may suggest some memorization of the unlabeled dataset (Figure 6, Page 28).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["In order to perform experiments at this scale, we introduce the \u201cColossal Clean Crawled Corpus\u201d (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.1\nThe remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.\n2. Setup. Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our \u201cColossal Clean Crawled Corpus\u201d (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the \u201cText-to-Text Transfer Transformer\u201d (T5). 2.1 Model. Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the \u201cTransformer\u201d architecture (Vaswani et al., 2017).", "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.1 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks.", "We are excited to see continued work using transfer learning towards the goal of general language understanding. Acknowledgments. We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito, Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on this manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for his guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff Klingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil Houlsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield, Yi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and Pierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our download pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on SentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and many other members of the Google Brain team for their discussion and insight. Appendix A. Contributions. Colin designed the scope of this project and wrote this paper, ran all the experiments in Sections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed many of the ideas, including the text-to-text framework, unsupervised objectives, and data set mixing strategies; implemented our base Transformer model and its architectural variants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects for this project, created the C4 data set, implemented our data set pipeline, and added various benchmark data sets. Katherine coordinated experiments, wrote and updated documentation, ran experiments to help design our baseline, and contributed to many parts of our codebase. Sharan contributed some of the required data sets and preprocessors, and ran assorted preliminary experiments, in addition to co-leading the open-sourcing of our codebase."]}
{"pkey": "t5_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors pre-train each model for 2^19 = 524,288 steps on C4 before fine-tuning\" (Section 3.1.2,Page 11). The authors use the train and validation sets from WMT 2016 (Bojar et al., 2016) (Section 2.3, Page 8). The paper authors use the data sets as distributed by the GLUE and SuperGLUE benchmarks (Section 2.3,Page 7).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["Our results suggest that scale and English-language pre-training may be insufficient to match the performance of these more sophisticated methods. On a more specific note, the best results on English to German newstest2014 set use the much larger training set from WMT 2018 (Edunov et al., 2018), making direct comparison to our results difficult. Finally, on CNN/Daily Mail we attain state-of-the-art performance, though only by a significant amount on the ROUGE-2-F score. It has been shown that improvements to the ROUGE score do not necessarily correspond to more coherent summaries (Paulus et al., 2017). Furthermore, while CNN/Daily Mail is posed as an abstractive summarization benchmark, purely extractive approaches have been shown to work well (Liu, 2019). It has also been argued that generative models trained with maximum likelihood are prone to producing repetitive summaries (See et al., 2017). Despite these potential issues, we find that our models do generate coherent and largely correct summaries. We provide some non-cherry-picked validation set examples in Appendix C.\nTo achieve its strong results, T5 combines insights from our experimental study with unprecedented scale. Note that in Section 3.6 we found that scaling up the pre-training amount or size of our baseline model produced substantial gains. Given this, we were interested to measure how much the \u201cnon-scaling\u201d changes we introduced into T5 contributed to its strong performance. We therefore carried out a final experiment where we compared the following three configurations: First, the standard baseline model, which was pre-trained on 235 \u2248 34B tokens; second, the baseline trained instead for about 1 trillion tokens (i.e. the same amount of pre-training used for T5), which we refer to as \u201cbaseline-1T\u201d; and third, T5-Base. Note that the differences between baseline-1T and T5-Base comprise the\ntokens (the same number used for the T5 model variants) instead of 235 \u2248 34B tokens (as was used for the baseline).", "We are excited to see continued work using transfer learning towards the goal of general language understanding. Acknowledgments. We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito, Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on this manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for his guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff Klingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil Houlsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield, Yi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and Pierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our download pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on SentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and many other members of the Google Brain team for their discussion and insight. Appendix A. Contributions. Colin designed the scope of this project and wrote this paper, ran all the experiments in Sections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed many of the ideas, including the text-to-text framework, unsupervised objectives, and data set mixing strategies; implemented our base Transformer model and its architectural variants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects for this project, created the C4 data set, implemented our data set pipeline, and added various benchmark data sets. Katherine coordinated experiments, wrote and updated documentation, ran experiments to help design our baseline, and contributed to many parts of our codebase. Sharan contributed some of the required data sets and preprocessors, and ran assorted preliminary experiments, in addition to co-leading the open-sourcing of our codebase.", "For the candidate noun (which is compared to our model\u2019s prediction to obtain a True or False label), we remove the matching substring from the hypothesis and optionally make it non-possessive (resulting in \u201cthe demonstrators\u201d). Appendix C. Example Predictions on CNN/Daily Mail. To show that our model is generating fluent summaries, we include a few example decodes from our best model (T5-11B) on the validation set along with the ground-truth summaries. These examples selected at random and were not cherry-picked. 1. Ground-truth: leopard gave up after spiky creature refused to back down in fight in kruger national park, south africa . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . the leopard and porcupine tumbled out of the bushes and began to fight by roadside - watched by ms moolman . Prediction: leopard tried to make lunch out of a plucky porcupine in kruger national park, south africa . but the predator was put firmly in its place after the spiky creature refused to back down during a fight . wildlife enthusiast lisl moolman, 41, caught the bizarre battle while out on the road and armed with her camera . 2. Ground-truth: researchers say homes are a hotbed of undiscovered species . study of 40 north carolina homes found 8,000 bacterial and archaeal taxa . 11 houses in california revealed a great variety of fungus among us . Prediction: researchers estimate that the indoor biome makes up about 0.5 percent of ice-free land, or about 247,000 square miles, almost the size of texas . a study of 40 houses in north carolina used genomic technologies to document more than 8,000 bacterial and archaeal taxa . another study of 50 houses in the same state described more than 750 types of arthropods, including insects and spiders . an examination of 11 houses in california found a great variety of fungus among us . 3."]}
{"pkey": "t5_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens (Sennrich et al., 2015; Kudo, 2018). For all experiments, the paper authors use a vocabulary of 32,000 wordpieces.", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["We also experimented with using a triangular learning rate (Howard and Ruder, 2018), which produced slightly better results but requires knowing the total number of training steps ahead of time. Since we will be varying the number of training steps in some of our experiments, we opt for the more generic inverse square root schedule. Our models are fine-tuned for 218 = 262,144 steps on all tasks. This value was chosen as a trade-off between the high-resource tasks (i.e. those with large data sets), which benefit from additional fine-tuning, and low-resource tasks (smaller data sets), which overfit quickly. During fine-tuning, we continue using batches with 128 length-512 sequences (i.e. 216 tokens per batch). We use a constant learning rate of 0.001 when fine-tuning. We save a checkpoint every 5,000 steps and report results on the model checkpoint corresponding to the highest validation performance. For models fine-tuned on multiple tasks, we choose the best checkpoint for each task independently. For all of the experiments except those in Section 3.7, we report results in the validation set to avoid performing model selection on the test set. 3.1.3 Vocabulary. We use SentencePiece (Kudo and Richardson, 2018) to encode text as WordPiece tokens (Sennrich et al., 2015; Kudo, 2018). For all experiments, we use a vocabulary of 32,000 wordpieces. Since we ultimately fine-tune our model on English to German, French, and Romanian translation, we also require that our vocabulary covers these non-English languages. To address this, we classified pages from the Common Crawl scrape used in C4 as German, French, and Romanian. Then, we trained our SentencePiece model on a mixture of 10 parts of English C4 data with 1 part each of data classified as German, French or Romanian. This vocabulary was shared across both the input and output of our model. Note that our vocabulary makes it so that our model can only process a predetermined, fixed set of languages. 3.1.4 Unsupervised Objective.", "Leveraging unlabeled data to pre-train our model necessitates an objective that does not require labels but (loosely speaking) teaches the model generalizable knowledge that will be\n10. https://www.pydoc.io/pypi/tensor2tensor-1.5.7/autoapi/data_generators/generator_utils/ index.html#data_generators.generator_utils.pack_examples\nuseful in downstream tasks. Preliminary work that applied the transfer learning paradigm of pre-training and fine-tuning all of the model\u2019s parameters to NLP problems used a causal language modeling objective for pre-training (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Howard and Ruder, 2018). However, it has recently been shown that \u201cdenoising\u201d objectives (Devlin et al., 2018; Taylor, 1953) (also called \u201cmasked language modeling\u201d) produce better performance and as a result they have quickly become standard. In a denoising objective, the model is trained to predict missing or otherwise corrupted tokens in the input. Inspired by BERT\u2019s \u201cmasked language modeling\u201d objective and the \u201cword dropout\u201d regularization technique (Bowman et al., 2015), we design an objective that randomly samples and then drops out 15% of tokens in the input sequence. All consecutive spans of dropped-out tokens are replaced by a single sentinel token. Each sentinel token is assigned a token ID that is unique to the sequence. The sentinel IDs are special tokens which are added to our vocabulary and do not correspond to any wordpiece. The target then corresponds to all of the dropped-out spans of tokens, delimited by the same sentinel tokens used in the input sequence plus a final sentinel token to mark the end of the target sequence. Our choices to mask consecutive spans of tokens and only predict dropped-out tokens were made to reduce the computational cost of pre-training. We perform thorough investigation into pre-training objectives in Section 3.3. An example of the transformation resulting from applying this objective is shown in Figure 2.", "For example, our large batch size of 211 length-512 sequences would result in the entire data set appearing multiple times in each batch for many of the low-resource GLUE and SuperGLUE tasks. We therefore use a smaller batch size of 8 length-512 sequences during fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints every 1,000 steps rather than every 5,000 steps to ensure we have access to the model\u2019s parameters before it overfits. Beam search All of our previous results were reported using greedy decoding. For tasks with long output sequences, we found improved performance from using beam search (Sutskever et al., 2014). Specifically, we use a beam width of 4 and a length penalty of \u03b1 = 0.6 (Wu et al., 2016) for the WMT translation and CNN/DM summarization tasks. Test set Since this is our final set of experiments, we report results on the test set rather than the validation set. For CNN/Daily Mail, we use the standard test set distributed with the data set. For the WMT tasks, this corresponds to using newstest2014 for English-German, newstest2015 for English-French, and newstest2016 for EnglishRomanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to compute official test set scores.15,16 For SQuAD, evaluating on the test set requires running inference on a benchmark server. Unfortunately, the computational resources on this server are insufficient for obtaining predictions from our largest models. As a result, we instead continue to report performance on the SQuAD validation set. Fortunately, the model with the highest performance on the SQuAD test set also reported results on the validation set, so we can still compare to what is ostensibly the state-of-the-art. Apart from those changes mentioned above, we use the same training procedure and hyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate schedule for pre-training, constant learning rate for fine-tuning, dropout regularization, vocabulary, etc.)."]}
{"pkey": "t5_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "In order to perform experiments at this scale, the paper authors introduce the \u201cColossal Clean Crawled Corpus\u201d (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web (Introduction, Page 3). To assemble our base data set, the paper authors downloaded the web extracted text from April 2019 and applied the aforementioned filtering. This produces a collection of text that is not only orders of magnitude larger than most data sets used for pre-training (about 750 GB) but also comprises reasonably clean and natural English text. The paper authors dub this data set the Colossal Clean Crawled Corpus\u201d (or C4 for short) (Section 2.2, Page 7).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["Ground-truth: executions have been temporarily halted in the state after the drug used for lethal injections appeared faulty on monday night . kelly gissendaner had her death delayed for a second time when it was found that the lethal pentobarbital dose appeared cloudy . state officials dithered over the decision , changing their mind three times before deciding not to take the risk that it wouldn\u2019t work . it is \u2019essential that executions are carried out in a constitutional manner,\u2019 said georgia attorney general sam olens . Prediction: the execution of kelly gissendaner, the only woman on georgia\u2019s death row, was delayed for a second time on monday . the execution team found that the lethal pentobarbital dose appeared cloudy . the cloudy drug bolstered death penalty opponents, who have been vocal in their opposition after three botched executions in other parts of the country . 4. Ground-truth: dani alves was not selected for the brazil squad to face france and chile . barcelona defender was filmed serving up hot dogs to people on saturday . this week alves released a charity single with former team-mat jose pinto . alves looks set to leave barcelona on a free transfer this summer . reports in spanish press claim he will sign for paris saint-germain . Prediction: dani alves was not selected for brazil\u2019s friendlies against france and chile . the barcelona right back has released a charity single with jose pinto . alves was filmed serving up snacks from behind the counter\nof a hot dog bar on saturday . alves is yet to be offered a new deal at the nou camp . click here for all the latest barcelona news . Appendix D. Preprocessed Examples. In this section, we provide examples of our preprocessing for each of the data sets we consider. D.1 CoLA. Original input:. Sentence: John made Bill master of himself. Processed input: cola sentence: John made Bill master of himself. Original target: 1.\nProcessed target: acceptable\nD.2 RTE. Original input:.", "We are excited to see continued work using transfer learning towards the goal of general language understanding. Acknowledgments. We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito, Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on this manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for his guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff Klingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil Houlsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield, Yi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and Pierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our download pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on SentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and many other members of the Google Brain team for their discussion and insight. Appendix A. Contributions. Colin designed the scope of this project and wrote this paper, ran all the experiments in Sections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed many of the ideas, including the text-to-text framework, unsupervised objectives, and data set mixing strategies; implemented our base Transformer model and its architectural variants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects for this project, created the C4 data set, implemented our data set pipeline, and added various benchmark data sets. Katherine coordinated experiments, wrote and updated documentation, ran experiments to help design our baseline, and contributed to many parts of our codebase. Sharan contributed some of the required data sets and preprocessors, and ran assorted preliminary experiments, in addition to co-leading the open-sourcing of our codebase.", "Note that for ease of comparison, we retain the heuristic filtering methods used in C4; the only difference is that we have ostensibly omitted any non-news content. WebText-like Similarly, the WebText data set (Radford et al., 2019) only uses content from webpages that were submitted to the content aggregation website Reddit and received a \u201cscore\u201d of at least 3. The score for a webpage submitted to Reddit is computed based on the proportion of users who endorse (upvote) or oppose (downvote) the webpage. The idea behind using the Reddit score as a quality signal is that users of the site would only upvote high-quality text content. To generate a comparable data set, we first tried removing all content from C4 that did not originate from a URL that appeared in the list prepared by the OpenWebText effort.12 However, this resulted in comparatively little content\u2014only about 2 GB\u2014because most pages never appear on Reddit. Recall that C4 was created based on a single month of Common Crawl data. To avoid using a prohibitively small data set, we therefore downloaded 12 months of data from Common Crawl from August 2018 to July 2019, applied our heuristic filtering for C4, then applied the Reddit filter. This produced a 17 GB WebText-like data set, which is of comparable size to the original 40GB WebText data set (Radford et al., 2019). Wikipedia The website Wikipedia consists of millions of encyclopedia articles written collaboratively. The content on the site is subject to strict quality guidelines and therefore has been used as a reliable source of clean and natural text. We use the English Wikipedia text data from TensorFlow Datasets,13 which omits any markup or reference sections from the articles. Wikipedia + Toronto Books Corpus A drawback of using pre-training data from Wikipedia is that it represents only one possible domain of natural text (encyclopedia articles)."]}
{"pkey": "t5_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "Base: This is our baseline model, whose hyperparameters are described in Section 3.1.1. It has roughly 220 million parameters.\n\u2022 Small: The paper authors consider a smaller model, which scales the baseline down by using d_model = 512, d_ff = 2,048, 8-headed attention, and only 6 layers each in the encoder and decoder. This variant has about 60 million parameters.\n\u2022 Large: Since our baseline uses a BERTBASE-sized encoder and decoder, we also consider a variant where the encoder and decoder are both similar in size and structure to BERTLARGE. Specifically, this variant uses d_model = 1,024, d_ff = 4,096, d_kv = 64, 16-headed attention, and 24 layers each in the encoder and decoder, resulting in around 770 million parameters.\n\u2022 3B and 11B: To further explore what kind of performance is possible when using larger models, the paper authors consider two additional variants. In both cases, the paper authors use d_model = 1024, a 24 layer encoder and decoder, and d_kv = 128. For the \u201c3B\u201d variant, the paper authors use d_ff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for \u201c11B\u201d the paper authors use d_ff = 65,536 with 128-headed attention producing a model with about 11 billion parameters. The paper authors chose to scale up d_ff specifically because modern accelerators (such as the TPUs the paper authors train our models on) are most efficient for large dense matrix multiplications like those in the Transformer\u2019s feed-forward networks.", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["After outlining our baseline experimental setup in the following subsection, we undertake an empirical comparison of model architectures (Section 3.2), unsupervised objectives (Section 3.3), pre-training data sets (Section 3.4), transfer approaches (Section 3.5), and scaling (Section 3.6). At the culmination of this section, we combine insights from our study with scale to obtain state-of-the-art results in many tasks we consider (Section 3.7). 3.1 Baseline. Our goal for our baseline is to reflect typical, modern practice. We pre-train a standard Transformer (described in Section 2.1) using a simple denoising objective and then separately fine-tune on each of our downstream tasks. We describe the details of this experimental setup in the following subsections. 3.1.1 Model. For our model, we use a standard encoder-decoder Transformer as proposed by Vaswani et al. (2017). While many modern approaches to transfer learning for NLP use a Transformer architecture consisting of only a single \u201cstack\u201d (e.g. for language modeling (Radford et al., 2018; Dong et al., 2019) or classification and span prediction (Devlin et al., 2018; Yang et al., 2019)), we found that using a standard encoder-decoder structure achieved good results on both generative and classification tasks. We explore the performance of different model architectures in Section 3.2. Our baseline model is designed so that the encoder and decoder are each similar in size and configuration to a \u201cBERTBASE\u201d (Devlin et al., 2018) stack. Specifically, both the encoder and decoder consist of 12 blocks (each block comprising self-attention, optional encoder-decoder attention, and a feed-forward network). The feed-forward networks in each block consist of a dense layer with an output dimensionality of dff = 3072 followed by a ReLU nonlinearity and another dense layer. The \u201ckey\u201d and \u201cvalue\u201d matrices of all attention mechanisms have an inner dimensionality of dkv = 64 and all attention mechanisms have 12 heads.", "The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more detailed introduction. The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural variants in Section 3.2. Overall, our encoder-decoder Transformer implementation closely follows its originallyproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of \u201cblocks\u201d, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent.", "However, the same L + L encoder-decoder model will have approximately the same computational cost as a language model with only L layers. This is a consequence of the fact that the L layers in the language model must be applied to both the input and output sequence, while the encoder is only applied to the input sequence and the decoder is only applied to the output sequence. Note that these equivalences are approximate\u2014there are some extra parameters in the decoder due to the encoder-decoder attention and there are also some computational costs in the attention layers that are quadratic in the sequence lengths. In practice, however, we observed nearly identical step\ntimes for L-layer language models versus L + L-layer encoder-decoder models, suggesting a roughly equivalent computational cost. Further, for the model sizes we consider, the number of parameters in the encoder-decoder attention layers is about 10% of the total parameter count, so we make the simplifying assumption that an L + L-layer encoder-decoder model has the same number of parameters as an 2L-layer language model. To provide a reasonable means of comparison, we consider multiple configurations for our encoder-decoder model. We will refer to the number of layers and parameters in a BERTBASE-sized layer stack as L and P , respectively. We will use M to refer to the number of FLOPs required for an L + L-layer encoder-decoder model or L-layer decoder-only model to process a given input-target pair. In total, we will compare:\n\u2022 An encoder-decoder model with L layers in the encoder and L layers in the decoder. This model has 2P parameters and a computation cost of M FLOPs. \u2022 An equivalent model, but with parameters shared across the encoder and decoder, resulting in P parameters and an M -FLOP computational cost. \u2022 An encoder-decoder model with L/2 layers each in the encoder and decoder, giving P parameters and an M/2-FLOP cost. \u2022"]}
{"pkey": "t5_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "During pre-training, the paper authors use an \"inverse square root\" learning rate schedule, which is defined as:\n\\frac{1}{\\sqrt{\\max(n, k)}}\nwhere \\( n \\) is the current training iteration and \\( k \\) is the number of warm-up steps, set to 10,000 in all of our experiments. This sets a constant learning rate of 0.01 for the first 10,000 steps, then exponentially decays the learning rate until pre-training is over. Our models are fine-tuned for \\( 2^{18} = 262,144 \\) steps on all tasks. This value was chosen as a trade-off between the high-resource tasks (i.e., those with large datasets), which benefit from additional fine-tuning, and low-resource tasks (smaller datasets), which overfit quickly.\nDuring fine-tuning, the paper authors continue using batches with 128 length-512 sequences, equivalent to \\( 2^{16} \\) tokens per batch. The paper authors use a constant learning rate of 0.001 when fine-tuning. The paper authors save a checkpoint every 5,000 steps and report results on the model checkpoint corresponding to the highest validation performance. For models fine-tuned on multiple tasks, the paper authors choose the best checkpoint for each task independently.", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["All other sub-layers and embeddings have a dimensionality of dmodel = 768. In total, this results in a model with about 220 million parameters. This is roughly twice the number of parameters of BERTBASE since our baseline model contains two layer stacks instead of one. For regularization, we use a dropout probability of 0.1 everywhere dropout is applied in the model. 3.1.2 Training. As described in Section 2.4, all tasks are formulated as text-to-text tasks. This allows us to always train using standard maximum likelihood, i.e. using teacher forcing (Williams and Zipser, 1989) and a cross-entropy loss. For optimization, we use AdaFactor (Shazeer and Stern, 2018). At test time, we use greedy decoding (i.e. choosing the highest-probability logit at every timestep). We pre-train each model for 219 = 524,288 steps on C4 before fine-tuning. We use a maximum sequence length of 512 and a batch size of 128 sequences. Whenever possible,\nwe \u201cpack\u201d multiple sequences into each entry of the batch10 so that our batches contain roughly 216 = 65,536 tokens. In total, this batch size and number of steps corresponds to pre-training on 235 \u2248 34B tokens. This is considerably less than BERT (Devlin et al., 2018), which used roughly 137B tokens, or RoBERTa (Liu et al., 2019c), which used roughly 2.2T tokens. Using only 235 tokens results in a reasonable computational budget while still providing a sufficient amount of pre-training for acceptable performance. We consider the effect of pre-training for more steps in Sections 3.6 and 3.7. Note that 235 tokens only covers a fraction of the entire C4 data set, so we never repeat any data during pre-training. During pre-training, we use an \u201cinverse square root\u201d learning rate schedule: 1 /\u221a\nmax(n, k) where n is the current training iteration and k is the number of warm-up steps (set to 104 in all of our experiments). This sets a constant learning rate of 0.01 for the first 104 steps, then exponentially decays the learning rate until pre-training is over.", "For example, our large batch size of 211 length-512 sequences would result in the entire data set appearing multiple times in each batch for many of the low-resource GLUE and SuperGLUE tasks. We therefore use a smaller batch size of 8 length-512 sequences during fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints every 1,000 steps rather than every 5,000 steps to ensure we have access to the model\u2019s parameters before it overfits. Beam search All of our previous results were reported using greedy decoding. For tasks with long output sequences, we found improved performance from using beam search (Sutskever et al., 2014). Specifically, we use a beam width of 4 and a length penalty of \u03b1 = 0.6 (Wu et al., 2016) for the WMT translation and CNN/DM summarization tasks. Test set Since this is our final set of experiments, we report results on the test set rather than the validation set. For CNN/Daily Mail, we use the standard test set distributed with the data set. For the WMT tasks, this corresponds to using newstest2014 for English-German, newstest2015 for English-French, and newstest2016 for EnglishRomanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to compute official test set scores.15,16 For SQuAD, evaluating on the test set requires running inference on a benchmark server. Unfortunately, the computational resources on this server are insufficient for obtaining predictions from our largest models. As a result, we instead continue to report performance on the SQuAD validation set. Fortunately, the model with the highest performance on the SQuAD test set also reported results on the validation set, so we can still compare to what is ostensibly the state-of-the-art. Apart from those changes mentioned above, we use the same training procedure and hyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate schedule for pre-training, constant learning rate for fine-tuning, dropout regularization, vocabulary, etc.).", "We might also hope that mixing in many sources of supervision could help the pre-trained model obtain a more general set of \u201cskills\u201d (loosely speaking) before it is adapted to an individual task. To measure this directly, we consider a second variant where we pre-train the model on the same examples-proportional mixture (with K = 219) except that we omit one of the downstream tasks from this pre-training mixture. Then, we fine-tune the model on the task that was left out during pre-training. We repeat this for each of the downstream tasks we consider. We call this approach \u201cleave-one-out\u201d multi-task training. This simulates the real-world setting where a pre-trained model is fine-tuned on a task it had not seen during pre-training. Note that multi-task pre-training provides a diverse mixture of supervised tasks. Since other fields (e.g. computer vision (Oquab et al., 2014; Jia et al., 2014; Huh et al., 2016; Yosinski et al., 2014)) use a supervised data set for pre-training, we were interested to see whether omitting the unsupervised task from the multi-task pre-training mixture still produced good results. For our third variant we therefore pre-train on an examples-proportional mixture of all of the supervised tasks we consider with K = 219. In all of these variants, we follow our standard procedure of pre-training for 219 steps before fine-tuning for 218 steps. We compare the results of these approaches in Table 12. For comparison, we also include results for our baseline (pre-train then fine-tune) and for standard multi-task learning (without fine-tuning) on an examples-proportional mixture with K = 219. We find that fine-tuning after multi-task pre-training results in comparable performance to our baseline. This suggests that using fine-tuning after multi-task learning can help mitigate some of the trade-offs between different mixing rates described in Section 3.5.2."]}
{"pkey": "t5_13", "question": "Describe the computational resources used to train the model.", "answer": "The paper authors use a combination of model and data parallelism and train models on slices of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines (Section 2.1 , Page 5).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["However, these data set variants are sufficiently small that they would be repeated hundreds of times over the course of pre-training on 1 trillion tokens. Since we showed in Section 3.4.2 that this repetition could be harmful, we opted instead to continue using the C4 data set. Model sizes In Section 3.6 we also showed how scaling up the baseline model size improved performance. However, using smaller models can be helpful in settings where limited computational resources are available for fine-tuning or inference. Based on these factors, we train models with a wide range of sizes:\n\u2022 Base. This is our baseline model, whose hyperparameters are described in Section 3.1.1. It has roughly 220 million parameters. \u2022 Small. We consider a smaller model, which scales the baseline down by using dmodel = 512, dff = 2,048, 8-headed attention, and only 6 layers each in the encoder and decoder. This variant has about 60 million parameters. \u2022 Large. Since our baseline uses a BERTBASE-sized encoder and decoder, we also consider a variant where the encoder and decoder are both similar in size and structure to BERTLARGE. Specifically, this variant uses dmodel = 1,024, dff = 4,096, dkv = 64, 16-headed attention, and 24 layers each in the encoder and decoder, resulting in around 770 million parameters. \u2022 3B and 11B. To further explore what kind of performance is possible when using larger models, we consider two additional variants. In both cases, we use dmodel = 1024, a 24 layer encoder and decoder, and dkv = 128. For the \u201c3B\u201d variant, we use dff = 16,384 with 32-headed attention, which results in around 2.8 billion parameters; for \u201c11B\u201d we use dff = 65,536 with 128-headed attention producing a model with about 11 billion parameters. We chose to scale up dff specifically because modern accelerators (such as the TPUs we train our models on) are most efficient for large dense matrix multiplications like those in the Transformer\u2019s feed-forward networks.", "However, it will always be the case that there are applications and scenarios where using a smaller or less expensive model is helpful, for example when performing client-side inference or federated learning (Kone\u010dny\u0300 et al., 2015, 2016). Relatedly, one beneficial use of transfer learning is the possibility of attaining good performance on low-resource tasks. Low-resource tasks often occur (by definition) in settings where one lacks the assets to label more data. It follows that low-resource applications often also have limited access to computational resources which can incur additional costs. As a result, we advocate for research on methods that achieve stronger performance with cheaper models so that transfer learning can be applied where it will have the most impact. Some current work along these lines include distillation (Hinton et al., 2015; Sanh et al., 2019; Jiao et al., 2019), parameter sharing (Lan et al., 2019), and conditional computation (Shazeer et al., 2017). More efficient knowledge extraction Recall that one of the goals of pre-training is (loosely speaking) to provide the model with general-purpose \u201cknowledge\u201d that improves its performance on downstream tasks. The method we use in this work, which is currently common practice, is to train the model to denoise corrupted spans of text. We suspect that this simplistic technique may not be a very efficient way to teach the model general-purpose knowledge. More concretely, it would be useful to be able to attain good fine-tuning performance without needing to train our models on 1 trillion tokens of text first. Some concurrent work along these lines improves efficiency by pre-training a model to distinguish between real and machine-generated text (Clark et al., 2020). Formalizing the similarity between tasks We observed that pre-training on unlabeled in-domain data can improve performance on downstream tasks (Section 3.4).", "For example, our large batch size of 211 length-512 sequences would result in the entire data set appearing multiple times in each batch for many of the low-resource GLUE and SuperGLUE tasks. We therefore use a smaller batch size of 8 length-512 sequences during fine-tuning for each GLUE and SuperGLUE task. We also save checkpoints every 1,000 steps rather than every 5,000 steps to ensure we have access to the model\u2019s parameters before it overfits. Beam search All of our previous results were reported using greedy decoding. For tasks with long output sequences, we found improved performance from using beam search (Sutskever et al., 2014). Specifically, we use a beam width of 4 and a length penalty of \u03b1 = 0.6 (Wu et al., 2016) for the WMT translation and CNN/DM summarization tasks. Test set Since this is our final set of experiments, we report results on the test set rather than the validation set. For CNN/Daily Mail, we use the standard test set distributed with the data set. For the WMT tasks, this corresponds to using newstest2014 for English-German, newstest2015 for English-French, and newstest2016 for EnglishRomanian. For GLUE and SuperGLUE, we used the benchmark evaluation servers to compute official test set scores.15,16 For SQuAD, evaluating on the test set requires running inference on a benchmark server. Unfortunately, the computational resources on this server are insufficient for obtaining predictions from our largest models. As a result, we instead continue to report performance on the SQuAD validation set. Fortunately, the model with the highest performance on the SQuAD test set also reported results on the validation set, so we can still compare to what is ostensibly the state-of-the-art. Apart from those changes mentioned above, we use the same training procedure and hyperparameters as our baseline (AdaFactor optimizer, inverse square root learning rate schedule for pre-training, constant learning rate for fine-tuning, dropout regularization, vocabulary, etc.)."]}
{"pkey": "t5_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "To facilitate future work on transfer learning for NLP, the paper authors release our data set, pre-trained models, and code as mentioned in Abstract, Page 1.", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more detailed introduction. The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural variants in Section 3.2. Overall, our encoder-decoder Transformer implementation closely follows its originallyproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of \u201cblocks\u201d, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent.", "We are excited to see continued work using transfer learning towards the goal of general language understanding. Acknowledgments. We thank Grady Simon, Noah Fiedel, Samuel R. Bowman, Augustus Odena, Daphne Ippolito, Noah Constant, Orhan Firat, Ankur Bapna, and Sebastian Ruder for their comments on this manuscript; Zak Stone and the TFRC team for their support; Austin Tarango for his guidance on data set creation; Melvin Johnson, Dima Lepikhin, Katrin Tomanek, Jeff Klingner, and Naveen Arivazhagan for insight into multi-task machine translation; Neil Houlsby for comments on adapter layers; Olga Wichowska, Ola Spyra, Michael Banfield, Yi Lin, and Frank Chen for assistance with infrastructure; Etienne Pot, Ryan Sepassi, and Pierre Ruyssen for collaboration on TensorFlow Datasets; Rohan Anil for help with our download pipeline for Common Crawl; Robby Neale and Taku Kudo for their work on SentencePiece; Jeffrey Li for pointing out missing details about the creation of C4; and many other members of the Google Brain team for their discussion and insight. Appendix A. Contributions. Colin designed the scope of this project and wrote this paper, ran all the experiments in Sections 3.1 to 3.6, and contributed a large portion of our codebase. Noam contributed many of the ideas, including the text-to-text framework, unsupervised objectives, and data set mixing strategies; implemented our base Transformer model and its architectural variants; and ran the experiments in Section 3.7. Adam oversaw all engineering aspects for this project, created the C4 data set, implemented our data set pipeline, and added various benchmark data sets. Katherine coordinated experiments, wrote and updated documentation, ran experiments to help design our baseline, and contributed to many parts of our codebase. Sharan contributed some of the required data sets and preprocessors, and ran assorted preliminary experiments, in addition to co-leading the open-sourcing of our codebase.", "Similarly, for SQuAD we find the performance of the \u201cexact match\u201d and \u201cF1\u201d scores to be highly correlated so we report the \u201cexact match\u201d score alone. We provide every score achieved on every task for all experiments in Table 16, Appendix E.\nOur results tables are all formatted so that each row corresponds to a particular experimental configuration with columns giving the scores for each benchmark. We will include the mean performance of the baseline configuration in most tables. Wherever a baseline configuration appears, we will mark it with a \u22c6 (as in the first row of Table 1). We also will boldface any score that is within two standard deviations of the maximum (best) in a given experiment. Our baseline results are shown in Table 1. Overall, our results are comparable to existing models of similar size. For example, BERTBASE achieved an exact match score of 80.8 on SQuAD and an accuracy of 84.4 on MNLI-matched, whereas we achieve 80.88 and 84.24, respectively (see Table 16). Note that we cannot directly compare our baseline to BERTBASE because ours is an encoder-decoder model and was pre-trained for roughly 1\u20444 as many steps. Unsurprisingly, we find that pre-training provides significant gains across almost all benchmarks. The only exception is WMT English to French, which is a large\nenough data set that gains from pre-training tend to be marginal. We include this task in our experiments to test the behavior of transfer learning in the high-resource regime. Since we perform early stopping by selecting the best-performing checkpoint, the large disparity between our baseline and \u201cno pre-training\u201d emphasize how much pre-training improves performance on tasks with limited data. While we do not explicitly measure improvements in data efficiency in this paper, we emphasize that this is one of the primary benefits of the transfer learning paradigm. As for inter-run variance, we find that for most tasks the standard deviation across runs is smaller than 1% of the task\u2019s baseline score."]}
{"pkey": "t5_15", "question": "What is the pretraining objective of the model? ", "answer": "Specifically, the model is trained with a maximum likelihood objective (using \u201cteacher forcing\u201d (Williams and Zipser, 1989)) regardless of the task (Section 2.4, Page 8).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["This is unsurprising but also unsatisfying if our goal is to pre-train a model that can rapidly adapt to language tasks from arbitrary domains. Liu et al. (2019c) also observed that pre-training on a more diverse data set yielded improvements on downstream tasks. This observation also motivates the parallel line of research on domain adaptation for natural language processing; for surveys of this field see e.g. Ruder (2019); Li (2012). A drawback to only pre-training on a single domain is that the resulting data sets are often substantially smaller. Similarly, while the WebText-like variant performed as well or better than the C4 data set in our baseline setting, the Reddit-based filtering produced a data set that was about 40\u00d7 smaller than C4 despite being based on 12\u00d7 more data from Common Crawl. Note, however, that in our baseline setup we only pre-train on 235 \u2248 34B tokens, which is only about 8 times larger than the smallest pre-training data set we consider. We investigate at what point using a smaller pre-training data sets poses an issue in the following section. 3.4.2 Pre-training Data set Size. The pipeline we use to create C4 was designed to be able to create extremely large pretraining data sets. The access to so much data allows us to pre-train our models without repeating examples. It is not clear whether repeating examples during pre-training would be helpful or harmful to downstream performance because our pre-training objective is itself stochastic and can help prevent the model from seeing the same exact data multiple times. To test the effect of limited unlabeled data set sizes, we pre-trained our baseline model on artificially truncated versions of C4. Recall that we pre-train our baseline model on 235 \u2248 34B tokens (a small fraction of the total size of C4). We consider training on truncated variants of C4 consisting of 229, 227, 225 and 223 tokens.", "Interestingly, the performance of \u201cleave-one-out\u201d training was only slightly worse, suggesting that a model that was trained on a variety of tasks can still adapt to new tasks (i.e. multi-task pretraining might not result in a dramatic task interference). Finally, supervised multi-task pre-training performed significantly worse in every case except for the translation tasks. This\ncould suggest that the translation tasks benefit less from (English) pre-training, whereas unsupervised pre-training is an important factor in the other tasks. 3.6 Scaling. The \u201cbitter lesson\u201d of machine learning research argues that general methods that can leverage additional computation ultimately win out against methods that rely on human expertise (Sutton, 2019; Hestness et al., 2017; Shazeer et al., 2017; Jozefowicz et al., 2016; Mahajan et al., 2018; Shazeer et al., 2018, 2017; Huang et al., 2018b; Keskar et al., 2019a). Recent results suggest that this may hold true for transfer learning in NLP (Liu et al., 2019c; Radford et al., 2019; Yang et al., 2019; Lan et al., 2019), i.e. it has repeatedly been shown that scaling up produces improved performance compared to more carefully-engineered methods. However, there are a variety of possible ways to scale, including using a bigger model, training the model for more steps, and ensembling. In this section, we compare these different approaches by addressing the following premise: \u201cYou were just given 4\u00d7 more compute. How should you use it?\u201d\nWe start with our baseline model, which has 220M parameters and is pre-trained and fine-tuned for 219 and 218 steps respectively. The encoder and decoder are both sized similarly to \u201cBERTBASE\u201d. To experiment with increased model size, we follow the guidelines of \u201cBERTLARGE\u201d Devlin et al. (2018) and use dff = 4096, dmodel = 1024, dkv = 64 and 16-head attention mechanisms.", "louis van gaal\u2019s side currently sit two points clear of liverpool in fourth . D.15 SQuAD. Original input:. Question: What does increased oxygen concentrations in the patient\u2019s lungs displace? Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Processed input: question: What does increased oxygen concentrations in the patient\u2019s lungs displace? context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Original target: carbon monoxide\nProcessed target: carbon monoxide\nD.16 WMT English to German."]}
{"pkey": "t5_16", "question": "What is the loss function that is used to train the model?", "answer": "The objective is standard maximum likelihood, i.e. using teacher forcing (Williams and Zipser, 1989) and loss is cross-entropy loss (Section 3.1.2, Page 11).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["3.5.2 Multi-task Learning. So far, we have been pre-training our model on a single unsupervised learning task before fine-tuning it individually on each downstream task. An alternative approach, called \u201cmultitask learning\u201d (Ruder, 2017; Caruana, 1997), is to train the model on multiple tasks at a time. This approach typically has the goal of training a single model that can simultaneously\nperform many tasks at once, i.e. the model and most of its parameters are shared across all tasks. We relax this goal somewhat and instead investigate methods for training on multiple tasks at once in order to eventually produce separate parameter settings that perform well on each individual task. For example, we might train a single model on many tasks, but when reporting performance we are allowed to select a different checkpoint for each task. This loosens the multi-task learning framework and puts it on more even footing compared to the pre-train-then-fine-tune approach we have considered so far. We also note that in our unified text-to-text framework, \u201cmulti-task learning\u201d simply corresponds to mixing data sets together. It follows that we can still train on unlabeled data when using multi-task learning by treating the unsupervised task as one of the tasks being mixed together. In contrast, most applications of multi-task learning to NLP add task-specific classification networks or use different loss functions for each task (Liu et al., 2019b). As pointed out by Arivazhagan et al. (2019), an extremely important factor in multi-task learning is how much data from each task the model should be trained on. Our goal is to not under- or over-train the model\u2014that is, we want the model to see enough data from a given task that it can perform the task well, but not to see so much data that it memorizes the training set.", "\u201cnon-scaling\u201d changes we made when designing T5. As such, comparing the performance of these two models gives us a concrete measurement of the impact of the insights from our systematic study. The performance of these three model configurations is shown in Table 15. Consistent with the findings in Section 3.6, we find that additional pre-training improves performance over the baseline. Nevertheless, T5-Base substantially outperforms baseline-1T on all downstream tasks. This suggests that scale is not the only factor that contributes to T5\u2019s success. We hypothesize that the larger models benefit not only from their increased size but also from these non-scaling factors. 4. Reflection\nHaving completed our systematic study, we wrap up by first recapping some of our most significant findings. Our results provide some high-level perspective on which avenues of research might be more or less promising. To conclude, we outline some topics we think might provide effective approaches for further progressing the field. 4.1 Takeaways. Text-to-text Our text-to-text framework provides a simple way to train a single model on a wide variety of text tasks using the same loss function and decoding procedure. We showed how this approach can be successfully applied to generative tasks like abstractive summarization, classification tasks like natural language inference, and even regression tasks like STS- B. In spite of its simplicity, we found the text-totext framework obtained comparable performance to task-specific architectures and ultimately produced state-of-the-art results when combined with scale. Architectures While some work on transfer learning for NLP has considered architectural variants of the Transformer, we found the original encoder-decoder form worked best in our text-to-text framework. Though an encoder-decoder model uses twice as many parameters as \u201cencoder-only\u201d (e.g. BERT) or \u201cdecoder-only\u201d (language model) architectures, it has a similar computational cost.", "These sizes correspond to repeating the data set 64, 256, 1,024, and 4,096 times respectively over the course of pre-training. The resulting downstream performance is shown in Table 9. As expected, performance degrades as the data set size shrinks. We suspect this may be due to the fact that the model begins to memorize the pre-training data set. To measure if this is true, we plot the training loss for each of these data set sizes in Figure 6. Indeed, the model attains significantly smaller training losses as the size of the pre-training data set shrinks, suggesting possible memorization. Baevski et al. (2019) similarly observed that truncating the pre-training data set size can degrade downstream task performance. We note that these effects are limited when the pre-training data set is repeated only 64 times. This suggests that some amount of repetition of pre-training data might not be harmful. However, given that additional pre-training can be beneficial (as we will show in Section 3.6) and that obtaining additional unlabeled data is cheap and easy, we suggest using large pre-training data sets whenever possible. We also note that this effect may be more pronounced for larger model sizes, i.e. a bigger model may be more prone to overfitting to a smaller pre-training data set. 3.5 Training Strategy. So far we have considered the setting where all parameters of a model are pre-trained on an unsupervised task before being fine-tuned on individual supervised tasks. While this approach is straightforward, various alternative methods for training the model on downstream/supervised tasks have been proposed. In this section, we compare different schemes for fine-tuning the model in addition to the approach of training the model simultaneously on multiple tasks. 3.5.1 Fine-tuning Methods. It has been argued that fine-tuning all of the model\u2019s parameters can lead to suboptimal results, particularly on low-resource tasks (Peters et al., 2019)."]}
{"pkey": "t5_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "In this paper, the paper authors explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format (Abstract, Page 1). The output of the final decoder block is fed into a dense layer with a softmax output, whose weights are shared with the input embedding matrix (Section 2.1, Page 5). The paper authors average the logits across the ensemble before feeding them into the output softmax nonlinearity to obtain an aggregate prediction (Section 3.6, Page 34).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["\u201cnon-scaling\u201d changes we made when designing T5. As such, comparing the performance of these two models gives us a concrete measurement of the impact of the insights from our systematic study. The performance of these three model configurations is shown in Table 15. Consistent with the findings in Section 3.6, we find that additional pre-training improves performance over the baseline. Nevertheless, T5-Base substantially outperforms baseline-1T on all downstream tasks. This suggests that scale is not the only factor that contributes to T5\u2019s success. We hypothesize that the larger models benefit not only from their increased size but also from these non-scaling factors. 4. Reflection\nHaving completed our systematic study, we wrap up by first recapping some of our most significant findings. Our results provide some high-level perspective on which avenues of research might be more or less promising. To conclude, we outline some topics we think might provide effective approaches for further progressing the field. 4.1 Takeaways. Text-to-text Our text-to-text framework provides a simple way to train a single model on a wide variety of text tasks using the same loss function and decoding procedure. We showed how this approach can be successfully applied to generative tasks like abstractive summarization, classification tasks like natural language inference, and even regression tasks like STS- B. In spite of its simplicity, we found the text-totext framework obtained comparable performance to task-specific architectures and ultimately produced state-of-the-art results when combined with scale. Architectures While some work on transfer learning for NLP has considered architectural variants of the Transformer, we found the original encoder-decoder form worked best in our text-to-text framework. Though an encoder-decoder model uses twice as many parameters as \u201cencoder-only\u201d (e.g. BERT) or \u201cdecoder-only\u201d (language model) architectures, it has a similar computational cost.", "The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more detailed introduction. The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural variants in Section 3.2. Overall, our encoder-decoder Transformer implementation closely follows its originallyproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of \u201cblocks\u201d, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent.", "Since transfer learning for NLP is a rapidly growing area of research, it is not feasible for us to cover every possible technique or idea in our empirical study. For a broader literature review, we recommend a recent survey by Ruder et al. (2019). We systematically study these contributions by taking a reasonable baseline (described in Section 3.1) and altering one aspect of the setup at a time. For example, in Section 3.3 we measure the performance of different unsupervised objectives while keeping the rest of our experimental pipeline fixed. This \u201ccoordinate ascent\u201d approach might miss second-order effects (for example, some particular unsupervised objective may work best on a model larger than our baseline setting), but performing a combinatorial exploration of all of the factors in our study would be prohibitively expensive. In future work, we expect it could be fruitful to more thoroughly consider combinations of the approaches we study. Our goal is to compare a variety of different approaches on a diverse set of tasks while keeping as many factors fixed as possible. In order to satisfy this aim, in some cases we do not exactly replicate existing approaches. For example, \u201cencoder-only\u201d models like BERT (Devlin et al., 2018) are designed to produce a single prediction per input token or a single prediction for an entire input sequence. This makes them applicable for classification or span prediction tasks but not for generative tasks like translation or abstractive summarization. As such, none of the model architectures we consider are identical to BERT or consist of an encoder-only structure. Instead, we test approaches that are similar in spirit\u2014for example, we consider an analogous objective to BERT\u2019s \u201cmasked language modeling\u201d objective in\nSection 3.3 and we consider a model architecture that behaves similarly to BERT on text classification tasks in Section 3.2."]}
{"pkey": "t5_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "Specifically, we measure performance on the GLUE and SuperGLUE text classification meta-benchmarks; CNN/Daily Mail abstractive summarization; SQuAD question answering; and WMT English to German, French, and Romanian translation (Section 2.3, Page 7).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["We empirically compare this objective to many other variants in Section 3.3. 3.1.5 Baseline Performance. In this section, we present results using the baseline experimental procedure described above to get a sense of what kind of performance to expect on our suite of downstream tasks. Ideally, we would repeat every experiment in our study multiple times to get a confidence interval on our results. Unfortunately, this would be prohibitively expensive due to the large\nnumber of experiments we run. As a cheaper alternative, we train our baseline model 10 times from scratch (i.e. with different random initializations and data set shuffling) and assume that the variance over these runs of the base model also applies to each experimental variant. We don\u2019t expect most of the changes we make to have a dramatic effect on the inter-run variance, so this should provide a reasonable indication of the significance of different changes. Separately, we also measure the performance of training our model for 218 steps (the same number we use for fine-tuning) on all downstream tasks without pre-training. This gives us an idea of how much pre-training benefits our model in the baseline setting. When reporting results in the main text, we only report a subset of the scores across all the benchmarks to conserve space and ease interpretation. For GLUE and SuperGLUE, we report the average score across all subtasks (as stipulated by the official benchmarks) under the headings \u201cGLUE\u201d and \u201cSGLUE\u201d. For all translation tasks, we report the BLEU score (Papineni et al., 2002) as provided by SacreBLEU v1.3.0 (Post, 2018) with \u201cexp\u201d smoothing and \u201cintl\u201d tokenization. We refer to scores for WMT English to German, English to French, and English to Romanian as EnDe, EnFr, and EnRo, respectively. For CNN/Daily Mail, we find the performance of models on the ROUGE-1-F, ROUGE-2-F, and ROUGE-L-F metrics (Lin, 2004) to be highly correlated so we report the ROUGE-2-F score alone under the heading \u201cCNNDM\u201d.", "Note that the choice of text prefix used for a given task is essentially a hyperparameter; we found that changing the exact wording of the prefix had limited impact and so did not perform extensive experiments into different prefix choices. A diagram of our text-to-text framework with a few input/output\nexamples is shown in Figure 1. We provide full examples of preprocessed inputs for every task we studied in Appendix D.\nOur text-to-text framework follows previous work that casts multiple NLP tasks into a common format: McCann et al. (2018) propose the \u201cNatural Language Decathlon\u201d, a benchmark that uses a consistent question-answering format for a suite of ten NLP tasks. The Natural Language Decathlon also stipulates that all models must be multi-task, i.e. are able to simultaneously tackle all of the tasks at once. We instead allow for separately fine-tuning the model on each individual task and use short task prefixes instead of an explicit question-answer format. Radford et al. (2019) evaluate the zero-shot learning capabilities of language models by feeding some input to the model as a prefix and then autoregressively sampling an output. For example, automatic summarization is done by feeding in a document followed by the text \u201cTL;DR:\u201d (short for \u201ctoo long, didn\u2019t read\u201d, a common abbreviation) and then the summary is predicted via autoregressive decoding. We mainly consider models that explicitly process an input with an encoder before generating an output with a separate decoder and we focus on transfer learning rather than zero-shot learning. Finally, Keskar et al. (2019b) unify many NLP tasks as \u201cspan extraction\u201d, where text corresponding to possible output choices are appended to the input and the model is trained to extract the input span corresponding to the correct choice. In contrast, our framework also allows for generative tasks like machine translation and abstractive summarization where it is not possible to enumerate all possible output choices.", "sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. Original target: 0. Processed target: entailment\nD.6 QQP. Original input:.\nQuestion 1: What attributes would have made you highly desirable in ancient Rome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nProcessed input: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nOriginal target: 0.\nProcessed target: not_duplicate\nD.7 SST2. Original input:. Sentence: it confirms fincher \u2019s status as a film maker who artfully bends technical know-how to the service of psychological insight . Processed input: sst2 sentence: it confirms fincher \u2019s status as a film maker who artfully bends technical know-how to the service of psychological insight . Original target: 1.\nProcessed target: positive\nD.8 STSB. Original input:. Sentence 1: Representatives for Puretunes could not immediately be reached for comment Wednesday. Sentence 2: Puretunes representatives could not be located Thursday to comment on the suit. Processed input: stsb sentence1: Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2: Puretunes representatives could not be located Thursday to comment on the suit. Original target: 3.25\nProcessed target: 3.2\nD.9 CB. Original input:. Hypothesis: Valence was helping Premise: Valence the void-brain, Valence the virtuous valet. Why couldn\u2019t\nthe figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet. Why couldn\u2019t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Original target: 1.\nProcessed target: contradiction\nD.10 COPA. Original input:. Question: effect Premise: Political violence broke out in the nation."]}
{"pkey": "t5_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "Since these architectural changes are orthogonal to the experimental factors the paper authors consider in our empirical survey of transfer learning, the paper authors leave the ablation of their impact for future work (Section 2.1, Page 5).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["For efficiency, we also share the position embedding parameters across all layers in our model, though within a given layer each attention head uses a different learned position embedding. Typically, a fixed number of embeddings are learned, each corresponding to a range of possible key-query offsets. In this work, we use 32 embeddings for all of our models with ranges that increase in size logarithmically up to an offset of 128 beyond which we assign all relative positions to the same embedding. Note that a given layer is insensitive to relative position beyond 128 tokens, but subsequent layers can build a sensitivity to larger offsets by combining local information from previous layers. To summarize, our model is roughly equivalent to the original Transformer proposed by Vaswani et al. (2017) with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme. Since these architectural changes are orthogonal to the experimental factors we consider in our empirical survey of transfer learning, we leave the ablation of their impact for future work. As part of our study, we experiment with the scalability of these models, i.e. how their performance changes as they are made to have more parameters or layers. Training large models can be non-trivial since they might not fit on a single machine and require a great deal of computation. As a result, we use a combination of model and data parallelism and train models on \u201cslices\u201d of Cloud TPU Pods.5 TPU pods are are multi-rack ML supercomputers that contain 1,024 TPU v3 chips connected via a high-speed 2D mesh interconnect with supporting CPU host machines. We leverage the Mesh TensorFlow library (Shazeer et al., 2018) for ease of implementation of both model parallelism and data parallelism (Krizhevsky, 2014). 2.2 The Colossal Clean Crawled Corpus.", "louis van gaal\u2019s side currently sit two points clear of liverpool in fourth . D.15 SQuAD. Original input:. Question: What does increased oxygen concentrations in the patient\u2019s lungs displace? Context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Processed input: question: What does increased oxygen concentrations in the patient\u2019s lungs displace? context: Hyperbaric (high-pressure) medicine uses special oxygen chambers to increase the partial pressure of O 2 around the patient and, when needed, the medical staff. Carbon monoxide poisoning, gas gangrene, and decompression sickness (the \u2019bends\u2019) are sometimes treated using these devices. Increased O 2 concentration in the lungs helps to displace carbon monoxide from the heme group of hemoglobin. Oxygen gas is poisonous to the anaerobic bacteria that cause gas gangrene, so increasing its partial pressure helps kill them. Decompression sickness occurs in divers who decompress too quickly after a dive, resulting in bubbles of inert gas, mostly nitrogen and helium, forming in their blood. Increasing the pressure of O 2 as soon as possible is part of the treatment. Original target: carbon monoxide\nProcessed target: carbon monoxide\nD.16 WMT English to German.", "sentence: Genghis Khan recalled Subutai back to Mongolia soon afterwards, and Jebe died on the road back to Samarkand. Original target: 0. Processed target: entailment\nD.6 QQP. Original input:.\nQuestion 1: What attributes would have made you highly desirable in ancient Rome?\nQuestion 2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nProcessed input: qqp question1: What attributes would have made you highly desirable in ancient Rome? question2: How I GET OPPERTINUTY TO JOIN IT COMPANY AS A FRESHER?\nOriginal target: 0.\nProcessed target: not_duplicate\nD.7 SST2. Original input:. Sentence: it confirms fincher \u2019s status as a film maker who artfully bends technical know-how to the service of psychological insight . Processed input: sst2 sentence: it confirms fincher \u2019s status as a film maker who artfully bends technical know-how to the service of psychological insight . Original target: 1.\nProcessed target: positive\nD.8 STSB. Original input:. Sentence 1: Representatives for Puretunes could not immediately be reached for comment Wednesday. Sentence 2: Puretunes representatives could not be located Thursday to comment on the suit. Processed input: stsb sentence1: Representatives for Puretunes could not immediately be reached for comment Wednesday. sentence2: Puretunes representatives could not be located Thursday to comment on the suit. Original target: 3.25\nProcessed target: 3.2\nD.9 CB. Original input:. Hypothesis: Valence was helping Premise: Valence the void-brain, Valence the virtuous valet. Why couldn\u2019t\nthe figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Processed input: cb hypothesis: Valence was helping premise: Valence the void-brain, Valence the virtuous valet. Why couldn\u2019t the figger choose his own portion of titanic anatomy to shaft? Did he think he was helping? Original target: 1.\nProcessed target: contradiction\nD.10 COPA. Original input:. Question: effect Premise: Political violence broke out in the nation."]}
{"pkey": "t5_20", "question": "List the future work mentioned in the paper.", "answer": "A better notion of the relatedness of tasks could also help choose supervised pre-training tasks, which has been shown to be helpful for the GLUE benchmark (Phang et al., 2018) (Section 4.2, Page 43). The paper authors ultimately did not find a strategy for setting mixing proportions that matched the performance of the basic approach of unsupervised pre-training followed by supervised fine-tuning (Section 4.1, Page 42).", "title": "Exploring the Limits of Transfer Learning with a UnifiedText-to-Text Transformer", "context": ["Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \u201cColossal Clean Crawled Corpus\u201d, we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.1 Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. Transfer learning, where a model is first pre-trained on a data-rich task before being finetuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks.", "In order to perform experiments at this scale, we introduce the \u201cColossal Clean Crawled Corpus\u201d (C4), a data set consisting of hundreds of gigabytes of clean English text scraped from the web. Recognizing that the main utility of transfer learning is the possibility of leveraging pre-trained models in data-scarce settings, we release our code, data sets, and pre-trained models.1\nThe remainder of the paper is structured as follows: In the following section, we discuss our base model and its implementation, our procedure for formulating every text processing problem as a text-to-text task, and the suite of tasks we consider. In Section 3, we present a large set of experiments that explore the field of transfer learning for NLP. At the end of the section (Section 3.7), we combine insights from our systematic study to obtain state-of-the-art results on a wide variety of benchmarks. Finally, we provide a summary of our results and wrap up with a look towards the future in Section 4.\n2. Setup. Before presenting the results from our large-scale empirical study, we review the necessary background topics required to understand our results, including the Transformer model architecture and the downstream tasks we evaluate on. We also introduce our approach for treating every problem as a text-to-text task and describe our \u201cColossal Clean Crawled Corpus\u201d (C4), the Common Crawl-based data set we created as a source of unlabeled text data. We refer to our model and framework as the \u201cText-to-Text Transfer Transformer\u201d (T5). 2.1 Model. Early results on transfer learning for NLP leveraged recurrent neural networks (Peters et al., 2018; Howard and Ruder, 2018), but it has recently become more common to use models based on the \u201cTransformer\u201d architecture (Vaswani et al., 2017).", "The Transformer was initially shown to be effective for machine translation, but it has subsequently been used in a wide variety of NLP settings (Radford et al., 2018; Devlin et al., 2018; McCann et al., 2018; Yu et al., 2018). Due to its increasing ubiquity, all of the models we study are based on the Transformer architecture. Apart from the details mentioned below and the variants we explore in Section 3.2, we do not deviate significantly from this architecture as originally proposed. Instead of providing a comprehensive definition of this model, we refer the interested reader to the original paper (Vaswani et al., 2017) or follow-up tutorials3,4 for a more detailed introduction. The primary building block of the Transformer is self-attention (Cheng et al., 2016). Self-attention is a variant of attention (Graves, 2013; Bahdanau et al., 2015) that processes a sequence by replacing each element by a weighted average of the rest of the sequence. The original Transformer consisted of an encoder-decoder architecture and was intended for sequence-to-sequence (Sutskever et al., 2014; Kalchbrenner et al., 2014) tasks. It has recently also become common to use models consisting of a single Transformer layer stack, with varying forms of self-attention used to produce architectures appropriate for language modeling (Radford et al., 2018; Al-Rfou et al., 2019) or classification and span prediction tasks (Devlin et al., 2018; Yang et al., 2019). We empirically explore these architectural variants in Section 3.2. Overall, our encoder-decoder Transformer implementation closely follows its originallyproposed form (Vaswani et al., 2017). First, an input sequence of tokens is mapped to a sequence of embeddings, which is then passed into the encoder. The encoder consists of a stack of \u201cblocks\u201d, each of which comprises two subcomponents: a self-attention layer followed by a small feed-forward network. Layer normalization (Ba et al., 2016) is applied to the input of each subcomponent."]}
{"pkey": "sparsetransformer_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. The main contribution of this work is to introduce several sparse factorizations of the attention matrix, which scale as O(n\u221an) with the sequence length without sacrificing performance", "title": "Generating Long Sequences with Sparse Transformers", "context": ["To address this, WaveNet (Van Den Oord et al., 2016) introduced dilated convolutions, which allowed the network to model long-range dependencies in a logarithmic number of layers. Separately, the Transformer (Vaswani et al., 2017) has been shown to excel on many natural language tasks, which may be in part due to its ability to model arbitrary dependencies in a constant number of layers. As each self-attention layer has a global receptive field, the network can allocate representational capacity to the input regions for which it is\nar X\niv :1\n90 4.\n10 50\n9v 1\n[ cs\n.L G\n] 2\n3 A\npr 2\n01 9\nmost useful. Thus the architecture may be more flexible at generating diverse data types than networks with fixed connectivity patterns. However, the memory and computational requirements of such networks grows quadratically with sequence length, which excludes their use on long sequences. The main contribution of this work is to introduce several sparse factorizations of the attention matrix, which scale as O(n p \u221a n) with the sequence length without sacrificing performance. These work by separating the full attention computation into several faster attention operations which, when combined, can approximate the dense attention operation. We use this to apply self-attention to sequences of unprecedented length. Additionally, we introduce several other changes to the Transformer, including:\n\u2022 A restructured residual block and weight initialization to improve training of very deep networks\n\u2022 A set of sparse attention kernels which efficiently compute subsets of the attention matrix\n\u2022 Recomputation of attention weights during the backwards pass to reduce memory usage\nWe empirically validate that models augmented in this manner can achieve state-of-the-art compression and generation of natural language, raw audio, and natural images. The simplicity of the architecture leads us to believe it may be useful for many problems of interest. 2. Related Work.", "Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\u221a n).", "We also\nintroduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. 1. Introduction. Estimating complex, high-dimensional data distributions is a central problem in unsupervised learning, as many downstream applications of interest involve generation of text, images, audio, and other data. Additionally, it is believed to be a key component of unsupervised representation learning. Recently, neural autoregressive models have achieved impressive results in this domain, achieving state-of-the-art in modeling natural language (Jozefowicz et al., 2016) (Radford et al., 2018) (Dai et al., 2018), raw audio (Van Den Oord et al., 2016) (Mehri et al., 2016), and images (Oord et al., 2016) (Menick & Kalchbrenner, 2018) (Salimans et al., 2017) (Reed et al., 2017) (Chen et al., 2017). These methods decompose a joint probability distribution into a product of conditional ones. Modeling these conditional distributions is extremely challenging, however, as they contain many complex, long-range dependencies and require a suitably expressive model architecture to learn them. Architectures based off CNNs (Oord et al., 2016) have made\ngreat progress in this direction, but require significant depth to expand their receptive field."]}
{"pkey": "sparsetransformer_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Neural autoregressive models methods decompose a joint probability distribution into a product of conditional ones. Modeling these conditional distributions is extremely challenging, however, as they contain many complex, long-range dependencies and require a suitably expressive model architecture to learn them. Architectures based off CNNs (Oord et al., 2016) have made great progress in this direction, but require significant depth to expand their receptive field.\n\nSeparately, the Transformer (Vaswani et al., 2017) has been shown to excel on many natural language tasks, which may be in part due to its ability to model arbitrary dependencies in a constant number of layers. As each self-attention layer has a global receptive field, the network can allocate representational capacity to the input regions for which it is most useful. Thus the architecture may be more flexible at generating diverse data types than networks with fixed\nconnectivity patterns. However, the memory and computational requirements of such networks grows quadratically with sequence length, which excludes their use on long sequences.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\u221a n).", "In this paper, we restricted our investigation to a class of sparse attention patterns that have connectivity between all positions over several steps of attention. These methods can be more efficient than full attention while still providing global context to any given position. We aimed to empirically validate the performance of these factorized patterns on a range of tasks, given that they are unable to learn the exact same mappings as those in Figure 2. We present the formulation of factorized attention below. 4.2. Factorized self-attention. A self-attention layer maps a matrix of input embeddings X to an output matrix and is parameterized by a connectivity pattern S = {S1, ..., Sn}, where Si denotes the set of indices of the input vectors to which the ith output vector attends. The output vector is a weighted sum of transformations of the input vectors:\nAttend(X,S) = ( a(xi, Si) ) i\u2208{1,...,n}\n(2)\na(xi, Si) = softmax\n( (Wqxi)K\nT Si\u221a\nd\n) VSi (3)\nKSi = ( Wkxj ) j\u2208Si VSi = ( Wvxj ) j\u2208Si\n(4) Here Wq , Wk, and Wv represent the weight matrices which transform a given xi into a query, key, or value, and d is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries. Full self-attention for autoregressive models defines Si = {j : j \u2264 i}, allowing every element to attend to all previous positions and its own position. Factorized self-attention instead has p separate attention heads, where the mth head defines a subset of the indices A\n(m) i \u2282 {j : j \u2264 i} and lets Si = A (m) i . We are chiefly interested in efficient choices for the subset A, where |A(m)i | \u221d p \u221a n.\nAdditionally, for the time being we consider valid choices of A, where all input positions are connected to all future output positions across the p steps of attention. For every j \u2264 i pair, we set every A such that i can attend to j through a path of locations with maximum length p+ 1."]}
{"pkey": "sparsetransformer_3", "question": "What are the main contributions of the paper?", "answer": "The main contribution of this work is to introduce several sparse factorizations of the attention matrix, which scale as O(n \u221ap n) with the sequence length without sacrificing performance. These work by separating the full attention computation into several faster attention operations which, when combined, can approximate the dense attention operation. The paper authors use this to apply self-attention to sequences of unprecedented length.\n\nIn this paper the paper authors introduce sparse factorizations of the attention matrix which reduce this to O(n\u221an).", "title": "Generating Long Sequences with Sparse Transformers", "context": ["To address this, WaveNet (Van Den Oord et al., 2016) introduced dilated convolutions, which allowed the network to model long-range dependencies in a logarithmic number of layers. Separately, the Transformer (Vaswani et al., 2017) has been shown to excel on many natural language tasks, which may be in part due to its ability to model arbitrary dependencies in a constant number of layers. As each self-attention layer has a global receptive field, the network can allocate representational capacity to the input regions for which it is\nar X\niv :1\n90 4.\n10 50\n9v 1\n[ cs\n.L G\n] 2\n3 A\npr 2\n01 9\nmost useful. Thus the architecture may be more flexible at generating diverse data types than networks with fixed connectivity patterns. However, the memory and computational requirements of such networks grows quadratically with sequence length, which excludes their use on long sequences. The main contribution of this work is to introduce several sparse factorizations of the attention matrix, which scale as O(n p \u221a n) with the sequence length without sacrificing performance. These work by separating the full attention computation into several faster attention operations which, when combined, can approximate the dense attention operation. We use this to apply self-attention to sequences of unprecedented length. Additionally, we introduce several other changes to the Transformer, including:\n\u2022 A restructured residual block and weight initialization to improve training of very deep networks\n\u2022 A set of sparse attention kernels which efficiently compute subsets of the attention matrix\n\u2022 Recomputation of attention weights during the backwards pass to reduce memory usage\nWe empirically validate that models augmented in this manner can achieve state-of-the-art compression and generation of natural language, raw audio, and natural images. The simplicity of the architecture leads us to believe it may be useful for many problems of interest. 2. Related Work.", "In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\u221a n)."]}
{"pkey": "sparsetransformer_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "The paper authors use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64.\n\nThe paper authors empirically validate that models augmented in this manner can achieve state-of-the-art compression and generation of natural language, raw audio, and natural images. The simplicity of the architecture leads us to believe it may be useful for many problems of interest.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["Specifically, if (j, a, b, c, ..., i) is the path of indices, then j \u2208 A(1)a , a \u2208 A(2)b , b \u2208 A (3) c , and so forth. These two criteria allow us keep the ability of Transformers to propagate signals from arbitrary input positions to arbitrary output positions in a constant number of steps, while reducing the total effective computation to O(n p \u221a n). We also note that softening the validity criterion (for instance, having a series of only locally connected layers) may be a useful inductive bias for certain domains. In this work, we explore two factorizations for p = 2, which we describe in the following section, though we note that the same techniques can be easily extended to higher dimensions. 4.3. Two-dimensional factorized attention. A natural approach to defining a factorized attention pattern in two dimensions is to have one head attend to the previous l locations, and the other head attend to every lth location, where l is the stride and chosen to be close to \u221a n, a method we call strided attention. Formally, A(1)i = {t, t + 1, ..., i} for t = max(0, i \u2212 l) and A(2)i = {j : (i \u2212 j) mod l = 0}. This pattern can be visualized in Figure 3(b). This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, we find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future. In those cases, we instead use a fixed attention pattern (Figure 3(c)), where specific cells summarize previous locations and propagate that information to all future cells. Formally,A(1)i = {j : (bj/lc = bi/lc)}, where the brackets denote the floor operation, and A(2)i = {j : j mod l \u2208 {t, t+ 1, ..., l}, where t = l \u2212 c and c is a hyperparameter.", "We also\nintroduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. 1. Introduction. Estimating complex, high-dimensional data distributions is a central problem in unsupervised learning, as many downstream applications of interest involve generation of text, images, audio, and other data. Additionally, it is believed to be a key component of unsupervised representation learning. Recently, neural autoregressive models have achieved impressive results in this domain, achieving state-of-the-art in modeling natural language (Jozefowicz et al., 2016) (Radford et al., 2018) (Dai et al., 2018), raw audio (Van Den Oord et al., 2016) (Mehri et al., 2016), and images (Oord et al., 2016) (Menick & Kalchbrenner, 2018) (Salimans et al., 2017) (Reed et al., 2017) (Chen et al., 2017). These methods decompose a joint probability distribution into a product of conditional ones. Modeling these conditional distributions is extremely challenging, however, as they contain many complex, long-range dependencies and require a suitably expressive model architecture to learn them. Architectures based off CNNs (Oord et al., 2016) have made\ngreat progress in this direction, but require significant depth to expand their receptive field.", "The samples clearly demonstrate global coherence over the sampled period, and exhibit a variety of play styles and tones, swapping from rhythmic playing to forceful. To listen to samples, visit https://openai.com/blog/ sparse-transformer. Sample quality quickly degrades for greater sequence lengths due to reduced model capacity. 8. Conclusion. We introduced Sparse Transformers and showed they attain equivalent or better performance on density modeling of long sequences than standard Transformers while requiring significantly fewer operations. This performance is stateof-the-art in images and text and is easily adaptable to raw audio. The model demonstrates usage of long-term context and generates globally coherent samples. 9. Acknowledgements. We would like to thank Ashish Vaswani for insightful discussions during the genesis of the project. We also thank Joshua Meier and Mark Chen for helpful discussions, and Johannes Otterbach, Prafulla Dhariwal, and David Luan for feedback on drafts of this paper."]}
{"pkey": "sparsetransformer_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64", "title": "Generating Long Sequences with Sparse Transformers", "context": ["In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "The model achieves 2.80 bits per dim (2.798\u00b1 0.004 over seeds 1, 2, 3) versus the previous 2.85 state of the art (Chen et al., 2017). We also compare performance of different attention patterns in Table 2. The strided attention reaches the lowest error in the shortest amount of time, surpassing the error of dense attention at 2.82 bits per dim.\n7.2. Text. In order to assess Sparse Transformers on datasets without a strong two-dimensional structure, we trained models on the EnWik8 dataset, which represents the first 108 bytes of Wikipedia and contains a great degree of variability in periodic structure. We trained with a context length of 12,288, which is longer than previous approaches. We trained on the first 90 million tokens and reserved the last 10 million for validation and test. We used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout rate of 0.40. We trained for 80 epochs until validation loss stopped decreasing. We used a stride of 128, c = 32, and merged the factorized attention heads. Our best model reached 0.99 bits per dim (0.992 \u00b1 0.001 over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for a similarly-sized Transformer-XL (Dai et al., 2018) and matching the 0.99 of a model trained with more than double\nthe number of parameters. Strided attention failed to do well on this dataset, whereas fixed patterns were able to recover and surpass the performance of dense attention, as listed in Table 2. Additionally, during evaluation of the test set, we modified the minimum context length the network could use by evaluating fewer tokens in parallel. We saw monotonic increases in performance with more tokens used, up to 12,160 out of the 12,288 tokens used for training (see Table 3), which suggests the network is effectively incorporating long-term dependencies.\n7.3. ImageNet 64x64.", "We use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which we found important for model stability. We use a weight decay penalty of 0.01. We annealed the learning rate according to a cosine decay as in (Radford et al., 2018). We train on 8 V100 GPUs unless otherwise noted. All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless we use \u201chalf-size\u201d transformations, where it is 2d. Additionally, sometimes we halve the size of the query and key transformations. We initialize the token embeddingWe fromN (0, 0.125\u221ad ) and the position embeddings from N (0, 0.125\u221a\ndnemb ). Within the\nattention and feedforward components, all biases are initial-\nized to 0 and all weights are initialized from N (0, 0.125\u221a din ) where din is the fan-in dimension. The weight matrix for the output logits was initialized to 0. 7. Experiments. We empirically test our architecture on density modeling tasks including natural images, text, and raw audio. A summary of the results is available in Table 1. We found that, in addition to running significantly faster than full attention, sparse patterns also converged to lower error, as shown in Table 2. This may point to a useful inductive bias from the sparsity patterns we introduced, or an underlying optimization issue with full attention.\n7.1. CIFAR-10. We train strided Sparse Transformers on CIFAR-10 images represented as sequences of 3072 bytes. Models have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs with a learning rate of 0.00035 and a dropout rate of 0.25 until validation error stops decreasing. We use 48000 examples for training and 2000 examples for validation, evaluating the performance of our best models on\nthe test set."]}
{"pkey": "sparsetransformer_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "The paper authors also note that softening the validity criterion (for instance, having a series of only locally connected layers) may be a useful inductive bias for certain domains.\n\nThe upper triangle of the attention matrix is never computed, moreover, removing the need for the negative bias term of (Vaswani et al., 2017) and halving the number of operations to be performed.\n\nWithin the attention and feedforward components, all biases are initialized to 0 and all weights are initialized from N (0,0.125/\u221adin) where din is the fan-in dimension. The weight matrix for\nthe output logits was initialized to 0", "title": "Generating Long Sequences with Sparse Transformers", "context": ["Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\u221a n).", "In this paper, we restricted our investigation to a class of sparse attention patterns that have connectivity between all positions over several steps of attention. These methods can be more efficient than full attention while still providing global context to any given position. We aimed to empirically validate the performance of these factorized patterns on a range of tasks, given that they are unable to learn the exact same mappings as those in Figure 2. We present the formulation of factorized attention below. 4.2. Factorized self-attention. A self-attention layer maps a matrix of input embeddings X to an output matrix and is parameterized by a connectivity pattern S = {S1, ..., Sn}, where Si denotes the set of indices of the input vectors to which the ith output vector attends. The output vector is a weighted sum of transformations of the input vectors:\nAttend(X,S) = ( a(xi, Si) ) i\u2208{1,...,n}\n(2)\na(xi, Si) = softmax\n( (Wqxi)K\nT Si\u221a\nd\n) VSi (3)\nKSi = ( Wkxj ) j\u2208Si VSi = ( Wvxj ) j\u2208Si\n(4) Here Wq , Wk, and Wv represent the weight matrices which transform a given xi into a query, key, or value, and d is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries. Full self-attention for autoregressive models defines Si = {j : j \u2264 i}, allowing every element to attend to all previous positions and its own position. Factorized self-attention instead has p separate attention heads, where the mth head defines a subset of the indices A\n(m) i \u2282 {j : j \u2264 i} and lets Si = A (m) i . We are chiefly interested in efficient choices for the subset A, where |A(m)i | \u221d p \u221a n.\nAdditionally, for the time being we consider valid choices of A, where all input positions are connected to all future output positions across the p steps of attention. For every j \u2264 i pair, we set every A such that i can attend to j through a path of locations with maximum length p+ 1.", "The samples clearly demonstrate global coherence over the sampled period, and exhibit a variety of play styles and tones, swapping from rhythmic playing to forceful. To listen to samples, visit https://openai.com/blog/ sparse-transformer. Sample quality quickly degrades for greater sequence lengths due to reduced model capacity. 8. Conclusion. We introduced Sparse Transformers and showed they attain equivalent or better performance on density modeling of long sequences than standard Transformers while requiring significantly fewer operations. This performance is stateof-the-art in images and text and is easily adaptable to raw audio. The model demonstrates usage of long-term context and generates globally coherent samples. 9. Acknowledgements. We would like to thank Ashish Vaswani for insightful discussions during the genesis of the project. We also thank Joshua Meier and Mark Chen for helpful discussions, and Johannes Otterbach, Prafulla Dhariwal, and David Luan for feedback on drafts of this paper."]}
{"pkey": "sparsetransformer_7", "question": "List the limitations of the model discussed in the paper.", "answer": "A second approach is to have a single head attend to the locations of the pixels that both factorized heads would attend to, which the paper authors call a merged head: This is slightly more computationally intensive, but only by a constant factor.\n\nThe paper authors typically find multiple heads to work well, though for extremely long sequences where the attention dominates the computation time, it is more worthwhile to perform them one at a time and sequentially.\n\n\nThe paper authors typically find multiple heads to work well, though for extremely long sequences where the attention dominates the computation time, it is more worthwhile to perform them one at a time and sequentially.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["The samples clearly demonstrate global coherence over the sampled period, and exhibit a variety of play styles and tones, swapping from rhythmic playing to forceful. To listen to samples, visit https://openai.com/blog/ sparse-transformer. Sample quality quickly degrades for greater sequence lengths due to reduced model capacity. 8. Conclusion. We introduced Sparse Transformers and showed they attain equivalent or better performance on density modeling of long sequences than standard Transformers while requiring significantly fewer operations. This performance is stateof-the-art in images and text and is easily adaptable to raw audio. The model demonstrates usage of long-term context and generates globally coherent samples. 9. Acknowledgements. We would like to thank Ashish Vaswani for insightful discussions during the genesis of the project. We also thank Joshua Meier and Mark Chen for helpful discussions, and Johannes Otterbach, Prafulla Dhariwal, and David Luan for feedback on drafts of this paper.", "The model achieves 2.80 bits per dim (2.798\u00b1 0.004 over seeds 1, 2, 3) versus the previous 2.85 state of the art (Chen et al., 2017). We also compare performance of different attention patterns in Table 2. The strided attention reaches the lowest error in the shortest amount of time, surpassing the error of dense attention at 2.82 bits per dim.\n7.2. Text. In order to assess Sparse Transformers on datasets without a strong two-dimensional structure, we trained models on the EnWik8 dataset, which represents the first 108 bytes of Wikipedia and contains a great degree of variability in periodic structure. We trained with a context length of 12,288, which is longer than previous approaches. We trained on the first 90 million tokens and reserved the last 10 million for validation and test. We used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout rate of 0.40. We trained for 80 epochs until validation loss stopped decreasing. We used a stride of 128, c = 32, and merged the factorized attention heads. Our best model reached 0.99 bits per dim (0.992 \u00b1 0.001 over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for a similarly-sized Transformer-XL (Dai et al., 2018) and matching the 0.99 of a model trained with more than double\nthe number of parameters. Strided attention failed to do well on this dataset, whereas fixed patterns were able to recover and surpass the performance of dense attention, as listed in Table 2. Additionally, during evaluation of the test set, we modified the minimum context length the network could use by evaluating fewer tokens in parallel. We saw monotonic increases in performance with more tokens used, up to 12,160 out of the 12,288 tokens used for training (see Table 3), which suggests the network is effectively incorporating long-term dependencies.\n7.3. ImageNet 64x64.", "Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\u221a n)."]}
{"pkey": "sparsetransformer_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["The model achieves 2.80 bits per dim (2.798\u00b1 0.004 over seeds 1, 2, 3) versus the previous 2.85 state of the art (Chen et al., 2017). We also compare performance of different attention patterns in Table 2. The strided attention reaches the lowest error in the shortest amount of time, surpassing the error of dense attention at 2.82 bits per dim.\n7.2. Text. In order to assess Sparse Transformers on datasets without a strong two-dimensional structure, we trained models on the EnWik8 dataset, which represents the first 108 bytes of Wikipedia and contains a great degree of variability in periodic structure. We trained with a context length of 12,288, which is longer than previous approaches. We trained on the first 90 million tokens and reserved the last 10 million for validation and test. We used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout rate of 0.40. We trained for 80 epochs until validation loss stopped decreasing. We used a stride of 128, c = 32, and merged the factorized attention heads. Our best model reached 0.99 bits per dim (0.992 \u00b1 0.001 over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for a similarly-sized Transformer-XL (Dai et al., 2018) and matching the 0.99 of a model trained with more than double\nthe number of parameters. Strided attention failed to do well on this dataset, whereas fixed patterns were able to recover and surpass the performance of dense attention, as listed in Table 2. Additionally, during evaluation of the test set, we modified the minimum context length the network could use by evaluating fewer tokens in parallel. We saw monotonic increases in performance with more tokens used, up to 12,160 out of the 12,288 tokens used for training (see Table 3), which suggests the network is effectively incorporating long-term dependencies.\n7.3. ImageNet 64x64.", "In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "We use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which we found important for model stability. We use a weight decay penalty of 0.01. We annealed the learning rate according to a cosine decay as in (Radford et al., 2018). We train on 8 V100 GPUs unless otherwise noted. All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless we use \u201chalf-size\u201d transformations, where it is 2d. Additionally, sometimes we halve the size of the query and key transformations. We initialize the token embeddingWe fromN (0, 0.125\u221ad ) and the position embeddings from N (0, 0.125\u221a\ndnemb ). Within the\nattention and feedforward components, all biases are initial-\nized to 0 and all weights are initialized from N (0, 0.125\u221a din ) where din is the fan-in dimension. The weight matrix for the output logits was initialized to 0. 7. Experiments. We empirically test our architecture on density modeling tasks including natural images, text, and raw audio. A summary of the results is available in Table 1. We found that, in addition to running significantly faster than full attention, sparse patterns also converged to lower error, as shown in Table 2. This may point to a useful inductive bias from the sparsity patterns we introduced, or an underlying optimization issue with full attention.\n7.1. CIFAR-10. We train strided Sparse Transformers on CIFAR-10 images represented as sequences of 3072 bytes. Models have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs with a learning rate of 0.00035 and a dropout rate of 0.25 until validation error stops decreasing. We use 48000 examples for training and 2000 examples for validation, evaluating the performance of our best models on\nthe test set."]}
{"pkey": "sparsetransformer_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors treat images, text, and audio as a sequence of discrete tokens, typically raw bytes. The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary\n\nThe paper authors initialize the token embedding The paper authors from N (0,0.125/\u221ad) and the position embeddings from N (0,0.125 \u221adnemb). Within the attention and feedforward components,", "title": "Generating Long Sequences with Sparse Transformers", "context": ["The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to \u03b8. A simple and powerful choice for model \u03b8 is a Transformer (Vaswani et al., 2017) in decoder-only mode, as demonstrated by (Radford et al., 2018) and (Liu et al., 2018). These models transform the input sequence with blocks of multihead self-attention over the entire sequence, followed by dense transformations over each sequence element. The selfattention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows. In the following sections, we describe our modifications to the Transformer architecture which make it more suitable for modeling long sequences. 4. Factorized Self-Attention. Sparse Transformers separate the full self-attention operation across several steps of attention, as visualized in Figure 3(b) and 3(c). To motivate our approach, we first perform a qualitative assessment of attention patterns learned by a standard Transformer on an image dataset.\n4.1. Qualitative assessment of learned attention patterns. We visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in Figure 2. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers (Figure 2c) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity (Figure 2d), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices.", "The model achieves 2.80 bits per dim (2.798\u00b1 0.004 over seeds 1, 2, 3) versus the previous 2.85 state of the art (Chen et al., 2017). We also compare performance of different attention patterns in Table 2. The strided attention reaches the lowest error in the shortest amount of time, surpassing the error of dense attention at 2.82 bits per dim.\n7.2. Text. In order to assess Sparse Transformers on datasets without a strong two-dimensional structure, we trained models on the EnWik8 dataset, which represents the first 108 bytes of Wikipedia and contains a great degree of variability in periodic structure. We trained with a context length of 12,288, which is longer than previous approaches. We trained on the first 90 million tokens and reserved the last 10 million for validation and test. We used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout rate of 0.40. We trained for 80 epochs until validation loss stopped decreasing. We used a stride of 128, c = 32, and merged the factorized attention heads. Our best model reached 0.99 bits per dim (0.992 \u00b1 0.001 over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for a similarly-sized Transformer-XL (Dai et al., 2018) and matching the 0.99 of a model trained with more than double\nthe number of parameters. Strided attention failed to do well on this dataset, whereas fixed patterns were able to recover and surpass the performance of dense attention, as listed in Table 2. Additionally, during evaluation of the test set, we modified the minimum context length the network could use by evaluating fewer tokens in parallel. We saw monotonic increases in performance with more tokens used, up to 12,160 out of the 12,288 tokens used for training (see Table 3), which suggests the network is effectively incorporating long-term dependencies.\n7.3. ImageNet 64x64.", "We use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which we found important for model stability. We use a weight decay penalty of 0.01. We annealed the learning rate according to a cosine decay as in (Radford et al., 2018). We train on 8 V100 GPUs unless otherwise noted. All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless we use \u201chalf-size\u201d transformations, where it is 2d. Additionally, sometimes we halve the size of the query and key transformations. We initialize the token embeddingWe fromN (0, 0.125\u221ad ) and the position embeddings from N (0, 0.125\u221a\ndnemb ). Within the\nattention and feedforward components, all biases are initial-\nized to 0 and all weights are initialized from N (0, 0.125\u221a din ) where din is the fan-in dimension. The weight matrix for the output logits was initialized to 0. 7. Experiments. We empirically test our architecture on density modeling tasks including natural images, text, and raw audio. A summary of the results is available in Table 1. We found that, in addition to running significantly faster than full attention, sparse patterns also converged to lower error, as shown in Table 2. This may point to a useful inductive bias from the sparsity patterns we introduced, or an underlying optimization issue with full attention.\n7.1. CIFAR-10. We train strided Sparse Transformers on CIFAR-10 images represented as sequences of 3072 bytes. Models have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs with a learning rate of 0.00035 and a dropout rate of 0.25 until validation error stops decreasing. We use 48000 examples for training and 2000 examples for validation, evaluating the performance of our best models on\nthe test set."]}
{"pkey": "sparsetransformer_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "There is no pre-traning section specifically:\nFor images, the paper authors used data embeddings, where ddata = 3 for the row, column, and channel location of each input byte. For text and audio, the paper authors used two-dimensional attention embeddings, where dattn = 2 and the index corresponds to each position\u2019s row and column index in a matrix of width equal to the stride.\n\nAll embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless the paper authors use \u201chalf-size\u201d transformations, where it is 2d. Additionally, sometimes the paper authors halve the size of the query and key transformations.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["The model achieves 2.80 bits per dim (2.798\u00b1 0.004 over seeds 1, 2, 3) versus the previous 2.85 state of the art (Chen et al., 2017). We also compare performance of different attention patterns in Table 2. The strided attention reaches the lowest error in the shortest amount of time, surpassing the error of dense attention at 2.82 bits per dim.\n7.2. Text. In order to assess Sparse Transformers on datasets without a strong two-dimensional structure, we trained models on the EnWik8 dataset, which represents the first 108 bytes of Wikipedia and contains a great degree of variability in periodic structure. We trained with a context length of 12,288, which is longer than previous approaches. We trained on the first 90 million tokens and reserved the last 10 million for validation and test. We used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout rate of 0.40. We trained for 80 epochs until validation loss stopped decreasing. We used a stride of 128, c = 32, and merged the factorized attention heads. Our best model reached 0.99 bits per dim (0.992 \u00b1 0.001 over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for a similarly-sized Transformer-XL (Dai et al., 2018) and matching the 0.99 of a model trained with more than double\nthe number of parameters. Strided attention failed to do well on this dataset, whereas fixed patterns were able to recover and surpass the performance of dense attention, as listed in Table 2. Additionally, during evaluation of the test set, we modified the minimum context length the network could use by evaluating fewer tokens in parallel. We saw monotonic increases in performance with more tokens used, up to 12,160 out of the 12,288 tokens used for training (see Table 3), which suggests the network is effectively incorporating long-term dependencies.\n7.3. ImageNet 64x64.", "In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to \u03b8. A simple and powerful choice for model \u03b8 is a Transformer (Vaswani et al., 2017) in decoder-only mode, as demonstrated by (Radford et al., 2018) and (Liu et al., 2018). These models transform the input sequence with blocks of multihead self-attention over the entire sequence, followed by dense transformations over each sequence element. The selfattention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows. In the following sections, we describe our modifications to the Transformer architecture which make it more suitable for modeling long sequences. 4. Factorized Self-Attention. Sparse Transformers separate the full self-attention operation across several steps of attention, as visualized in Figure 3(b) and 3(c). To motivate our approach, we first perform a qualitative assessment of attention patterns learned by a standard Transformer on an image dataset.\n4.1. Qualitative assessment of learned attention patterns. We visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in Figure 2. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers (Figure 2c) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity (Figure 2d), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices."]}
{"pkey": "sparsetransformer_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "CIFAR-10\nModels have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs\n\nEnWik8\nThe paper authors used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout\nrate of 0.40. The paper authors trained for 80 epochs until validation loss\nstopped decreasing\n\nImageNet 64x64\nThe paper authors used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters", "title": "Generating Long Sequences with Sparse Transformers", "context": ["In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "We scale the initialization of W2 and Wp in Eq. 5 by 1\u221a2N to keep the ratio of input embedding scale to residual block scale invariant across values of N .\n5.3. Modeling diverse data types. In addition to the embedding of input symbols, positional embeddings are typically used in Transformers and other location-agnostic architectures to encode the spatial relationships of data (Gehring et al., 2017), (Parmar et al., 2018). We found using learned embeddings which either encoded the structure of the data or the factorized attention patterns were important for performance of our models. We added either nemb = ddata or nemb = dattn embeddings to each input location, where ddata refers to the number of dimensions of the data, and dattn is the number of dimensions of the factorized attention. If xi is the one-hot encoded ith element in the sequence, and o(j)i represents the one-hot encoded position of xi in the jth dimension (1 \u2264 j \u2264 nemb), then:\nembed(X,We) = xiWe + nemb\u2211 j=1 o (j) i Wj  xi\u2208X (15) For images, we used data embeddings, where ddata = 3 for the row, column, and channel location of each input byte. For text and audio, we used two-dimensional attention embeddings, where dattn = 2 and the index corresponds to each position\u2019s row and column index in a matrix of width equal to the stride. 5.4. Saving memory by recomputing attention weights. Gradient checkpointing has been shown to be effective in reducing the memory requirements of training deep neural networks (Chen et al., 2016), (Gruslys et al., 2016). It is worth noting, however, that this technique is particularly effective for self-attention layers when long sequences are processed, as memory usage is high for these layers relative to the cost of computing them. Using recomputation alone, we are able to train dense attention networks with hundreds of layers on sequence lengths of 16,384, which would be infeasible on modern hardware otherwise.", "Concretely, if the stride is 128 and c = 8, then all future positions greater than 128 can attend to positions 120-128, all positions greater than 256 can attend to 248-256, and so forth. A fixed-attention pattern with c = 1 limits the expressivity of the network significantly, as many representations in\nthe network are only used for one block whereas a small number of locations are used by all blocks. We instead found choosing c \u2208 {8, 16, 32} for typical values of l \u2208 {128, 256} to perform well, although it should be noted that this increases the computational cost of this method by c in comparison to the strided attention. Additionally, we found that when using multiple heads, having them attend to distinct subblocks of length c within the block of size l was preferable to having them attend to the same subblock. In the subsequent section, we describe how to incorporate factorized attention into the Sparse Transformer architecture. 5. Sparse Transformer. Here we fully describe the Sparse Transformer architecture, which is a modified version of the Transformer (Vaswani et al., 2017). 5.1. Factorized attention heads. Standard dense attention simply performs a linear transformation of the attend function defined in Equation 2:\nattention(X) = Wp \u00b7 attend(X,S) (5)\nwhere Wp denotes the post-attention weight matrix. The simplest technique for integrating factorized self-attention is to use one attention type per residual block, and interleave them sequentially or at a ratio determined as a hyperparameter:\nattention(X) = Wp \u00b7 attend(X,A(r mod p)) (6) Here r is the index of the current residual block and p is the number of factorized attention heads. A second approach is to have a single head attend to the locations of the pixels that both factorized heads would attend to, which we call a merged head:\nattention(X) = Wp \u00b7 attend(X, p\u22c3\nm=1\nA(m)) (7) This is slightly more computationally intensive, but only by a constant factor."]}
{"pkey": "sparsetransformer_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which the paper authors found important for model stability. The paper authors use a weight decay penalty of 0.01. The paper authors annealed the learning rate according to a cosine decay as in (Radford et al., 2018). The paper authors train on 8 V100 GPUs unless otherwise noted. All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless the paper authors use \u201chalf-size\u201d transformations, where it is 2d.  The weight matrix for the output logits was initialized to 0.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["We use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which we found important for model stability. We use a weight decay penalty of 0.01. We annealed the learning rate according to a cosine decay as in (Radford et al., 2018). We train on 8 V100 GPUs unless otherwise noted. All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless we use \u201chalf-size\u201d transformations, where it is 2d. Additionally, sometimes we halve the size of the query and key transformations. We initialize the token embeddingWe fromN (0, 0.125\u221ad ) and the position embeddings from N (0, 0.125\u221a\ndnemb ). Within the\nattention and feedforward components, all biases are initial-\nized to 0 and all weights are initialized from N (0, 0.125\u221a din ) where din is the fan-in dimension. The weight matrix for the output logits was initialized to 0. 7. Experiments. We empirically test our architecture on density modeling tasks including natural images, text, and raw audio. A summary of the results is available in Table 1. We found that, in addition to running significantly faster than full attention, sparse patterns also converged to lower error, as shown in Table 2. This may point to a useful inductive bias from the sparsity patterns we introduced, or an underlying optimization issue with full attention.\n7.1. CIFAR-10. We train strided Sparse Transformers on CIFAR-10 images represented as sequences of 3072 bytes. Models have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs with a learning rate of 0.00035 and a dropout rate of 0.25 until validation error stops decreasing. We use 48000 examples for training and 2000 examples for validation, evaluating the performance of our best models on\nthe test set.", "The model achieves 2.80 bits per dim (2.798\u00b1 0.004 over seeds 1, 2, 3) versus the previous 2.85 state of the art (Chen et al., 2017). We also compare performance of different attention patterns in Table 2. The strided attention reaches the lowest error in the shortest amount of time, surpassing the error of dense attention at 2.82 bits per dim.\n7.2. Text. In order to assess Sparse Transformers on datasets without a strong two-dimensional structure, we trained models on the EnWik8 dataset, which represents the first 108 bytes of Wikipedia and contains a great degree of variability in periodic structure. We trained with a context length of 12,288, which is longer than previous approaches. We trained on the first 90 million tokens and reserved the last 10 million for validation and test. We used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout rate of 0.40. We trained for 80 epochs until validation loss stopped decreasing. We used a stride of 128, c = 32, and merged the factorized attention heads. Our best model reached 0.99 bits per dim (0.992 \u00b1 0.001 over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for a similarly-sized Transformer-XL (Dai et al., 2018) and matching the 0.99 of a model trained with more than double\nthe number of parameters. Strided attention failed to do well on this dataset, whereas fixed patterns were able to recover and surpass the performance of dense attention, as listed in Table 2. Additionally, during evaluation of the test set, we modified the minimum context length the network could use by evaluating fewer tokens in parallel. We saw monotonic increases in performance with more tokens used, up to 12,160 out of the 12,288 tokens used for training (see Table 3), which suggests the network is effectively incorporating long-term dependencies.\n7.3. ImageNet 64x64.", "The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to \u03b8. A simple and powerful choice for model \u03b8 is a Transformer (Vaswani et al., 2017) in decoder-only mode, as demonstrated by (Radford et al., 2018) and (Liu et al., 2018). These models transform the input sequence with blocks of multihead self-attention over the entire sequence, followed by dense transformations over each sequence element. The selfattention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows. In the following sections, we describe our modifications to the Transformer architecture which make it more suitable for modeling long sequences. 4. Factorized Self-Attention. Sparse Transformers separate the full self-attention operation across several steps of attention, as visualized in Figure 3(b) and 3(c). To motivate our approach, we first perform a qualitative assessment of attention patterns learned by a standard Transformer on an image dataset.\n4.1. Qualitative assessment of learned attention patterns. We visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in Figure 2. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers (Figure 2c) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity (Figure 2d), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices."]}
{"pkey": "sparsetransformer_13", "question": "Describe the computational resources used to train the model.", "answer": "Mixed-precision training: The paper authors store network weights in single-precision floating-point, but otherwise compute network activations and gradients in half-precision, as in (Micikevicius et al., 2017). This accelerates our training due to the usage of Tensor Core operations on the V100 GPU. During the gradient calculation, the paper authors use dynamic loss scaling to reduce numerical underflow, and the paper authors communicate half-precision gradients when averaging across multiple GPUs. When sampling, the paper authors cast the queries and keys to single-precision, as the query-key product can sometimes overflow the max value of half-precision.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to \u03b8. A simple and powerful choice for model \u03b8 is a Transformer (Vaswani et al., 2017) in decoder-only mode, as demonstrated by (Radford et al., 2018) and (Liu et al., 2018). These models transform the input sequence with blocks of multihead self-attention over the entire sequence, followed by dense transformations over each sequence element. The selfattention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows. In the following sections, we describe our modifications to the Transformer architecture which make it more suitable for modeling long sequences. 4. Factorized Self-Attention. Sparse Transformers separate the full self-attention operation across several steps of attention, as visualized in Figure 3(b) and 3(c). To motivate our approach, we first perform a qualitative assessment of attention patterns learned by a standard Transformer on an image dataset.\n4.1. Qualitative assessment of learned attention patterns. We visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in Figure 2. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers (Figure 2c) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity (Figure 2d), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices.", "Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\u221a n).", "In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz."]}
{"pkey": "sparsetransformer_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "The pseudo codes are available in the paper:\n1)  Factorized attention head\n2)  Scaling to hundreds of layers\n3)  Modeling diverse data types\n4)  Saving memory by recomputing attention weights\n5)  Efficient block-sparse attention kernels\n6)  Mixed-precision training\nAlso, the datasets are mentioned along with architecture is mentioned:\nThe paper authors empirically validate that models augmented in this manner can achieve state-of-the-art compression and generation of natural language, raw audio, and natural images.\nImplementation details:\nThis accelerates our training due to the usage of Tensor Core operations on the V100 GPU.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["In this paper, we restricted our investigation to a class of sparse attention patterns that have connectivity between all positions over several steps of attention. These methods can be more efficient than full attention while still providing global context to any given position. We aimed to empirically validate the performance of these factorized patterns on a range of tasks, given that they are unable to learn the exact same mappings as those in Figure 2. We present the formulation of factorized attention below. 4.2. Factorized self-attention. A self-attention layer maps a matrix of input embeddings X to an output matrix and is parameterized by a connectivity pattern S = {S1, ..., Sn}, where Si denotes the set of indices of the input vectors to which the ith output vector attends. The output vector is a weighted sum of transformations of the input vectors:\nAttend(X,S) = ( a(xi, Si) ) i\u2208{1,...,n}\n(2)\na(xi, Si) = softmax\n( (Wqxi)K\nT Si\u221a\nd\n) VSi (3)\nKSi = ( Wkxj ) j\u2208Si VSi = ( Wvxj ) j\u2208Si\n(4) Here Wq , Wk, and Wv represent the weight matrices which transform a given xi into a query, key, or value, and d is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries. Full self-attention for autoregressive models defines Si = {j : j \u2264 i}, allowing every element to attend to all previous positions and its own position. Factorized self-attention instead has p separate attention heads, where the mth head defines a subset of the indices A\n(m) i \u2282 {j : j \u2264 i} and lets Si = A (m) i . We are chiefly interested in efficient choices for the subset A, where |A(m)i | \u221d p \u221a n.\nAdditionally, for the time being we consider valid choices of A, where all input positions are connected to all future output positions across the p steps of attention. For every j \u2264 i pair, we set every A such that i can attend to j through a path of locations with maximum length p+ 1.", "Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\u221a n).", "In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz."]}
{"pkey": "sparsetransformer_15", "question": "What is the pretraining objective of the model? ", "answer": "Mixed-precision training: The paper authors store network weights in single-precision floating-point, but otherwise compute network activations and gradients in half-precision, as in (Micikevicius et al., 2017). This accelerates our training due to the usage of Tensor Core operations on the V100 GPU. The relevant details from pretraining section are presented next. The paper authors initialize the token embedding The paper authors from N (0,0.125/\u221ad) and the position embeddings from N (0, 0.125\u221adnemb). Within the attention and feedforward components,", "title": "Generating Long Sequences with Sparse Transformers", "context": ["The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to \u03b8. A simple and powerful choice for model \u03b8 is a Transformer (Vaswani et al., 2017) in decoder-only mode, as demonstrated by (Radford et al., 2018) and (Liu et al., 2018). These models transform the input sequence with blocks of multihead self-attention over the entire sequence, followed by dense transformations over each sequence element. The selfattention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows. In the following sections, we describe our modifications to the Transformer architecture which make it more suitable for modeling long sequences. 4. Factorized Self-Attention. Sparse Transformers separate the full self-attention operation across several steps of attention, as visualized in Figure 3(b) and 3(c). To motivate our approach, we first perform a qualitative assessment of attention patterns learned by a standard Transformer on an image dataset.\n4.1. Qualitative assessment of learned attention patterns. We visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in Figure 2. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers (Figure 2c) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity (Figure 2d), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices.", "In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "We use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which we found important for model stability. We use a weight decay penalty of 0.01. We annealed the learning rate according to a cosine decay as in (Radford et al., 2018). We train on 8 V100 GPUs unless otherwise noted. All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless we use \u201chalf-size\u201d transformations, where it is 2d. Additionally, sometimes we halve the size of the query and key transformations. We initialize the token embeddingWe fromN (0, 0.125\u221ad ) and the position embeddings from N (0, 0.125\u221a\ndnemb ). Within the\nattention and feedforward components, all biases are initial-\nized to 0 and all weights are initialized from N (0, 0.125\u221a din ) where din is the fan-in dimension. The weight matrix for the output logits was initialized to 0. 7. Experiments. We empirically test our architecture on density modeling tasks including natural images, text, and raw audio. A summary of the results is available in Table 1. We found that, in addition to running significantly faster than full attention, sparse patterns also converged to lower error, as shown in Table 2. This may point to a useful inductive bias from the sparsity patterns we introduced, or an underlying optimization issue with full attention.\n7.1. CIFAR-10. We train strided Sparse Transformers on CIFAR-10 images represented as sequences of 3072 bytes. Models have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs with a learning rate of 0.00035 and a dropout rate of 0.25 until validation error stops decreasing. We use 48000 examples for training and 2000 examples for validation, evaluating the performance of our best models on\nthe test set."]}
{"pkey": "sparsetransformer_16", "question": "What is the loss function that is used to train the model?", "answer": "The paper authors treat images, text, and audio as a sequence of discrete tokens, typically raw bytes. The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to \u03b8.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["A third approach is to use multi-head attention (Vaswani et al., 2017), where nh attention products are computed in parallel, then concatenated along the feature dimension:\nattention(X) = Wp\n( attend(X,A)(i) ) i\u2208{1,...,nh} (8)\nHere, the A can be the separate attention patterns, the merged patterns, or interleaved as in Eq. 2. Also, the dimensions of the weight matrices inside the attend function are reduced by a factor of 1/nh, such that the number of parameters are invariant across values of nh. We typically find multiple heads to work well, though for extremely long sequences where the attention dominates the computation time, it is more worthwhile to perform them one at a time and sequentially.\n5.2. Scaling to hundreds of layers. We found that Transformers were difficult to train with many layers, as noted by (Al-Rfou et al., 2018). Instead of incorporating auxillary losses, we adopted the following\narchitectural changes. First, we use the pre-activation residual block of (He et al., 2016), defining a network of N layers in the following way:\nH0 = embed(X,We) (9) Hk = Hk\u22121 + resblock(Hk\u22121) (10) y = softmax(norm(HN )Wout) (11)\nwhere embed is a function we describe in the next section, Wout is a weight matrix, and resblock(h) normalizes the input to the attention block and a positionwise feedforward network in the following way:\na(H) = dropout(attention(norm(H))) (12)\nb(H) = dropout(ff(norm(H + a(H)))) (13)\nresblock(H) = a(H) + b(H) (14) The norm function denotes Layer Normalization (Ba et al., 2016), and ff(x) = W2 f(W1x + b1) + b2. Our choice of f is the Gaussian Error Linear Unit (Hendrycks & Gimpel, 2016), f(X) = X sigmoid(1.702 \u00b7X), as used in (Radford et al., 2018). The output dimension of W1 is 4.0 times the input dimension, unless otherwise noted. Observe that HN is the sum of N applications of functions a and b, and thus each function block receives a gradient directly from the output layer .", "In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to \u03b8. A simple and powerful choice for model \u03b8 is a Transformer (Vaswani et al., 2017) in decoder-only mode, as demonstrated by (Radford et al., 2018) and (Liu et al., 2018). These models transform the input sequence with blocks of multihead self-attention over the entire sequence, followed by dense transformations over each sequence element. The selfattention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows. In the following sections, we describe our modifications to the Transformer architecture which make it more suitable for modeling long sequences. 4. Factorized Self-Attention. Sparse Transformers separate the full self-attention operation across several steps of attention, as visualized in Figure 3(b) and 3(c). To motivate our approach, we first perform a qualitative assessment of attention patterns learned by a standard Transformer on an image dataset.\n4.1. Qualitative assessment of learned attention patterns. We visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in Figure 2. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers (Figure 2c) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity (Figure 2d), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices."]}
{"pkey": "sparsetransformer_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "Additionally, the paper authors introduce several other changes to the Transformer, including:\n\u2022 A restructured residual block and weight initialization to improve training of very deep networks\n\u2022 A set of sparse attention kernels which efficiently compute subsets of the attention matrix\n\u2022 Recomputation of attention weights during the backwards pass to reduce memory usage\nSparse Transformers separate the full self-attention operation across several steps of attention, as visualized in Figure 3(b) and 3(c).", "title": "Generating Long Sequences with Sparse Transformers", "context": ["We also\nintroduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. 1. Introduction. Estimating complex, high-dimensional data distributions is a central problem in unsupervised learning, as many downstream applications of interest involve generation of text, images, audio, and other data. Additionally, it is believed to be a key component of unsupervised representation learning. Recently, neural autoregressive models have achieved impressive results in this domain, achieving state-of-the-art in modeling natural language (Jozefowicz et al., 2016) (Radford et al., 2018) (Dai et al., 2018), raw audio (Van Den Oord et al., 2016) (Mehri et al., 2016), and images (Oord et al., 2016) (Menick & Kalchbrenner, 2018) (Salimans et al., 2017) (Reed et al., 2017) (Chen et al., 2017). These methods decompose a joint probability distribution into a product of conditional ones. Modeling these conditional distributions is extremely challenging, however, as they contain many complex, long-range dependencies and require a suitably expressive model architecture to learn them. Architectures based off CNNs (Oord et al., 2016) have made\ngreat progress in this direction, but require significant depth to expand their receptive field.", "The most related work involves other techniques for scaling up autoregressive generative models. For images, (Reed et al., 2017) models conditional independence between the pixels in order to generate many locations in parallel, and (Menick & Kalchbrenner, 2018) imposes an ordering and multi-scale upsampling procedure to generate high fidelity samples. (Parmar et al., 2018) uses blocks of local attention to apply Transformers to images. For text, (Dai et al., 2018) introduces a state reuse \u201dmemory\u201d for modeling long-term dependencies. And for audio, in addition to (Van Den Oord et al., 2016), (Mehri et al., 2016) used a hierarchical structure and RNNs of varying clock-rates to use long contexts during inference, similar to (Koutnik et al., 2014). (Huang et al., 2018) apply Transformers to MIDI generation with an efficient relative attention. Our work is simpler than many of the techniques above and can be applied equally across images, text, and audio. Many of the above techniques are orthogonal to ours, moreover, and could be used in conjunction with ours. Outside of generative modeling, there are several works relevant to improving the efficiency of attention based off chunking (Chiu & Raffel, 2017) or using fixed length representations (Britz et al., 2017). Other works have investigated attention with multiple \u201dhops\u201d, such as (Sukhbaatar et al., 2015) and (Gehring et al., 2017). It is worth noting that the Gated Pixel CNN (Oord et al., 2016) and WaveNet (Van Den Oord et al., 2016) use multiplicative interactions in their networks, which are related to self-attention. 3. Background. We consider the task of autoregressive sequence generation, where the joint probability of a sequence x = {x1, x2, ..., xn} is modeled as the product of conditional probability distributions and parameterized by a network \u03b8.\np(x) = n\u220f i=1 p(xi|x1, ..., xi\u22121; \u03b8) (1) We treat images, text, and audio as a sequence of discrete tokens, typically raw bytes.", "The network \u03b8 takes in the sequence of tokens and outputs a categorical distribution over the v possible values of the next token using the softmax function, where v is the size of the vocabulary. The training objective is to maximize the log-probability of the data with respect to \u03b8. A simple and powerful choice for model \u03b8 is a Transformer (Vaswani et al., 2017) in decoder-only mode, as demonstrated by (Radford et al., 2018) and (Liu et al., 2018). These models transform the input sequence with blocks of multihead self-attention over the entire sequence, followed by dense transformations over each sequence element. The selfattention portion of the network must compute n weightings for each of n elements, however, which can quickly become intractable as the sequence length grows. In the following sections, we describe our modifications to the Transformer architecture which make it more suitable for modeling long sequences. 4. Factorized Self-Attention. Sparse Transformers separate the full self-attention operation across several steps of attention, as visualized in Figure 3(b) and 3(c). To motivate our approach, we first perform a qualitative assessment of attention patterns learned by a standard Transformer on an image dataset.\n4.1. Qualitative assessment of learned attention patterns. We visualized the attention patterns learned by a 128-layer self-attention network on CIFAR-10, and present several examples in Figure 2. Visual inspection showed that most layers had sparse attention patterns across most data points, suggesting that some form of sparsity could be introduced without significantly affecting performance. Several layers (Figure 2c) clearly exhibited global patterns, however, and others exhibited data-dependent sparsity (Figure 2d), both of which would be impacted by introducing a predetermined sparsity pattern into all of the attention matrices."]}
{"pkey": "sparsetransformer_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["We use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which we found important for model stability. We use a weight decay penalty of 0.01. We annealed the learning rate according to a cosine decay as in (Radford et al., 2018). We train on 8 V100 GPUs unless otherwise noted. All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless we use \u201chalf-size\u201d transformations, where it is 2d. Additionally, sometimes we halve the size of the query and key transformations. We initialize the token embeddingWe fromN (0, 0.125\u221ad ) and the position embeddings from N (0, 0.125\u221a\ndnemb ). Within the\nattention and feedforward components, all biases are initial-\nized to 0 and all weights are initialized from N (0, 0.125\u221a din ) where din is the fan-in dimension. The weight matrix for the output logits was initialized to 0. 7. Experiments. We empirically test our architecture on density modeling tasks including natural images, text, and raw audio. A summary of the results is available in Table 1. We found that, in addition to running significantly faster than full attention, sparse patterns also converged to lower error, as shown in Table 2. This may point to a useful inductive bias from the sparsity patterns we introduced, or an underlying optimization issue with full attention.\n7.1. CIFAR-10. We train strided Sparse Transformers on CIFAR-10 images represented as sequences of 3072 bytes. Models have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs with a learning rate of 0.00035 and a dropout rate of 0.25 until validation error stops decreasing. We use 48000 examples for training and 2000 examples for validation, evaluating the performance of our best models on\nthe test set.", "In this paper, we restricted our investigation to a class of sparse attention patterns that have connectivity between all positions over several steps of attention. These methods can be more efficient than full attention while still providing global context to any given position. We aimed to empirically validate the performance of these factorized patterns on a range of tasks, given that they are unable to learn the exact same mappings as those in Figure 2. We present the formulation of factorized attention below. 4.2. Factorized self-attention. A self-attention layer maps a matrix of input embeddings X to an output matrix and is parameterized by a connectivity pattern S = {S1, ..., Sn}, where Si denotes the set of indices of the input vectors to which the ith output vector attends. The output vector is a weighted sum of transformations of the input vectors:\nAttend(X,S) = ( a(xi, Si) ) i\u2208{1,...,n}\n(2)\na(xi, Si) = softmax\n( (Wqxi)K\nT Si\u221a\nd\n) VSi (3)\nKSi = ( Wkxj ) j\u2208Si VSi = ( Wvxj ) j\u2208Si\n(4) Here Wq , Wk, and Wv represent the weight matrices which transform a given xi into a query, key, or value, and d is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries. Full self-attention for autoregressive models defines Si = {j : j \u2264 i}, allowing every element to attend to all previous positions and its own position. Factorized self-attention instead has p separate attention heads, where the mth head defines a subset of the indices A\n(m) i \u2282 {j : j \u2264 i} and lets Si = A (m) i . We are chiefly interested in efficient choices for the subset A, where |A(m)i | \u221d p \u221a n.\nAdditionally, for the time being we consider valid choices of A, where all input positions are connected to all future output positions across the p steps of attention. For every j \u2264 i pair, we set every A such that i can attend to j through a path of locations with maximum length p+ 1.", "In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz."]}
{"pkey": "sparsetransformer_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "There is no mention of ablation study of any of the parameters in the paper.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["In order to test the ability of the model to learn long range dependencies and scale to a large dataset, we train on the version of downsampled ImageNet released by (Oord et al., 2016) and evaluate on the validation set. We used a 48 layer strided Sparse Transformer with 16 attention heads and d = 512, totaling 152 million parameters. We used a stride of 128, a dropout of 0.01, and trained for 70 epochs, which took 7 days on 64 V100 GPUs. Our model achieves a loss of 3.44 bits per dim (3.437 across 1 run), in comparison to the previous 3.52 (Menick & Kalchbrenner, 2018). Additionally, we generate unconditional samples (Figure 5) at an unmodified softmax temperature of 1.0, from the model and from one trained with twice the layers (300M parameters total). We include here samples from the 300M parameter model. On visual assessment we find no artifacts from the sparsity patterns and see evidence of long-term structure in most images. 7.4. Classical music from raw audio. To test the extent to which Sparse Transformers are able to scale to very long contexts, we trained models on the classical music dataset released by (Dieleman et al., 2018). As details of the dataset processing are unavailable, we omit any direct comparison to other work and instead study what size of Sparse Transformer we can train with increasing context size. For each sequence length, we attempted to train the largest model which could entirely fit into 16GB V100 accelerators without model parallelism. Overall, we found that increasing the sequence length by a factor of 4 requires a reduction in model capacity of approximately 4 \u221a 4 = 8. Thus we found we could use factorized self-attention on sequences over 1 million timesteps long, albeit with extremely few parameters (3 million). Samples are available for sequences of length 65,536, which correspond to around 5 seconds of generated audio at 12kHz.", "Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more. Generating Long Sequences with Sparse Transformers. Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to O(n \u221a n). We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.\n\u221a n).", "We use the Adam optimizer with a linear warmup of 5000 iterations and a gradient clipping of 1.0, both of which we found important for model stability. We use a weight decay penalty of 0.01. We annealed the learning rate according to a cosine decay as in (Radford et al., 2018). We train on 8 V100 GPUs unless otherwise noted. All embeddings are of a constant dimension d, usually one of {256, 512, 1024}. By default, all linear transforms are to the same dimension, with the exception of the feed-forward network, which projects the input to 4d, unless we use \u201chalf-size\u201d transformations, where it is 2d. Additionally, sometimes we halve the size of the query and key transformations. We initialize the token embeddingWe fromN (0, 0.125\u221ad ) and the position embeddings from N (0, 0.125\u221a\ndnemb ). Within the\nattention and feedforward components, all biases are initial-\nized to 0 and all weights are initialized from N (0, 0.125\u221a din ) where din is the fan-in dimension. The weight matrix for the output logits was initialized to 0. 7. Experiments. We empirically test our architecture on density modeling tasks including natural images, text, and raw audio. A summary of the results is available in Table 1. We found that, in addition to running significantly faster than full attention, sparse patterns also converged to lower error, as shown in Table 2. This may point to a useful inductive bias from the sparsity patterns we introduced, or an underlying optimization issue with full attention.\n7.1. CIFAR-10. We train strided Sparse Transformers on CIFAR-10 images represented as sequences of 3072 bytes. Models have 2 heads, 128 layers, d = 256, half-size feedforward network and query-key projections, and are trained for 120 epochs with a learning rate of 0.00035 and a dropout rate of 0.25 until validation error stops decreasing. We use 48000 examples for training and 2000 examples for validation, evaluating the performance of our best models on\nthe test set."]}
{"pkey": "sparsetransformer_20", "question": "List the future work mentioned in the paper.", "answer": "The authors mention that their work is simpler than many of the existing techniques and can be applied equally across images, text, and audio. Many of the existing techniques are orthogonal to their, moreover, and could be used in conjunction with this paper's work.", "title": "Generating Long Sequences with Sparse Transformers", "context": ["The model achieves 2.80 bits per dim (2.798\u00b1 0.004 over seeds 1, 2, 3) versus the previous 2.85 state of the art (Chen et al., 2017). We also compare performance of different attention patterns in Table 2. The strided attention reaches the lowest error in the shortest amount of time, surpassing the error of dense attention at 2.82 bits per dim.\n7.2. Text. In order to assess Sparse Transformers on datasets without a strong two-dimensional structure, we trained models on the EnWik8 dataset, which represents the first 108 bytes of Wikipedia and contains a great degree of variability in periodic structure. We trained with a context length of 12,288, which is longer than previous approaches. We trained on the first 90 million tokens and reserved the last 10 million for validation and test. We used 30-layer fixed Sparse Transformers with 8 heads, d = 512, and a dropout rate of 0.40. We trained for 80 epochs until validation loss stopped decreasing. We used a stride of 128, c = 32, and merged the factorized attention heads. Our best model reached 0.99 bits per dim (0.992 \u00b1 0.001 over seeds 1, 2, 3), surpassing the 1.03 state-of-the-art for a similarly-sized Transformer-XL (Dai et al., 2018) and matching the 0.99 of a model trained with more than double\nthe number of parameters. Strided attention failed to do well on this dataset, whereas fixed patterns were able to recover and surpass the performance of dense attention, as listed in Table 2. Additionally, during evaluation of the test set, we modified the minimum context length the network could use by evaluating fewer tokens in parallel. We saw monotonic increases in performance with more tokens used, up to 12,160 out of the 12,288 tokens used for training (see Table 3), which suggests the network is effectively incorporating long-term dependencies.\n7.3. ImageNet 64x64.", "Specifically, if (j, a, b, c, ..., i) is the path of indices, then j \u2208 A(1)a , a \u2208 A(2)b , b \u2208 A (3) c , and so forth. These two criteria allow us keep the ability of Transformers to propagate signals from arbitrary input positions to arbitrary output positions in a constant number of steps, while reducing the total effective computation to O(n p \u221a n). We also note that softening the validity criterion (for instance, having a series of only locally connected layers) may be a useful inductive bias for certain domains. In this work, we explore two factorizations for p = 2, which we describe in the following section, though we note that the same techniques can be easily extended to higher dimensions. 4.3. Two-dimensional factorized attention. A natural approach to defining a factorized attention pattern in two dimensions is to have one head attend to the previous l locations, and the other head attend to every lth location, where l is the stride and chosen to be close to \u221a n, a method we call strided attention. Formally, A(1)i = {t, t + 1, ..., i} for t = max(0, i \u2212 l) and A(2)i = {j : (i \u2212 j) mod l = 0}. This pattern can be visualized in Figure 3(b). This formulation is convenient if the data naturally has a structure that aligns with the stride, like images or some types of music. For data without a periodic structure, like text, however, we find that the network can fail to properly route information with the strided pattern, as spatial coordinates for an element do not necessarily correlate with the positions where the element may be most relevant in the future. In those cases, we instead use a fixed attention pattern (Figure 3(c)), where specific cells summarize previous locations and propagate that information to all future cells. Formally,A(1)i = {j : (bj/lc = bi/lc)}, where the brackets denote the floor operation, and A(2)i = {j : j mod l \u2208 {t, t+ 1, ..., l}, where t = l \u2212 c and c is a hyperparameter.", "In this paper, we restricted our investigation to a class of sparse attention patterns that have connectivity between all positions over several steps of attention. These methods can be more efficient than full attention while still providing global context to any given position. We aimed to empirically validate the performance of these factorized patterns on a range of tasks, given that they are unable to learn the exact same mappings as those in Figure 2. We present the formulation of factorized attention below. 4.2. Factorized self-attention. A self-attention layer maps a matrix of input embeddings X to an output matrix and is parameterized by a connectivity pattern S = {S1, ..., Sn}, where Si denotes the set of indices of the input vectors to which the ith output vector attends. The output vector is a weighted sum of transformations of the input vectors:\nAttend(X,S) = ( a(xi, Si) ) i\u2208{1,...,n}\n(2)\na(xi, Si) = softmax\n( (Wqxi)K\nT Si\u221a\nd\n) VSi (3)\nKSi = ( Wkxj ) j\u2208Si VSi = ( Wvxj ) j\u2208Si\n(4) Here Wq , Wk, and Wv represent the weight matrices which transform a given xi into a query, key, or value, and d is the inner dimension of the queries and keys. The output at each position is a sum of the values weighted by the scaled dot-product similarity of the keys and queries. Full self-attention for autoregressive models defines Si = {j : j \u2264 i}, allowing every element to attend to all previous positions and its own position. Factorized self-attention instead has p separate attention heads, where the mth head defines a subset of the indices A\n(m) i \u2282 {j : j \u2264 i} and lets Si = A (m) i . We are chiefly interested in efficient choices for the subset A, where |A(m)i | \u221d p \u221a n.\nAdditionally, for the time being we consider valid choices of A, where all input positions are connected to all future output positions across the p steps of attention. For every j \u2264 i pair, we set every A such that i can attend to j through a path of locations with maximum length p+ 1."]}
{"pkey": "ernie_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "The existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. The paper authors argue that informative entities in KGs can enhance language representation with external knowledge. The paper authors further utilize both corpora and KGs to train an enhanced language representation model based on BERT.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Hence, considering rich knowledge information can lead to better language understanding and accordingly benefits various knowledge-driven applications, e.g. entity typing and relation classification. For incorporating external knowledge into language representation models, there are two main\nar X\niv :1\n90 5.\n07 12\n9v 3\n[ cs\n.C L\n] 4\nJ un\n2 01\n9\nchallenges. (1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces. How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge. To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:\n(1) For extracting and encoding knowledge information, we firstly recognize named entity mentions in text and then align these mentions to their corresponding entities in KGs. Instead of directly using the graph-based facts in KGs, we encode the graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013), and then take the informative entity embeddings as input for ERNIE. Based on the alignments between text and KGs, ERNIE integrates entity representations in the knowledge module into the underlying layers of the semantic module. (2) Similar to BERT, we adopt the masked language model and the next sentence prediction as the pre-training objectives.", "In conclusion, we find that the pre-trained language models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller\ntraining set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC. In short, ERNIE achieves comparable results with BERTBASE on GLUE. On the one hand, it means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion.\n4.6 Ablation Study. In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively.", "Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1. In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3 Methodology. In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5. 1It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\n3.1 Notations. We denote a token sequence as {w1, . . . , wn} 2, where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1, . . . , em}, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V , and the entity list containing all entities in KGs as E . If a token w \u2208 V has a corresponding entity e \u2208 E , their alignment is defined as f(w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2.\n3.2 Model Architecture."]}
{"pkey": "ernie_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "The existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. This is addressed in the paper.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["As this paper focuses on comparing the general language representation abilities of various neural models, we thus do not use the hand-craft features in this work. UFET. For Open Entity, we add a new hybrid model UFET (Choi et al., 2018) for comparison. UFET is proposed with the Open Entity dataset, which uses a Bi-LSTM for context representation instead of two Bi-LSTMs separated by entity mentions in NFGEC. Besides NFGEC and UFET, we also report the result of fine-tuning BERT with the same input format introduced in Section 3.5 for fair com-\nparison. Following the same evaluation criteria used in the previous work, we compare NFGEC, BERT, ERNIE on FIGER, and adopt strict accuracy, loose macro, loose micro scores for evaluation. We compare NFGEC, BERT, UFET, ERNIE on Open Entity, and adopt precision, recall, microF1 scores for evaluation. The results on FIGER are shown in Table 2. From the results, we observe that: (1) BERT achieves comparable results with NFGEC on the macro and micro metrics. However, BERT has lower accuracy than the best NFGEC model. As strict accuracy is the ratio of instances whose predictions are identical to human annotations, it illustrates some wrong labels from distant supervision are learned by BERT due to its powerful fitting ability. (2) Compared with BERT, ERNIE significantly improves the strict accuracy, indicating the external knowledge regularizes ERNIE to avoid fitting the noisy labels and accordingly benefits entity typing. The results on Open Entity are shown in Table 3. From the table, we observe that: (1) BERT and ERNIE achieve much higher recall scores than the previous entity typing models, which means pre-training language models make full use of both the unsupervised pre-training and manuallyannotated training data for better entity typing. (2) Compared to BERT, ERNIE improves the precision by 2% and the recall by 2%, which means the informative entities help ERNIE predict the labels more precisely.", "4.1 Pre-training Dataset. The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google3 to initialize the Transformer blocks for encoding tokens. Since pre-\n3https://github.com/google-research/bert\ntraining is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly. 4.2 Parameter Settings and Training Details. In this work, we denote the hidden dimension of token embeddings and entity embeddings as Hw, He respectively, and the number of self-attention heads as Aw, Ae respectively. In detail, we have the following model size: N = 6,M = 6, Hw = 768, He = 100, Aw = 12, Ae = 4. The total parameters are about 114M.\nThe total amount of parameters of BERTBASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512.", "To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison. From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%. However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%. (2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective. In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro. The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively. From Table 5, we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy. The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA. (2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification."]}
{"pkey": "ernie_3", "question": "What are the main contributions of the paper?", "answer": "This work utilizes both large-scale textual corpora and KGs to train an enhanced language model. For incorporating external knowledge into language representation models, there are two main challenges. (1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces. How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge. ERNIE can take full advantage of lexical, syntactic, and knowledge information simultaneously.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations. 4.4 Relation Classification. Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TACRED (Zhang et al., 2017). The statistics of these two datasets are shown in Table 4. As the original experimental setting of FewRel is few-shot learning, we rearrange the FewRel dataset for the common relation classification setting. Specifically, we sample 100 instances from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation \u201cno relation\u201d) in TACRED. We compare our model with the following baseline models for relation classification:\nCNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.", "The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE. 1 Introduction. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question\n\u2217 indicates equal contribution \u2020 Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn)\nanswering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin\u2019 in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task. For the existing pre-trained language representation models, these two sentences are syntactically ambiguous, like \u201cUNK wrote UNK in UNK\u201d.", "Hence, considering rich knowledge information can lead to better language understanding and accordingly benefits various knowledge-driven applications, e.g. entity typing and relation classification. For incorporating external knowledge into language representation models, there are two main\nar X\niv :1\n90 5.\n07 12\n9v 3\n[ cs\n.C L\n] 4\nJ un\n2 01\n9\nchallenges. (1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces. How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge. To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:\n(1) For extracting and encoding knowledge information, we firstly recognize named entity mentions in text and then align these mentions to their corresponding entities in KGs. Instead of directly using the graph-based facts in KGs, we encode the graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013), and then take the informative entity embeddings as input for ERNIE. Based on the alignments between text and KGs, ERNIE integrates entity representations in the knowledge module into the underlying layers of the semantic module. (2) Similar to BERT, we adopt the masked language model and the next sentence prediction as the pre-training objectives."]}
{"pkey": "ernie_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "No, the paper focuses on incorporating knowledge from knowledge graphs which could be of any domain.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1. In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3 Methodology. In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5. 1It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\n3.1 Notations. We denote a token sequence as {w1, . . . , wn} 2, where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1, . . . , em}, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V , and the entity list containing all entities in KGs as E . If a token w \u2208 V has a corresponding entity e \u2208 E , their alignment is defined as f(w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2.\n3.2 Model Architecture.", "The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE. 1 Introduction. Pre-trained language representation models, including feature-based (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2017, 2018) and fine-tuning (Dai and Le, 2015; Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019) approaches, can capture rich language information from text and then benefit many NLP applications. BERT (Devlin et al., 2019), as one of the most recently proposed models, obtains the stateof-the-art results on various NLP applications by simple fine-tuning, including named entity recognition (Sang and De Meulder, 2003), question\n\u2217 indicates equal contribution \u2020 Corresponding author: Z.Liu(liuzy@tsinghua.edu.cn)\nanswering (Rajpurkar et al., 2016; Zellers et al., 2018), natural language inference (Bowman et al., 2015), and text classification (Wang et al., 2018). Although pre-trained language representation models have achieved promising results and worked as a routine component in many NLP tasks, they neglect to incorporate knowledge information for language understanding. As shown in Figure 1, without knowing Blowin\u2019 in the Wind and Chronicles: Volume One are song and book respectively, it is difficult to recognize the two occupations of Bob Dylan, i.e., songwriter and writer, on the entity typing task. Furthermore, it is nearly impossible to extract the fine-grained relations, such as composer and author on the relation classification task. For the existing pre-trained language representation models, these two sentences are syntactically ambiguous, like \u201cUNK wrote UNK in UNK\u201d.", "Except for setting the learning rate as 5e\u22125, we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e\u22125, 3e\u22125, 2e\u22125, number of epochs ranging from 3 to 10. We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train-\n4https://www.wikidata.org/\ning epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing. Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing:\nNFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER."]}
{"pkey": "ernie_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "Since pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, the paper authors use English Wikipedia as the pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. FIGER and Open Entity also used.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["In conclusion, we find that the pre-trained language models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller\ntraining set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC. In short, ERNIE achieves comparable results with BERTBASE on GLUE. On the one hand, it means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion.\n4.6 Ablation Study. In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively.", "Except for setting the learning rate as 5e\u22125, we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e\u22125, 3e\u22125, 2e\u22125, number of epochs ranging from 3 to 10. We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train-\n4https://www.wikidata.org/\ning epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing. Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing:\nNFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.", "In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations. 4.4 Relation Classification. Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TACRED (Zhang et al., 2017). The statistics of these two datasets are shown in Table 4. As the original experimental setting of FewRel is few-shot learning, we rearrange the FewRel dataset for the common relation classification setting. Specifically, we sample 100 instances from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation \u201cno relation\u201d) in TACRED. We compare our model with the following baseline models for relation classification:\nCNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification."]}
{"pkey": "ernie_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not mentioned in the paper.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison. From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%. However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%. (2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective. In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro. The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively. From Table 5, we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy. The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA. (2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification.", "The overall pre-training loss is the sum of the dEA, MLM and NSP loss. 3.5 Fine-tuning for Specific Tasks. As shown in Figure 3, for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT. We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:\nFor relation classification, the task requires systems to classify relation labels of given entity pairs based on context. The most straightforward way\nto fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively. The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016; Yaghoobzadeh and Schu\u0308tze, 2017; Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively. 4 Experiments. In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks.", "In conclusion, we find that the pre-trained language models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller\ntraining set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC. In short, ERNIE achieves comparable results with BERTBASE on GLUE. On the one hand, it means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion.\n4.6 Ablation Study. In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively."]}
{"pkey": "ernie_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Not mentioned in the paper.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1. In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3 Methodology. In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5. 1It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\n3.1 Notations. We denote a token sequence as {w1, . . . , wn} 2, where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1, . . . , em}, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V , and the entity list containing all entities in KGs as E . If a token w \u2208 V has a corresponding entity e \u2208 E , their alignment is defined as f(w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2.\n3.2 Model Architecture.", "As shown in Table 7, we have the following observations: (1) Without entity sequence input, dEA still injects knowledge information into language representation during pre-training, which increases the F1 score of BERT by 0.9%. (2) Although the informative entities bring much knowledge informa-\ntion which intuitively benefits relation classification, ERNIE without dEA takes little advantage of this, leading to the F1 increase of 0.7%. 5 Conclusion. In this paper, we propose ERNIE to incorporate knowledge information into language representation models. Accordingly, we propose the knowledgeable aggregator and the pre-training task dEA for better fusion of heterogeneous information from both text and KGs. The experimental results demonstrate that ERNIE has better abilities of both denoising distantly supervised data and fine-tuning on limited data than BERT. There are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from the world knowledge database Wikidata; (3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.\nAcknowledgement. This work is funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFG TRR-169, the National Natural Science Foundation of China (NSFC No. 61572273) and China Association for Science and Technology (2016QNRC001).", "To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison. From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%. However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%. (2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective. In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro. The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively. From Table 5, we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy. The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA. (2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification."]}
{"pkey": "ernie_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "Pre-training is a multi-task procedure consisting of NSP, MLM, and dEA, the paper authors use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, the paper authors adopt the knowledge embeddings trained on Wikidata by TransE as the input embeddings for entities. To be specific, the paper authors sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations. 4.4 Relation Classification. Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TACRED (Zhang et al., 2017). The statistics of these two datasets are shown in Table 4. As the original experimental setting of FewRel is few-shot learning, we rearrange the FewRel dataset for the common relation classification setting. Specifically, we sample 100 instances from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation \u201cno relation\u201d) in TACRED. We compare our model with the following baseline models for relation classification:\nCNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.", "Except for setting the learning rate as 5e\u22125, we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e\u22125, 3e\u22125, 2e\u22125, number of epochs ranging from 3 to 10. We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train-\n4https://www.wikidata.org/\ning epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing. Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing:\nNFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.", "4.1 Pre-training Dataset. The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google3 to initialize the Transformer blocks for encoding tokens. Since pre-\n3https://github.com/google-research/bert\ntraining is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly. 4.2 Parameter Settings and Training Details. In this work, we denote the hidden dimension of token embeddings and entity embeddings as Hw, He respectively, and the number of self-attention heads as Aw, Ae respectively. In detail, we have the following model size: N = 6,M = 6, Hw = 768, He = 100, Aw = 12, Ae = 4. The total parameters are about 114M.\nThe total amount of parameters of BERTBASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512."]}
{"pkey": "ernie_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Not mentioned in the paper.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1. In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3 Methodology. In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5. 1It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\n3.1 Notations. We denote a token sequence as {w1, . . . , wn} 2, where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1, . . . , em}, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V , and the entity list containing all entities in KGs as E . If a token w \u2208 V has a corresponding entity e \u2208 E , their alignment is defined as f(w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2.\n3.2 Model Architecture.", "Except for setting the learning rate as 5e\u22125, we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e\u22125, 3e\u22125, 2e\u22125, number of epochs ranging from 3 to 10. We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train-\n4https://www.wikidata.org/\ning epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing. Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing:\nNFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.", "4.1 Pre-training Dataset. The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google3 to initialize the Transformer blocks for encoding tokens. Since pre-\n3https://github.com/google-research/bert\ntraining is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly. 4.2 Parameter Settings and Training Details. In this work, we denote the hidden dimension of token embeddings and entity embeddings as Hw, He respectively, and the number of self-attention heads as Aw, Ae respectively. In detail, we have the following model size: N = 6,M = 6, Hw = 768, He = 100, Aw = 12, Ae = 4. The total parameters are about 114M.\nThe total amount of parameters of BERTBASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512."]}
{"pkey": "ernie_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "Not mentioned in the paper.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1. In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3 Methodology. In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5. 1It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\n3.1 Notations. We denote a token sequence as {w1, . . . , wn} 2, where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1, . . . , em}, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V , and the entity list containing all entities in KGs as E . If a token w \u2208 V has a corresponding entity e \u2208 E , their alignment is defined as f(w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2.\n3.2 Model Architecture.", "In conclusion, we find that the pre-trained language models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller\ntraining set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC. In short, ERNIE achieves comparable results with BERTBASE on GLUE. On the one hand, it means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion.\n4.6 Ablation Study. In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively.", "Except for setting the learning rate as 5e\u22125, we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e\u22125, 3e\u22125, 2e\u22125, number of epochs ranging from 3 to 10. We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train-\n4https://www.wikidata.org/\ning epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing. Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing:\nNFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER."]}
{"pkey": "ernie_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "The paper denotes the number of T-Encoder layers as N, and the number of K-Encoder layers as M.  In the i-th aggregator, the input token embeddings {w (i\u22121) 1 , . . . , w (i\u22121) n } and entity embeddings {e (i\u22121) 1, . . . , e (i\u22121) m } from the preceding aggregator are fed into two multi-head self-attentions (MH-ATTs) (Vaswani et al., 2017) respectively.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["4.1 Pre-training Dataset. The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google3 to initialize the Transformer blocks for encoding tokens. Since pre-\n3https://github.com/google-research/bert\ntraining is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly. 4.2 Parameter Settings and Training Details. In this work, we denote the hidden dimension of token embeddings and entity embeddings as Hw, He respectively, and the number of self-attention heads as Aw, Ae respectively. In detail, we have the following model size: N = 6,M = 6, Hw = 768, He = 100, Aw = 12, Ae = 4. The total parameters are about 114M.\nThe total amount of parameters of BERTBASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512.", "Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1. In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3 Methodology. In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5. 1It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\n3.1 Notations. We denote a token sequence as {w1, . . . , wn} 2, where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1, . . . , em}, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V , and the entity list containing all entities in KGs as E . If a token w \u2208 V has a corresponding entity e \u2208 E , their alignment is defined as f(w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2.\n3.2 Model Architecture.", "Except for setting the learning rate as 5e\u22125, we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e\u22125, 3e\u22125, 2e\u22125, number of epochs ranging from 3 to 10. We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train-\n4https://www.wikidata.org/\ning epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing. Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing:\nNFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER."]}
{"pkey": "ernie_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "To accelerate the training process, the paper authors reduce the max sequence length from 512 to 256. To keep the number of tokens in a batch as same as BERT, the paper authors double the batch size to 512. Except for setting the learning rate as 5e\u22125. batch size: 32, learning rate (Adam): 5e\u22125, 3e\u22125,2e\u22125, number of epochs ranging from 3 to 10. The paper authors use a larger batch size and less training epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Except for setting the learning rate as 5e\u22125, we largely follow the pre-training hyper-parameters used in BERT. For fine-tuning, most hyper-parameters are the same as pre-training, except batch size, learning rate, and number of training epochs. We find the following ranges of possible values work well on the training datasets with gold annotations, i.e., batch size: 32, learning rate (Adam): 5e\u22125, 3e\u22125, 2e\u22125, number of epochs ranging from 3 to 10. We also evaluate ERNIE on the distantly supervised dataset, i.e., FIGER (Ling et al., 2015). As the powerful expression ability of deeply stacked Transformer blocks, we found small batch size would lead the model to overfit the training data. Hence, we use a larger batch size and less train-\n4https://www.wikidata.org/\ning epochs to avoid overfitting, and keep the range of learning rate unchanged, i.e., batch size: 2048, number of epochs: 2, 3. As most datasets do not have entity annotations, we use TAGME (Ferragina and Scaiella, 2010) to extract the entity mentions in the sentences and link them to their corresponding entities in KGs. 4.3 Entity Typing. Given an entity mention and its context, entity typing requires systems to label the entity mention with its respective semantic types. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FIGER (Ling et al., 2015) and Open Entity (Choi et al., 2018). The training set of FIGER is labeled with distant supervision, and its test set is annotated by human. Open Entity is a completely manually-annotated dataset. The statistics of these two datasets are shown in Table 1. We compare our model with the following baseline models for entity typing:\nNFGEC. NFGEC is a hybrid model proposed by Shimaoka et al. (2016). NFGEC combines the representations of entity mention, context and extra hand-craft features as input, and is the stateof-the-art model on FIGER.", "Hence, considering rich knowledge information can lead to better language understanding and accordingly benefits various knowledge-driven applications, e.g. entity typing and relation classification. For incorporating external knowledge into language representation models, there are two main\nar X\niv :1\n90 5.\n07 12\n9v 3\n[ cs\n.C L\n] 4\nJ un\n2 01\n9\nchallenges. (1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces. How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge. To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:\n(1) For extracting and encoding knowledge information, we firstly recognize named entity mentions in text and then align these mentions to their corresponding entities in KGs. Instead of directly using the graph-based facts in KGs, we encode the graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013), and then take the informative entity embeddings as input for ERNIE. Based on the alignments between text and KGs, ERNIE integrates entity representations in the knowledge module into the underlying layers of the semantic module. (2) Similar to BERT, we adopt the masked language model and the next sentence prediction as the pre-training objectives.", "(2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed. Howard and Ruder (2018) present AWDLSTM (Merity et al., 2018) to build a universal language model (ULMFiT). Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language\ninference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018)."]}
{"pkey": "ernie_13", "question": "Describe the computational resources used to train the model.", "answer": "Not mentioned in the paper.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["4.1 Pre-training Dataset. The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google3 to initialize the Transformer blocks for encoding tokens. Since pre-\n3https://github.com/google-research/bert\ntraining is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly. 4.2 Parameter Settings and Training Details. In this work, we denote the hidden dimension of token embeddings and entity embeddings as Hw, He respectively, and the number of self-attention heads as Aw, Ae respectively. In detail, we have the following model size: N = 6,M = 6, Hw = 768, He = 100, Aw = 12, Ae = 4. The total parameters are about 114M.\nThe total amount of parameters of BERTBASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512.", "In order to inject knowledge into language representation by informative entities, we propose a new pre-training task for ERNIE, which randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. As our task is similar to training a denoising auto-encoder (Vincent et al., 2008), we refer to this procedure as a denoising entity auto-encoder (dEA). Considering that the size of E is quite large for the softmax layer, we thus only require the system to predict entities based on the given entity sequence instead of all entities in KGs. Given the token sequence {w1, . . . , wn} and its corresponding entity sequence {e1, . . . , em}, we define the aligned entity distribution for the token wi as follows,\np(ej |wi) = exp(linear(woi ) \u00b7 ej)\u2211m\nk=1 exp(linear(w o i ) \u00b7 ek)\n, (7)\nwhere linear(\u00b7) is a linear layer. Eq. 7 will be used to compute the cross-entropy loss function for dEA. Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding. Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text. More details of these pre-training tasks can be found from Devlin et al. (2019).", "As shown in Figure 2, the whole model architecture of ERNIE consists of two stacked modules: (1) the underlying textual encoder (T-Encoder) responsible to capture basic lexical and syntactic information from the input tokens, and (2) the upper knowledgeable encoder (K-Encoder) responsible to integrate extra token-oriented knowledge information into textual information from the underlying layer, so that we can represent heterogeneous information of tokens and entities into a united feature space. Besides, we denote the number of T-Encoder layers as N , and the number\n2In this paper, tokens are at the subword level.\nof K-Encoder layers as M . To be specific, given a token sequence {w1, . . . , wn} and its corresponding entity sequence {e1, . . . , em}, the textual encoder firstly sums the token embedding, segment embedding, positional embedding for each token to compute its input embedding, and then computes lexical and syntactic features {w1, . . . ,wn} as follows,\n{w1, . . . ,wn} = T-Encoder({w1, . . . , wn}), (1)\nwhere T-Encoder(\u00b7) is a multi-layer bidirectional Transformer encoder. As T-Encoder(\u00b7) is identical to its implementation in BERT and BERT is prevalent, we exclude a comprehensive description of this module and refer readers to Devlin et al. (2019) and Vaswani et al. (2017). After computing {w1, . . . ,wn}, ERNIE adopts a knowledgeable encoder K-Encoder to inject the knowledge information into language representation. To be specific, we represent {e1, . . . , em} with their entity embeddings {e1, . . . , em}, which are pre-trained by the effective knowledge embedding model TransE (Bordes et al., 2013). Then, both {w1, . . . ,wn} and {e1, . . . , em} are fed into K-Encoder for fusing heterogeneous information and computing final output embeddings,\n{wo1, . . . ,won}, {eo1, . . . , eon} = K-Encoder( {w1, . . . ,wn}, {e1, . . . , em}). (2)\n{wo1, . . . ,won} and {eo1, . . . , eon} will be used as features for specific tasks."]}
{"pkey": "ernie_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "Not mentioned in the paper.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["In order to inject knowledge into language representation by informative entities, we propose a new pre-training task for ERNIE, which randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. As our task is similar to training a denoising auto-encoder (Vincent et al., 2008), we refer to this procedure as a denoising entity auto-encoder (dEA). Considering that the size of E is quite large for the softmax layer, we thus only require the system to predict entities based on the given entity sequence instead of all entities in KGs. Given the token sequence {w1, . . . , wn} and its corresponding entity sequence {e1, . . . , em}, we define the aligned entity distribution for the token wi as follows,\np(ej |wi) = exp(linear(woi ) \u00b7 ej)\u2211m\nk=1 exp(linear(w o i ) \u00b7 ek)\n, (7)\nwhere linear(\u00b7) is a linear layer. Eq. 7 will be used to compute the cross-entropy loss function for dEA. Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding. Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text. More details of these pre-training tasks can be found from Devlin et al. (2019).", "4.1 Pre-training Dataset. The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google3 to initialize the Transformer blocks for encoding tokens. Since pre-\n3https://github.com/google-research/bert\ntraining is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly. 4.2 Parameter Settings and Training Details. In this work, we denote the hidden dimension of token embeddings and entity embeddings as Hw, He respectively, and the number of self-attention heads as Aw, Ae respectively. In detail, we have the following model size: N = 6,M = 6, Hw = 768, He = 100, Aw = 12, Ae = 4. The total parameters are about 114M.\nThe total amount of parameters of BERTBASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512.", "ERNIE: Enhanced Language Representation with Informative Entities. Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously. The experimental results have demonstrated that ERNIE achieves significant improvements on various knowledge-driven tasks, and meanwhile is comparable with the state-of-the-art model BERT on other common NLP tasks. The source code and experiment details of this paper can be obtained from https:// github.com/thunlp/ERNIE. ERNIE: Enhanced Language Representation with Informative Entities. Neural language representation models such as BERT pre-trained on large-scale corpora can well capture rich semantic patterns from plain text, and be fine-tuned to consistently improve the performance of various NLP tasks. However, the existing pre-trained language models rarely consider incorporating knowledge graphs (KGs), which can provide rich structured knowledge facts for better language understanding. We argue that informative entities in KGs can enhance language representation with external knowledge. In this paper, we utilize both large-scale textual corpora and KGs to train an enhanced language representation model (ERNIE), which can take full advantage of lexical, syntactic, and knowledge information simultaneously."]}
{"pkey": "ernie_15", "question": "What is the pretraining objective of the model? ", "answer": "Adopt the masked language model and the next sentence prediction as the pre-training objectives. Besides, for the better fusion of textual and knowledge features, the paper authors design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Hence, considering rich knowledge information can lead to better language understanding and accordingly benefits various knowledge-driven applications, e.g. entity typing and relation classification. For incorporating external knowledge into language representation models, there are two main\nar X\niv :1\n90 5.\n07 12\n9v 3\n[ cs\n.C L\n] 4\nJ un\n2 01\n9\nchallenges. (1) Structured Knowledge Encoding: regarding to the given text, how to effectively extract and encode its related informative facts in KGs for language representation models is an important problem; (2) Heterogeneous Information Fusion: the pre-training procedure for language representation is quite different from the knowledge representation procedure, leading to two individual vector spaces. How to design a special pre-training objective to fuse lexical, syntactic, and knowledge information is another challenge. To overcome the challenges mentioned above, we propose Enhanced Language RepresentatioN with Informative Entities (ERNIE), which pretrains a language representation model on both large-scale textual corpora and KGs:\n(1) For extracting and encoding knowledge information, we firstly recognize named entity mentions in text and then align these mentions to their corresponding entities in KGs. Instead of directly using the graph-based facts in KGs, we encode the graph structure of KGs with knowledge embedding algorithms like TransE (Bordes et al., 2013), and then take the informative entity embeddings as input for ERNIE. Based on the alignments between text and KGs, ERNIE integrates entity representations in the knowledge module into the underlying layers of the semantic module. (2) Similar to BERT, we adopt the masked language model and the next sentence prediction as the pre-training objectives.", "To encode the word order and reduce the side effect of errors in dependency parsing, Contextualized GCN (C-GCN) firstly uses Bi-LSTM to generate contextualized representations as input for GCN models. In addition to these three baselines, we also finetune BERT with the same input format introduced in Section 3.5 for fair comparison. As FewRel does not have any null instance where there is not any relation between entities, we adopt macro averaged metrics to present the model performances. Since FewRel is built by checking whether the sentences contain facts in Wikidata, we drop the related facts in KGs before pre-training for fair comparison. From Table 5, we have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%. However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%. (2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, which means fusing external knowledge is very effective. In TACRED, there are nearly 80% null instances so that we follow the previous work (Zhang et al., 2017) to adopt micro averaged metrics to represent the model performances instead of the macro. The results of CNN, PA-LSTM, and C-GCN come from the paper by Zhang et al. (2018), which are the best results of CNN, RNN, and GCN respectively. From Table 5, we observe that: (1) The C-GCN model outperforms the strong BERT model by an F1 increase of 0.4%, as C-GCN utilizes the dependency trees and the entity mask strategy. The entity mask strategy refers to replacing each subject (and object similarly) entity with a special NER token, which is similar to our proposed pre-training task dEA. (2) ERNIE achieves the best recall and F1 scores, and increases the F1 of BERT by nearly 2.0%, which proves the effectiveness of the knowledgeable module for relation classification.", "Besides, for the better fusion of textual and knowledge features, we design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments. Unlike the existing pre-trained language representation models only utilizing local context to predict tokens, our objectives require models to aggregate both context and knowledge facts for predicting both tokens and entities, and lead to a knowledgeable language representation model. We conduct experiments on two knowledgedriven NLP tasks, i.e., entity typing and relation classification. The experimental results show that ERNIE significantly outperforms the state-of-theart model BERT on these knowledge-driven tasks, by taking full advantage of lexical, syntactic, and knowledge information. We also evaluate ERNIE on other common NLP tasks, and ERNIE still achieves comparable results. 2 Related Work. Many efforts are devoted to pre-training language representation models for capturing language information from text and then utilizing the information for specific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al."]}
{"pkey": "ernie_16", "question": "What is the loss function that is used to train the model?", "answer": "Equation 7 in the paper is used to compute the cross-entropy loss function for dEA.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["In order to inject knowledge into language representation by informative entities, we propose a new pre-training task for ERNIE, which randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. As our task is similar to training a denoising auto-encoder (Vincent et al., 2008), we refer to this procedure as a denoising entity auto-encoder (dEA). Considering that the size of E is quite large for the softmax layer, we thus only require the system to predict entities based on the given entity sequence instead of all entities in KGs. Given the token sequence {w1, . . . , wn} and its corresponding entity sequence {e1, . . . , em}, we define the aligned entity distribution for the token wi as follows,\np(ej |wi) = exp(linear(woi ) \u00b7 ej)\u2211m\nk=1 exp(linear(w o i ) \u00b7 ek)\n, (7)\nwhere linear(\u00b7) is a linear layer. Eq. 7 will be used to compute the cross-entropy loss function for dEA. Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding. Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text. More details of these pre-training tasks can be found from Devlin et al. (2019).", "The overall pre-training loss is the sum of the dEA, MLM and NSP loss. 3.5 Fine-tuning for Specific Tasks. As shown in Figure 3, for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT. We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:\nFor relation classification, the task requires systems to classify relation labels of given entity pairs based on context. The most straightforward way\nto fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively. The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016; Yaghoobzadeh and Schu\u0308tze, 2017; Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively. 4 Experiments. In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks.", "4.1 Pre-training Dataset. The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google3 to initialize the Transformer blocks for encoding tokens. Since pre-\n3https://github.com/google-research/bert\ntraining is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly. 4.2 Parameter Settings and Training Details. In this work, we denote the hidden dimension of token embeddings and entity embeddings as Hw, He respectively, and the number of self-attention heads as Aw, Ae respectively. In detail, we have the following model size: N = 6,M = 6, Hw = 768, He = 100, Aw = 12, Ae = 4. The total parameters are about 114M.\nThe total amount of parameters of BERTBASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512."]}
{"pkey": "ernie_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "the whole model architecture of ERNIE consists of two stacked modules:\n(1) the underlying textual encoder (T-Encoder) responsible to capture basic lexical and syntactic information from the input tokens, and \n(2) the upper knowledgeable encoder (K-Encoder) responsible to integrate extra token-oriented knowledge information into textual information from the underlying layer, so that the paper authors can represent heterogeneous information of tokens and entities into a united feature space. Besides, the paper authors denote the number of T-Encoder layers as N, and the number of K-Encoder layers as M", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1. In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3 Methodology. In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5. 1It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\n3.1 Notations. We denote a token sequence as {w1, . . . , wn} 2, where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1, . . . , em}, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V , and the entity list containing all entities in KGs as E . If a token w \u2208 V has a corresponding entity e \u2208 E , their alignment is defined as f(w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2.\n3.2 Model Architecture.", "(2018) further adopt the sequence-level model (ELMo) to capture complex word features across different linguistic contexts and use ELMo to generate context-aware word embeddings. Different from the above-mentioned featurebased language approaches only using the pretrained language representations as input features, Dai and Le (2015) train auto-encoders on unlabeled text, and then use the pre-trained model architecture and parameters as a starting point for other specific NLP models. Inspired by Dai and Le (2015), more pre-trained language representation models for fine-tuning have been proposed. Howard and Ruder (2018) present AWDLSTM (Merity et al., 2018) to build a universal language model (ULMFiT). Radford et al. (2018) propose a generative pre-trained Transformer (Vaswani et al., 2017) (GPT) to learn language representations. Devlin et al. (2019) propose a deep bidirectional model with multiplelayer Transformers (BERT), which achieves the state-of-the-art results for various NLP tasks. Though both feature-based and fine-tuning language representation models have achieved great success, they ignore the incorporation of knowledge information. As demonstrated in recent work, injecting extra knowledge information can significantly enhance original models, such as reading comprehension (Mihaylov and Frank, 2018; Zhong et al., 2018), machine translation (Zaremoodi et al., 2018), natural language\ninference (Chen et al., 2018), knowledge acquisition (Han et al., 2018a), and dialog systems (Madotto et al., 2018). Hence, we argue that extra knowledge information can effectively benefit existing pre-training models. In fact, some work has attempted to joint representation learning of words and entities for effectively leveraging external KGs and achieved promising results (Wang et al., 2014; Toutanova et al., 2015; Han et al., 2016; Yamada et al., 2016; Cao et al., 2017, 2018).", "In order to inject knowledge into language representation by informative entities, we propose a new pre-training task for ERNIE, which randomly masks some token-entity alignments and then requires the system to predict all corresponding entities based on aligned tokens. As our task is similar to training a denoising auto-encoder (Vincent et al., 2008), we refer to this procedure as a denoising entity auto-encoder (dEA). Considering that the size of E is quite large for the softmax layer, we thus only require the system to predict entities based on the given entity sequence instead of all entities in KGs. Given the token sequence {w1, . . . , wn} and its corresponding entity sequence {e1, . . . , em}, we define the aligned entity distribution for the token wi as follows,\np(ej |wi) = exp(linear(woi ) \u00b7 ej)\u2211m\nk=1 exp(linear(w o i ) \u00b7 ek)\n, (7)\nwhere linear(\u00b7) is a linear layer. Eq. 7 will be used to compute the cross-entropy loss function for dEA. Considering that there are some errors in tokenentity alignments, we perform the following operations for dEA: (1) In 5% of the time, for a given token-entity alignment, we replace the entity with another random entity, which aims to train our model to correct the errors that the token is aligned with a wrong entity; (2) In 15% of the time, we mask token-entity alignments, which aims to train our model to correct the errors that the entity alignment system does not extract all existing alignments; (3) In the rest of the time, we keep tokenentity alignments unchanged, which aims to encourage our model to integrate the entity information into token representations for better language understanding. Similar to BERT, ERNIE also adopts the masked language model (MLM) and the next sentence prediction (NSP) as pre-training tasks to enable ERNIE to capture lexical and syntactic information from tokens in text. More details of these pre-training tasks can be found from Devlin et al. (2019)."]}
{"pkey": "ernie_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors conduct experiments on two knowledge-driven NLP tasks, i.e., entity typing and relation classification. Given the large cost of training ERNIE from scratch, the paper authors adopt the parameters of BERT released by Google to initialize the Transformer blocks for encoding tokens classification. The experimental results show that ERNIE significantly outperforms the state-of-the-art model BERT on these knowledge-driven tasks by taking full advantage of lexical, syntactic, and knowledge information. The paper authors also evaluate ERNIE on other common NLP tasks, where ERNIE still achieves comparable results. \nSince pre-training is a multi-task procedure consisting of NSP (Next Sentence Prediction), MLM (Masked Language Modeling), and dEA (dependency edge annotation), the paper authors use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4,500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, the paper authors adopt the knowledge embeddings trained on Wikidata by TransE as the input embeddings for entities. Specifically, the paper authors sample part of Wikidata which contains 5,040,986 entities and 24,267,796 fact triples.\nThe model configuration includes the hidden dimension of token embeddings and entity embeddings as ( H_w ) and ( H_e ) respectively, and the number of self-attention heads as ( A_w ) and ( A_e ) respectively. The model sizes are as follows: ( N = 6 ), ( M = 6 ), ( H_w = 768 ), ( H_e = 100 ), ( A_w = 12 ), ( A_e = 4 ). The total parameters are about 114M. The training parameters are: batch size: 32, learning rates (Adam): ( 5 times 10^{-5} ), ( 3 times 10^{-5} ), ( 2 times 10^{-5} ), with the number of epochs ranging from 3 to 10.\nThe results on Open Entity show that BERT and ERNIE achieve much higher recall scores than previous entity typing models, indicating that pre-training language models make full use of both the unsupervised pre-training and manually annotated training data for better entity typing. Compared to BERT, ERNIE improves precision by 2% and recall by 2%, suggesting that informative entities help ERNIE predict the labels more precisely.\nSpecifically, the paper authors sample 100 instances from each class for the training set, and sample 200 instances for the development and test sets respectively. There are 80 classes in FewRel, and 42 classes (including a special relation \"no relation\") in TACRED.\nFrom Table 5, the paper authors have two observations: (1) As the training data does not have enough instances to train the CNN encoder from scratch, CNN just achieves an F1 score of 69.35%. However, the pre-training models including BERT and ERNIE increase the F1 score by at least 15%. (2) ERNIE achieves an absolute F1 increase of 3.4% over BERT, indicating that fusing external knowledge is very effective. ERNIE achieves comparable results with BERTBASE on GLUE. On one hand, this means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["In summary, ERNIE effectively reduces the noisy label challenge in FIGER, which is a widely-used distantly supervised entity typing dataset, by injecting the information from KGs. Besides, ERNIE also outperforms the baselines on Open Entity which has gold annotations. 4.4 Relation Classification. Relation classification aims to determine the correct relation between two entities in a given sentence, which is an important knowledge-driven NLP task. To evaluate performance on this task, we fine-tune ERNIE on two well-established datasets FewRel (Han et al., 2018c) and TACRED (Zhang et al., 2017). The statistics of these two datasets are shown in Table 4. As the original experimental setting of FewRel is few-shot learning, we rearrange the FewRel dataset for the common relation classification setting. Specifically, we sample 100 instances from each class for the training set, and sample 200 instances for the development and test respectively. There are 80 classes in FewRel, and there are 42 classes (including a special relation \u201cno relation\u201d) in TACRED. We compare our model with the following baseline models for relation classification:\nCNN. With a convolution layer, a max-pooling layer, and a non-linear activation layer, CNN gets the output sentence embedding, and then feeds it into a relation classifier. To better capture the position of head and tail entities, position embeddings are introduced into CNN (Zeng et al., 2015; Lin et al., 2016; Wu et al., 2017; Han et al., 2018b). PA-LSTM. Zhang et al. (2017) propose PALSTM introducing a position-aware attention mechanism over an LSTM network, which evaluates the relative contribution of each word in the sequence for the final sentence representation. C-GCN. Zhang et al. (2018) adopt the graph convolution operations to model dependency trees for relation classification.", "Besides, for the better fusion of textual and knowledge features, we design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments. Unlike the existing pre-trained language representation models only utilizing local context to predict tokens, our objectives require models to aggregate both context and knowledge facts for predicting both tokens and entities, and lead to a knowledgeable language representation model. We conduct experiments on two knowledgedriven NLP tasks, i.e., entity typing and relation classification. The experimental results show that ERNIE significantly outperforms the state-of-theart model BERT on these knowledge-driven tasks, by taking full advantage of lexical, syntactic, and knowledge information. We also evaluate ERNIE on other common NLP tasks, and ERNIE still achieves comparable results. 2 Related Work. Many efforts are devoted to pre-training language representation models for capturing language information from text and then utilizing the information for specific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al.", "The overall pre-training loss is the sum of the dEA, MLM and NSP loss. 3.5 Fine-tuning for Specific Tasks. As shown in Figure 3, for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT. We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:\nFor relation classification, the task requires systems to classify relation labels of given entity pairs based on context. The most straightforward way\nto fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively. The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016; Yaghoobzadeh and Schu\u0308tze, 2017; Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively. 4 Experiments. In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks."]}
{"pkey": "ernie_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "The paper explores the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively. As shown in Table 7, the paper authors have the following observations:\n(1) Without entity sequence input, dEA still injects knowledge information into language representation during pre-training, which increases the F1 score of BERT by 0.9%. \n(2) Although the informative entities bring much knowledge information which intuitively benefits relation classification, ERNIE without dEA takes little advantage of this, leading to the F1 increase of 0.7%.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["In conclusion, we find that the pre-trained language models can provide more information for relation classification than the vanilla encoder CNN and RNN. And ERNIE outperforms BERT on both of the relation classification datasets, especially on the FewRel which has a much smaller\ntraining set. It demonstrates extra knowledge helps the model make full use of small training data, which is important for most NLP tasks as large-scale annotated data is unavailable. 4.5 GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of diverse natural language understanding tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan and Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Rajpurkar et al., 2016; Dagan et al., 2006; Levesque et al., 2011), which is the main benchmark used in Devlin et al. (2019). To explore whether our knowledgeable module degenerates the performance on common NLP tasks, we evaluate ERNIE on 8 datasets of GLUE and compare it with BERT. In Table 6, we report the results of our evaluation submissions and those of BERT from the leaderboard. We notice that ERNIE is consistent with BERTBASE on big datasets like MNLI, QQP, QNLI, and SST-2. The results become more unstable on small datasets, that is, ERNIE is better on CoLA and RTE, but worse on STS-B and MRPC. In short, ERNIE achieves comparable results with BERTBASE on GLUE. On the one hand, it means GLUE does not require external knowledge for language representation. On the other hand, it illustrates ERNIE does not lose the textual information after heterogeneous information fusion.\n4.6 Ablation Study. In this subsection, we explore the effects of the informative entities and the knowledgeable pretraining task (dEA) for ERNIE using FewRel dataset. w/o entities and w/o dEA refer to finetuning ERNIE without entity sequence input and the pre-training task dEA respectively.", "Besides, for the better fusion of textual and knowledge features, we design a new pre-training objective by randomly masking some of the named entity alignments in the input text and asking the model to select appropriate entities from KGs to complete the alignments. Unlike the existing pre-trained language representation models only utilizing local context to predict tokens, our objectives require models to aggregate both context and knowledge facts for predicting both tokens and entities, and lead to a knowledgeable language representation model. We conduct experiments on two knowledgedriven NLP tasks, i.e., entity typing and relation classification. The experimental results show that ERNIE significantly outperforms the state-of-theart model BERT on these knowledge-driven tasks, by taking full advantage of lexical, syntactic, and knowledge information. We also evaluate ERNIE on other common NLP tasks, and ERNIE still achieves comparable results. 2 Related Work. Many efforts are devoted to pre-training language representation models for capturing language information from text and then utilizing the information for specific NLP tasks. These pre-training approaches can be divided into two classes, i.e., feature-based approaches and finetuning approaches. The early work (Collobert and Weston, 2008; Mikolov et al., 2013; Pennington et al., 2014) focuses on adopting feature-based approaches to transform words into distributed representations. As these pre-trained word representations capture syntactic and semantic information in textual corpora, they are often used as input embeddings and initialization parameters for various NLP models, and offer significant improvements over random initialization parameters (Turian et al., 2010). Since these word-level models often suffer from the word polysemy, Peters et al.", "4.1 Pre-training Dataset. The pre-training procedure primarily acts in accordance with the existing literature on pre-training language models. For the large cost of training ERNIE from scratch, we adopt the parameters of BERT released by Google3 to initialize the Transformer blocks for encoding tokens. Since pre-\n3https://github.com/google-research/bert\ntraining is a multi-task procedure consisting of NSP, MLM, and dEA, we use English Wikipedia as our pre-training corpus and align text to Wikidata. After converting the corpus into the formatted data for pre-training, the annotated input has nearly 4, 500M subwords and 140M entities, and discards the sentences having less than 3 entities. Before pre-training ERNIE, we adopt the knowledge embeddings trained on Wikidata4 by TransE as the input embeddings for entities. To be specific, we sample part of Wikidata which contains 5, 040, 986 entities and 24, 267, 796 fact triples. The entity embeddings are fixed during training and the parameters of the entity encoding modules are all initialized randomly. 4.2 Parameter Settings and Training Details. In this work, we denote the hidden dimension of token embeddings and entity embeddings as Hw, He respectively, and the number of self-attention heads as Aw, Ae respectively. In detail, we have the following model size: N = 6,M = 6, Hw = 768, He = 100, Aw = 12, Ae = 4. The total parameters are about 114M.\nThe total amount of parameters of BERTBASE is about 110M, which means the knowledgeable module of ERNIE is much smaller than the language module and has little impact on the run-time performance. And, we only pre-train ERNIE on the annotated corpus for one epoch. To accelerate the training process, we reduce the max sequence length from 512 to 256 as the computation of selfattention is a quadratic function of the length. To keep the number of tokens in a batch as same as BERT, we double the batch size to 512."]}
{"pkey": "ernie_20", "question": "List the future work mentioned in the paper.", "answer": "There are three important directions remain for future research: \n(1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); \n(2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from the world knowledge database Wikidata;\n(3) annotate more real-world corpora heuristically for building larger pre-training data.", "title": "ERNIE: Enhanced Language Representation with Informative Entities", "context": ["Sun et al. (2019) propose the knowledge masking strategy for masked language model to enhance language representation by knowledge 1. In this paper, we further utilize both corpora and KGs to train an enhanced language representation model based on BERT. 3 Methodology. In this section, we present the overall framework of ERNIE and its detailed implementation, including the model architecture in Section 3.2, the novel pre-training task designed for encoding informative entities and fusing heterogeneous information in Section 3.4, and the details of the fine-tuning procedure in Section 3.5. 1It is a coincidence that both Sun et al. (2019) and we chose ERNIE as the model names, which follows the interesting naming habits like ELMo and BERT. Sun et al. (2019) released their code on March 16th and submitted their paper to Arxiv on April 19th while we submitted our paper to ACL whose deadline is March 4th.\n3.1 Notations. We denote a token sequence as {w1, . . . , wn} 2, where n is the length of the token sequence. Meanwhile, we denote the entity sequence aligning to the given tokens as {e1, . . . , em}, where m is the length of the entity sequence. Note that m is not equal to n in most cases, as not every token can be aligned to an entity in KGs. Furthermore, we denote the whole vocabulary containing all tokens as V , and the entity list containing all entities in KGs as E . If a token w \u2208 V has a corresponding entity e \u2208 E , their alignment is defined as f(w) = e. In this paper, we align an entity to the first token in its named entity phrase, as shown in Figure 2.\n3.2 Model Architecture.", "As shown in Table 7, we have the following observations: (1) Without entity sequence input, dEA still injects knowledge information into language representation during pre-training, which increases the F1 score of BERT by 0.9%. (2) Although the informative entities bring much knowledge informa-\ntion which intuitively benefits relation classification, ERNIE without dEA takes little advantage of this, leading to the F1 increase of 0.7%. 5 Conclusion. In this paper, we propose ERNIE to incorporate knowledge information into language representation models. Accordingly, we propose the knowledgeable aggregator and the pre-training task dEA for better fusion of heterogeneous information from both text and KGs. The experimental results demonstrate that ERNIE has better abilities of both denoising distantly supervised data and fine-tuning on limited data than BERT. There are three important directions remain for future research: (1) inject knowledge into feature-based pre-training models such as ELMo (Peters et al., 2018); (2) introduce diverse structured knowledge into language representation models such as ConceptNet (Speer and Havasi, 2012) which is different from the world knowledge database Wikidata; (3) annotate more real-world corpora heuristically for building larger pre-training data. These directions may lead to more general and effective language understanding.\nAcknowledgement. This work is funded by the Natural Science Foundation of China (NSFC) and the German Research Foundation (DFG) in Project Crossmodal Learning, NSFC 61621136008 / DFG TRR-169, the National Natural Science Foundation of China (NSFC No. 61572273) and China Association for Science and Technology (2016QNRC001).", "The overall pre-training loss is the sum of the dEA, MLM and NSP loss. 3.5 Fine-tuning for Specific Tasks. As shown in Figure 3, for various common NLP tasks, ERNIE can adopt the fine-tuning procedure similar to BERT. We can take the final output embedding of the first token, which corresponds to the special [CLS] token, as the representation of the input sequence for specific tasks. For some knowledge-driven tasks (e.g., relation classification and entity typing), we design special finetuning procedure:\nFor relation classification, the task requires systems to classify relation labels of given entity pairs based on context. The most straightforward way\nto fine-tune ERNIE for relation classification is to apply the pooling layer to the final output embeddings of the given entity mentions, and represent the given entity pair with the concatenation of their mention embeddings for classification. In this paper, we design another method, which modifies the input token sequence by adding two mark tokens to highlight entity mentions. These extra mark tokens play a similar role like position embeddings in the conventional relation classification models (Zeng et al., 2015). Then, we also take the [CLS] token embedding for classification. Note that we design different tokens [HD] and [TL] for head entities and tail entities respectively. The specific fine-tuning procedure for entity typing is a simplified version of relation classification. As previous typing models make full use of both context embeddings and entity mention embeddings (Shimaoka et al., 2016; Yaghoobzadeh and Schu\u0308tze, 2017; Xin et al., 2018), we argue that the modified input sequence with the mention mark token [ENT] can guide ERNIE to combine both context information and entity mention information attentively. 4 Experiments. In this section, we present the details of pretraining ERNIE and the fine-tuning results on five NLP datasets, which contain both knowledgedriven tasks and the common NLP tasks."]}
{"pkey": "pegasus_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "In this work, the paper authors proposed PEGASUS, a sequence-to-sequence model with gap-sentences generation as a pretraining objective tailored for abstractive text summarization.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["we discuss a universal version of the problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the unified problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems . domain restrictions and weakening of the properties are also investigated in order to get some positive results . ROUGE1-F1 48.61 Document (ID #289) machine learning methods are used widely within high energy physics ( hep ) . one promising approach , used extensively outside of hep forapplications such as handwriting recognition , is that of support vector machines ( svms ) , a supervised learning model used with associated learning\nalgorithms for multivariate analysis ( mva ) . developed originally in the 1960s , with the current standard version proposed in 1995 xcite , svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace . however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis . this motivates a number methods for ensuring the classifier is sufficiently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given in section [ sec : svm ] , with an example using svms shown in section [ sec : checker ] .", "the pulses are based upon control signals produced by either the timing of paint balls entering the firing chamber of the gun or the detection of the presence of a single paint ball within the chamber . a manually operated electrical switch trigger activates the electronic circuitry . an adjustably predetermined number of paint balls will fire based upon each depression of the electrical switch trigger while minimizing the chopping of paint balls in the firing chamber . Model a paint ball gun control system which permits selective firing of paint balls in response to the depression of the trigger . the system includes an electronic circuit which converts a dc current into a regulated pulse current , and an electromagnetic device which is actuated by the regulated pulse current to produce a reciprocating linear motion . a push rod attached to the emd is adapted to contact the trigger of the gun . ROUGEL-F1 22.10\nTable I.25: Generated summaries by PEGASUSLARGE (HugeNews) on arXiv sampled by ROUGE1-F1. arXiv Document (ID #34) consider a set of objects which should be ranked on the basis of information about their bilateral relationships . similar problems arise , among others, in social choice theory xcite , sports xcite , psychology xcite , internet search xcite , and bibliometrics xcite . we discuss a universal version of\nthe problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the unified problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems .", "generalisation is discussed in section [ sec : gen ] with an illustrative example of how this can improve performance given in section [ sec : check2 ] . consider the problem of linear classification with the svm where the training set , xmath0 , is linearly separable . we define a separating hyperplane given by xmath1 , where xmath2 , the weight vector , is perpendicular to the hyperplane , and xmath3 , the bias , determines the distance of the hyperplane from the origin ( fig . ) . a decision function defined by xmath4 is used to positively and negatively classify xmath5 , the points in the training set . without further constraint the ... ... ...\nGold we review the concept of support vector machines ( svms ) and discuss examples of their use . one of the benefits of svm algorithms , compared with neural networks and decision trees is that they can be less susceptible to over fitting than those other algorithms are to over training . this issue is related to the generalisation of a multivariate algorithm ( mva ) ; a problem that has often been overlooked in particle physics . we discuss cross validation and how this can be used to improve the generalisation of a mva in the context of high energy physics analyses . the examples presented use the toolkit for multivariate analysis ( tmva ) based on root and describe our improvements to the svm functionality and new tools introduced for cross validation within this framework . Model support vector machines ( svms ) are one of the most promising methods for machine learning in high energy physics . svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace . however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis ."]}
{"pkey": "pegasus_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Large-scale document-summary datasets are rare and in practice there is a mismatch between research datasets and real-world use-cases where collecting summaries is expensive; the most common setting is that of low-resource summarization. The paper authors simulate this setting and show that our model is able to adapt very quickly when fine tuning with small numbers of supervised pairs, obtaining state-of-the-art results in 6 datasets with only 1000 examples. The paper authors found that PEGASUS summaries are at least as good as reference summaries for the datasets the paper authors assessed \u2013 XSum, CNN/DailyMail, and Reddit TIFU \u2013 even at low-levels of supervision.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["Model Y\u2019all think a little toy buzzer is going to keep the truth from tumbling out from these jaws?\nROUGEL-F1 10.26\nTable I.10: Generated summaries by PEGASUSLARGE (HugeNews) on Multi-News sampled by ROUGE1-F1. Multi-News Document (ID #114) Size really does seem to matter when it comes to cancer risk. Being tall undoubtedly has its benefits. You can see in a crowd and grab objects offhigh shelves. But with the good comes the bad. The taller you are, the higher your odds of developing cancer, and a new paper has added weight to\nthis. key points Key points: Taller people have more cells in their body, as well as higher levels of a protein that encourages cells to divide and grow For every 10cm over the average height, a person\u2019s risk for cancer increases 10 per cent New analysis of data from big cancer studies supports this, and also finds a few specific cancers to be more or less strongly correlated with height Leonard Nunney, an evolutionary biologist at the University of California, Riverside, looked at massive cancer databases to find out how the number of cells in a person\u2019s body, using height as a proxy, might affect their risk of developing cancer. Reporting in the Proceedings of the Royal Society B, he found being taller and having more cells did mean more cancer overall: For every 10 centimetres over the average height, the risk of developing any cancer increased by around 10 per cent. This fits with previous studies. \u201dIf you were comparing a 5-foot guy to a basketball player who\u2019s over 7 feet tall, then that basketball player has around twice the risk of cancer across the board,\u201d Professor Nunney said. He also found that taller people were at much higher risk of melanoma, and women specifically had greater odds of developing thyroid cancer. But it doesn\u2019t mean tall people should panic. \u201dNow, you can\u2019t do anything about your height, but what you can do is tell extremely tall individuals that they should be aware ... ... ...", "as i\u2019m standing outside, a friend walks up and tells us what he found out from a teacher. one of the students, while trying to connect to the wifi, had \u201ddiscovered\u201d a network... my network: \u201dtaliban secure communications.\u201d needless to say, the student went to a teacher, things escalated, and the school ordered a full evacuation. i talked to the principal (a very, very awkward conversation) and got everything sorted out. everyone\u2019s parents are panicking, checking their kids out of school now. my personal hotspot is now called \u201di love school.\u201d Gold turned on personal hotspot with the name \u201dtaliban secure communications\u201d for an assembly, school got evacuated because of a supposed terror threat. Model i connected my school\u2019s live camera to my personal hotspot, one of the students found out and evacuated the school. ROUGE2-F1 4.88 Document (ID #228) unlike the majority of these, this occurred about 20 minutes ago i am a pc gamer, which means i have a desk with tons of junk on it. sometimes i willwalk in and grab a snack to eat before i start anything, which this time turned out to be a nice bag of chipsticks. i sit down and was so excited to eat\nthem that i teared open the bag. apparently these have a very low tolerance to force, so as i rip them open the i watch each individual yellow stick of glory fly everywhere. these not only landed in all the small keyboard gaps but in every little gap imaginable. obviously before i moved i ate each individual stick to try and minimise the situation. so im now sitting here typing this up with a tiny dog vacuum trying clean up this mess. Gold back of the packet doesn\u2019t state the clock inside\nModel i ripped open a bag of chipsticks and they flew everywhere\nROUGE2-F1 0.00 Document (ID #29) this started two days ago and it followed up today. so we had a new sales guy start at the office. i handle most of the i.t. helpdesk/network adminstuff as well as work in sales (its a private company).", "Sugar can give you a rush ofenergy, but then your energy will crash. Complex carbohydrates, on the other hand, offer more sustained energy, especially when you pair them with\nprotein. For instance, try some natural peanut butter on whole-wheat bread or a piece of fruit with a slice of cheese. Dehydration can lead to fatigue. Therefore, staying hydrated will help keep your energy up. Try sipping on water throughout the day to make sure you get enough. The Institute of Medicine recommends that men drink 13 cups and women drink 9 cups of water every day. Other liquids can contribute to your water intake, such as juice and coffee. However, drinking too much juice can pack on extra calories and cause a sugar crash, so try to limit your intake. Also, though you can count caffeinated beverages, you shouldn\u2019t make them the majority of the liquid you drink in a day., Keeping something in your mouth, such as a piece of gum, can help increase your alertness. The best time for this type of trick is when you\u2019re in a meeting that you absolutely need to stay awake for. Gold Skip the sugar. Make hydration a priority. Try a piece of gum. Model Eat the right foods. Stay hydrated. Chew something. ROUGE2-F1 0.00\nTable I.18: Generated summaries by PEGASUSLARGE (HugeNews) on WikiHow sampled by ROUGEL-F1. WikiHow Document (ID #241) No matter what size the paint spill, carefully use a putty knife or any sort of flat tool to scoop up paint that is sitting on top of the carpet, not yetsoaked down into the fibers. Scoop up as much excess paint as you can without spreading the paint around. Wipe off the collected paint with a paper\ntowels, and throw the towels away in the garbage. ; , Use a paper towel or clean rag to blot and absorb as much wet paint as you can. Lift your towel up and down and gently dab at the paint. Adjust the towel so you are always dabbing at the paint with a clean section. Dont rub or try to scrub the paint out; that will merely spread the paint around and push it deeper into the carpet."]}
{"pkey": "pegasus_3", "question": "What are the main contributions of the paper?", "answer": "The paper authors propose a new self-supervised pre-training objective for abstractive summarization, gap-sentences generation, and study strategies for selecting those sentences. The paper authors evaluate the proposed pre-training objective on a broad range of downstream summarization tasks, with careful ablations to choose the best model settings, which the paper authors use to train a 568M parameter PEGASUS model that surpasses or is on-par with the state-of-theart on all 12 downstream datasets considered.The paper authors show how good abstractive summarization performance can be achieved across broad domains with very little supervision by fine-tuning the PEGASUS model and surpassing previous state-of-the-art results on many tasks with as little as 1000 examples. The paper authors conducted human evaluation studies to validate our experimental design and demonstrate human-level summarization performance on XSum, CNN/DailyMail, and Reddit TIFU.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["we discuss a universal version of the problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the unified problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems . domain restrictions and weakening of the properties are also investigated in order to get some positive results . ROUGE1-F1 48.61 Document (ID #289) machine learning methods are used widely within high energy physics ( hep ) . one promising approach , used extensively outside of hep forapplications such as handwriting recognition , is that of support vector machines ( svms ) , a supervised learning model used with associated learning\nalgorithms for multivariate analysis ( mva ) . developed originally in the 1960s , with the current standard version proposed in 1995 xcite , svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace . however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis . this motivates a number methods for ensuring the classifier is sufficiently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given in section [ sec : svm ] , with an example using svms shown in section [ sec : checker ] .", "the pulses are based upon control signals produced by either the timing of paint balls entering the firing chamber of the gun or the detection of the presence of a single paint ball within the chamber . a manually operated electrical switch trigger activates the electronic circuitry . an adjustably predetermined number of paint balls will fire based upon each depression of the electrical switch trigger while minimizing the chopping of paint balls in the firing chamber . Model a paint ball gun control system which permits selective firing of paint balls in response to the depression of the trigger . the system includes an electronic circuit which converts a dc current into a regulated pulse current , and an electromagnetic device which is actuated by the regulated pulse current to produce a reciprocating linear motion . a push rod attached to the emd is adapted to contact the trigger of the gun . ROUGEL-F1 22.10\nTable I.25: Generated summaries by PEGASUSLARGE (HugeNews) on arXiv sampled by ROUGE1-F1. arXiv Document (ID #34) consider a set of objects which should be ranked on the basis of information about their bilateral relationships . similar problems arise , among others, in social choice theory xcite , sports xcite , psychology xcite , internet search xcite , and bibliometrics xcite . we discuss a universal version of\nthe problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the unified problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems .", "Sugar can give you a rush ofenergy, but then your energy will crash. Complex carbohydrates, on the other hand, offer more sustained energy, especially when you pair them with\nprotein. For instance, try some natural peanut butter on whole-wheat bread or a piece of fruit with a slice of cheese. Dehydration can lead to fatigue. Therefore, staying hydrated will help keep your energy up. Try sipping on water throughout the day to make sure you get enough. The Institute of Medicine recommends that men drink 13 cups and women drink 9 cups of water every day. Other liquids can contribute to your water intake, such as juice and coffee. However, drinking too much juice can pack on extra calories and cause a sugar crash, so try to limit your intake. Also, though you can count caffeinated beverages, you shouldn\u2019t make them the majority of the liquid you drink in a day., Keeping something in your mouth, such as a piece of gum, can help increase your alertness. The best time for this type of trick is when you\u2019re in a meeting that you absolutely need to stay awake for. Gold Skip the sugar. Make hydration a priority. Try a piece of gum. Model Eat the right foods. Stay hydrated. Chew something. ROUGE2-F1 0.00\nTable I.18: Generated summaries by PEGASUSLARGE (HugeNews) on WikiHow sampled by ROUGEL-F1. WikiHow Document (ID #241) No matter what size the paint spill, carefully use a putty knife or any sort of flat tool to scoop up paint that is sitting on top of the carpet, not yetsoaked down into the fibers. Scoop up as much excess paint as you can without spreading the paint around. Wipe off the collected paint with a paper\ntowels, and throw the towels away in the garbage. ; , Use a paper towel or clean rag to blot and absorb as much wet paint as you can. Lift your towel up and down and gently dab at the paint. Adjust the towel so you are always dabbing at the paint with a clean section. Dont rub or try to scrub the paint out; that will merely spread the paint around and push it deeper into the carpet."]}
{"pkey": "pegasus_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "The paper authors demonstrated the effects of the pre-training corpora, gap-sentences ratios, vocabulary sizes and scaled up the best configuration to achieve state-of-the-art results on all 12 diverse downstream spanning news, science, stories, instructions, emails, patents, and legislative bills.The paper authors also showed that our model was able to adapt to unseen summarization datasets very quickly, achieving strong results in as little as 1000 examples.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["we discuss a universal version of the problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the unified problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems . domain restrictions and weakening of the properties are also investigated in order to get some positive results . ROUGE1-F1 48.61 Document (ID #289) machine learning methods are used widely within high energy physics ( hep ) . one promising approach , used extensively outside of hep forapplications such as handwriting recognition , is that of support vector machines ( svms ) , a supervised learning model used with associated learning\nalgorithms for multivariate analysis ( mva ) . developed originally in the 1960s , with the current standard version proposed in 1995 xcite , svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace . however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis . this motivates a number methods for ensuring the classifier is sufficiently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given in section [ sec : svm ] , with an example using svms shown in section [ sec : checker ] .", "We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets. Large-scale document-summary datasets are rare and in practice there is a mismatch between research datasets and real-world use-cases where collecting summaries is expensive; the most common setting is that of low-resource summarization. We simulate this setting and show that our model is able to adapt very quickly when fine-tuning with small numbers of supervised pairs, obtaining state-of-the-art results in 6 datasets with only 1000 examples. Qualitatively we observed high quality outputs from our\nbest models and validated this in human evaluation studies. We found that PEGASUS summaries are at least as good as reference summaries for the datasets we assessed \u2013 XSum, CNN/DailyMail, and Reddit TIFU \u2013 even at low-levels of supervision. To summarize our contributions:\n\u2022 We propose a new self-supervised pre-training objective for abstractive summarization, gap-sentences generation, and study strategies for selecting those sentences. \u2022 We evaluate the proposed pre-training objective on a broad range of downstream summarization tasks, with careful ablations to choose the best model settings, which we use to train a 568M parameter PEGASUS model that surpasses or is on-par with the state-of-theart on all 12 downstream datasets considered. \u2022 We show how good abstractive summarization performance can be achieved across broad domains with very little supervision by fine-tuning the PEGASUS model and surpassing previous state-of-the-art results on many tasks with as little as 1000 examples. \u2022 We conducted human evaluation studies to validate our experimental design and demonstrate human-level summarization performance on XSum, CNN/DailyMail, and Reddit TIFU.", "Figure 1: The base architecture of PEGASUS is a standard Transformer encoder-decoder. Both GSG and MLM are applied simultaneously to this example as pre-training objectives. Originally there are three sentences. One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some tokens are randomly masked by [MASK2] (MLM). Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. *Equal contribution 1Data Science Institute, Imperial College London, London, UK 2Brain Team, Google Research, Mountain View, CA, USA. Correspondence to: Jingqing Zhang <jingqing.zhang15@imperial.ac.uk>, Yao Zhao <yaozhaoyz@google.com>, Mohammad Saleh <msaleh@google.com>, Peter J. Liu <peterjliu@google.com>."]}
{"pkey": "pegasus_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "we study pre-training objectives specifically for abstractive text summarization and evaluate on 12 downstream datasets spanning news (Hermann et al., 2015; Narayan et al., 2018; Grusky et al., 2018; Rush et al., 2015; Fabbri et al., 2019), science (Cohan et al., 2018), short stories (Kim et al., 2019), instructions (Koupaee & Wang, 2018), emails (Zhang & Tetreault, 2019), patents (Sharma et al., 2019), and legislative bills (Kornilova & Eidelman, 2019)", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets. Large-scale document-summary datasets are rare and in practice there is a mismatch between research datasets and real-world use-cases where collecting summaries is expensive; the most common setting is that of low-resource summarization. We simulate this setting and show that our model is able to adapt very quickly when fine-tuning with small numbers of supervised pairs, obtaining state-of-the-art results in 6 datasets with only 1000 examples. Qualitatively we observed high quality outputs from our\nbest models and validated this in human evaluation studies. We found that PEGASUS summaries are at least as good as reference summaries for the datasets we assessed \u2013 XSum, CNN/DailyMail, and Reddit TIFU \u2013 even at low-levels of supervision. To summarize our contributions:\n\u2022 We propose a new self-supervised pre-training objective for abstractive summarization, gap-sentences generation, and study strategies for selecting those sentences. \u2022 We evaluate the proposed pre-training objective on a broad range of downstream summarization tasks, with careful ablations to choose the best model settings, which we use to train a 568M parameter PEGASUS model that surpasses or is on-par with the state-of-theart on all 12 downstream datasets considered. \u2022 We show how good abstractive summarization performance can be achieved across broad domains with very little supervision by fine-tuning the PEGASUS model and surpassing previous state-of-the-art results on many tasks with as little as 1000 examples. \u2022 We conducted human evaluation studies to validate our experimental design and demonstrate human-level summarization performance on XSum, CNN/DailyMail, and Reddit TIFU.", "Figure 1: The base architecture of PEGASUS is a standard Transformer encoder-decoder. Both GSG and MLM are applied simultaneously to this example as pre-training objectives. Originally there are three sentences. One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some tokens are randomly masked by [MASK2] (MLM). Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. *Equal contribution 1Data Science Institute, Imperial College London, London, UK 2Brain Team, Google Research, Mountain View, CA, USA. Correspondence to: Jingqing Zhang <jingqing.zhang15@imperial.ac.uk>, Yao Zhao <yaozhaoyz@google.com>, Mohammad Saleh <msaleh@google.com>, Peter J. Liu <peterjliu@google.com>.", "One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some tokens are randomly masked by [MASK2] (MLM). PEGASUS: Pre-training with Extracted Gap-sentences for  Abstractive Summarization. Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. Equal contribution Data Science Institute, Imperial College London, London, UK Brain Team, Google Research, Mountain View, CA, USA. Correspondence to: Jingqing Zhang <jingqing.zhang15@imperial.ac.uk>, Yao Zhao <yaozhaoyz@google.com>, Mohammad Saleh <msaleh@google.com>, Peter J. Liu <peterjliu@google.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s)."]}
{"pkey": "pegasus_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "The paper does not mention any bias or prejudice associated with the model. Howver, while comparing six variants of GSG, the authors mention that \"Lead had decent performance on the two news datasets but was significantly worse on the two non news datasets, which agrees findings of lead bias in news datasets (See et al., 2017; Zhong et al., 2019).\"", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["He was owned by a Georgetown family but eventually gained his freedom. He was well known in Georgetown, a practicing Muslim who made bricks and owned land. His portrait was painted in 1819 by Charles Willson Peale and displayed in Peales Philadelphia museum. Yarrow was also painted by James Alexander Simpson, a Georgetown painter. Today the Alexander painting of Yarrow hangs in the Peabody Room of the Georgetown public library branch. But in 1825 it hung at Ninth and Pennsylvania NW. We know this because the newspapers at the time were full of mentions of a new attraction: the Columbia Museum. The ... ... ... Gold In 1825, a painting of an African American woman was displayed in Washington. Where is it now? Model Go up to your attic right now, or down to your basement, or to your late parents storage unit that you still pay for each month even though youre unfamiliar with its exact contents. Wherever it is you keep things that are ancient and dusty and mysterious, go there immediately and see if an elderly African American woman stares back at you from the canvas of an oil painting. Because if she does, you may have found one of the worlds rarest paintings. ROUGE2-F1 6.00 Document (ID #163) The age when North American clubs look to England and the wider British Isles with a childs embrace, beseeching input from the mother countryscoaching bosom to take the domestic game on, has long since passed. Or so some critics would argue. A new, forward-thinking generation of\ncoaches not steeped in the 4-4-2 and an unadorned, direct style of football rule the waves in this epoch, they postulate. And this legion of coaches are increasingly young, fresh and, most importantly, American. While there might be some historical merit to the spirit of the argument against the British-style coach, it is perhaps itself a little outdated, not to say somewhat harsh on someone like Carl Robinson and his stylish Vancouver Whitecaps, for instance.", "Model Y\u2019all think a little toy buzzer is going to keep the truth from tumbling out from these jaws?\nROUGEL-F1 10.26\nTable I.10: Generated summaries by PEGASUSLARGE (HugeNews) on Multi-News sampled by ROUGE1-F1. Multi-News Document (ID #114) Size really does seem to matter when it comes to cancer risk. Being tall undoubtedly has its benefits. You can see in a crowd and grab objects offhigh shelves. But with the good comes the bad. The taller you are, the higher your odds of developing cancer, and a new paper has added weight to\nthis. key points Key points: Taller people have more cells in their body, as well as higher levels of a protein that encourages cells to divide and grow For every 10cm over the average height, a person\u2019s risk for cancer increases 10 per cent New analysis of data from big cancer studies supports this, and also finds a few specific cancers to be more or less strongly correlated with height Leonard Nunney, an evolutionary biologist at the University of California, Riverside, looked at massive cancer databases to find out how the number of cells in a person\u2019s body, using height as a proxy, might affect their risk of developing cancer. Reporting in the Proceedings of the Royal Society B, he found being taller and having more cells did mean more cancer overall: For every 10 centimetres over the average height, the risk of developing any cancer increased by around 10 per cent. This fits with previous studies. \u201dIf you were comparing a 5-foot guy to a basketball player who\u2019s over 7 feet tall, then that basketball player has around twice the risk of cancer across the board,\u201d Professor Nunney said. He also found that taller people were at much higher risk of melanoma, and women specifically had greater odds of developing thyroid cancer. But it doesn\u2019t mean tall people should panic. \u201dNow, you can\u2019t do anything about your height, but what you can do is tell extremely tall individuals that they should be aware ... ... ...", "1) What is that table on set made of and where can it be ordered? The abuse it has taken without breaking is remarkable. 2) There are an extraordinary number of famous people taken aback by Skips passion in 90 seconds. At a glance ... I have a feeling Im about to unleash, youre Cowboy hating, is it Dak is it Zeke? My Cowboys have arrived, are you sleep deprived? You have to eat humble pie Monday after Monday, they just keep making plays, here we go, every dog has its day Dak attack, the MVP, Dak attack, you decree, ah ah ah, kee kee kee, week after week, I do agree Romo need to check that team chemistry, Dez Bryant chasing greatness oh thats fancy 3) A special consideration should be made by the Recording Academy to bestow an honorary Grammy to the whites of Shannon Sharpes eyes for their performance throughout the video. Is it a hit single? Only time will tell. Will the Cowboys use this as inspiration to reach their first Super Bowl since 1996? If they do, someone better reinforce that table. Gold It won\u2019t end the QB controversy, but it is catchy. Model Skip Bayless is a Dallas Cowboys fan. And unlike in past years, that is worth celebrating (which Bayless does quite frequently on Undisputed). At 12-2 the Cowboys are running away from the rest of the NFC, have clinched a playoff berth and have a rookie duo in Dak Prescott and Ezekiel Elliott that appears poised to make a deep playoff push in 2017 and beyond. With such a bright future, many Cowboys fans will be searching for a way to suitably express their jubilation. Enter Bayless and DJ Steve Porter with How Bout Them Cowboys. There are a few observations to be made right off the bat.\nROUGE1-F1 3.36\nTable I.8: Generated summaries by PEGASUSLARGE (HugeNews) on NEWSROOM sampled by ROUGE2-F1."]}
{"pkey": "pegasus_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Overall, the paper authors observed high-linguistic quality (in terms of fluency and coherence), closely emulating the style of groundtruth summaries. While some previous work suggested that maximum likelihood training results in repetitive text in model outputs (Welleck et al., 2019) the paper authors found this to be rare in our outputs and did not require additional countermeasures to mitigate dis-fluencies. According tothe significance level of p < 0.01, both PEGASUSLARGE (HugeNews) and PEGASUSLARGE (C4) outputs were at least as good as the reference summaries in all cases Although ROUGE clearly has its draw-backs (Kryscinski et al., 2019), over-penalizing abstractive approaches compared to extractive ones and having no sense of linguistic quality, the paper authors found that choosing perplexity-optimized models using aggregated ROUGE (rather than directly optimizing ROUGE as in Paulus et al. (2017)) resulted in qualitatively good models. In the Reddit TIFU case, however, perhaps due to its diverse writing styles, human performance required full supervision.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["thank you\nGold insurance\nModel Insurance\nROUGEL-F1 100.00 Document (ID #106) As discussed during our recent demonstration of our new portfolio tracking system (PortRAC), we have completed a security system that will providea \u201dfirewall\u201d controlling access to each business unit\u2019s data. Our intent is to open up the system to designated members of each business unit or others\nas approved by the COO or Managing Director of that group. In order to complete the initial setup, we need to receive a listing of approved users for your business unit and their login Id\u2019s. If you prefer to delegate security approval authority to someone else on your staff, please let us know and we will establish procedures with them. If additional demonstrations of PortRAC are necessary for your group, please let us know. Thank you for your assistance. Rick C.\nGold PortRAC Security\nModel PortRAC Security System\nROUGEL-F1 80.00 Document (ID #254) Mike, Here\u2019s a revised version of the option agreement. I changed the shipping dates, quantity (10), Outside Exercise Date, and the amount of theoption payment. I did not change the prices, since I had not discussed this with Ben, and couldn\u2019t get the numbers to work. Have there been some\nchanges to the data sheets? It appears to me that there might be a version which reflects the inclusion of the low sound option. In the last draft we did not include the low sound option because the impact on the load losses had not been agreed, so the prices did not reflect the option. Please call or email me if you have any questions or comments. Kay\nGold Option\nModel Rev option agreement\nROUGEL-F1 50.00 Document (ID #267) Susan: I\u2019m not sure that what I told you to do in the CSA Annex with respect to Custodians and their qualifications work so for now let\u2019s keep it outand go back to our old language.", "Model Penelope Cruz had just scored her first lead in a summer blockbuster, as a sexy, feisty swashbuckler opposite her old pal Johnny Depp. ROUGE1-F1 84.44 Document (ID #228) Seven species of bees in Hawaii have been classified as endangered, the first time the insect has been protected by federal law. The U.S. Fish& Wildlife Service has granted seven species of yellow-faced bees native to the islands protection under the Endangered Species Act, which will\nhopefully allow authorities to implement recovery programs, access funding and limit their harm from outside sources, Gregory Koob of USFW told The Associated Press. The yellow-faced bees population faces a range of threats, like habitat destruction, invasive species both animal and vegetable, and all manner of natural phenomena such as wildfires, hurricanes, tsunamis and droughts. This news represents an important step for bees in general. They and many other pollinating species like butterflies are in danger across the world: About 40 percent of invertebrate pollinating species are facing extinction, according to a U.N. report released in February. This could have devastating effects on global agriculture, as about 75 percent of the worlds crops rely on pollination to grow. The ruling also grants protection to 39 plant species found in Hawaii and three other native animals: the band-romped storm-petrel, the orangeblack Hawaiian damselfly and the anchialine pool shrimp. Bees in certain parts of the U.S. are facing a different threat, as aerial spraying, aimed to combat Zika-spreading mosquitos, is killing hives en masse. Gold A species of bee in Hawaii has been added to the endangered species list, a first for the insect. Model Seven species of bees in Hawaii have been classified as endangered, the first time the insect has been protected by federal law.", "we discuss a universal version of the problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the unified problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems . domain restrictions and weakening of the properties are also investigated in order to get some positive results . ROUGE1-F1 48.61 Document (ID #289) machine learning methods are used widely within high energy physics ( hep ) . one promising approach , used extensively outside of hep forapplications such as handwriting recognition , is that of support vector machines ( svms ) , a supervised learning model used with associated learning\nalgorithms for multivariate analysis ( mva ) . developed originally in the 1960s , with the current standard version proposed in 1995 xcite , svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace . however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis . this motivates a number methods for ensuring the classifier is sufficiently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given in section [ sec : svm ] , with an example using svms shown in section [ sec : checker ] ."]}
{"pkey": "pegasus_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "we study pre-training objectives specifically for abstractive text summarization and evaluate on 12 downstream datasets spanning news (Hermann et al., 2015; Narayan et al., 2018; Grusky et al., 2018; Rush et al., 2015; Fabbri et al., 2019), science (Cohan et al., 2018), short stories (Kim et al., 2019), instructions (Koupaee & Wang, 2018), emails (Zhang & Tetreault, 2019), patents (Sharma et al., 2019), and legislative bills (Kornilova & Eidelman, 2019).", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["a recent trend of studies are thus motivated to focus on compressing the size of dnns while mostly keeping their predictive performance xcite . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the ... ... ... Gold as the complexity of deep neural networks ( dnns ) trend to grow to absorb the increasing sizes of data , memory and energy consumption has been receiving more and more attentions for industrial applications , especially on mobile devices . this paper presents a novel structure based on functional hashing to compress dnns , namely funhashnn . for each entry in a deep net , funhashnn uses multiple low - cost hash functions to fetch values in the compression space , and then employs a small reconstruction network to recover that entry . the reconstruction network is plugged into the whole network and trained jointly . funhashnn includes the recently proposed hashednets xcite as a degenerated case , and benefits from larger value capacity and less reconstruction loss . we further discuss extensions with dual space hashing and multi - hops . on several benchmark datasets , funhashnn demonstrates high compression ratios with little loss on prediction accuracy . Model deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision to speech recognition . as the sizes of data mount up , people usually have to increase the number of parameters in dnns so as to absorb the vast volume of supervision . compressing the model size becomes more important for applications on mobile and embedded devices . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the first type tries to revise the training target into more informative supervision using dark knowledge .", "Following Grusky et al., the number of examples and extractive fragment coverage/density for all downstream datasets is illustrated in Appendix A.\n6 Experiments In a similar strategy to Raffel et al. (2019), to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUSBASE, smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUSLARGE. The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size. available C4 corpus. Note that the y-axis in Figures 3, 4, 5 are normalized by the left-most bar using 13 ( R1 R1base + R2R2base + RL RLbase ) where R1, R2, RL are ROUGE F1 scores and R1base, R2base, RLbase are the scores of the configuration corresponding to the first bar. With more pre-training steps, the model observed more documents in the pre-training corpus. A PEGASUSBASE model trained for 500k (highest we tried) steps did not observe all training examples on C4 nor HugeNews. Appendix B shows the number of pre-training steps had an unsurprisingly positive impact on downstream dataset performance. We used 500k steps for the ablation studies and the large model. 6.1.1 PRE-TRAINING CORPUS\nXSum CNN/DailyMail WikiHow Reddit TIFU 0.90\n0.95\n1.00\n1.05\n1.10 C4 HugeNews\nFigure 3: Effect of pre-training corpus. PEGASUSBASE pre-trained on C4 (350M Web-pages) and HugeNews (1.5B news-like documents). 6.1.2 EFFECT OF PRE-TRAINING OBJECTIVES\nGSG We compared six variants of GSG (Lead, Random, Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq) while choosing 30% sentences as gap sentences. As shown in Figure 4a, IndOrig achieved the best performance followed by Seq-Uniq. Ind-Orig and Seq-Uniq were consistently better (or similar) than Random and Lead across the four downstream datasets.", "Lead had decent performance on the two news datasets but was significantly worse on the two non-news datasets, which agrees findings of lead bias in news datasets (See et al., 2017; Zhong et al., 2019). The results suggest choosing principal sentences works best for downstream summarization tasks, and we chose Ind-Orig for the PEGASUSLARGE. A significant hyper-parameter in GSG is the gap-sentences ratio (GSR). A low GSR makes the pre-training less challenging and computationally efficient. On the other hand, choosing gap sentences at a high GSR loses contextual in-\nformation necessary to guide the generation. We compared GSRs from 15% to 75%. For a fair comparison, the original documents were truncated to have up to 400 words. The maximum input length, Linput in the encoder and the maximum target length, Ltarget in the decoder were set as 512 tokens. on non-news datasets, especially WikiHow. On XSum and CNN/DailyMail, Unigram 96k achieved the highest ROUGE scores. On WikiHow and Reddit TIFU, the best configurations were Unigram 128k and 64k respectively. Therefore, we used the overall best vocabulary option Unigram 96k in PEGASUSLARGE. 6.2 Larger Model Results\nCompared with PEGASUSBASE, the large model PEGASUSLARGE had increased capacity from larger hidden size (H : 768 \u2192 1024, F : 3072 \u2192 4096, A : 12 \u2192 16), number of layers (L : 12 \u2192 16) and traversed much more data, due to larger batch size (B : 256 \u2192 8192) (same number of pre-training steps, 500k). We adopted the best practices found in the PEGASUSBASE ablation studies using the GSG (Ind-Orig) pre-training objective without MLM and Unigram vocabulary size of 96k. In total, PEGASUSLARGE had 568M parameters. To encourage the model to copy, which is an important aspect of the more extractive datasets, we left 20% of selected sentences unchanged in the input instead of replacing with [MASK1]. We increased the GSR to 45% to achieve a similar number of \u201cgaps\u201d as the optimal 30% found above."]}
{"pkey": "pegasus_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors compared two tokenization methods: Byte-pairencoding algorithm (BPE) (Wu et al., 2016; Sennrich et al.,2016), and SentencePiece Unigram algorithm (Unigram) proposed in Kudo (2018). The paper authors evaluated Unigram with different vocabulary sizes ranging from 32k to 256k.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["Lead had decent performance on the two news datasets but was significantly worse on the two non-news datasets, which agrees findings of lead bias in news datasets (See et al., 2017; Zhong et al., 2019). The results suggest choosing principal sentences works best for downstream summarization tasks, and we chose Ind-Orig for the PEGASUSLARGE. A significant hyper-parameter in GSG is the gap-sentences ratio (GSR). A low GSR makes the pre-training less challenging and computationally efficient. On the other hand, choosing gap sentences at a high GSR loses contextual in-\nformation necessary to guide the generation. We compared GSRs from 15% to 75%. For a fair comparison, the original documents were truncated to have up to 400 words. The maximum input length, Linput in the encoder and the maximum target length, Ltarget in the decoder were set as 512 tokens. on non-news datasets, especially WikiHow. On XSum and CNN/DailyMail, Unigram 96k achieved the highest ROUGE scores. On WikiHow and Reddit TIFU, the best configurations were Unigram 128k and 64k respectively. Therefore, we used the overall best vocabulary option Unigram 96k in PEGASUSLARGE. 6.2 Larger Model Results\nCompared with PEGASUSBASE, the large model PEGASUSLARGE had increased capacity from larger hidden size (H : 768 \u2192 1024, F : 3072 \u2192 4096, A : 12 \u2192 16), number of layers (L : 12 \u2192 16) and traversed much more data, due to larger batch size (B : 256 \u2192 8192) (same number of pre-training steps, 500k). We adopted the best practices found in the PEGASUSBASE ablation studies using the GSG (Ind-Orig) pre-training objective without MLM and Unigram vocabulary size of 96k. In total, PEGASUSLARGE had 568M parameters. To encourage the model to copy, which is an important aspect of the more extractive datasets, we left 20% of selected sentences unchanged in the input instead of replacing with [MASK1]. We increased the GSR to 45% to achieve a similar number of \u201cgaps\u201d as the optimal 30% found above.", "2 Related Work Dai & Le (2015); Ramachandran et al. (2017) used LM and autoencoder pre-training on in-domain data to improve performance of RNN sequence models. However, the combination of pre-training with much larger external text corpora (such as Wikipedia, books, or Web-pages) and Transformerbased sequence models has led to a dramatic improvement in performance when fine-tuned for both natural language understanding and text generation tasks (Radford et al., 2018a; Devlin et al., 2019; Rothe et al., 2019; Yang et al., 2019; Joshi et al., 2019; Song et al., 2019; Dong et al., 2019; Lewis et al., 2019). Most similar to our approach are Transformer encoder-decoder models pre-trained on some masked input pre-training objective. MASS (Song et al., 2019) proposed masked sequence-tosequence generation that reconstructs a sentence fragment given the remaining part of the sentence. A single sentence fragment was randomly selected. UniLM (Dong et al., 2019) proposed jointly training on three types of language modeling tasks: unidirectional (leftto-right and right-to-left), bidirectional (word-level mask,\nwith next sentence prediction), and sequence-to-sequence (word-level mask) prediction. T5 (Raffel et al., 2019) generalized the text-to-text framework to a variety of NLP tasks and showed the advantage of scaling up model size (to 11 billion parameters) and pre-training corpus, introducing C4, a massive text corpus derived from Common Crawl, which we also use in some of our models. T5 was pre-trained with randomly corrupted text spans of varying mask ratios and sizes of spans. BART (Lewis et al., 2019) introduced a denoising autoencoder to pre-train sequence-to-sequence models. BART corrupted text with an arbitrary noising function and learned to reconstruct the original text. For generation tasks, the noising function was text infilling which used single mask tokens to mask random sampled spans of text.", "An example containing lead, random and principal gap sentence selection strategies are shown in Figure 2.\n3.2 Masked Language Model (MLM) Following BERT, we select 15% tokens in the input text, and the selected tokens are (1) 80% of time replaced by a mask token [MASK2], or (2) 10% of time replaced by a random token, or (3) 10% of time unchanged. We apply MLM to train the Transformer encoder as the sole pre-training objective or along with GSG. When MLM is the sole pre-training objective, the Transformer decoder shares all parameters with encoder when fine-tuning on downstream tasks following Rothe et al. (2019). Figure 1 simultaneously shows how both GSG and MLM are applied to the same example when used in conjunction. However, we found that MLM does not improve downstream tasks at large number of pre-training steps (section 6.1.2), and chose not to include MLM in the final model PEGASUSLARGE (section 6.2). 4 Pre-training Corpus For pre-training we considered two large text corpora:\n\u2022 C4, or the Colossal and Cleaned version of Common Crawl, introduced in Raffel et al. (2019); consists of text from 350M Web-pages (750GB). \u2022 HugeNews, a dataset of 1.5B articles (3.8TB) collected from news and news-like websites from 2013- 2019. A whitelist of domains ranging from highquality news publishers to lower-quality sites such as high-school newspapers, and blogs was curated and used to seed a web-crawler. Heuristics were used to identify news-like articles, and only the main article text was extracted as plain text. CNN/DailyMail (Hermann et al., 2015) dataset contains 93k articles from the CNN, and 220k articles the Daily Mail newspapers. Both publishers supplement their articles with bullet point summaries. We use the non-anonymized variant used in See et al. (2017). NEWSROOM (Grusky et al., 2018) is a large dataset containing 1.3M article-summary pairs written by authors and editors in the newsrooms of 38 major publications between 1998 and 2017."]}
{"pkey": "pegasus_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "The paper does not explicitly mentions the pre-processing done. However the authors mention that \"we used public abstractive summarization datasets, and access them through TensorFlow Summarization Datasets 1 ,which provides publicly reproducible code for dataset processing and train/validation/test splits.\"", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["It\u2019ll then become illegal for telemarketers to contact you: https://www.donotcall.gov/. If you do get contacted by a telemarketer after putting yourself on the list, you can report the number that called you to the FTC or FCC. If they tell you a mutual acquaintance or other contact, consider taking the call. If they dont or cant tell you, or if they got your contact information from some data pool or list, ask to be removed from their call list and hang up., Be clear about conveying your annoyance with phone calls, especially if you might want to do business with or purchase something from that company but dont want annoying callers. Make sure you provide them with an email address specifically for spam or sales deals. Gold Dont just hang up right away. Add yourself to the national do not call registry. Ask where they got your contact info. Tell them No calls, emails only. If youre running a business, in charge of purchasing, or otherwise need to field sales pitches from time to time, tell telemarketers that you only respond to emails. Model Hang up immediately. Report persistent callers. Ask to be removed from the call list. Be clear about your preferences.\nROUGE1-F1 20.78\nTable I.17: Generated summaries by PEGASUSLARGE (HugeNews) on WikiHow sampled by ROUGE2-F1. WikiHow Document (ID #29) Microwave frozen spinach for two minutes at 50% power. Break the spinach apart into smaller pieces. Microwave it again, at 50% power, checkingand stirring it whenever a minute passes by, until its defrosted. Alternately, you can use one packed cup of fresh baby spinach, washed and finely\nchopped with the stems removed. ; , Wrap the spinach in a clean kitchen towel. Squeeze and wring the towel to remove the liquid. Another option is to press the wet spinach into a sieve over the sink., Make sure your hands are clean and free of jewelry. Place the ground meat in the bowl first, and press it down in the middle to form a well.", "Establish if there are any upfront costs The real costs associated with accepting online card payments do not start and end at the rate quote. Manage reporting and administration facilities Accepting online payments is only one part of the e-commerce equation. Enquire about their technical support A service is not a service unless its fully backed up. Find out if you would be protected against fraud When accepting card payments online, fraud is potentially the biggest threat to your profits. Make sure you can trust their services Online, the difference between a prospect and a customer is trust. Look out for a full service history Okay, its not a car, but the principle is the same. Model Establish what your PSP needs to do to meet the needs of your online merchant business. Look at the pricing offered by the PSP. Consider the level of support youll get from the PSP. Look at the tools youll be using to manage your merchant services. Consider the need for fraud management systems (PMS).Theres no doubt that the quality of a PSPs fraud solution will depend upon its support for merchant services solutions in other words, its ability to meet the needs of your online merchant business. Check the reliability of the PSPs customer service. ROUGE1-F1 31.58 Document\n(ID #3) If you hang up on a telemarketer right away, youll probably be placed on the no answer list, and will be called again eventually. You dont want toengage a telemarketer in conversation either, especially if you have absolutely no interest in their product or service. The easiest way to handle a telemarketer is to say, Please put me on your do not call list. If the telemarketer keeps interrupting you or a robot calls you, you might just have to hang up. If the same number persistently calls, report it to the FCC by calling 1-888-CALL-FCC or going to https://www.fcc.gov/. If you live in the United States, you can add yourself to the do not call registry.", "Lead had decent performance on the two news datasets but was significantly worse on the two non-news datasets, which agrees findings of lead bias in news datasets (See et al., 2017; Zhong et al., 2019). The results suggest choosing principal sentences works best for downstream summarization tasks, and we chose Ind-Orig for the PEGASUSLARGE. A significant hyper-parameter in GSG is the gap-sentences ratio (GSR). A low GSR makes the pre-training less challenging and computationally efficient. On the other hand, choosing gap sentences at a high GSR loses contextual in-\nformation necessary to guide the generation. We compared GSRs from 15% to 75%. For a fair comparison, the original documents were truncated to have up to 400 words. The maximum input length, Linput in the encoder and the maximum target length, Ltarget in the decoder were set as 512 tokens. on non-news datasets, especially WikiHow. On XSum and CNN/DailyMail, Unigram 96k achieved the highest ROUGE scores. On WikiHow and Reddit TIFU, the best configurations were Unigram 128k and 64k respectively. Therefore, we used the overall best vocabulary option Unigram 96k in PEGASUSLARGE. 6.2 Larger Model Results\nCompared with PEGASUSBASE, the large model PEGASUSLARGE had increased capacity from larger hidden size (H : 768 \u2192 1024, F : 3072 \u2192 4096, A : 12 \u2192 16), number of layers (L : 12 \u2192 16) and traversed much more data, due to larger batch size (B : 256 \u2192 8192) (same number of pre-training steps, 500k). We adopted the best practices found in the PEGASUSBASE ablation studies using the GSG (Ind-Orig) pre-training objective without MLM and Unigram vocabulary size of 96k. In total, PEGASUSLARGE had 568M parameters. To encourage the model to copy, which is an important aspect of the more extractive datasets, we left 20% of selected sentences unchanged in the input instead of replacing with [MASK1]. We increased the GSR to 45% to achieve a similar number of \u201cgaps\u201d as the optimal 30% found above."]}
{"pkey": "pegasus_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "In a similar strategy to Raffel et al. (2019), to save time and computation the paper authors conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUSBASE, smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUSLARGE. PEGASUSBASE had L = 12, H = 768, F = 3072, A = 12 and PEGASUSLARGE had L = 16, H = 1024, F = 4096, A = 16, where L denotes the number of layers for encoder and decoder (i.e. Transformer blocks), H for the hidden size, F for the feed-forward layer size and A for the number of self-attention heads. The paper authors pre-trained PEGASUSBASE with a batch size of 256 and PEGASUSLARGE with a batch size of 8192. For optimization, both pre-training and finetuning used Adafactor (Shazeer & Stern, 2018) with square root learning rate decay and dropout rate of 0.1. Byte-pairencoding algorithm (BPE) (Wu et al., 2016; Sennrich et al., 2016), and SentencePiece Unigram algorithm (Unigram) proposed in Kudo (2018). The paper authors evaluated Unigram with different vocabulary sizes ranging from 32k to 256k. In these experiments, models were pre-trained for 500k steps on the C4 corpus. Compared with PEGASUSBASE, the large model PEGASUSLARGE had increased capacity from larger hidden size (H : 768 \u2192 1024, F : 3072 \u2192 4096, A : 12 \u2192 16), number of layers (L : 12 \u2192 16) and traversed much more data, due to larger batch size (B : 256 \u2192 8192) (same number of pre-training steps, 500k). The paper authors adopted the best practices found in the PEGASUSBASE ablation studies using the GSG (Ind-Orig) pre-training objective without MLM and Unigram vocabulary size of 96k. In total, PEGASUSLARGE had 568M parameters", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["Lead had decent performance on the two news datasets but was significantly worse on the two non-news datasets, which agrees findings of lead bias in news datasets (See et al., 2017; Zhong et al., 2019). The results suggest choosing principal sentences works best for downstream summarization tasks, and we chose Ind-Orig for the PEGASUSLARGE. A significant hyper-parameter in GSG is the gap-sentences ratio (GSR). A low GSR makes the pre-training less challenging and computationally efficient. On the other hand, choosing gap sentences at a high GSR loses contextual in-\nformation necessary to guide the generation. We compared GSRs from 15% to 75%. For a fair comparison, the original documents were truncated to have up to 400 words. The maximum input length, Linput in the encoder and the maximum target length, Ltarget in the decoder were set as 512 tokens. on non-news datasets, especially WikiHow. On XSum and CNN/DailyMail, Unigram 96k achieved the highest ROUGE scores. On WikiHow and Reddit TIFU, the best configurations were Unigram 128k and 64k respectively. Therefore, we used the overall best vocabulary option Unigram 96k in PEGASUSLARGE. 6.2 Larger Model Results\nCompared with PEGASUSBASE, the large model PEGASUSLARGE had increased capacity from larger hidden size (H : 768 \u2192 1024, F : 3072 \u2192 4096, A : 12 \u2192 16), number of layers (L : 12 \u2192 16) and traversed much more data, due to larger batch size (B : 256 \u2192 8192) (same number of pre-training steps, 500k). We adopted the best practices found in the PEGASUSBASE ablation studies using the GSG (Ind-Orig) pre-training objective without MLM and Unigram vocabulary size of 96k. In total, PEGASUSLARGE had 568M parameters. To encourage the model to copy, which is an important aspect of the more extractive datasets, we left 20% of selected sentences unchanged in the input instead of replacing with [MASK1]. We increased the GSR to 45% to achieve a similar number of \u201cgaps\u201d as the optimal 30% found above.", "Following Grusky et al., the number of examples and extractive fragment coverage/density for all downstream datasets is illustrated in Appendix A.\n6 Experiments In a similar strategy to Raffel et al. (2019), to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUSBASE, smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUSLARGE. The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size. available C4 corpus. Note that the y-axis in Figures 3, 4, 5 are normalized by the left-most bar using 13 ( R1 R1base + R2R2base + RL RLbase ) where R1, R2, RL are ROUGE F1 scores and R1base, R2base, RLbase are the scores of the configuration corresponding to the first bar. With more pre-training steps, the model observed more documents in the pre-training corpus. A PEGASUSBASE model trained for 500k (highest we tried) steps did not observe all training examples on C4 nor HugeNews. Appendix B shows the number of pre-training steps had an unsurprisingly positive impact on downstream dataset performance. We used 500k steps for the ablation studies and the large model. 6.1.1 PRE-TRAINING CORPUS\nXSum CNN/DailyMail WikiHow Reddit TIFU 0.90\n0.95\n1.00\n1.05\n1.10 C4 HugeNews\nFigure 3: Effect of pre-training corpus. PEGASUSBASE pre-trained on C4 (350M Web-pages) and HugeNews (1.5B news-like documents). 6.1.2 EFFECT OF PRE-TRAINING OBJECTIVES\nGSG We compared six variants of GSG (Lead, Random, Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq) while choosing 30% sentences as gap sentences. As shown in Figure 4a, IndOrig achieved the best performance followed by Seq-Uniq. Ind-Orig and Seq-Uniq were consistently better (or similar) than Random and Lead across the four downstream datasets.", "the superposition rule given in terms of tomograms and in terms of weight functions of the diagonal representation where explicit kernels of the corresponding star - products are employed to obtain the addition rules for the tomograms and weight functions are considered . we discuss also the formulation of the separability and entanglement properties of composed system in the tomographic probability and diagonal representations . ROUGE2-F1 26.52 Document (ID #32) deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision xcite , to speech recognitionxcite , natural language processing xcite , and domain adaptation xcite . as the sizes of data mount up , people usually have to increase the number\nof parameters in dnns so as to absorb the vast volume of supervision . high performance computing techniques are investigated to speed up dnn training , concerning optimization algorithms , parallel synchronisations on clusters w / o gpus , and stochastic binarization / ternarization , etc xcite . on the other hand the memory and energy consumption is usually , if not always , constrained in industrial applications xcite . for instance , for commercial search engines ( e.g. , google and baidu ) and recommendation systems ( e.g. , netflix and youtube ) , the ratio between the increased model size and the improved performance should be considered given limited online resources . compressing the model size becomes more important for applications on mobile and embedded devices xcite . having dnns running on mobile apps owns many great features such as better privacy , less network bandwidth and real time processing . however , the energy consumption of battery - constrained mobile devices is usually dominated by memory access , which would be greatly saved if a dnn model can fit in on - chip storage rather than dram storage ( c.f . xcite for details ) ."]}
{"pkey": "pegasus_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors hypothesize that using a pre-training objective that more closely resembles the downstream task leads to better and faster fine-tuning performance. Given our intended use for abstractive summarization, our proposed pre-training objective involves generating summary-like text from an input document. In order to leverage massive text corpora for pretraining, the paper authors design a sequence-to-sequence self supervised objective in the absence of abstactive summaries.\nMore details on GSG are in section 3.1.\nFor MLM: we select 15% tokens in the input text, and the selected tokens are (1) 80% of time replaced by a mask token [MASK], or (2) 10% of time replaced by a random token, or (3) 10% of time unchanged. The paper authors apply MLM to train the Transformer encoder as the sole pre-training objective or along with GSG. When MLM is the sole pre-training objective, the Transformer decoder shares all parameters with encoder when fine-tuning on downstream tasks following Rothe et al. (2019). In a similar strategy to Raffel et al. (2019), to save time and computation the paper authors conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUSBASE, smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUSLARGE The paper authors pre-trained PEGASUSBASE with a batch size of 256 and PEGASUSLARGE with a batch size of 8192.For optimization, both pre-training and finetuning used Adafactor (Shazeer & Stern, 2018) with square root learning rate decay and dropout rate of 0.1. To simulate the low-resource summarization setting, the paper authors picked the first 10k (k = 1, 2, 3, 4) training examples from each dataset to fine-tune PEGASUSLARGE (HugeNews) . The paper authors fine-tuned the models up to 2000 steps with batch size 256, learning rate 0.0005, and picked the checkpoint with best validation performance Further changes were made: The model was pre-trained for 1.5M steps instead of 500k steps, as the paper authors observed slower convergence of pre-training perplexity. (5) The SentencePiece tokenizer was updated to encode the newline character.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["the superposition rule given in terms of tomograms and in terms of weight functions of the diagonal representation where explicit kernels of the corresponding star - products are employed to obtain the addition rules for the tomograms and weight functions are considered . we discuss also the formulation of the separability and entanglement properties of composed system in the tomographic probability and diagonal representations . ROUGE2-F1 26.52 Document (ID #32) deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision xcite , to speech recognitionxcite , natural language processing xcite , and domain adaptation xcite . as the sizes of data mount up , people usually have to increase the number\nof parameters in dnns so as to absorb the vast volume of supervision . high performance computing techniques are investigated to speed up dnn training , concerning optimization algorithms , parallel synchronisations on clusters w / o gpus , and stochastic binarization / ternarization , etc xcite . on the other hand the memory and energy consumption is usually , if not always , constrained in industrial applications xcite . for instance , for commercial search engines ( e.g. , google and baidu ) and recommendation systems ( e.g. , netflix and youtube ) , the ratio between the increased model size and the improved performance should be considered given limited online resources . compressing the model size becomes more important for applications on mobile and embedded devices xcite . having dnns running on mobile apps owns many great features such as better privacy , less network bandwidth and real time processing . however , the energy consumption of battery - constrained mobile devices is usually dominated by memory access , which would be greatly saved if a dnn model can fit in on - chip storage rather than dram storage ( c.f . xcite for details ) .", "the second type observes the redundancy existence in network weights , and exploits techniques to constrain or reduce the number of free - parameters in dnns during learning . in applications , we observe hashednets compresses model sizes greatly at marginal loss of accuracy for some situations , whereas also significantly loses accuracy for others . after revisiting its mechanism , we conjecture this instability comes from at least three factors . first , hashing and training are disjoint in a two - phase manner , i.e. , once inappropriate collisions exist , there may be no much optimization room left for training . second , one single hash\nROUGE2-F1 7.21\nTable I.27: Generated summaries by PEGASUSLARGE (HugeNews) on arXiv sampled by ROUGEL-F1. arXiv Document (ID #248) stripped supernovae ( sne ) and long - duration gamma - ray bursts ( long grbs ) are nature s most powerful explosions from massive stars . theyenergize and enrich the interstellar medium , and , like beacons , they are visible over large cosmological distances . however , the mass and\nmetallicity range of their progenitors is not known , nor the detailed physics of the explosion ( see reviews by xcite and xcite ) . stripped - envelope sne ( i.e , sne of types iib , ib , and ic , e.g. , xcite ) are core - collapse events whose massive progenitors have been stripped of progressively larger amounts of their outermost h and he envelopes ( fig . [ fig1 ] ) . in particular , broad - lined sne ic ( sne ic - bl ) are sne ic whose line widths approach 20,000xmath030,000 xmath1 around maximum light ( see below ) and whose optical spectra show no trace of h and he . for the last 15 years , the exciting connection between long grbs and sne ic - bl , the only type of sne observed accompanying long grbs ( for reviews , see xcite ) , and the existence of many more sne ic - bl without grbs raises the question of what distinguishes sn - grb progenitors from those of ordinary sne ic - bl without grbs .", "Following Grusky et al., the number of examples and extractive fragment coverage/density for all downstream datasets is illustrated in Appendix A.\n6 Experiments In a similar strategy to Raffel et al. (2019), to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUSBASE, smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUSLARGE. The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size. available C4 corpus. Note that the y-axis in Figures 3, 4, 5 are normalized by the left-most bar using 13 ( R1 R1base + R2R2base + RL RLbase ) where R1, R2, RL are ROUGE F1 scores and R1base, R2base, RLbase are the scores of the configuration corresponding to the first bar. With more pre-training steps, the model observed more documents in the pre-training corpus. A PEGASUSBASE model trained for 500k (highest we tried) steps did not observe all training examples on C4 nor HugeNews. Appendix B shows the number of pre-training steps had an unsurprisingly positive impact on downstream dataset performance. We used 500k steps for the ablation studies and the large model. 6.1.1 PRE-TRAINING CORPUS\nXSum CNN/DailyMail WikiHow Reddit TIFU 0.90\n0.95\n1.00\n1.05\n1.10 C4 HugeNews\nFigure 3: Effect of pre-training corpus. PEGASUSBASE pre-trained on C4 (350M Web-pages) and HugeNews (1.5B news-like documents). 6.1.2 EFFECT OF PRE-TRAINING OBJECTIVES\nGSG We compared six variants of GSG (Lead, Random, Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq) while choosing 30% sentences as gap sentences. As shown in Figure 4a, IndOrig achieved the best performance followed by Seq-Uniq. Ind-Orig and Seq-Uniq were consistently better (or similar) than Random and Lead across the four downstream datasets."]}
{"pkey": "pegasus_13", "question": "Describe the computational resources used to train the model.", "answer": "Each checkpoint is 2.2 GB on disk and 568M parameters. FP16 is not supported (help/ideas on this appreciated!). Summarizing xsum in fp32 takes about 400ms/sample, with default parameters on a v100 GPU.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["the superposition rule given in terms of tomograms and in terms of weight functions of the diagonal representation where explicit kernels of the corresponding star - products are employed to obtain the addition rules for the tomograms and weight functions are considered . we discuss also the formulation of the separability and entanglement properties of composed system in the tomographic probability and diagonal representations . ROUGE2-F1 26.52 Document (ID #32) deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision xcite , to speech recognitionxcite , natural language processing xcite , and domain adaptation xcite . as the sizes of data mount up , people usually have to increase the number\nof parameters in dnns so as to absorb the vast volume of supervision . high performance computing techniques are investigated to speed up dnn training , concerning optimization algorithms , parallel synchronisations on clusters w / o gpus , and stochastic binarization / ternarization , etc xcite . on the other hand the memory and energy consumption is usually , if not always , constrained in industrial applications xcite . for instance , for commercial search engines ( e.g. , google and baidu ) and recommendation systems ( e.g. , netflix and youtube ) , the ratio between the increased model size and the improved performance should be considered given limited online resources . compressing the model size becomes more important for applications on mobile and embedded devices xcite . having dnns running on mobile apps owns many great features such as better privacy , less network bandwidth and real time processing . however , the energy consumption of battery - constrained mobile devices is usually dominated by memory access , which would be greatly saved if a dnn model can fit in on - chip storage rather than dram storage ( c.f . xcite for details ) .", "generalisation is discussed in section [ sec : gen ] with an illustrative example of how this can improve performance given in section [ sec : check2 ] . consider the problem of linear classification with the svm where the training set , xmath0 , is linearly separable . we define a separating hyperplane given by xmath1 , where xmath2 , the weight vector , is perpendicular to the hyperplane , and xmath3 , the bias , determines the distance of the hyperplane from the origin ( fig . ) . a decision function defined by xmath4 is used to positively and negatively classify xmath5 , the points in the training set . without further constraint the ... ... ...\nGold we review the concept of support vector machines ( svms ) and discuss examples of their use . one of the benefits of svm algorithms , compared with neural networks and decision trees is that they can be less susceptible to over fitting than those other algorithms are to over training . this issue is related to the generalisation of a multivariate algorithm ( mva ) ; a problem that has often been overlooked in particle physics . we discuss cross validation and how this can be used to improve the generalisation of a mva in the context of high energy physics analyses . the examples presented use the toolkit for multivariate analysis ( tmva ) based on root and describe our improvements to the svm functionality and new tools introduced for cross validation within this framework . Model support vector machines ( svms ) are one of the most promising methods for machine learning in high energy physics . svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace . however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis .", "a recent trend of studies are thus motivated to focus on compressing the size of dnns while mostly keeping their predictive performance xcite . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the ... ... ... Gold as the complexity of deep neural networks ( dnns ) trend to grow to absorb the increasing sizes of data , memory and energy consumption has been receiving more and more attentions for industrial applications , especially on mobile devices . this paper presents a novel structure based on functional hashing to compress dnns , namely funhashnn . for each entry in a deep net , funhashnn uses multiple low - cost hash functions to fetch values in the compression space , and then employs a small reconstruction network to recover that entry . the reconstruction network is plugged into the whole network and trained jointly . funhashnn includes the recently proposed hashednets xcite as a degenerated case , and benefits from larger value capacity and less reconstruction loss . we further discuss extensions with dual space hashing and multi - hops . on several benchmark datasets , funhashnn demonstrates high compression ratios with little loss on prediction accuracy . Model deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision to speech recognition . as the sizes of data mount up , people usually have to increase the number of parameters in dnns so as to absorb the vast volume of supervision . compressing the model size becomes more important for applications on mobile and embedded devices . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the first type tries to revise the training target into more informative supervision using dark knowledge ."]}
{"pkey": "pegasus_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "The training code and instructions for using model checkpoints can be found at https://github.com/google-research/pegasus\nDataset : https://www.tensorflow.org/datasets/catalog/overview\nImplemented in https://github.com/google/sentencepiece", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["37.52/10.63/22.79\narXiv 28.05/6.63/17.72 31.38/8.16/17.97 33.06/9.66/20.11 39.46/12.38/22.20 40.24/14.04/23.11 41.59/14.26/23.55 PubMed 28.17/7.57/17.85 33.31/10.58/20.05 34.05/12.75/21.12 40.15/15.56/24.05 41.75/16.74/24.80 40.59/15.59/23.59 AESLC 10.35/3.86/9.29 11.97/4.91/10.84 16.05/7.20/15.32 28.58/15.45/28.14 36.47/20.85/35.53 23.67/10.29/23.44 BillSum 41.02/17.44/25.24 40.48/18.49/27.27 44.78/26.40/34.40 46.47/30.58/37.21 50.81/34.49/40.96 40.80/23.83/33.73\nF Human Evaluation Details In all human evaluation experiments we used the same task template shown in Figure F.1, where workers were asked to rate 4 summaries for a document on a scale of 1 (poor summary) to 5 (great summary). The order in which the summaries are presented for each task was random per example. Each task was independently done by 3 different workers and we retained the median score across workers for each summary. We paid 1 USD per task and used the following critieria for workers to ensure high-quality:\n\u2022 Location: US\n\u2022 Minimum approval rate: 95%\n\u2022 Minimum HIITs: 1000\nWith this criteria we observed high reproducibility in the conclusions of the huamn evaluation. Multiple runs of the same experiment with different workers meeting this criteria yielded very similar results. The HITT template is provided at https://github.com/google-research/pegasus. In experiment 1, the four summaries corresponded to 3 models (PEGASUSLARGE pre-trained on HugeNews, C4, and TransformerBASE) that were fine-tuned using all the supervised examples along with the reference (human) summary. We sampled 100 examples from each dataset (XSum, CNN/DailyMail, Reddit TIFU). In experiment 2, we evaluated 4 models (PEGASUSLARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries.", "we discuss a universal version of the problem involving arbitrary preference intensities as well as incomplete and multiple comparisons . the main contribution of this paper is the presentation of an impossibility theorem : consistency requiring that if an object is ranked at least as high as another in two independent problems , then it is ranked as high as the other in the unified problem , too and self - consistency a less known but intuitive property , introduced in xcite , which prohibits to assign a lower rank for an object with a better or equivalent performance than another can not be met simultaneously by any ranking method on the set of all problems . domain restrictions and weakening of the properties are also investigated in order to get some positive results . ROUGE1-F1 48.61 Document (ID #289) machine learning methods are used widely within high energy physics ( hep ) . one promising approach , used extensively outside of hep forapplications such as handwriting recognition , is that of support vector machines ( svms ) , a supervised learning model used with associated learning\nalgorithms for multivariate analysis ( mva ) . developed originally in the 1960s , with the current standard version proposed in 1995 xcite , svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace . however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis . this motivates a number methods for ensuring the classifier is sufficiently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given in section [ sec : svm ] , with an example using svms shown in section [ sec : checker ] .", "this motivates a number methods for ensuring the classifier is sufficiently generalised such that when used on an unseen dataset the performance can be accurately predicted . in this paper a brief overview of svms is given , with an example using svms shown in section [ sec : checker ] . generalisation is discussed with an illustrative example of how this can improve performance given in section [ sec : gen ] . ROUGE1-F1 35.29\nTable I.26: Generated summaries by PEGASUSLARGE (HugeNews) on arXiv sampled by ROUGE2-F1. arXiv Document (ID #294) the pure quantum states are traditionally associated with the wave function xcite or a vector in the hilbert space xcite . the mixed quantum statesare described by the density matrix xcite or the density operator xcite . there exist several representations of quantum states in terms of the quasidis-\ntribution functions like the wigner function xcite and the husimi kano function xcite . the diagonal representation of quantum states was suggested in xcite ( see also xcite ) . it was studied and applied in xcite . in this representation , a quantum state is represented in terms of weighted sum of coherent - state xmath0 projectors . the properties of all the quantum - state representations considered are associated with the properties of the density operator which is hermitian , trace - class nonnegative operator . this means , in particular , that all the eigenvalues of the density operators must be nonnegative . in the quantum domain , the multipartite systems have a specific property connected with strong correlations of the quantum subsystems . this property provides the entanglement phenomenon xcite . in the diagonal representation of the density states , the weight function xmath1 is an analog of the probability - distribution function in the phase space . for some class of states , this function is identical to the probability - distribution function like in classical statistical mechanics ."]}
{"pkey": "pegasus_15", "question": "What is the pretraining objective of the model? ", "answer": "our proposed pre-training objective involves generating summary-like text from an input document. Two pre-training taks are introduced GSG and MLM", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["In contrast to MASS, UniLM, BART and T5, the proposed PEGASUS masks multiple whole sentences rather than smaller continuous text spans. In our final objective we deterministically choose sentences based on importance, rather than randomly. As in T5, PEGASUS does not reconstruct full input sequences, and only generates the masked sentences as a single output sequence. In this work we focus entirely on downstream summarization (generative) tasks and do not evaluate on NLU classification tasks. There has been some work on the low-resource, summarization setting using the CNN/DailyMail dataset. Radford et al. (2018b) showed that a large Transformer language model pre-trained on Web text could generate summaries if prompted with \u201dTL;DR\u201d, achieving a ROUGE-2 of 8.27 on CNN/DailyMail. Khandelwal et al. (2019) pre-trained a Transformer language model on Wikipedia, and fine-tuned using 3000 examples, achieving 13.1 ROUGE-2. 3 Pre-training Objectives We propose a new pre-training objective, GSG, in this work, but for comparison, we also evaluate BERT\u2019s maskedlanguage model objective, in isolation and in conjunction with GSG. 3.1 Gap Sentences Generation (GSG)\nWe hypothesize that using a pre-training objective that more closely resembles the downstream task leads to better and faster fine-tuning performance. Given our intended use for abstractive summarization, our proposed pre-training objective involves generating summary-like text from an input document. In order to leverage massive text corpora for pretraining, we design a sequence-to-sequence self-supervised objective in the absence of abstactive summaries. A naive option would be to pre-train as an extractive summarizer;\nhowever, such a procedure would only train a model to copy sentences, thus not suitable for abstractive summarization.", "NEWSROOM Document (ID #193) 2011 - 12:33 pm \u2014 Gregory Booth, the CEO of Zippo, which makes perhaps the worlds most iconic cigarette lighters, stopped by the Forbes studiorecently to talk about how to sell a commodity product at decidedly non-commodity prices. You must be logged in to post a comment Log in with\nyour Forbes account\nGold The CEO of the world\u2019s most iconic maker of cigarette lighters talks about how to sell a commodity product \u2013 at decidedly non-commodity prices. Model Gregory Booth, the CEO of Zippo, which makes perhaps the world\u2019s most iconic cigarette lighters, stopped by the Forbes studio recently to talk about how to sell a commodity product - at decidedly non-commodity prices. ROUGE2-F1 61.02 Document (ID #32) TESTED 2013 Lincoln MKZ Hybrid WHAT IS IT? A Lincoln-branded upscale hybrid sedan with its Ford roots showing. HOW MUCH? $36,820base, $42,415 as tested. The price is the same as an equivalent nonhybrid MKZ. WHAT MAKES IT RUN? A 141-horsepower 2-liter 4-cylinder,\nwith an 88-kilowatt electric motor and a 1.4-kilowatt-hour lithium-ion battery for 188 net horsepower; continuously variable transmission. IS IT THIRSTY? The E.P.A. rating is 45 m.p.g. all around in the city, on the highway and combined. ALTERNATIVES Lexus ES 300h, Ford Fusion Hybrid. Part of the fun of a hybrid car is listening for the gasoline engine to start and analyzing how smoothly it makes the transition from electric to gas. Unless you floor the accelerator, the MKZ Hybrid is so quiet it s hard to tell what s moving you forward. With active noise control, which sends sound-canceling frequencies through the infotainment system, the car errs on the side of isolation: if engagement with the road is your goal, this isn t your ride. Rather, the appeal is the combination of an impressive mileage rating and an understated wood-and-leather ambience. Ford calls the MKZ Hybrid America s most fuel-efficient luxury sedan.", "Sara and Mark: Shari had a counterparty point out to her that in our CSA Annex language where we talk about the\ncircumstances under which a party can \u201dhold\u201d collateral, all of our triggers are based on something happening to our counterparty and nothing is said about what would happen if a Custodian holds the colateral and fails to be creditworthy. Do you have any thoughts on this? At first I thought that we should require the Custodian to be a Qualified institution but I\u2019m not sure that that is the right approach. Let me know what you think. Carol\nGold Custodian Language\nModel Custodians\nROUGEL-F1 0.00 Document (ID #162) Ms. Peppers, I am very sorry, but I must once again reschedule the appointment that you have with Mr. Lay. He will be out of town the entire weekof September 26. The first available time to reschedule is October 19, 2000. Would an afternoon appointment work for you - about 2:00 p.m.? I look\nforward to hearing from you. Rosalee\nGold Reschedule appointment\nModel Mr. Lay\u2019s schedule\nROUGEL-F1 0.00 Document (ID #22) the tax issue that we discussed at the rmt meeting has been cleared. now we need to see if the $250 million is somehow at the rmt level or would betrapped in some way in a rmt filing. i keep thinking that i will have time to track down with weil and has not happened. can you track this down and\nthen let the group know. add stuart to this list. can you do? ? Gold rmt\nModel ISDA\nROUGEL-F1 0.00\nTable I.34: Generated summaries by PEGASUSLARGE (HugeNews) on BillSum sampled by ROUGE1-F1. BillSum Document (ID #193) SECTION 1. SHORT TITLE. This Act may be cited as the Veterans Homebuyer Accessibility Act of 2015\u201d. SEC. 2. VETERAN FIRST-TIMEHOMEBUYER TAX CREDIT."]}
{"pkey": "pegasus_16", "question": "What is the loss function that is used to train the model?", "answer": "The paper authors used PEGASUSBASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. For reproducibility, the paper authors evaluated the latter two using the publicly available C4 corpus. Note that the y-axis in Figures 3, 4, 5 are normalized by the left-most bar using 1/3(R1/R1base + R2/R2base + RL/RLbase) where R1, R2, RL are ROUGE F1 scores and R1base, R2base, RLbase are the scores of the configuration corresponding to the first bar.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["a recent trend of studies are thus motivated to focus on compressing the size of dnns while mostly keeping their predictive performance xcite . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the ... ... ... Gold as the complexity of deep neural networks ( dnns ) trend to grow to absorb the increasing sizes of data , memory and energy consumption has been receiving more and more attentions for industrial applications , especially on mobile devices . this paper presents a novel structure based on functional hashing to compress dnns , namely funhashnn . for each entry in a deep net , funhashnn uses multiple low - cost hash functions to fetch values in the compression space , and then employs a small reconstruction network to recover that entry . the reconstruction network is plugged into the whole network and trained jointly . funhashnn includes the recently proposed hashednets xcite as a degenerated case , and benefits from larger value capacity and less reconstruction loss . we further discuss extensions with dual space hashing and multi - hops . on several benchmark datasets , funhashnn demonstrates high compression ratios with little loss on prediction accuracy . Model deep neural networks ( dnns ) have been receiving ubiquitous success in wide applications , ranging from computer vision to speech recognition . as the sizes of data mount up , people usually have to increase the number of parameters in dnns so as to absorb the vast volume of supervision . compressing the model size becomes more important for applications on mobile and embedded devices . with different intuitions , there are mainly two types of dnn compression methods , which could be used in conjunction for better parameter savings . the first type tries to revise the training target into more informative supervision using dark knowledge .", "generalisation is discussed in section [ sec : gen ] with an illustrative example of how this can improve performance given in section [ sec : check2 ] . consider the problem of linear classification with the svm where the training set , xmath0 , is linearly separable . we define a separating hyperplane given by xmath1 , where xmath2 , the weight vector , is perpendicular to the hyperplane , and xmath3 , the bias , determines the distance of the hyperplane from the origin ( fig . ) . a decision function defined by xmath4 is used to positively and negatively classify xmath5 , the points in the training set . without further constraint the ... ... ...\nGold we review the concept of support vector machines ( svms ) and discuss examples of their use . one of the benefits of svm algorithms , compared with neural networks and decision trees is that they can be less susceptible to over fitting than those other algorithms are to over training . this issue is related to the generalisation of a multivariate algorithm ( mva ) ; a problem that has often been overlooked in particle physics . we discuss cross validation and how this can be used to improve the generalisation of a mva in the context of high energy physics analyses . the examples presented use the toolkit for multivariate analysis ( tmva ) based on root and describe our improvements to the svm functionality and new tools introduced for cross validation within this framework . Model support vector machines ( svms ) are one of the most promising methods for machine learning in high energy physics . svms aim to classify data points using a maximal margin hyperplane mapped from a linear classification problem to a possibly infinite dimensional hyperspace . however this means svms , like other mva classifiers , have a number of free parameters which need to be tuned on a case by case basis .", "in xcite , the tomographic - probability representation of quantum states , where the quantum state is associated with the so - called symplectic tomogram , was introduced . the tomogram is a fair probability distribution containing the same information on quantum state that the density operator does ( or such its characteristics as the wigner or husimi kano functions ) . the aim of this work ... ... ... Gold the quasidistributions corresponding to the diagonal representation of quantum states are discussed within the framework of operator - symbol construction . the tomographic - probability distribution describing the quantum state in the probability representation of quantum mechanics is reviewed . the connection of the diagonal and probability representations is discussed . the superposition rule is considered in terms of the density - operator symbols . the separability and entanglement properties of multipartite quantum systems are formulated as the properties of the density - operator symbols of the system states . Model the diagonal and tomographic representations of quantum states are considered . the quantum states are represented in terms of the weighted sum of coherent - state xmath0 projectors . in the tomographic - probability representation , the quantum state is associated with the so - called symplectic tomogram . the tomographic - probability distribution is a fair probability distribution containing the same information on quantum state that the density operator does ( or such its characteristics as the wigner or husimi kano functions ) . the aim of this work is to find the explicit formulae realizing the connection of the diagonal and tomographic probability representations . the functions like the wigner function , husimi kano function and tomographic - probability - distribution function are considered as symbols of the density operators of a corresponding star - product scheme ."]}
{"pkey": "pegasus_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The paper authors propose a new pre-training objective, GSG, in this work, but for comparison, the paper authors also evaluate BERT\u2019s maskedlanguage model objective, in isolation and in conjunction with GSG Inspired by recent success in masking words and contiguous spans (Joshi et al., 2019; Raffel et al., 2019), the paper authors select and mask whole sentences from documents, and concatenate the gap-sentences into a pseudo-summary. The corresponding position of each selected gap sentence is replaced by a mask token [MASK1] to inform the model. Gap sentences ratio, or GSR, refers to the number of selected gap sentences to the total number of sentences in the document, which is similar to mask rate in other works. To even more closely approximate a summary, the paper authors select sentences that appear to be important/principal to the document. The resulting objective has both the empirically demonstrated benefits of masking, and anticipates the form of the downstream task.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Figure 1: The base architecture of PEGASUS is a standard Transformer encoder-decoder. Both GSG and MLM are applied simultaneously to this example as pre-training objectives. Originally there are three sentences. One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some tokens are randomly masked by [MASK2] (MLM). 1 Introduction Text summarization aims at generating accurate and concise summaries from input document(s). In contrast to extractive summarization which merely copies informative fragments from the input, abstractive summarization may generate novel words. A good abstractive summary covers principal information in the input and is linguistically fluent. In abstractive summarization, sequence-to-sequence (Sutskever et al., 2014) has become a dominant framework using encoder-decoder architectures based on RNNs (Chung et al., 2014; Hochreiter & Schmidhuber, 1997) and more recently Transformers (Vaswani et al., 2017). Most prior work on neural abstractive summarization relied on large-scale, high-quality datasets of supervised document-summary pairs (Hermann et al., 2015) and achieved promising results (Rush et al., 2015; Nallapati et al., 2016; See et al., 2017). In recent years, there has been increased interest in collecting new summarization datasets that have more abstractive summaries (Narayan et al., 2018), have longer documents, (Cohan et al., 2018; Sharma et al., 2019), utilize multiple documents (Fabbri et al., 2019), and are sourced from diverse domains (Grusky et al., 2018; Koupaee & Wang, 2018; Kim et al., 2019; Kornilova & Eidelman, 2019; Zhang & Tetreault, 2019); ar X\niv :1\n91 2.\n08 77\n7v 3\n[ cs\n.C L\n] 1\n0 Ju\nl 2 02\n0\nhowever, there has been little work on systematic evaluation of models across these broad settings.", "Figure 1: The base architecture of PEGASUS is a standard Transformer encoder-decoder. Both GSG and MLM are applied simultaneously to this example as pre-training objectives. Originally there are three sentences. One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some tokens are randomly masked by [MASK2] (MLM). Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. *Equal contribution 1Data Science Institute, Imperial College London, London, UK 2Brain Team, Google Research, Mountain View, CA, USA. Correspondence to: Jingqing Zhang <jingqing.zhang15@imperial.ac.uk>, Yao Zhao <yaozhaoyz@google.com>, Mohammad Saleh <msaleh@google.com>, Peter J. Liu <peterjliu@google.com>.", "PEGASUS: Pre-training with Extracted Gap-sentences for  Abstractive Summarization. Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. Equal contribution Data Science Institute, Imperial College London, London, UK Brain Team, Google Research, Mountain View, CA, USA. Correspondence to: Jingqing Zhang <jingqing.zhang15@imperial.ac.uk>, Yao Zhao <yaozhaoyz@google.com>, Mohammad Saleh <msaleh@google.com>, Peter J. Liu <peterjliu@google.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s). Figure 1: The base architecture of PEGASUS is a standard Transformer encoder-decoder. Both GSG and MLM are applied simultaneously to this example as pre-training objectives. Originally there are three sentences."]}
{"pkey": "pegasus_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "XSum (Narayan et al., 2018) consists of 227k BBC articles from 2010 to 2017 covering a wide variety of subjects along with professionally written single-sentence summaries.  CNN/DailyMail (Hermann et al., 2015) dataset contains 93k articles from the CNN, and 220k articles the Daily Mail newspapers. Both publishers supplement their articles with bullet point summaries. The paper authors use the non-anonymized variant used in See et al. (2017). Gigaword (Rush et al., 2015) contains 4M examples extracted from news articles (seven publishers) from the Gigaword corpus (Graff et al., 2003). The task is to generate the headline from the first sentence. BIGPATENT (Sharma et al., 2019) consists of 1.3 million U.S. patents along with human summaries under nine patent classification categories. WikiHow (Koupaee & Wang, 2018) is a large-scale dataset of instructions from the online WikiHow.com website. Each of 200k examples consists of multiple instruction-step paragraphs along with a summarizing sentence. The task is to generate the concatenated summary-sentences from the paragraphs.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["37.52/10.63/22.79\narXiv 28.05/6.63/17.72 31.38/8.16/17.97 33.06/9.66/20.11 39.46/12.38/22.20 40.24/14.04/23.11 41.59/14.26/23.55 PubMed 28.17/7.57/17.85 33.31/10.58/20.05 34.05/12.75/21.12 40.15/15.56/24.05 41.75/16.74/24.80 40.59/15.59/23.59 AESLC 10.35/3.86/9.29 11.97/4.91/10.84 16.05/7.20/15.32 28.58/15.45/28.14 36.47/20.85/35.53 23.67/10.29/23.44 BillSum 41.02/17.44/25.24 40.48/18.49/27.27 44.78/26.40/34.40 46.47/30.58/37.21 50.81/34.49/40.96 40.80/23.83/33.73\nF Human Evaluation Details In all human evaluation experiments we used the same task template shown in Figure F.1, where workers were asked to rate 4 summaries for a document on a scale of 1 (poor summary) to 5 (great summary). The order in which the summaries are presented for each task was random per example. Each task was independently done by 3 different workers and we retained the median score across workers for each summary. We paid 1 USD per task and used the following critieria for workers to ensure high-quality:\n\u2022 Location: US\n\u2022 Minimum approval rate: 95%\n\u2022 Minimum HIITs: 1000\nWith this criteria we observed high reproducibility in the conclusions of the huamn evaluation. Multiple runs of the same experiment with different workers meeting this criteria yielded very similar results. The HITT template is provided at https://github.com/google-research/pegasus. In experiment 1, the four summaries corresponded to 3 models (PEGASUSLARGE pre-trained on HugeNews, C4, and TransformerBASE) that were fine-tuned using all the supervised examples along with the reference (human) summary. We sampled 100 examples from each dataset (XSum, CNN/DailyMail, Reddit TIFU). In experiment 2, we evaluated 4 models (PEGASUSLARGE pre-trained on HugeNews fine-tuned using different amounts of supervision, 10, 100, 1000, and all examples) alongside the human summary. To do this with the same template, for each example we randomly selected 4 out of the 5 summaries.", "Figure 1: The base architecture of PEGASUS is a standard Transformer encoder-decoder. Both GSG and MLM are applied simultaneously to this example as pre-training objectives. Originally there are three sentences. One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some tokens are randomly masked by [MASK2] (MLM). Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. *Equal contribution 1Data Science Institute, Imperial College London, London, UK 2Brain Team, Google Research, Mountain View, CA, USA. Correspondence to: Jingqing Zhang <jingqing.zhang15@imperial.ac.uk>, Yao Zhao <yaozhaoyz@google.com>, Mohammad Saleh <msaleh@google.com>, Peter J. Liu <peterjliu@google.com>.", "One sentence is masked with [MASK1] and used as target generation text (GSG). The other two sentences remain in the input, but some tokens are randomly masked by [MASK2] (MLM). PEGASUS: Pre-training with Extracted Gap-sentences for  Abstractive Summarization. Recent work pre-training Transformers with self-supervised objectives on large text corpora has shown great success when fine-tuned on downstream NLP tasks including text summarization. However, pre-training objectives tailored for abstractive text summarization have not been explored. Furthermore there is a lack of systematic evaluation across diverse domains. In this work, we propose pre-training large Transformer-based encoder-decoder models on massive text corpora with a new selfsupervised objective. In PEGASUS, important sentences are removed/masked from an input document and are generated together as one output sequence from the remaining sentences, similar to an extractive summary. We evaluated our best PEGASUS model on 12 downstream summarization tasks spanning news, science, stories, instructions, emails, patents, and legislative bills. Experiments demonstrate it achieves state-of-the-art performance on all 12 downstream datasets measured by ROUGE scores. Our model also shows surprising performance on low-resource summarization, surpassing previous state-of-the-art results on 6 datasets with only 1000 examples. Finally we validated our results using human evaluation and show that our model summaries achieve human performance on multiple datasets. Equal contribution Data Science Institute, Imperial College London, London, UK Brain Team, Google Research, Mountain View, CA, USA. Correspondence to: Jingqing Zhang <jingqing.zhang15@imperial.ac.uk>, Yao Zhao <yaozhaoyz@google.com>, Mohammad Saleh <msaleh@google.com>, Peter J. Liu <peterjliu@google.com>. Proceedings of the 37 th International Conference on Machine Learning, Vienna, Austria, PMLR 119, 2020. Copyright 2020 by the author(s)."]}
{"pkey": "pegasus_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "PEGASUSBASE had L = 12, H = 768, F = 3072, A = 12 and PEGASUSLARGE had L = 16, H = 1024, F = 4096, A = 16, where L denotes the number of layers for encoder and decoder (i.e. Transformer blocks), H for the hidden size, F for the feed-forward layer size and A for the number of self-attention heads. The paper authors pre-trained PEGASUSBASE with a batch size of 256 and PEGASUSLARGE with a batch size of 8192. The paper authors used sinusoidal positional encoding following Vaswani et al. (2017). For optimization, both pre-training and finetuning used Adafactor (Shazeer & Stern, 2018) with square root learning rate decay and dropout rate of 0.1. The paper authors used PEGASUSBASE to evaluate choices of pre-training corpus, pre-training objective, and vocabulary size. All experiments\u2019 hyper parameters can be found in Appendix C and reported numbers are in Appendix D and E.", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["Following Grusky et al., the number of examples and extractive fragment coverage/density for all downstream datasets is illustrated in Appendix A.\n6 Experiments In a similar strategy to Raffel et al. (2019), to save time and computation we conducted pre-training ablation experiments using a reduced-size model with 223M parameters, PEGASUSBASE, smaller batch size, and only 4 of 12 datasets before scaling up pre-training with the best settings to the final 568M parameters, PEGASUSLARGE. The datasets (XSum, CNN/DailyMail, WikiHow and Reddit TIFU) were chosen for diversity in abstractiveness, writing style, and size. available C4 corpus. Note that the y-axis in Figures 3, 4, 5 are normalized by the left-most bar using 13 ( R1 R1base + R2R2base + RL RLbase ) where R1, R2, RL are ROUGE F1 scores and R1base, R2base, RLbase are the scores of the configuration corresponding to the first bar. With more pre-training steps, the model observed more documents in the pre-training corpus. A PEGASUSBASE model trained for 500k (highest we tried) steps did not observe all training examples on C4 nor HugeNews. Appendix B shows the number of pre-training steps had an unsurprisingly positive impact on downstream dataset performance. We used 500k steps for the ablation studies and the large model. 6.1.1 PRE-TRAINING CORPUS\nXSum CNN/DailyMail WikiHow Reddit TIFU 0.90\n0.95\n1.00\n1.05\n1.10 C4 HugeNews\nFigure 3: Effect of pre-training corpus. PEGASUSBASE pre-trained on C4 (350M Web-pages) and HugeNews (1.5B news-like documents). 6.1.2 EFFECT OF PRE-TRAINING OBJECTIVES\nGSG We compared six variants of GSG (Lead, Random, Ind-Orig, Ind-Uniq, Seq-Orig, Seq-Uniq) while choosing 30% sentences as gap sentences. As shown in Figure 4a, IndOrig achieved the best performance followed by Seq-Uniq. Ind-Orig and Seq-Uniq were consistently better (or similar) than Random and Lead across the four downstream datasets.", "We further push forward the state-of-the-art using a newly collected text corpus comprised of news-like articles we call HugeNews, including the highly competitive XSum and CNN/DailyMail summarization datasets. Large-scale document-summary datasets are rare and in practice there is a mismatch between research datasets and real-world use-cases where collecting summaries is expensive; the most common setting is that of low-resource summarization. We simulate this setting and show that our model is able to adapt very quickly when fine-tuning with small numbers of supervised pairs, obtaining state-of-the-art results in 6 datasets with only 1000 examples. Qualitatively we observed high quality outputs from our\nbest models and validated this in human evaluation studies. We found that PEGASUS summaries are at least as good as reference summaries for the datasets we assessed \u2013 XSum, CNN/DailyMail, and Reddit TIFU \u2013 even at low-levels of supervision. To summarize our contributions:\n\u2022 We propose a new self-supervised pre-training objective for abstractive summarization, gap-sentences generation, and study strategies for selecting those sentences. \u2022 We evaluate the proposed pre-training objective on a broad range of downstream summarization tasks, with careful ablations to choose the best model settings, which we use to train a 568M parameter PEGASUS model that surpasses or is on-par with the state-of-theart on all 12 downstream datasets considered. \u2022 We show how good abstractive summarization performance can be achieved across broad domains with very little supervision by fine-tuning the PEGASUS model and surpassing previous state-of-the-art results on many tasks with as little as 1000 examples. \u2022 We conducted human evaluation studies to validate our experimental design and demonstrate human-level summarization performance on XSum, CNN/DailyMail, and Reddit TIFU.", "me: no, africa (my mothers indian however my fathers half tanzanian half arab so to make things easy i just say i\u2019m african since its more plausible as i have afroish hair) her: ah, you come here to study? me: yes her: i see you around college and yeah me: yeah her: are you going to university? me: yes her: how long have you been here? me: i\u2019m in my second year her: so you stay at your cousins? (this is where i thought huh i thought she asked my ethnicity and the situation was getting really awkward) me: yes then she left and i realised i made her think i\u2019ve come over to england to study and am staying at my cousins when in fact i was born in england and have stayed here my whole life... throughout the ordeal my twin was sat quiet in his phone -.- edit: sorry for the formatting, i\u2019m posting on my phone. Gold made a lunch lady believe i\u2019ve immigrated to study and stay at my cousins because of how awkward the situation was when in fact i\u2019ve lived in england my whole life\nModel made a lunch lady think i was born in england and have stayed here my whole life. ROUGEL-F1 44.00 Document (ID #224) so this happened last night and proceeded into today. went out with this girl for drinks, we started fooling around. went back to my car and she wentape-shit on my neck. didn\u2019t notice anything at the time, woke up in the morning with a nice bruise on my neck. went to work in a very corporate\nsetting. thought that my dress shirt buttoned all the way up would be enough to conceal it but nope. boss inquires about the bruise, i say \u201d yeah i don\u2019t know where i got that, they keep popping up all over my body\u201d. i then proceeded to show him bruises on my arms (i was trying to climb trees whilst drunk over the weekend) as a cover up. boss became concerned and said i need to see a doctor right away as it could be something more serious like leukemia. he has given me next monday off to see the doctor...\nGold girl attacked my neck, boss sent me to the doctor."]}
{"pkey": "pegasus_20", "question": "List the future work mentioned in the paper.", "answer": "The paper does not contain any information on the future work", "title": "PEGASUS: Pre-training with Extracted Gap-sentences forAbstractive Summarization", "context": ["Jayden was given months to live before he received part of Kristina Chesterman\u2019s liver. Jayden and Christina share the same birth stone and that gem is embeded into the bracelet. (Aric Crabb/Bay Area News Group) ( ARIC CRABB ) LIVERMORE \u2013 When she was still in high school, Kristina Chesterman wrote out her bucket list. Flying a plane was on it; so was running through a poppy field and breaking up a fight between two boys over her affections. She also wanted to save a life. The aspiring nurse\u2019s ambitions came to a halt in September, when she was killed by a suspected drunken driver near Chico State, where she attended school. Though Chesterman, 21, didn\u2019t get to mark much off her list, she has saved more lives than she hoped \u2013 and is profoundly affecting many others. Five Northern Californians have been saved through Kristina\u2019s choice to donate her organs. And her grateful friends and family are making the rest of her bucket list their own. A photograph of Kristina Chesterman is displayed during a birthday party for Jayden Kirby on Sunday, Feb. 9, 2014, in Fremont, Calif. Chesterman was killed by an alleged drunk driver last September while coming home from nursing school in Chico. Jayden, 1, was given months to live before he received part of Kristina Chesterman\u2019s liver. (Courtesy of the Chesterman Family) ( Chesterman Family ) Chesterman\u2019s mother, Sandra, of Livermore, said her daughter wanted to help people from an early age. She routinely gave ... ... ... Gold Kristina Chesterman, 21, was studying to be a nurse when she was killed by a suspected drunk driver last yearbut she managed to save lives anyway. A registered organ donor, Chesterman gave five people, including a baby, new life, and now the woman who received her heart wants to do something in return. Susan Vieira, 64, has vowed to check off everything on Chesterman\u2019s bucket listwritten down on a piece of paper her mom only recently found. (One of the items? \u201dSave someone\u2019s life.\u201d Another? \u201dBe in four places at once.\u201d)", "It\u2019ll then become illegal for telemarketers to contact you: https://www.donotcall.gov/. If you do get contacted by a telemarketer after putting yourself on the list, you can report the number that called you to the FTC or FCC. If they tell you a mutual acquaintance or other contact, consider taking the call. If they dont or cant tell you, or if they got your contact information from some data pool or list, ask to be removed from their call list and hang up., Be clear about conveying your annoyance with phone calls, especially if you might want to do business with or purchase something from that company but dont want annoying callers. Make sure you provide them with an email address specifically for spam or sales deals. Gold Dont just hang up right away. Add yourself to the national do not call registry. Ask where they got your contact info. Tell them No calls, emails only. If youre running a business, in charge of purchasing, or otherwise need to field sales pitches from time to time, tell telemarketers that you only respond to emails. Model Hang up immediately. Report persistent callers. Ask to be removed from the call list. Be clear about your preferences.\nROUGE1-F1 20.78\nTable I.17: Generated summaries by PEGASUSLARGE (HugeNews) on WikiHow sampled by ROUGE2-F1. WikiHow Document (ID #29) Microwave frozen spinach for two minutes at 50% power. Break the spinach apart into smaller pieces. Microwave it again, at 50% power, checkingand stirring it whenever a minute passes by, until its defrosted. Alternately, you can use one packed cup of fresh baby spinach, washed and finely\nchopped with the stems removed. ; , Wrap the spinach in a clean kitchen towel. Squeeze and wring the towel to remove the liquid. Another option is to press the wet spinach into a sieve over the sink., Make sure your hands are clean and free of jewelry. Place the ground meat in the bowl first, and press it down in the middle to form a well.", "He was owned by a Georgetown family but eventually gained his freedom. He was well known in Georgetown, a practicing Muslim who made bricks and owned land. His portrait was painted in 1819 by Charles Willson Peale and displayed in Peales Philadelphia museum. Yarrow was also painted by James Alexander Simpson, a Georgetown painter. Today the Alexander painting of Yarrow hangs in the Peabody Room of the Georgetown public library branch. But in 1825 it hung at Ninth and Pennsylvania NW. We know this because the newspapers at the time were full of mentions of a new attraction: the Columbia Museum. The ... ... ... Gold In 1825, a painting of an African American woman was displayed in Washington. Where is it now? Model Go up to your attic right now, or down to your basement, or to your late parents storage unit that you still pay for each month even though youre unfamiliar with its exact contents. Wherever it is you keep things that are ancient and dusty and mysterious, go there immediately and see if an elderly African American woman stares back at you from the canvas of an oil painting. Because if she does, you may have found one of the worlds rarest paintings. ROUGE2-F1 6.00 Document (ID #163) The age when North American clubs look to England and the wider British Isles with a childs embrace, beseeching input from the mother countryscoaching bosom to take the domestic game on, has long since passed. Or so some critics would argue. A new, forward-thinking generation of\ncoaches not steeped in the 4-4-2 and an unadorned, direct style of football rule the waves in this epoch, they postulate. And this legion of coaches are increasingly young, fresh and, most importantly, American. While there might be some historical merit to the spirit of the argument against the British-style coach, it is perhaps itself a little outdated, not to say somewhat harsh on someone like Carl Robinson and his stylish Vancouver Whitecaps, for instance."]}
{"pkey": "fnet_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "The standard attention mechanism (Vaswani et al.,2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long-range dependencies. In this work, the paper authors investigate whether simpler token mixing mechanisms can wholly replace the relatively complex self-attention layers in Transformer encoder architectures.Surprisingly, the paper authors find that the Fourier Transform, despite having no parameters at all, achieves nearly the same performance as dense linear mixing and scales very efficiently to long inputs, especially on GPUs owing to the O(N log N ) Fast Fourier Transform (FFT) algorithm. The paper authors believe our work is the first to wholly replace particular neural network sublayers with a Fourier Transform.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["The contributions of our paper are:\n\u2022 We show that simple linear transformations, including even (parameter-free) Fourier Transforms, along with standard MLPs in feedforward layers, are competent at modeling diverse relationships in text. That such a simple linear transformation works at all is surprising, and suggests that, for at least some NLP problems, attention may not be the principal component driving the performance of Transformers. \u2022 We introduce a new model, FNet, that uses the Fourier Transform as a mixing mechanism. FNet offers an excellent compromise between speed, memory footprint, and accuracy, achieving 92% and 97%, respectively, of the accuracy of BERT-Base and BERT-Large (Devlin et al., 2019) on the GLUE benchmark (Wang et al., 2018), while training 80% faster on GPUs and 70% faster on TPUs. \u2022 We find that FNet hybrid models containing only two self-attention sublayers achieve 97\u2212 99% of their BERT counterparts\u2019 accuracy on GLUE, while still running 40\u2212 70% faster. This indicates that, while attention can improve accuracy, it may not be necessary to use in every layer. \u2022 We demonstrate FNet scales very well to long inputs and offers a better compromise between speed and accuracy than the efficient Transformers evaluated on the Long-Range Arena (LRA) benchmark (Tay et al., 2021a). Specifically, FNet achieves accuracy comparable to the most accurate efficient Transformer architectures but is significantly faster at both\ntraining and inference than all of the evaluated Transformer architectures across all sequence lengths on GPUs. On TPUs, FNet is faster for relatively shorter sequence lengths; for longer sequences, the only efficient Transformers that are faster than FNet on TPUs are less accurate on the LRA benchmark. Based on this, we argue that rather than seeking more efficient approximations of the attention, there may be more value in seeking out completely new mixing mechanisms. 2 Related work. 2.1 Fourier Transforms in neural networks.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "Indeed, Table 5 shows that BERT-Base is actually more accurate than FNet-Large, which contains more than twice as many parameters. BERT is presumably more expressive because the mixing (attention) weights are both task specific and token dependent, determined\n768 12 111 93 83 88.\n512 12 55 49 42 44.\n512 8 42 38 34 36.\n256 8 15 15 13 13.\n512 4 30 28 26 28.\n256 4 12 12 11 11.\n256 2 10 10 10 -.\nby token-token (query-key) dot products; see also Tay et al. (2020a). FNet\u2019s mixing weights, on the other hand, are neither task specific nor token dependent. Finally, Table 6 shows the model sizes that were used to construct Figure 2 (main text) and Figure 3 (Appendix A.2). A.2 TPU results In this section, we report FNet efficiency results for TPUs; the main text focuses on GPUs. Figure 3 shows the speed vs MLM pre-training accuracy curve when training on TPU (4\u00d7 4 v3 chips). As on GPUs, FNet and the Linear model define the Pareto efficiency frontier for smaller, faster models, while BERT defines the frontier for larger, slower models. Table 7 shows Long Range Arena Text classification efficiency results on TPUs (4\u00d7 4 v3 chips). The Linear model and FNet train faster than all the efficient Transformers for sequence lengths\u2264 2048 and 512, respectively. For longer sequences, FNet is slower than the Performer and, based on results in Tay et al. (2021a), likely also slower than the other efficient Transformers that linearize attention, namely Local Attention (Parmar et al., 2018), Linformer (Wang et al., 2020) and Linear Transformer (Katharopoulos et al., 2020). However, it is worth noting that Table 4a suggests that FNet is more accurate than all of the aforementioned models. Moreover, we expect that the GPU speed gains will\ntransfer to TPUs as the TPU FFT implementation improves. A.3 Additional configurations that we experimented with\nWe experimented with a number of additional ideas to improve FNet. Fourier Transform algorithm."]}
{"pkey": "fnet_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "The literature on \u201clong sequence\u201d or \u201cefficient\u201d Transformers, which aim to make the attention mechanism scale better via sparsity patterns or via linearization of the attention matrix. As the paper authors will show in our experiments, while some of those works achieve O(N) scaling of attention, this complexity often hides large constants, which make them less scalable in practice than FNet. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do the paper authors really need the flexibility, and associated cost, of attention?", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["Although the Hadamard Transform was slightly faster than the DFT, it yielded less accurate results (\u223c 2% accuracy degradation). \u2022 Hartley Transform. The Hartley Transform, which transforms real input to real output, can be described in terms of the Fourier Transform: H = <{F} \u2212 ={F}. We found that the Hartley Transform matched the Fourier Transform on GLUE (76.7 vs. 76.7). Introducing learnable parameters to the Fourier sublayer. Our attempts to introduce learnable parameters into the Fourier sublayer were either detrimental or inconsequential, and generally slightly slowed the model. For the (sequence length, hidden dimension) input in each Fourier sublayer, we tried two approaches to introduce learnable parameters: (1) element wise multiplication with a (sequence length, hidden dimension) matrix, and (2) regular matrix multiplication with (sequence length, sequence length) and (hidden dimension, hidden dimension) matrices. We experimented with these approaches in various configurations: preceding and/or following the DFT, and also in combination with inverse DFT (e.g. transform to frequency domain, apply element wise multiplication, transform back to time domain), but most setups degraded accuracy and reduced training stability, while a few did not change accuracy but lead to small speed decreases. In a slightly different set of experiments and in an effort to provide more flexibility to the model, we added (complex) learnable weights to the 2D DFT matrix. This model was stable but did not yield any accuracy gains, suggesting that the DFT is locally optimal in some sense. FNet block modifications. The standard FNet encoder block structure follows that of the Transformer: a Fourier sublayer followed by a feedforward sublayer, with residual connections and layer norms after each sublayer; see Figure 1. We tried several modifications to this structure, based on the intuition of moving in and out of the frequency domain between multiplications.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "With the addition of just two self-attention sublayers, the hybrid FNet models achieve 97% and 99% of their respective BERT counterpart\u2019s accuracies with only limited speed degradations (see Table 3). Interestingly, the gap between BERT and FNet shrinks to just 3% for Large models; this is likely due to FNet-Large being more stable during training than BERT-Large.9 The Linear-Large model severely underperforms its Base counterpart on GLUE benchmark due to training instabilities. We generally found that the Linear model and BERT were less stable than the models with no param-\n9Devlin et al. (2019) obtain a roughly 2.5 average point boost on the Test split going from BERT-Base to BERT-Large. We only see a roughly 1.5 boost on the Validation split, which may be due to reduced headroom.\neters in their mixing sublayers, namely the FNet, Random and FF-only models. The speed vs MLM accuracy curve for GPU (8 V100 chips) pre-training is shown in Figure 2 (see Appendix A.2 for TPU results). Both TPU and GPU models are trained for 1 million steps as in Devlin et al. (2019). Motivated by the models considered in Turc et al. (2019), we evaluated several model sizes; see Table 6 in Appendix A.1. We found that the smaller model architectures benefited from larger learning rates, so we select the best result using 10\u22123 and 10\u22124 for all models.10\nThe GPU (Figure 2), and TPU (Figure 3 in Appendix A.2) results display the same trends. For larger, slower models, BERT and FNet-Hybrid define the Pareto speed-accuracy efficiency frontier. For smaller, faster models, FNet and the Linear model define the efficiency frontier. 4.2 Long-Range Arena (LRA) benchmark. Of the efficient Transformers evaluated on LRA benchmark by Tay et al. (2021a), their results suggest that (1) the vanilla Transformer is (by a small margin) the second most accurate model, and (2) the Performer (Choromanski et al., 2021) is the fastest model."]}
{"pkey": "fnet_3", "question": "What are the main contributions of the paper?", "answer": "The contributions of our paper are:\n\u2022 The paper authors show that simple linear transformations, including even (parameter-free) Fourier Transforms, along with standard MLPs in feed-forward layers, are competent at modeling diverse relationships in text. That such a simple linear transformation works at all is surprising, and suggests that, for at least some NLP problems, attention may not be the principal component driving the performance of Transformers.\n\u2022 The paper authors introduce a new model, FNet, that uses the Fourier Transform as a mixing mechanism. FNet offers an excellent compromise between speed, memory footprint, and accuracy, achieving 92% and 97%, respectively, of the accuracy of BERT-Base and BERT-Large (Devlin et al., 2019) on the GLUE benchmark (Wang et al., 2018), while training 80% faster on GPUs and 70% faster on TPUs.\n\u2022 The paper authors find that FNet hybrid models containing only two self-attention sublayers achieve 97 \u2212 99% of their BERT counterparts\u2019 accuracy on GLUE, while still running 40 \u2212 70% faster. This indicates that, while attention can improve accuracy, it may not be necessary to use in every layer.\n\u2022 The paper authors demonstrate FNet scales very well to long inputs and offers a better compromise between speed and accuracy than the efficient Trans- formers evaluated on the Long-Range Arena (LRA) benchmark (Tay et al., 2021a). Specifically, FNet achieves accuracy comparable to the most accurate efficient Transformer ar chitectures but is significantly faster at both training and inference than all of the evaluated Transformer architectures across all sequence lengths on GPUs. On TPUs, FNet is faster for relatively shorter sequence lengths; for longer sequences, the only efficient Transformers that are faster than FNet on TPUs are less accurate on the LRA benchmark. Based on this, the paper authors argue that rather than seeking more efficient approximations of the attention, there may be more value in seeking out completely new mixing mechanisms.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["The contributions of our paper are:\n\u2022 We show that simple linear transformations, including even (parameter-free) Fourier Transforms, along with standard MLPs in feedforward layers, are competent at modeling diverse relationships in text. That such a simple linear transformation works at all is surprising, and suggests that, for at least some NLP problems, attention may not be the principal component driving the performance of Transformers. \u2022 We introduce a new model, FNet, that uses the Fourier Transform as a mixing mechanism. FNet offers an excellent compromise between speed, memory footprint, and accuracy, achieving 92% and 97%, respectively, of the accuracy of BERT-Base and BERT-Large (Devlin et al., 2019) on the GLUE benchmark (Wang et al., 2018), while training 80% faster on GPUs and 70% faster on TPUs. \u2022 We find that FNet hybrid models containing only two self-attention sublayers achieve 97\u2212 99% of their BERT counterparts\u2019 accuracy on GLUE, while still running 40\u2212 70% faster. This indicates that, while attention can improve accuracy, it may not be necessary to use in every layer. \u2022 We demonstrate FNet scales very well to long inputs and offers a better compromise between speed and accuracy than the efficient Transformers evaluated on the Long-Range Arena (LRA) benchmark (Tay et al., 2021a). Specifically, FNet achieves accuracy comparable to the most accurate efficient Transformer architectures but is significantly faster at both\ntraining and inference than all of the evaluated Transformer architectures across all sequence lengths on GPUs. On TPUs, FNet is faster for relatively shorter sequence lengths; for longer sequences, the only efficient Transformers that are faster than FNet on TPUs are less accurate on the LRA benchmark. Based on this, we argue that rather than seeking more efficient approximations of the attention, there may be more value in seeking out completely new mixing mechanisms. 2 Related work. 2.1 Fourier Transforms in neural networks.", "From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1."]}
{"pkey": "fnet_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "FNet offers an excellent compromise between speed, memory footprint, and accuracy, achieving 92% and 97%, respectively, of the accuracy of BERT Base and BERT-Large (Devlin et al., 2019) on the GLUE benchmark. GLUE: A multi-task benchmark and analysis platform for natural language understanding.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["Indeed, Table 5 shows that BERT-Base is actually more accurate than FNet-Large, which contains more than twice as many parameters. BERT is presumably more expressive because the mixing (attention) weights are both task specific and token dependent, determined\n768 12 111 93 83 88.\n512 12 55 49 42 44.\n512 8 42 38 34 36.\n256 8 15 15 13 13.\n512 4 30 28 26 28.\n256 4 12 12 11 11.\n256 2 10 10 10 -.\nby token-token (query-key) dot products; see also Tay et al. (2020a). FNet\u2019s mixing weights, on the other hand, are neither task specific nor token dependent. Finally, Table 6 shows the model sizes that were used to construct Figure 2 (main text) and Figure 3 (Appendix A.2). A.2 TPU results In this section, we report FNet efficiency results for TPUs; the main text focuses on GPUs. Figure 3 shows the speed vs MLM pre-training accuracy curve when training on TPU (4\u00d7 4 v3 chips). As on GPUs, FNet and the Linear model define the Pareto efficiency frontier for smaller, faster models, while BERT defines the frontier for larger, slower models. Table 7 shows Long Range Arena Text classification efficiency results on TPUs (4\u00d7 4 v3 chips). The Linear model and FNet train faster than all the efficient Transformers for sequence lengths\u2264 2048 and 512, respectively. For longer sequences, FNet is slower than the Performer and, based on results in Tay et al. (2021a), likely also slower than the other efficient Transformers that linearize attention, namely Local Attention (Parmar et al., 2018), Linformer (Wang et al., 2020) and Linear Transformer (Katharopoulos et al., 2020). However, it is worth noting that Table 4a suggests that FNet is more accurate than all of the aforementioned models. Moreover, we expect that the GPU speed gains will\ntransfer to TPUs as the TPU FFT implementation improves. A.3 Additional configurations that we experimented with\nWe experimented with a number of additional ideas to improve FNet. Fourier Transform algorithm.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "Although the Hadamard Transform was slightly faster than the DFT, it yielded less accurate results (\u223c 2% accuracy degradation). \u2022 Hartley Transform. The Hartley Transform, which transforms real input to real output, can be described in terms of the Fourier Transform: H = <{F} \u2212 ={F}. We found that the Hartley Transform matched the Fourier Transform on GLUE (76.7 vs. 76.7). Introducing learnable parameters to the Fourier sublayer. Our attempts to introduce learnable parameters into the Fourier sublayer were either detrimental or inconsequential, and generally slightly slowed the model. For the (sequence length, hidden dimension) input in each Fourier sublayer, we tried two approaches to introduce learnable parameters: (1) element wise multiplication with a (sequence length, hidden dimension) matrix, and (2) regular matrix multiplication with (sequence length, sequence length) and (hidden dimension, hidden dimension) matrices. We experimented with these approaches in various configurations: preceding and/or following the DFT, and also in combination with inverse DFT (e.g. transform to frequency domain, apply element wise multiplication, transform back to time domain), but most setups degraded accuracy and reduced training stability, while a few did not change accuracy but lead to small speed decreases. In a slightly different set of experiments and in an effort to provide more flexibility to the model, we added (complex) learnable weights to the 2D DFT matrix. This model was stable but did not yield any accuracy gains, suggesting that the DFT is locally optimal in some sense. FNet block modifications. The standard FNet encoder block structure follows that of the Transformer: a Fourier sublayer followed by a feedforward sublayer, with residual connections and layer norms after each sublayer; see Figure 1. We tried several modifications to this structure, based on the intuition of moving in and out of the frequency domain between multiplications."]}
{"pkey": "fnet_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "With the addition of just two self-attention sublayers, the hybrid FNet models achieve 97% and 99% of their respective BERT counterpart\u2019s accuracies with only limited speed degradations (see Table 3). Interestingly, the gap between BERT and FNet shrinks to just 3% for Large models; this is likely due to FNet-Large being more stable during training than BERT-Large.9 The Linear-Large model severely underperforms its Base counterpart on GLUE benchmark due to training instabilities. We generally found that the Linear model and BERT were less stable than the models with no param-\n9Devlin et al. (2019) obtain a roughly 2.5 average point boost on the Test split going from BERT-Base to BERT-Large. We only see a roughly 1.5 boost on the Validation split, which may be due to reduced headroom.\neters in their mixing sublayers, namely the FNet, Random and FF-only models. The speed vs MLM accuracy curve for GPU (8 V100 chips) pre-training is shown in Figure 2 (see Appendix A.2 for TPU results). Both TPU and GPU models are trained for 1 million steps as in Devlin et al. (2019). Motivated by the models considered in Turc et al. (2019), we evaluated several model sizes; see Table 6 in Appendix A.1. We found that the smaller model architectures benefited from larger learning rates, so we select the best result using 10\u22123 and 10\u22124 for all models.10\nThe GPU (Figure 2), and TPU (Figure 3 in Appendix A.2) results display the same trends. For larger, slower models, BERT and FNet-Hybrid define the Pareto speed-accuracy efficiency frontier. For smaller, faster models, FNet and the Linear model define the efficiency frontier. 4.2 Long-Range Arena (LRA) benchmark. Of the efficient Transformers evaluated on LRA benchmark by Tay et al. (2021a), their results suggest that (1) the vanilla Transformer is (by a small margin) the second most accurate model, and (2) the Performer (Choromanski et al., 2021) is the fastest model."]}
{"pkey": "fnet_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not specified in the paper.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.1\n1 Introduction. The Transformer architecture (Vaswani et al., 2017) has achieved rapid and widespread dominance in NLP. At its heart is a attention mechanism \u2013 an inductive bias that connects each token in the input through a relevance weighted basis of every other token. Many papers have prodded and probed the Transformer, and in particular the attention sublayers, in an effort to better understand the architecture; see, for example, Tenney et al. (2019); Vig and Belinkov (2019); Clark et al. (2019); Voita et al. (2019). Although potentially limited in their effectiveness (Hewitt and Liang, 2019), these probes generally back the intuition that, by allowing higher order units to form out of compositions of the input,\n1Code is available at https://github.com/ google-research/google-research/tree/ master/f_net. Transformer models can flexibly capture diverse syntactic and semantic relationships. In this work, we investigate whether simpler token mixing mechanisms can wholly replace the relatively complex self-attention layers in Transformer encoder architectures. We first replace the attention sublayer with two parameterized matrix multiplications \u2013 one mixing the sequence dimension and one mixing the hidden dimension. Seeing promising results in this simple linear mixing scheme, we further investigate the efficacy of faster, structured linear transformations. Surprisingly, we find that the Fourier Transform, despite having no parameters at all, achieves nearly the same performance as dense linear mixing and scales very efficiently to long inputs, especially on GPUs (owing to the O(N logN) Fast Fourier Transform (FFT) algorithm). We call the resulting model FNet.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "This reflects our observation that BERT-Large was less stable than BERT-Base, as noted in Devlin et al. (2019). We report the results for the best base learning rate (no early stopping) on the GLUE Validation split in Table 2.8 For Base models, results mirror the pre-training metrics (see Appendix A.1): BERT performs best. FNet and the Linear model both underperform BERT by 7.5 \u2212 8%. Referring to Table 3, we see that although less accurate, FNet trains significantly faster than BERT \u2013 80% faster on GPUs and 70% faster on TPUs \u2013 and performs\n7On the other hand, the smaller sized Linear models do generally perform well on 512 sequence lengths; see Figure 2. 8WNLI is excluded in Devlin et al. (2019). BERT\u2019s accuracy on WNLI is below baseline, unless a special training recipe is used. See also (12) in https:// gluebenchmark.com/faq. 63% of BERT\u2019s FLOPS. Measured in isolation, the Fourier sublayers perform forward and backward passes an order of magnitude faster than the self-attention sublayers (see Appendix A.4), but FNet\u2019s overall training speed is impeded by the feed-forward sublayers that all models share. Returning to Table 2: the FF-only model severely underperforms all other models: as expected, token mixing is critical to the expressivity of the model. For example, 50% accuracy scores on the binary classification tasks (QNLI, SST-2, RTE), indicate that the model fails to learn the tasks. The weak accuracy of the Random model suggests that not just any mixing will do; rather, a structured mixing is required. We also include metrics from a hybrid FNet attention model. In the hybrid model, we replace the final two Fourier sublayers of FNet with self-attention sublayers \u2013 other configurations\nare possible, but we generally found that replacing the final layers worked best; see Appendix A.5."]}
{"pkey": "fnet_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Throughout this work the paper authors have restricted our focus to encoders. FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that cross-attention may be crucial to performance. This model is primarily aimed at being fine-tuned on tasks that use the whole sentence (potentially masked) to make decisions, such as sequence classification, token classification or question answering.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i", "Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.1\n1 Introduction. The Transformer architecture (Vaswani et al., 2017) has achieved rapid and widespread dominance in NLP. At its heart is a attention mechanism \u2013 an inductive bias that connects each token in the input through a relevance weighted basis of every other token. Many papers have prodded and probed the Transformer, and in particular the attention sublayers, in an effort to better understand the architecture; see, for example, Tenney et al. (2019); Vig and Belinkov (2019); Clark et al. (2019); Voita et al. (2019). Although potentially limited in their effectiveness (Hewitt and Liang, 2019), these probes generally back the intuition that, by allowing higher order units to form out of compositions of the input,\n1Code is available at https://github.com/ google-research/google-research/tree/ master/f_net. Transformer models can flexibly capture diverse syntactic and semantic relationships. In this work, we investigate whether simpler token mixing mechanisms can wholly replace the relatively complex self-attention layers in Transformer encoder architectures. We first replace the attention sublayer with two parameterized matrix multiplications \u2013 one mixing the sequence dimension and one mixing the hidden dimension. Seeing promising results in this simple linear mixing scheme, we further investigate the efficacy of faster, structured linear transformations. Surprisingly, we find that the Fourier Transform, despite having no parameters at all, achieves nearly the same performance as dense linear mixing and scales very efficiently to long inputs, especially on GPUs (owing to the O(N logN) Fast Fourier Transform (FFT) algorithm). We call the resulting model FNet.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1."]}
{"pkey": "fnet_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and training configurations as for the original BERT (Devlin et al., 2019), except that the paper authors pretrain on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) (see Appendix A.1 for full pre-training details). fine-tuning on the GLUE benchmark", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "Feed Forward-only (FF-only) encoder: we completely remove the self-attention sublayer; so that this model has no token mixing. 5https://github.com/google/flax 6https://github.com/google-research/\ngoogle-research/tree/master/f_net\nDespite its simplicity, the Linear baseline turns out to be surprisingly accurate and fast. Our Linear model is similar to the MLP-Mixer (Tolstikhin et al., 2021) (for vision) and also the Random Synthesizer (Tay et al., 2020a), but simplifies the latter model further by removing the multiple heads and softmax projections, resulting in just two matrix multiplications in the mixing sublayer. It is reasonable to expect that the Linear encoder, which uses densely parameterized mixing layers, will learn more flexibly than FNet, which uses parameter-free mixing layers. As we will show, although the Linear-Base model outperforms FNetBase slightly on GLUE (0.3 points), it has several efficiency drawbacks relative to FNet: it has a much larger memory footprint (see Table 4b), it is slower to train on regular 512 sequence lengths (see Table 3), and scales significantly worse on long sequence lengths (see Tables 4b-4c).7 We also found that Linear-Large was more difficult to train due to gradient blow up (see \u201cLarge\u201d scores in Table 2). We adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and training configurations as for the original BERT (Devlin et al., 2019), except that we pretrain on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) (see Appendix A.1 for full pre-training details). For fine-tuning on the GLUE benchmark (Wang et al., 2018), we found that different BERT runs with the same base learning rate could yield slightly different results. Consequently, for the Base (Large) models, we performed 3 (6) trials, respectively, for each base learning rate and reported the best result across all experiments.", "Although the Hadamard Transform was slightly faster than the DFT, it yielded less accurate results (\u223c 2% accuracy degradation). \u2022 Hartley Transform. The Hartley Transform, which transforms real input to real output, can be described in terms of the Fourier Transform: H = <{F} \u2212 ={F}. We found that the Hartley Transform matched the Fourier Transform on GLUE (76.7 vs. 76.7). Introducing learnable parameters to the Fourier sublayer. Our attempts to introduce learnable parameters into the Fourier sublayer were either detrimental or inconsequential, and generally slightly slowed the model. For the (sequence length, hidden dimension) input in each Fourier sublayer, we tried two approaches to introduce learnable parameters: (1) element wise multiplication with a (sequence length, hidden dimension) matrix, and (2) regular matrix multiplication with (sequence length, sequence length) and (hidden dimension, hidden dimension) matrices. We experimented with these approaches in various configurations: preceding and/or following the DFT, and also in combination with inverse DFT (e.g. transform to frequency domain, apply element wise multiplication, transform back to time domain), but most setups degraded accuracy and reduced training stability, while a few did not change accuracy but lead to small speed decreases. In a slightly different set of experiments and in an effort to provide more flexibility to the model, we added (complex) learnable weights to the 2D DFT matrix. This model was stable but did not yield any accuracy gains, suggesting that the DFT is locally optimal in some sense. FNet block modifications. The standard FNet encoder block structure follows that of the Transformer: a Fourier sublayer followed by a feedforward sublayer, with residual connections and layer norms after each sublayer; see Figure 1. We tried several modifications to this structure, based on the intuition of moving in and out of the frequency domain between multiplications."]}
{"pkey": "fnet_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i", "Feed Forward-only (FF-only) encoder: we completely remove the self-attention sublayer; so that this model has no token mixing. 5https://github.com/google/flax 6https://github.com/google-research/\ngoogle-research/tree/master/f_net\nDespite its simplicity, the Linear baseline turns out to be surprisingly accurate and fast. Our Linear model is similar to the MLP-Mixer (Tolstikhin et al., 2021) (for vision) and also the Random Synthesizer (Tay et al., 2020a), but simplifies the latter model further by removing the multiple heads and softmax projections, resulting in just two matrix multiplications in the mixing sublayer. It is reasonable to expect that the Linear encoder, which uses densely parameterized mixing layers, will learn more flexibly than FNet, which uses parameter-free mixing layers. As we will show, although the Linear-Base model outperforms FNetBase slightly on GLUE (0.3 points), it has several efficiency drawbacks relative to FNet: it has a much larger memory footprint (see Table 4b), it is slower to train on regular 512 sequence lengths (see Table 3), and scales significantly worse on long sequence lengths (see Tables 4b-4c).7 We also found that Linear-Large was more difficult to train due to gradient blow up (see \u201cLarge\u201d scores in Table 2). We adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and training configurations as for the original BERT (Devlin et al., 2019), except that we pretrain on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) (see Appendix A.1 for full pre-training details). For fine-tuning on the GLUE benchmark (Wang et al., 2018), we found that different BERT runs with the same base learning rate could yield slightly different results. Consequently, for the Base (Large) models, we performed 3 (6) trials, respectively, for each base learning rate and reported the best result across all experiments."]}
{"pkey": "fnet_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "The texts are lowercased and tokenized using SentencePiece and a vocabulary size of 32,000. The inputs of the model are then of the form: [CLS] Sentence A [SEP] Sentence B [SEP] With probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens. The details of the masking procedure for each sentence are the following: 15% of the tokens are masked. In 80% of the cases, the masked tokens are replaced by [MASK]. In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. In the 10% remaining cases, the masked tokens are left as is.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "Feed Forward-only (FF-only) encoder: we completely remove the self-attention sublayer; so that this model has no token mixing. 5https://github.com/google/flax 6https://github.com/google-research/\ngoogle-research/tree/master/f_net\nDespite its simplicity, the Linear baseline turns out to be surprisingly accurate and fast. Our Linear model is similar to the MLP-Mixer (Tolstikhin et al., 2021) (for vision) and also the Random Synthesizer (Tay et al., 2020a), but simplifies the latter model further by removing the multiple heads and softmax projections, resulting in just two matrix multiplications in the mixing sublayer. It is reasonable to expect that the Linear encoder, which uses densely parameterized mixing layers, will learn more flexibly than FNet, which uses parameter-free mixing layers. As we will show, although the Linear-Base model outperforms FNetBase slightly on GLUE (0.3 points), it has several efficiency drawbacks relative to FNet: it has a much larger memory footprint (see Table 4b), it is slower to train on regular 512 sequence lengths (see Table 3), and scales significantly worse on long sequence lengths (see Tables 4b-4c).7 We also found that Linear-Large was more difficult to train due to gradient blow up (see \u201cLarge\u201d scores in Table 2). We adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and training configurations as for the original BERT (Devlin et al., 2019), except that we pretrain on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) (see Appendix A.1 for full pre-training details). For fine-tuning on the GLUE benchmark (Wang et al., 2018), we found that different BERT runs with the same base learning rate could yield slightly different results. Consequently, for the Base (Large) models, we performed 3 (6) trials, respectively, for each base learning rate and reported the best result across all experiments.", "From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i"]}
{"pkey": "fnet_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "FNet is an attention-free Transformer architecture, wherein each layer consists of a Fourier mixing sublayer followed by a feed-forward sublayer. Essentially, the paper authors replace the self-attention sublayer of each Transformer encoder layer with a Fourier sublayer, which applies a 2D DFT to its (sequence length, hidden dimension) embedding input. The paper authors compare FNet and Transformer architectures in a common transfer learning setting. For a fuller picture, the paper authors compare multiple models (see Table 1 for parameter counts in \u201cBase\u201d configuration):\n\u2022 BERT-Base: a Transformer encoder model. \n\u2022 FNet encoder: the paper authors replace every self-attention sublayer with a Fourier sublayer. \n\u2022 Linear encoder: the paper authors replace each self-attention sublayer with two learnable, dense, linear sublayers, one applied to the hidden dimension and one to the sequence dimension. \n\u2022 Random encoder: the paper authors replace each self-attention sublayer with two constant random matrices, one applied to the hidden dimension and one applied to the sequence dimension. \n\u2022 Feed Forward-only (FF-only) encoder: the paper authors completely remove the self-attention sublayer, so this model has no token mixing.\nMotivated by the models considered in Turc et al. (2019), the paper authors evaluated several model sizes; see Table 6 in Appendix A.1 Table 6: Pre-training model sizes (ignoring output projection layers)", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i", "For example, the sandwiching of Fourier, feed-forward, Fourier (or inverse Fourier) sublayers and only applying the residual connections and layer norms to the final result, yields a structure that more closely\n12Whereas the DFT matrix in Equation (2) contains N roots of unity, the Hadamard Transform simply contains two roots of unity: {\u00b11}; see also Kunz (1979). mimics convolutions. However, these setups degraded accuracy and lead to a more unstable model during training. Adding extra feed-forward sublayers to this layering, or swapping out the feedforward sublayers for simpler dense sublayers, did not help either. A.4 Mixing layer speeds Table 8 summarizes the inference and training speeds for the different mixing layers. For each of the Base and Large configurations, we have removed all other sublayers and transformations and then calculated the speed per batch of input examples. The FNet training speeds are particularly fast because no parameters are updated. The Linear model has faster inference than FNet on TPUs because it is performing real matrix multiplications, whereas FNet performs complex matrix multiplications; see Equation (2). Although the Fourier mixing sublayer itself performs forward and backward passes significantly faster than the self-attention sublayer, FNet is overall 70-80% faster than BERT because the overall training and inference speeds are bottle-necked by the feed-forward sublayers that all models share. A.5 FNet-Hybrid ablations Table 9 shows the effects of varying the number of attention sublayers and the attention layout in the FNet-Hybrid model. For the \u201cBOTTOM\u201d layout, all attention sublayers are placed in the first few encoder layers, where they replace the Fourier mixing sublayers. For the \u201cTOP\u201d layout, attention sublayers are placed in the final encoder layers; for the \u201cMIDDLE\u201d layout they are placed in the middle layers; and for the \u201cMIXED\u201d layout, they are distributed through the model."]}
{"pkey": "fnet_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors found that the smaller model architectures benefited from larger learning rates, so the paper authors select the best result using 10^\u22123 and 10^\u22124 for all models. Secondly, for the Pathfinder task, the paper authors found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). The paper authors perform a sweep over sequence lengths {512, 1024, 2048, 4096, 8192}. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4 \u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["Although the Hadamard Transform was slightly faster than the DFT, it yielded less accurate results (\u223c 2% accuracy degradation). \u2022 Hartley Transform. The Hartley Transform, which transforms real input to real output, can be described in terms of the Fourier Transform: H = <{F} \u2212 ={F}. We found that the Hartley Transform matched the Fourier Transform on GLUE (76.7 vs. 76.7). Introducing learnable parameters to the Fourier sublayer. Our attempts to introduce learnable parameters into the Fourier sublayer were either detrimental or inconsequential, and generally slightly slowed the model. For the (sequence length, hidden dimension) input in each Fourier sublayer, we tried two approaches to introduce learnable parameters: (1) element wise multiplication with a (sequence length, hidden dimension) matrix, and (2) regular matrix multiplication with (sequence length, sequence length) and (hidden dimension, hidden dimension) matrices. We experimented with these approaches in various configurations: preceding and/or following the DFT, and also in combination with inverse DFT (e.g. transform to frequency domain, apply element wise multiplication, transform back to time domain), but most setups degraded accuracy and reduced training stability, while a few did not change accuracy but lead to small speed decreases. In a slightly different set of experiments and in an effort to provide more flexibility to the model, we added (complex) learnable weights to the 2D DFT matrix. This model was stable but did not yield any accuracy gains, suggesting that the DFT is locally optimal in some sense. FNet block modifications. The standard FNet encoder block structure follows that of the Transformer: a Fourier sublayer followed by a feedforward sublayer, with residual connections and layer norms after each sublayer; see Figure 1. We tried several modifications to this structure, based on the intuition of moving in and out of the frequency domain between multiplications.", "From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i", "With the addition of just two self-attention sublayers, the hybrid FNet models achieve 97% and 99% of their respective BERT counterpart\u2019s accuracies with only limited speed degradations (see Table 3). Interestingly, the gap between BERT and FNet shrinks to just 3% for Large models; this is likely due to FNet-Large being more stable during training than BERT-Large.9 The Linear-Large model severely underperforms its Base counterpart on GLUE benchmark due to training instabilities. We generally found that the Linear model and BERT were less stable than the models with no param-\n9Devlin et al. (2019) obtain a roughly 2.5 average point boost on the Test split going from BERT-Base to BERT-Large. We only see a roughly 1.5 boost on the Validation split, which may be due to reduced headroom.\neters in their mixing sublayers, namely the FNet, Random and FF-only models. The speed vs MLM accuracy curve for GPU (8 V100 chips) pre-training is shown in Figure 2 (see Appendix A.2 for TPU results). Both TPU and GPU models are trained for 1 million steps as in Devlin et al. (2019). Motivated by the models considered in Turc et al. (2019), we evaluated several model sizes; see Table 6 in Appendix A.1. We found that the smaller model architectures benefited from larger learning rates, so we select the best result using 10\u22123 and 10\u22124 for all models.10\nThe GPU (Figure 2), and TPU (Figure 3 in Appendix A.2) results display the same trends. For larger, slower models, BERT and FNet-Hybrid define the Pareto speed-accuracy efficiency frontier. For smaller, faster models, FNet and the Linear model define the efficiency frontier. 4.2 Long-Range Arena (LRA) benchmark. Of the efficient Transformers evaluated on LRA benchmark by Tay et al. (2021a), their results suggest that (1) the vanilla Transformer is (by a small margin) the second most accurate model, and (2) the Performer (Choromanski et al., 2021) is the fastest model."]}
{"pkey": "fnet_13", "question": "Describe the computational resources used to train the model.", "answer": "Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4 \u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Our model uses JAX and, in particular, the Flax framework", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["Fourier analysis features heavily in studies of the universal approximation properties of neural networks; see, for example, (Cybenko, 1989; Barron, 1993). In terms of practical applications, discrete Fourier Transforms (DFT), and in particular the Fast Fourier Transform (FFT), have been used to tackle signal processing problems such as fitting neural networks to FFTs of electrocardiogram signals (Minami et al., 1999; Gothwal et al., 2011; Mironovova and B\u00edla, 2015) and vibration signals (Zhang et al., 2013), or to evolve solutions of Partial Differential Equations (Li et al., 2021). Because ordinary multiplication in the frequency domain corresponds to a convolution in the time domain, FFTs have been deployed in Convolutional Neural Networks to speed up computations, in Recurrent Neural Networks to speed up training and reduce exploding and vanishing gradients, and generally to approximate dense, linear layers to reduce computational complexity; see references cited in Section 1. DFTs have also been used indirectly in several Transformer works. The Performer (Choromanski et al., 2020) linearizes the Transformer selfattention mechanism by leveraging random Fourier features to approximate a Gaussian representation of the softmax kernel. In our work, rather than approximating attention, we replace attention with the Fourier Transform, which acts as an alternate hidden representation mixing mechanism. Tamkin et al. (2020) use spectral filters to generate hierarchical features, showing that the filtered embeddings perform well in different tasks (word-level, sentence-level or document-level), depending on which frequency scales are filtered. In contrast to FNet, they separate Fourier frequencies, rather than using the transform to combine features. Finally, through personal communication, we were alerted\nto concurrent, unpublished work (Backurs et al., 2021) that describes an FFT based neural model that is very similar to FNet. 2.2 Modeling semantic relations via attention.", "On GPUs, the FFT was the fastest algorithm for computing the DFT across all sequence lengths that we experimented with (512 \u2212 8192). On TPUs, it is faster to compute the DFT directly using matrix multiplications for relatively shorter sequence lengths (up to lengths of 4096; see Table 7). This efficiency boundary between matrix multiplication and FFT on TPUs will change depending on the XLA precision for the matrix multiplications. We found that, although (slower) HIGHEST XLA precision was required to very accurately reproduce FFT in computing the DFT, (faster) DEFAULT XLA precision was sufficient to facilitate accurate model convergence. Modifying the Fourier Transform computation. To keep the entire FNet architecture simple, the Fourier sublayer accepts real input and returns real output. The standard Fourier sublayer in FNet simply extracts the real part after computing the 2D DFT. We found that FNet was less accurate and less stable during training if only the real part of the DFT was used throughout the computation. Simply extracting the absolute value (instead of the real part) also led to a significantly less accurate model. Because the feed-forward sublayer mixes the hidden dimension, we experimented with applying a 1D DFT along the token dimension only in the Fourier sublayer (i.e. no hidden dimension mixing in the Fourier sublayer). This yielded some training speed gains but hurt accuracy. The 1D (token mixing only) DFT model still significantly outperformed the (no token mixing) FF-only model, indicating that token mixing is most important mechanism in the Fourier sublayer. Other transforms. We experimented with three natural alternatives to the Fourier Transform:\n\u2022 Discrete Cosine Transform (DCT). The DCT is closely related to the DFT but transforms real input to real output. However, we found that the DCT model underperformed FNet (\u223c 4% accuracy degradation). \u2022 Hadamard Transform12.", "Empirically, we found that on GPUs: the FFT is faster than matrix multiplications for all sequence lengths we consider (512\u2212 8192 tokens), whereas on TPUs: for relatively shorter sequences (\u2264 4096 tokens), it is faster to cache the DFT matrix and then compute the DFT through matrix multiplications than using the FFT; for longer sequences, the FFT is faster. As a result, our GPU FNet implementation always uses the FFT, while our TPU implementation computes the 2D DFT using matrix multiplications for sequences up to lengths of 4096 and the FFT for longer lengths. Presumably the GPU vs TPU difference is primarily a result of two factors: (1) TPUs are even more highly optimized for matrix multiplications than GPUs, and (2) GPUs offer a more efficient FFT implementa-\n4This is merely an intuition; the reality is more complicated due to the presence of residual connections and since the transformation in Equation (3) is no longer invertible if we only use the real component.\ntion than TPUs. We suspect that FNet will only become more performant on TPUs as the TPU implementation of the FFT improves. Our model uses JAX and, in particular, the Flax framework5. Core model code is given in Appendix A.7 and the full source core is available online.6\n4 Results. 4.1 Transfer learning. We compare FNet and Transformer architectures in a common transfer learning setting. For a fuller picture, we compare multiple models (see Table 1 for parameter counts in \u201cBase\u201d configuration):\n\u2022 BERT-Base: a Transformer encoder model. \u2022 FNet encoder: we replace every self-attention sublayer with a Fourier sublayer. \u2022 Linear encoder: we replace each self-attention sublayer with a two learnable, dense, linear sublayers, one applied to the hidden dimension and one to the sequence dimension. \u2022 Random encoder: we replace each selfattention sublayer with a two constant random matrices, one applied to the hidden dimension and one applied to the sequence dimension. \u2022"]}
{"pkey": "fnet_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "Core model code is given in Appendix A.7 and the full source core is available online at https://github.com/google-research/google-research/tree/master/f_net", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["The contributions of our paper are:\n\u2022 We show that simple linear transformations, including even (parameter-free) Fourier Transforms, along with standard MLPs in feedforward layers, are competent at modeling diverse relationships in text. That such a simple linear transformation works at all is surprising, and suggests that, for at least some NLP problems, attention may not be the principal component driving the performance of Transformers. \u2022 We introduce a new model, FNet, that uses the Fourier Transform as a mixing mechanism. FNet offers an excellent compromise between speed, memory footprint, and accuracy, achieving 92% and 97%, respectively, of the accuracy of BERT-Base and BERT-Large (Devlin et al., 2019) on the GLUE benchmark (Wang et al., 2018), while training 80% faster on GPUs and 70% faster on TPUs. \u2022 We find that FNet hybrid models containing only two self-attention sublayers achieve 97\u2212 99% of their BERT counterparts\u2019 accuracy on GLUE, while still running 40\u2212 70% faster. This indicates that, while attention can improve accuracy, it may not be necessary to use in every layer. \u2022 We demonstrate FNet scales very well to long inputs and offers a better compromise between speed and accuracy than the efficient Transformers evaluated on the Long-Range Arena (LRA) benchmark (Tay et al., 2021a). Specifically, FNet achieves accuracy comparable to the most accurate efficient Transformer architectures but is significantly faster at both\ntraining and inference than all of the evaluated Transformer architectures across all sequence lengths on GPUs. On TPUs, FNet is faster for relatively shorter sequence lengths; for longer sequences, the only efficient Transformers that are faster than FNet on TPUs are less accurate on the LRA benchmark. Based on this, we argue that rather than seeking more efficient approximations of the attention, there may be more value in seeking out completely new mixing mechanisms. 2 Related work. 2.1 Fourier Transforms in neural networks.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i"]}
{"pkey": "fnet_15", "question": "What is the pretraining objective of the model? ", "answer": "Table 5: Loss and accuracy pre-training metrics on TPUs. The GPU metrics are very similar. \u201cB\u201d denotes Base, \u201cL\u201d is Large and \u201cH\u201d is Hybrid.\n           Loss                   Accuracy\nTotal | MLM | NSP       MLM | NSP\nBERT\u2019s higher accuracy on the MLM pre-training task is not simply a result of having more parameters than the other models.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "Feed Forward-only (FF-only) encoder: we completely remove the self-attention sublayer; so that this model has no token mixing. 5https://github.com/google/flax 6https://github.com/google-research/\ngoogle-research/tree/master/f_net\nDespite its simplicity, the Linear baseline turns out to be surprisingly accurate and fast. Our Linear model is similar to the MLP-Mixer (Tolstikhin et al., 2021) (for vision) and also the Random Synthesizer (Tay et al., 2020a), but simplifies the latter model further by removing the multiple heads and softmax projections, resulting in just two matrix multiplications in the mixing sublayer. It is reasonable to expect that the Linear encoder, which uses densely parameterized mixing layers, will learn more flexibly than FNet, which uses parameter-free mixing layers. As we will show, although the Linear-Base model outperforms FNetBase slightly on GLUE (0.3 points), it has several efficiency drawbacks relative to FNet: it has a much larger memory footprint (see Table 4b), it is slower to train on regular 512 sequence lengths (see Table 3), and scales significantly worse on long sequence lengths (see Tables 4b-4c).7 We also found that Linear-Large was more difficult to train due to gradient blow up (see \u201cLarge\u201d scores in Table 2). We adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and training configurations as for the original BERT (Devlin et al., 2019), except that we pretrain on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) (see Appendix A.1 for full pre-training details). For fine-tuning on the GLUE benchmark (Wang et al., 2018), we found that different BERT runs with the same base learning rate could yield slightly different results. Consequently, for the Base (Large) models, we performed 3 (6) trials, respectively, for each base learning rate and reported the best result across all experiments."]}
{"pkey": "fnet_16", "question": "What is the loss function that is used to train the model?", "answer": "Not specified in the paper.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["Several \u201cefficient Transformers\u201d achieve O(N \u221a N) or even O(N) theoretical complexity. However, the constants hidden by this notation can be large. For example, in models such as Longformer (Beltagy et al., 2020), ETC (Ainslie et al., 2020), and BigBird (Zaheer et al., 2020), attention is O(N) as a\nfunction of the input length, but quadratic in the number of \u201cglobal tokens\u201d; the latter must be sufficiently large to ensure good performance. The Long-Range Arena benchmark (Tay et al., 2021a) attempts to compare many of the efficient Transformers in a series of tasks requiring long range dependencies, finding that the Performer (Choromanski et al., 2021), Linear Transformer (Katharopoulos et al., 2020), Linformer (Wang et al., 2020), and Image Transformer (Local Attention) (Parmar et al., 2018) were the fastest on TPUs and had the lowest peak memory usages per device.2 Instead, in this paper we completely replace self-attention with a different mixing, namely the Fourier Transform, which offers: (1) performance, (2) reduced model size (no learnable parameters), and (3) simplicity. Finally, we note that, in an effort to investigate different token mixing mechanisms, we compare a vanilla BERT model (Devlin et al., 2019) with a vanilla FNet, ignoring more recent Transformer optimizations, which we consider orthogonal to this work; see, for example, (Narang et al., 2021; Kim and Hassan, 2020; Shleifer and Rush, 2020). 3 Model.\n3.1 Discrete Fourier Transform. The Fourier Transform decomposes a function into its constituent frequencies. Given a sequence {xn} with n \u2208 [0, N\u22121], the discrete Fourier Transform (DFT) is defined by the formula: Xk = N\u22121\u2211 n=0 xne \u2212 2\u03c0i N nk, 0 \u2264 k \u2264 N \u2212 1. (1)\nFor each k, the DFT generates a new representation Xk as a sum of all of the original input tokens xn, with so-called \u201ctwiddle factors\u201d. There are two primary approaches to computing the DFT: the Fast Fourier Transform (FFT) and matrix multiplication.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models."]}
{"pkey": "fnet_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "FNet is an attention-free Transformer architecture, wherein each layer consists of a Fourier mix-ing sublayer followed by a feed-forward sublayer.\nThe paper authors compare FNet and Transformer architectures in a common transfer learning setting. For a fuller picture, the paper authors compare multiple models\n* BERT-Base: a Transformer encoder model.\n* FNet encoder: the paper authors replace every self-attention sublayer with a Fourier sublayer.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i", "With the addition of just two self-attention sublayers, the hybrid FNet models achieve 97% and 99% of their respective BERT counterpart\u2019s accuracies with only limited speed degradations (see Table 3). Interestingly, the gap between BERT and FNet shrinks to just 3% for Large models; this is likely due to FNet-Large being more stable during training than BERT-Large.9 The Linear-Large model severely underperforms its Base counterpart on GLUE benchmark due to training instabilities. We generally found that the Linear model and BERT were less stable than the models with no param-\n9Devlin et al. (2019) obtain a roughly 2.5 average point boost on the Test split going from BERT-Base to BERT-Large. We only see a roughly 1.5 boost on the Validation split, which may be due to reduced headroom.\neters in their mixing sublayers, namely the FNet, Random and FF-only models. The speed vs MLM accuracy curve for GPU (8 V100 chips) pre-training is shown in Figure 2 (see Appendix A.2 for TPU results). Both TPU and GPU models are trained for 1 million steps as in Devlin et al. (2019). Motivated by the models considered in Turc et al. (2019), we evaluated several model sizes; see Table 6 in Appendix A.1. We found that the smaller model architectures benefited from larger learning rates, so we select the best result using 10\u22123 and 10\u22124 for all models.10\nThe GPU (Figure 2), and TPU (Figure 3 in Appendix A.2) results display the same trends. For larger, slower models, BERT and FNet-Hybrid define the Pareto speed-accuracy efficiency frontier. For smaller, faster models, FNet and the Linear model define the efficiency frontier. 4.2 Long-Range Arena (LRA) benchmark. Of the efficient Transformers evaluated on LRA benchmark by Tay et al. (2021a), their results suggest that (1) the vanilla Transformer is (by a small margin) the second most accurate model, and (2) the Performer (Choromanski et al., 2021) is the fastest model.", "FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models."]}
{"pkey": "fnet_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "Table 2: GLUE Validation results on TPUs, after finetuning on respective tasks. \nMNLI\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\nTable 4: Accuracy, inference speed and memory usage results on the Long-Range Arena (LRA) benchmark.\nListOps\nText\nRetrieval\nImage\nPathfinder\nPath-X\nThe paper authors experimented with a number of additional ideas to improve FNet.\n* Fourier Transform algorithm. \n* Modifying the Fourier Transform computation.\n* Other transforms. The paper authors experimented with three natural alternatives to the Fourier Transform: DCT, Hadamard, Hartley\n* Introducing learnable parameters to the Fourier sublayer. \n* FNet block modifications.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["This reflects our observation that BERT-Large was less stable than BERT-Base, as noted in Devlin et al. (2019). We report the results for the best base learning rate (no early stopping) on the GLUE Validation split in Table 2.8 For Base models, results mirror the pre-training metrics (see Appendix A.1): BERT performs best. FNet and the Linear model both underperform BERT by 7.5 \u2212 8%. Referring to Table 3, we see that although less accurate, FNet trains significantly faster than BERT \u2013 80% faster on GPUs and 70% faster on TPUs \u2013 and performs\n7On the other hand, the smaller sized Linear models do generally perform well on 512 sequence lengths; see Figure 2. 8WNLI is excluded in Devlin et al. (2019). BERT\u2019s accuracy on WNLI is below baseline, unless a special training recipe is used. See also (12) in https:// gluebenchmark.com/faq. 63% of BERT\u2019s FLOPS. Measured in isolation, the Fourier sublayers perform forward and backward passes an order of magnitude faster than the self-attention sublayers (see Appendix A.4), but FNet\u2019s overall training speed is impeded by the feed-forward sublayers that all models share. Returning to Table 2: the FF-only model severely underperforms all other models: as expected, token mixing is critical to the expressivity of the model. For example, 50% accuracy scores on the binary classification tasks (QNLI, SST-2, RTE), indicate that the model fails to learn the tasks. The weak accuracy of the Random model suggests that not just any mixing will do; rather, a structured mixing is required. We also include metrics from a hybrid FNet attention model. In the hybrid model, we replace the final two Fourier sublayers of FNet with self-attention sublayers \u2013 other configurations\nare possible, but we generally found that replacing the final layers worked best; see Appendix A.5.", "FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1."]}
{"pkey": "fnet_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "A.5 FNet-Hybrid ablations. Table 9 shows the effects of varying the number of attention sublayers and the attention layout in the FNet-Hybrid model. Table 9: GPU pre-training accuracy and speed ablations for FNet-Hybrid models in the Base configuration. Batch size is 64. Metrics are recorded after 100k steps, which the paper authors have generally found to be a good indicator of final relative performance. See text for a description of the layouts. From the Table 9, the paper authors can make two observations:\n(1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns;\n(2) placing attention layers at the top of the model gives the best accuracy results. \nGiven our focus on speed, the paper authors chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant.", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["For example, the sandwiching of Fourier, feed-forward, Fourier (or inverse Fourier) sublayers and only applying the residual connections and layer norms to the final result, yields a structure that more closely\n12Whereas the DFT matrix in Equation (2) contains N roots of unity, the Hadamard Transform simply contains two roots of unity: {\u00b11}; see also Kunz (1979). mimics convolutions. However, these setups degraded accuracy and lead to a more unstable model during training. Adding extra feed-forward sublayers to this layering, or swapping out the feedforward sublayers for simpler dense sublayers, did not help either. A.4 Mixing layer speeds Table 8 summarizes the inference and training speeds for the different mixing layers. For each of the Base and Large configurations, we have removed all other sublayers and transformations and then calculated the speed per batch of input examples. The FNet training speeds are particularly fast because no parameters are updated. The Linear model has faster inference than FNet on TPUs because it is performing real matrix multiplications, whereas FNet performs complex matrix multiplications; see Equation (2). Although the Fourier mixing sublayer itself performs forward and backward passes significantly faster than the self-attention sublayer, FNet is overall 70-80% faster than BERT because the overall training and inference speeds are bottle-necked by the feed-forward sublayers that all models share. A.5 FNet-Hybrid ablations Table 9 shows the effects of varying the number of attention sublayers and the attention layout in the FNet-Hybrid model. For the \u201cBOTTOM\u201d layout, all attention sublayers are placed in the first few encoder layers, where they replace the Fourier mixing sublayers. For the \u201cTOP\u201d layout, attention sublayers are placed in the final encoder layers; for the \u201cMIDDLE\u201d layout they are placed in the middle layers; and for the \u201cMIXED\u201d layout, they are distributed through the model.", "Attention models have achieved state of the art results across virtually all NLP tasks and even some image tasks (Dosovitskiy et al., 2021). This success is generally attributed to the flexibility and capacity of attention. Although some works (Ramsauer et al., 2021) have endeavoured to gain a deeper understanding of attention, the pervasive intuition is that the success of attention models derives from the token-dependent attention patterns in different layers; see, for example, (Tenney et al., 2019). However, it is natural to ask: Do we really need the flexibility, and associated cost, of attention?\nTay et al. (2020a) empirically investigated the importance of the dot product operation in the attention mechanism in their Synthesizer model (related to our \u201cLinear\u201d baseline below). They find that learnt token-dependent attention weights are highly expressive, but not necessarily crucial for realizing accurate NLP models. You et al. (2020) replace attention weights in the Transformer encoder and decoder with unparameterized Gaussian distributions, showing minimal performance degradation provided they retain learnable cross-attention weights. Similarly, Raganato et al. (2020) find little to no accuracy degradation when replacing all but one of the attention heads of each attention layer in the encoder with fixed, non-learnable positional patterns. Finally, Tolstikhin et al. (2021) present MLP-Mixer, where attention is replaced by MLPs, with limited performance degradation in image classification tasks. 2.3 Efficient and long sequence models. The standard attention mechanism (Vaswani et al., 2017) has a quadratic time and memory bottleneck with respect to sequence length. This limits its applicability in tasks involving long range dependencies. Most efforts to improve attention efficiency are based on sparsifying the attention matrix. Tay et al. (2020c) survey many of the recent efficient attention works; see also citations in Section 1.", "Fourier analysis features heavily in studies of the universal approximation properties of neural networks; see, for example, (Cybenko, 1989; Barron, 1993). In terms of practical applications, discrete Fourier Transforms (DFT), and in particular the Fast Fourier Transform (FFT), have been used to tackle signal processing problems such as fitting neural networks to FFTs of electrocardiogram signals (Minami et al., 1999; Gothwal et al., 2011; Mironovova and B\u00edla, 2015) and vibration signals (Zhang et al., 2013), or to evolve solutions of Partial Differential Equations (Li et al., 2021). Because ordinary multiplication in the frequency domain corresponds to a convolution in the time domain, FFTs have been deployed in Convolutional Neural Networks to speed up computations, in Recurrent Neural Networks to speed up training and reduce exploding and vanishing gradients, and generally to approximate dense, linear layers to reduce computational complexity; see references cited in Section 1. DFTs have also been used indirectly in several Transformer works. The Performer (Choromanski et al., 2020) linearizes the Transformer selfattention mechanism by leveraging random Fourier features to approximate a Gaussian representation of the softmax kernel. In our work, rather than approximating attention, we replace attention with the Fourier Transform, which acts as an alternate hidden representation mixing mechanism. Tamkin et al. (2020) use spectral filters to generate hierarchical features, showing that the filtered embeddings perform well in different tasks (word-level, sentence-level or document-level), depending on which frequency scales are filtered. In contrast to FNet, they separate Fourier frequencies, rather than using the transform to combine features. Finally, through personal communication, we were alerted\nto concurrent, unpublished work (Backurs et al., 2021) that describes an FFT based neural model that is very similar to FNet. 2.2 Modeling semantic relations via attention."]}
{"pkey": "fnet_20", "question": "List the future work mentioned in the paper.", "answer": "The paper authors only performed a cursory survey of other linear transformations (see also Appendix A.3), and additional fast alternatives are worth exploring.Throughout this work, the paper authors have restricted our focus to encoders. FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that cross- attention may be crucial to performance", "title": "FNet: Mixing Tokens with Fourier Transforms", "context": ["From the Table 9, we can make two observations: (1) more attention improves accuracy at the cost of speed, and ultimately with diminishing returns; (2) placing attention layers at the top of the model gives the best accuracy results. Given our focus on speed, we chose to focus FNet-Hybrid experiments in the main text of the paper on the 2 attention layer, \u201cTOP\u201d configuration variant. A.6 A note on Long-Range Arena hyperparameter settings\nConcerning the Long-Range Arena setup, several hyperparameters are not described in Tay et al. (2021a) and there a few mismatches between the configurations described in the paper and the code repository. Where possible, we prioritize configurations described in the paper with only two exceptions. Firstly, for the CIFAR10 (Image) task, we perform a sweep of the number of layers in the\nrange [1, 2, 3, 4]. We found that 1 layer worked best for all models; Tay et al. (2021a) suggest 3 layers yielded the best results. Secondly, for the Pathfinder task, we found that a base learning rate of 0.001 (as given in the code repository) yielded better results for all models than the 0.01 value indicated in Tay et al. (2021a). We also perform a very small sweep over the embedding dimension and batch size, which are not listed in Tay et al. (2021a). We also remark that the accuracy comparisons between our runs and those from Tay et al. (2021a) should be performed with the caveat that we found that results for certain tasks \u2013 Text and Retrieval in particular \u2013 can vary quite a bit between runs, especially for the Transformer; we report the best results. A.7 FNet code\n1 impor t f l a x . l i n e n as nn.\n2 impor t j ax. 3 impor t j ax . numpy as jnp 4 5 6 class Four ierTransformLayer ( nn . Module ) : 7 @nn. compact 8 def __ca l l__ ( s e l f , x ) : 9 r e t u r n jax . vmap( jnp . f f t . f f t n ) ( x ) . r e a l\n10 11 12 class FeedForwardLayer ( nn . Module ) : 13 d _ f f : i n t 14 dropout_ra te : f l o a t 15 16 @nn. compact 17 def __ca l l__ ( s e l f , x , d e t e r m i", "FNet decoders can be designed by \u201ccausally\u201d masking the Vandermonde matrix, but a lower level implementation is required to introduce causal masking to FFTs. How to adapt Fourier mixing for encoder-decoder cross-attention is an open question as evidence suggests that crossattention may be crucial to performance (You et al., 2020). We have focused on tasks which do not require generation so we leave FNet decoders and encoder-decoder setups to future work; although we do remark that the FNet encoder could be used as a drop in replacement in a Transformer as other works have successfully demonstrated; see, for example, (Zaheer et al., 2020; Guo et al., 2021). A Appendices. A.1 Pre-training details\nWe adopt the same fixed \u201cBase\u201d and \u201cLarge\u201d model and learning configurations as for the original BERT (Devlin et al., 2019). We train on the much larger C4 dataset (Raffel et al., 2020) and use a 32000 SentencePiece vocabulary model (Kudo and Richardson, 2018) trained on a 100 million sentence subset of C4. Our TPU experiments use a batch size of 256 as in Devlin et al. (2019) and are each run on 4\u00d7 4 TPU v3 chips. Our GPU experiments use a smaller batch size of 64 and are run on 8 V100 chips. Because the training configuration is lifted from Devlin et al. (2019), it may be slightly biased towards the BERT attention model. Table 5 summarizes the pre-training metrics for the different models; the pre-training speeds are shown in Table 3 in the main text. Although they have weaker accuracy metrics, the Linear model and FNet train nearly 80% faster than BERT on GPUs, and 70% faster on TPUs (see Table 3). We also find that the three models with no learnable parameters in their mixing layer, namely FNet, the Random model and the FF-only model, are the most stable during training. BERT\u2019s higher accuracy on the MLM pretraining task is not simply a result of having more parameters than the other models.", "Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.1\n1 Introduction. The Transformer architecture (Vaswani et al., 2017) has achieved rapid and widespread dominance in NLP. At its heart is a attention mechanism \u2013 an inductive bias that connects each token in the input through a relevance weighted basis of every other token. Many papers have prodded and probed the Transformer, and in particular the attention sublayers, in an effort to better understand the architecture; see, for example, Tenney et al. (2019); Vig and Belinkov (2019); Clark et al. (2019); Voita et al. (2019). Although potentially limited in their effectiveness (Hewitt and Liang, 2019), these probes generally back the intuition that, by allowing higher order units to form out of compositions of the input,\n1Code is available at https://github.com/ google-research/google-research/tree/ master/f_net. Transformer models can flexibly capture diverse syntactic and semantic relationships. In this work, we investigate whether simpler token mixing mechanisms can wholly replace the relatively complex self-attention layers in Transformer encoder architectures. We first replace the attention sublayer with two parameterized matrix multiplications \u2013 one mixing the sequence dimension and one mixing the hidden dimension. Seeing promising results in this simple linear mixing scheme, we further investigate the efficacy of faster, structured linear transformations. Surprisingly, we find that the Fourier Transform, despite having no parameters at all, achieves nearly the same performance as dense linear mixing and scales very efficiently to long inputs, especially on GPUs (owing to the O(N logN) Fast Fourier Transform (FFT) algorithm). We call the resulting model FNet."]}
{"pkey": "reformer_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["That is much more efficient, but how can we find the nearest neighbors among the keys? Locality sensitive hashing. The problem of finding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector x to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not. In our case, we actually only require that nearby vectors get the same hash with high probability and that hash-buckets are of similar size with high probability. We achieve this by employing random projections as follows (see Figure 1). To get b hashes, we first fix a random matrix R of size [dk, b/2]. We then define h(x) = argmax([xR;\u2212xR]) where [u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors. LSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We first rewrite the equation for normal attention, (1), for a single query position i at a time: oi = \u2211 j\u2208Pi exp (qi \u00b7 kj \u2212 z(i,Pi)) vj where Pi = {j : i \u2265 j} (2)\nWe introduce the notation Pi to represent the set that the query at position i attends to, and z to denote the partition function (i.e. the normalizing term in the softmax). For clarity, we also omit scaling by \u221a dk. For batching purposes we typically perform attention over a larger set P\u0303i = {0, 1, . . . , l} \u2287 Pi while masking out elements not in Pi:\noi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) Now we turn to LSH attention, which we can think of in terms of restricting the set Pi of target items a query position i can attend to, by only allowing attention within a single hash bucket. Pi = {j : h(qi) = h(kj)} (4)\nFigure 2(a-b) shows a schematic comparison of full-attention with a hashed variant.", "But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately, only calculating softmax( qiK T\n\u221a dk )V once in memory, and then re-computing it on the backward pass when needed for gradients. This way of computing attention may be less efficient but it only uses memory proportional to length. We use this memory-efficient implementation of attention to run the full-attention baselines presented in the experimental section. Where do Q, K, V come from? The multi-head attention described above operates on keys, queries and values, but usually we are only given a single tensor of activations A of the shape [batch size, length, dmodel] \u2013 e.g., coming from embedding the tokens in a sentence into vectors. To build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K and V with different parameters. For models with LSH attention, we want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V. We call a model that behaves like this a shared-QK Transformer. It turns out that sharing QK does not affect the performance of Transformer, even if we additionally normalize the length of the keys K, as we show in the experimental Section 5. Hashing attention. For the LSH attention, we start with two tensors, Q=K and V of the shape [batch size, length, dmodel]. We keep the multi-head mechanism intact and focus on the attention computation from Equation 1. As already mentioned, the main issue is the term QKT , which has the shape [batch size, length, length]. But note that we are actually only interested in softmax(QKT ). Since softmax is dominated by the largest elements, for each query qi we only need to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi we could only consider a small subset of, say, the 32 or 64 closest keys.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022"]}
{"pkey": "reformer_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model\u2019s self-attention mechanism (Sukhbaatar et al., 2019a;b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al., 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al., 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory. 5 EXPERIMENTS.", "That is much more efficient, but how can we find the nearest neighbors among the keys? Locality sensitive hashing. The problem of finding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector x to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not. In our case, we actually only require that nearby vectors get the same hash with high probability and that hash-buckets are of similar size with high probability. We achieve this by employing random projections as follows (see Figure 1). To get b hashes, we first fix a random matrix R of size [dk, b/2]. We then define h(x) = argmax([xR;\u2212xR]) where [u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors. LSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We first rewrite the equation for normal attention, (1), for a single query position i at a time: oi = \u2211 j\u2208Pi exp (qi \u00b7 kj \u2212 z(i,Pi)) vj where Pi = {j : i \u2265 j} (2)\nWe introduce the notation Pi to represent the set that the query at position i attends to, and z to denote the partition function (i.e. the normalizing term in the softmax). For clarity, we also omit scaling by \u221a dk. For batching purposes we typically perform attention over a larger set P\u0303i = {0, 1, . . . , l} \u2287 Pi while masking out elements not in Pi:\noi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) Now we turn to LSH attention, which we can think of in terms of restricting the set Pi of target items a query position i can attend to, by only allowing attention within a single hash bucket. Pi = {j : h(qi) = h(kj)} (4)\nFigure 2(a-b) shows a schematic comparison of full-attention with a hashed variant.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022"]}
{"pkey": "reformer_3", "question": "What are the main contributions of the paper?", "answer": "The paper authors introduce the Reformer model which solves these problems using the following techniques:\n\u2022 Reversible layers, first introduced in Gomez et al. (2017), enable storing only a single copy of activations in the whole model, so the N factor disappears.\n\u2022 Splitting activations inside feed-forward layers and processing them in chunks removes the d_ff factor and saves memory inside feed-forward layers.\n\u2022 Approximate attention computation based on locality-sensitive hashing replaces the O(L^2) factor in attention layers with O(L log L) and so allows operating on long sequences.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["That is much more efficient, but how can we find the nearest neighbors among the keys? Locality sensitive hashing. The problem of finding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector x to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not. In our case, we actually only require that nearby vectors get the same hash with high probability and that hash-buckets are of similar size with high probability. We achieve this by employing random projections as follows (see Figure 1). To get b hashes, we first fix a random matrix R of size [dk, b/2]. We then define h(x) = argmax([xR;\u2212xR]) where [u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors. LSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We first rewrite the equation for normal attention, (1), for a single query position i at a time: oi = \u2211 j\u2208Pi exp (qi \u00b7 kj \u2212 z(i,Pi)) vj where Pi = {j : i \u2265 j} (2)\nWe introduce the notation Pi to represent the set that the query at position i attends to, and z to denote the partition function (i.e. the normalizing term in the softmax). For clarity, we also omit scaling by \u221a dk. For batching purposes we typically perform attention over a larger set P\u0303i = {0, 1, . . . , l} \u2287 Pi while masking out elements not in Pi:\noi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) Now we turn to LSH attention, which we can think of in terms of restricting the set Pi of target items a query position i can attend to, by only allowing attention within a single hash bucket. Pi = {j : h(qi) = h(kj)} (4)\nFigure 2(a-b) shows a schematic comparison of full-attention with a hashed variant.", "But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately, only calculating softmax( qiK T\n\u221a dk )V once in memory, and then re-computing it on the backward pass when needed for gradients. This way of computing attention may be less efficient but it only uses memory proportional to length. We use this memory-efficient implementation of attention to run the full-attention baselines presented in the experimental section. Where do Q, K, V come from? The multi-head attention described above operates on keys, queries and values, but usually we are only given a single tensor of activations A of the shape [batch size, length, dmodel] \u2013 e.g., coming from embedding the tokens in a sentence into vectors. To build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K and V with different parameters. For models with LSH attention, we want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V. We call a model that behaves like this a shared-QK Transformer. It turns out that sharing QK does not affect the performance of Transformer, even if we additionally normalize the length of the keys K, as we show in the experimental Section 5. Hashing attention. For the LSH attention, we start with two tensors, Q=K and V of the shape [batch size, length, dmodel]. We keep the multi-head mechanism intact and focus on the attention computation from Equation 1. As already mentioned, the main issue is the term QKT , which has the shape [batch size, length, length]. But note that we are actually only interested in softmax(QKT ). Since softmax is dominated by the largest elements, for each query qi we only need to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi we could only consider a small subset of, say, the 32 or 64 closest keys.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022"]}
{"pkey": "reformer_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "The paper authors experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image generation task (imagenet-64 generation) with sequences of length 12K. In addition to generating very long coherent text, the Reformer can bring the power of Transformer models to other domains like time-series forecasting, music, image and video generation.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["We see that while regular attention becomes slower at longer sequence length, LSH attention speed remains flat. Large Reformer models. To verify that the Reformer can indeed fit large models on a single core and train fast on long sequences, we train up to 20-layer big Reformers on enwik8 and imagenet64. As can be seen in Figure 5, these models fit into memory and train. We were not able to train Transformer baselines in this case as they are too slow and memory-hungry, but we see clear improvement with the number of layers. A 12-layer model on enwik8 trained for 20K steps with a dropout rate of 0.1 achieves 1.19 bits/dim on the test set. We also trained a 12-layer Reformer model for longer with further tuning and improvements and we reached 1.05 bits/dim on the enwiki8 test set. 6 CONCLUSION. Reformer combines the modeling capacity of a Transformer with an architecture that can be executed efficiently on long sequences and with small memory use even for models with a large number of layers. We believe that this will help large, richly-parameterized Transformer models become more widespread and accessible. Also, the ability to handle long sequences opens the way for the use of the Reformer on many generative tasks. In addition to generating very long coherent text, the Reformer can bring the power of Transformer models to other domains like time-series forecasting, music, image and video generation. A MULTI-ROUND LSH ATTENTION. In this section we describe in more detail the multi-hash version of our LSH attention mechanism. We first repeat Equation (3) from the main text, which describes a general formulation of attention with sparsity: oi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) In the multi-round case, a query position i can attend to key positions Pi as defined in (6), which we also repeat here:\nPi = nrounds\u22c3\nr=1\nP(r)i where P (r) i = { j : h(r)(qi) = h (r)(qj) }\n(6)", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022", "But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately, only calculating softmax( qiK T\n\u221a dk )V once in memory, and then re-computing it on the backward pass when needed for gradients. This way of computing attention may be less efficient but it only uses memory proportional to length. We use this memory-efficient implementation of attention to run the full-attention baselines presented in the experimental section. Where do Q, K, V come from? The multi-head attention described above operates on keys, queries and values, but usually we are only given a single tensor of activations A of the shape [batch size, length, dmodel] \u2013 e.g., coming from embedding the tokens in a sentence into vectors. To build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K and V with different parameters. For models with LSH attention, we want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V. We call a model that behaves like this a shared-QK Transformer. It turns out that sharing QK does not affect the performance of Transformer, even if we additionally normalize the length of the keys K, as we show in the experimental Section 5. Hashing attention. For the LSH attention, we start with two tensors, Q=K and V of the shape [batch size, length, dmodel]. We keep the multi-head mechanism intact and focus on the attention computation from Equation 1. As already mentioned, the main issue is the term QKT , which has the shape [batch size, length, length]. But note that we are actually only interested in softmax(QKT ). Since softmax is dominated by the largest elements, for each query qi we only need to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi we could only consider a small subset of, say, the 32 or 64 closest keys."]}
{"pkey": "reformer_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors ran our experiments on the imagenet64 and enwik8-64K tasks. The paper authors also evaluate on the WMT 2014 English-to-German translation task", "title": "Reformer: The Efficient Transformer (2020)", "context": ["The above task can be solved perfectly (to accuracy 100% and loss 0) by a 1-layer Transformer model. Note though, that it requires non-local attention lookups, so it cannot be solved by any model relying on sparse attention with a limited span. To make it easy and fast to train but similar to models used in NLP, we use a 1-layer Transformer with dmodel = dff = 256 and 4 heads. We train it for 150K steps in 4 different settings: with full attention, LSH attention with nrounds = 1, nrounds = 2 and nrounds = 4.\nFrom the results summarized in Table 2 we see that a model trained with full attention can be immediately used with LSH attention, but at some loss of accuracy. When trained from scratch with LSH attention, the model trained with 4 hashes achieves almost perfect accuracy as well. Interestingly, the accuracy becomes perfect when evaluated with 8 hashes. It goes down when evaluated with 2 or 1 hashes. Models trained with less hashes show worse results but even the model trained with just 1 hash performs almost perfectly when evaluated with 8 hashes. 3 REVERSIBLE TRANSFORMER. As the above section shows, the complexity of attention can be reduced from square in length to linear, provided an approximation is acceptable. But it is clear from Table 1 that each field starts with a b \u00b7 nh \u00b7 l term: the b \u00b7 nh \u00b7 l \u00b7 dk, or alternatively b \u00b7 l \u00b7 dmodel cost cannot be avoided. Indeed, the activations before each layer are already of the size b \u00b7 l \u00b7dmodel, so the memory use of the whole model with nl layers is at least b \u00b7 l \u00b7 dmodel \u00b7 nl. Even worse: inside the feed-forward layers of Transformer this goes up to b \u00b7 l \u00b7 dff \u00b7 nl. In a big Transformer it is usual to set dff = 4K and nl = 16 so with l = 64K this again would use an impractical 16GB of memory In this section, we show how to reduce this cost by first dealing with the nl part of the term using reversible layers and then showing how chunking can allow us to handle the dff problem.", "In this section we present experimental results demonstrating the techniques described above. We analyze the techniques one-by-one to make clear which combinations have impact on performance. We start by showing that reversible layers and shared query-key spaces do not impact performance, then proceed to analyze hashing attention and finally the full Reformer model. We ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant of enwik8 that is chunked into subsequences of 216 = 64K tokens. We use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l2) attention. All experiments have dmodel = 1024, dff = 4096, nheads = 8, and a total batch size of 8 sequences. We used the Adafactor optimizer (Shazeer & Stern, 2018) for training these models. We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017). Training for all experiments\nwas parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made publicly available.2\nEffect of sharing QK. We first consider the effect of shared-QK attention on a regular Transformer model. Shared-QK attention sets kj =\nqj \u2016qj\u2016 and prevents tokens from attending to themselves\n(except when no other context is available). In the left part of Figure 3, we plot perplexity curves for both regular and shared-QK attention. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not sacrificing accuracy by switching to shared-QK attention. Effect of reversible layers. In the two plots on the right in Figure 3, we compare a regular Transformer per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022"]}
{"pkey": "reformer_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "For models with LSH attention, the paper authors want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V. The paper authors call a model that behaves like this a shared-QK Transformer.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately, only calculating softmax( qiK T\n\u221a dk )V once in memory, and then re-computing it on the backward pass when needed for gradients. This way of computing attention may be less efficient but it only uses memory proportional to length. We use this memory-efficient implementation of attention to run the full-attention baselines presented in the experimental section. Where do Q, K, V come from? The multi-head attention described above operates on keys, queries and values, but usually we are only given a single tensor of activations A of the shape [batch size, length, dmodel] \u2013 e.g., coming from embedding the tokens in a sentence into vectors. To build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K and V with different parameters. For models with LSH attention, we want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V. We call a model that behaves like this a shared-QK Transformer. It turns out that sharing QK does not affect the performance of Transformer, even if we additionally normalize the length of the keys K, as we show in the experimental Section 5. Hashing attention. For the LSH attention, we start with two tensors, Q=K and V of the shape [batch size, length, dmodel]. We keep the multi-head mechanism intact and focus on the attention computation from Equation 1. As already mentioned, the main issue is the term QKT , which has the shape [batch size, length, length]. But note that we are actually only interested in softmax(QKT ). Since softmax is dominated by the largest elements, for each query qi we only need to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi we could only consider a small subset of, say, the 32 or 64 closest keys.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022", "That is much more efficient, but how can we find the nearest neighbors among the keys? Locality sensitive hashing. The problem of finding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector x to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not. In our case, we actually only require that nearby vectors get the same hash with high probability and that hash-buckets are of similar size with high probability. We achieve this by employing random projections as follows (see Figure 1). To get b hashes, we first fix a random matrix R of size [dk, b/2]. We then define h(x) = argmax([xR;\u2212xR]) where [u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors. LSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We first rewrite the equation for normal attention, (1), for a single query position i at a time: oi = \u2211 j\u2208Pi exp (qi \u00b7 kj \u2212 z(i,Pi)) vj where Pi = {j : i \u2265 j} (2)\nWe introduce the notation Pi to represent the set that the query at position i attends to, and z to denote the partition function (i.e. the normalizing term in the softmax). For clarity, we also omit scaling by \u221a dk. For batching purposes we typically perform attention over a larger set P\u0303i = {0, 1, . . . , l} \u2287 Pi while masking out elements not in Pi:\noi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) Now we turn to LSH attention, which we can think of in terms of restricting the set Pi of target items a query position i can attend to, by only allowing attention within a single hash bucket. Pi = {j : h(qi) = h(kj)} (4)\nFigure 2(a-b) shows a schematic comparison of full-attention with a hashed variant."]}
{"pkey": "reformer_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Not Specified in paper", "title": "Reformer: The Efficient Transformer (2020)", "context": ["The above task can be solved perfectly (to accuracy 100% and loss 0) by a 1-layer Transformer model. Note though, that it requires non-local attention lookups, so it cannot be solved by any model relying on sparse attention with a limited span. To make it easy and fast to train but similar to models used in NLP, we use a 1-layer Transformer with dmodel = dff = 256 and 4 heads. We train it for 150K steps in 4 different settings: with full attention, LSH attention with nrounds = 1, nrounds = 2 and nrounds = 4.\nFrom the results summarized in Table 2 we see that a model trained with full attention can be immediately used with LSH attention, but at some loss of accuracy. When trained from scratch with LSH attention, the model trained with 4 hashes achieves almost perfect accuracy as well. Interestingly, the accuracy becomes perfect when evaluated with 8 hashes. It goes down when evaluated with 2 or 1 hashes. Models trained with less hashes show worse results but even the model trained with just 1 hash performs almost perfectly when evaluated with 8 hashes. 3 REVERSIBLE TRANSFORMER. As the above section shows, the complexity of attention can be reduced from square in length to linear, provided an approximation is acceptable. But it is clear from Table 1 that each field starts with a b \u00b7 nh \u00b7 l term: the b \u00b7 nh \u00b7 l \u00b7 dk, or alternatively b \u00b7 l \u00b7 dmodel cost cannot be avoided. Indeed, the activations before each layer are already of the size b \u00b7 l \u00b7dmodel, so the memory use of the whole model with nl layers is at least b \u00b7 l \u00b7 dmodel \u00b7 nl. Even worse: inside the feed-forward layers of Transformer this goes up to b \u00b7 l \u00b7 dff \u00b7 nl. In a big Transformer it is usual to set dff = 4K and nl = 16 so with l = 64K this again would use an impractical 16GB of memory In this section, we show how to reduce this cost by first dealing with the nl part of the term using reversible layers and then showing how chunking can allow us to handle the dff problem.", "Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model\u2019s self-attention mechanism (Sukhbaatar et al., 2019a;b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al., 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al., 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory. 5 EXPERIMENTS.", "That is much more efficient, but how can we find the nearest neighbors among the keys? Locality sensitive hashing. The problem of finding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector x to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not. In our case, we actually only require that nearby vectors get the same hash with high probability and that hash-buckets are of similar size with high probability. We achieve this by employing random projections as follows (see Figure 1). To get b hashes, we first fix a random matrix R of size [dk, b/2]. We then define h(x) = argmax([xR;\u2212xR]) where [u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors. LSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We first rewrite the equation for normal attention, (1), for a single query position i at a time: oi = \u2211 j\u2208Pi exp (qi \u00b7 kj \u2212 z(i,Pi)) vj where Pi = {j : i \u2265 j} (2)\nWe introduce the notation Pi to represent the set that the query at position i attends to, and z to denote the partition function (i.e. the normalizing term in the softmax). For clarity, we also omit scaling by \u221a dk. For batching purposes we typically perform attention over a larger set P\u0303i = {0, 1, . . . , l} \u2287 Pi while masking out elements not in Pi:\noi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) Now we turn to LSH attention, which we can think of in terms of restricting the set Pi of target items a query position i can attend to, by only allowing attention within a single hash bucket. Pi = {j : h(qi) = h(kj)} (4)\nFigure 2(a-b) shows a schematic comparison of full-attention with a hashed variant."]}
{"pkey": "reformer_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image generation task (imagenet-64 generation) with sequences of length 12K.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["In this section we present experimental results demonstrating the techniques described above. We analyze the techniques one-by-one to make clear which combinations have impact on performance. We start by showing that reversible layers and shared query-key spaces do not impact performance, then proceed to analyze hashing attention and finally the full Reformer model. We ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant of enwik8 that is chunked into subsequences of 216 = 64K tokens. We use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l2) attention. All experiments have dmodel = 1024, dff = 4096, nheads = 8, and a total batch size of 8 sequences. We used the Adafactor optimizer (Shazeer & Stern, 2018) for training these models. We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017). Training for all experiments\nwas parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made publicly available.2\nEffect of sharing QK. We first consider the effect of shared-QK attention on a regular Transformer model. Shared-QK attention sets kj =\nqj \u2016qj\u2016 and prevents tokens from attending to themselves\n(except when no other context is available). In the left part of Figure 3, we plot perplexity curves for both regular and shared-QK attention. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not sacrificing accuracy by switching to shared-QK attention. Effect of reversible layers. In the two plots on the right in Figure 3, we compare a regular Transformer per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022", "We see that while regular attention becomes slower at longer sequence length, LSH attention speed remains flat. Large Reformer models. To verify that the Reformer can indeed fit large models on a single core and train fast on long sequences, we train up to 20-layer big Reformers on enwik8 and imagenet64. As can be seen in Figure 5, these models fit into memory and train. We were not able to train Transformer baselines in this case as they are too slow and memory-hungry, but we see clear improvement with the number of layers. A 12-layer model on enwik8 trained for 20K steps with a dropout rate of 0.1 achieves 1.19 bits/dim on the test set. We also trained a 12-layer Reformer model for longer with further tuning and improvements and we reached 1.05 bits/dim on the enwiki8 test set. 6 CONCLUSION. Reformer combines the modeling capacity of a Transformer with an architecture that can be executed efficiently on long sequences and with small memory use even for models with a large number of layers. We believe that this will help large, richly-parameterized Transformer models become more widespread and accessible. Also, the ability to handle long sequences opens the way for the use of the Reformer on many generative tasks. In addition to generating very long coherent text, the Reformer can bring the power of Transformer models to other domains like time-series forecasting, music, image and video generation. A MULTI-ROUND LSH ATTENTION. In this section we describe in more detail the multi-hash version of our LSH attention mechanism. We first repeat Equation (3) from the main text, which describes a general formulation of attention with sparsity: oi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) In the multi-round case, a query position i can attend to key positions Pi as defined in (6), which we also repeat here:\nPi = nrounds\u22c3\nr=1\nP(r)i where P (r) i = { j : h(r)(qi) = h (r)(qj) }\n(6)"]}
{"pkey": "reformer_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Not Specified in paper", "title": "Reformer: The Efficient Transformer (2020)", "context": ["However, computations in feed-forward layers are completely independent across positions in a sequence, so the computation can be split into c chunks:\nY2 = [ Y (1) 2 ; . . . ;Y (c) 2 ] = [ X (1) 2 + FeedForward(Y (1) 1 ); . . . ;X (c) 2 + FeedForward(Y (c) 1 ) ] (10) This layer is typically batched by performing operations for all positions in parallel, but operating on one chunk at a time can reduce memory. The reverse computation in (8) and the backward pass are also chunked. In addition to the feed-forward layers, for models with large vocabulary (more than dmodel word types) we also chunk the log-probabilities at the output and calculate the loss for sections of the sequence at a time. Chunking, large batches and parameter reuse. With chunking and reversible layers the memory we use for activations in the whole network is independent of the number of layers. The same is not true for parameters though as their number grows with the number of layers. This problem is remedied though because we can swap layer parameters to and from CPU memory when this layer is not computing. In a standard Transformer this would be inefficient because memory transfer to CPU is slow. The batch size multiplied by length in Reformer is much larger though and therefore the amount of compute done with the parameters amortizes the cost of their transfer. 4 RELATED WORK. The Transformer model introduced in (Vaswani et al., 2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018; Radford et al., 2019).", "But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately, only calculating softmax( qiK T\n\u221a dk )V once in memory, and then re-computing it on the backward pass when needed for gradients. This way of computing attention may be less efficient but it only uses memory proportional to length. We use this memory-efficient implementation of attention to run the full-attention baselines presented in the experimental section. Where do Q, K, V come from? The multi-head attention described above operates on keys, queries and values, but usually we are only given a single tensor of activations A of the shape [batch size, length, dmodel] \u2013 e.g., coming from embedding the tokens in a sentence into vectors. To build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K and V with different parameters. For models with LSH attention, we want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V. We call a model that behaves like this a shared-QK Transformer. It turns out that sharing QK does not affect the performance of Transformer, even if we additionally normalize the length of the keys K, as we show in the experimental Section 5. Hashing attention. For the LSH attention, we start with two tensors, Q=K and V of the shape [batch size, length, dmodel]. We keep the multi-head mechanism intact and focus on the attention computation from Equation 1. As already mentioned, the main issue is the term QKT , which has the shape [batch size, length, length]. But note that we are actually only interested in softmax(QKT ). Since softmax is dominated by the largest elements, for each query qi we only need to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi we could only consider a small subset of, say, the 32 or 64 closest keys.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022"]}
{"pkey": "reformer_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "Not Specified in paper", "title": "Reformer: The Efficient Transformer (2020)", "context": ["REFORMER: THE EFFICIENT TRANSFORMER. Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L) to O(L logL), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences. REFORMER: THE EFFICIENT TRANSFORMER. Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L) to O(L logL), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences. 1 INTRODUCTION. The Transformer architecture (Vaswani et al., 2017) is widely used in natural language processing and yields state-of-the-art results on a number of tasks. To obtain these results, researchers have resorted to training ever larger Transformer models.", "Since the depth dff of intermediate feed-forward layers is often much larger than the depth dmodel of attention activations, it accounts for a large fraction of memory use. \u2022 Attention on sequences of length L is O(L2) in both computational and memory complexity, so even for a single sequence of 64K tokens can exhaust accelerator memory. \u2217Equal Contribution 1https://hackingsemantics.xyz/2019/leaderboards/\nar X\niv :2\n00 1.\n04 45\n1v 2\n[ cs\n.L G\n] 1\n8 Fe\nb 20\n20\nWe introduce the Reformer model which solves these problems using the following techniques:\n\u2022 Reversible layers, first introduced in Gomez et al. (2017), enable storing only a single copy of activations in the whole model, so the N factor disappears. \u2022 Splitting activations inside feed-forward layers and processing them in chunks removes the dff factor and saves memory inside feed-forward layers. \u2022 Approximate attention computation based on locality-sensitive hashing replaces the O(L2) factor in attention layers with O(L logL) and so allows operating on long sequences. We study these techniques and show that they have negligible impact on the training process compared to the standard Transformer. Splitting activations in fact only affects the implementation; it is numerically identical to the layers used in the Transformer. Applying reversible residuals instead of the standard ones does change the model but has a negligible effect on training in all configurations we experimented with. Finally, locality-sensitive hashing in attention is a more major change that can influence the training dynamics, depending on the number of concurrent hashes used. We study this parameter and find a value which is both efficient to use and yields results very close to full attention. We experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image generation task (imagenet-64 generation) with sequences of length 12K.", "In both cases we show that Reformer matches the results obtained with full Transformer but runs much faster, especially on the text task, and with orders of magnitude better memory efficiency. 2 LOCALITY-SENSITIVE HASHING ATTENTION. Dot-product attention. The standard attention used in the Transformer is the scaled dot-product attention (Vaswani et al., 2017). The input consists of queries and keys of dimension dk, and values of dimension dv . The dot products of the query with all keys are computed, scaled by \u221a dk, and a softmax function is applied to obtain the weights on the values. In practice, the attention function on a set of queries is computed simultaneously, packed together into a matrix Q. Assuming the keys and values are also packed together into matrices K and V , the matrix of outputs is defined as:\nAttention(Q,K, V ) = softmax( QKT\u221a\ndk )V (1)\nMulti-head attention. In the Transformer, instead of performing a single attention function with dmodel-dimensional keys, values and queries, one linearly projects the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. Attention is applied to each of these projected versions of queries, keys and values in parallel, yielding dvdimensional output values. These are concatenated and once again projected, resulting in the final values. This mechanism is known as multi-head attention. Memory-efficient attention. To calculate the memory use of the attention mechanism, let us focus on the attention computation from Equation 1. Let us assume that Q, K and V all have the shape [batch size, length, dmodel]. The main issue is the term QKT , which has the shape [batch size, length, length]. In the experimental section we train a model on sequences of length 64K \u2013 in this case, even at batch-size of 1, this is a 64K\u00d764K matrix, which in 32-bit floats would take 16GB of memory. This is impractical and has hindered the use of the Transformer for long sequences."]}
{"pkey": "reformer_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "The paper authors analyze the techniques one-by-one to make clear which combinations have impact on performance. The paper authors also evaluate reversible layers in the context of an encoder-decoder Transformer model for machine translation from English to German. The paper authors use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l^2) attention. All experiments have d_model = 1024, d_ff = 4096, n_heads = 8, and a total batch size of 8 sequences. The paper authors also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017) In the two plots on the right in Figure 3, the paper authors compare a regular Transformer per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["These results show that the memory savings in the reversible Transformer do not come at the expense of accuracy. Reversible layers in machine translation. We also evaluate reversible layers in the context of an encoder-decoder Transformer model for machine translation from English to German. We start by making both the encoder and the decoder fully reversible in the Transformer-base architecture, and\n2https://github.com/google/trax/tree/master/trax/models/reformer 3 BLEU+case.lc+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3 4 BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3\nsee that the resulting model performs comparably to Vaswani et al. (2017) when trained for 100K steps. We also evaluate training for a greater number of steps and with a larger model. Reformer models are very memory-efficient, so for the latter two experiments we do not need to save memory by sharing embedding and output projection weight matrices throughout the model. Results are shown in Table 4. We do not apply LSH attention in this setting because examples are single sentences, and sentences tend to be relatively short. Our typical LSH attention configuration uses chunks of 128 tokens after hashing and sorting, whereas the examples in the WMT14 test set are all shorter than 128 tokens. LSH attention in Transformer. LSH attention is an approximation for full attention that, as evidenced in Figure 4, becomes more accurate as the number of hashes increases. At nrounds = 8, it already almost matches full attention. The computational cost of a model grows with the number of hashes, so this hyperparameter can be adjusted depending on the available compute budget. Additionally, as in Table 2, the number of hashes can be increased at evaluation time to produce more accurate results. On the right half of Figure 5, we plot the speed of different attention types vs. the sequence length, while holding the total number of tokens fixed.", "The multi-round case essentially involves performing LSH attention nrounds times in parallel; the details of the procedure are described in in Appendix A.\nCausal masking for shared-QK attention. In a Transformer decoder, masking (denoted by m(j,Pi) in Equation 3) is used to prevent positions from attending into the future. To implement masking in LSH attention, we associate every query/key vector with a position index, re-order the position indices using the same permutations used to sort the query/key vectors, and then use a comparison operation to compute the mask. While attention to the future is not allowed, typical implementations of the Transformer do allow a position to attend to itself. Such behavior is undesirable in a shared-QK formulation because the dot-product of a query vector with itself will almost always be greater than the dot product of a query vector with a vector at another position. We therefore modify the masking to forbid a token from attending to itself, except in situations where a token has no other valid attention targets (e.g. the first token in a sequence). 2.1 ANALYSIS ON A SYNTHETIC TASK. To verify the performance of LSH attention and study its behavior, we start with the following synthetic task: duplicate a sequence of symbols. In this task, each training and testing example has the form 0w0w where w \u2208 {1, . . . , N}\u2217 is a sequence of symbols ranging from 1 to N (we use N = 127 in our experiments). An example with the word w of length 3 is given below. Example: 0 19 113 72 0 19 113 72\nTo study LSH attention, we train a language model on examples of the above form where each w is of length 511 (so the whole input 0w0w is of length 1024). As this is a language modeling task, we always predict the next symbol given all the previous ones, but we mask the loss and accuracy to only consider positions in the second half of the input, i.e., those that can actually be predicted.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022"]}
{"pkey": "reformer_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors used the Adafactor optimizer (Shazeer & Stern, 2018) for training these models. resulting model performs comparably to Vaswani et al. (2017) when trained for 100K steps.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["In this section we present experimental results demonstrating the techniques described above. We analyze the techniques one-by-one to make clear which combinations have impact on performance. We start by showing that reversible layers and shared query-key spaces do not impact performance, then proceed to analyze hashing attention and finally the full Reformer model. We ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant of enwik8 that is chunked into subsequences of 216 = 64K tokens. We use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l2) attention. All experiments have dmodel = 1024, dff = 4096, nheads = 8, and a total batch size of 8 sequences. We used the Adafactor optimizer (Shazeer & Stern, 2018) for training these models. We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017). Training for all experiments\nwas parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made publicly available.2\nEffect of sharing QK. We first consider the effect of shared-QK attention on a regular Transformer model. Shared-QK attention sets kj =\nqj \u2016qj\u2016 and prevents tokens from attending to themselves\n(except when no other context is available). In the left part of Figure 3, we plot perplexity curves for both regular and shared-QK attention. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not sacrificing accuracy by switching to shared-QK attention. Effect of reversible layers. In the two plots on the right in Figure 3, we compare a regular Transformer per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same.", "We see that while regular attention becomes slower at longer sequence length, LSH attention speed remains flat. Large Reformer models. To verify that the Reformer can indeed fit large models on a single core and train fast on long sequences, we train up to 20-layer big Reformers on enwik8 and imagenet64. As can be seen in Figure 5, these models fit into memory and train. We were not able to train Transformer baselines in this case as they are too slow and memory-hungry, but we see clear improvement with the number of layers. A 12-layer model on enwik8 trained for 20K steps with a dropout rate of 0.1 achieves 1.19 bits/dim on the test set. We also trained a 12-layer Reformer model for longer with further tuning and improvements and we reached 1.05 bits/dim on the enwiki8 test set. 6 CONCLUSION. Reformer combines the modeling capacity of a Transformer with an architecture that can be executed efficiently on long sequences and with small memory use even for models with a large number of layers. We believe that this will help large, richly-parameterized Transformer models become more widespread and accessible. Also, the ability to handle long sequences opens the way for the use of the Reformer on many generative tasks. In addition to generating very long coherent text, the Reformer can bring the power of Transformer models to other domains like time-series forecasting, music, image and video generation. A MULTI-ROUND LSH ATTENTION. In this section we describe in more detail the multi-hash version of our LSH attention mechanism. We first repeat Equation (3) from the main text, which describes a general formulation of attention with sparsity: oi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) In the multi-round case, a query position i can attend to key positions Pi as defined in (6), which we also repeat here:\nPi = nrounds\u22c3\nr=1\nP(r)i where P (r) i = { j : h(r)(qi) = h (r)(qj) }\n(6)", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022"]}
{"pkey": "reformer_13", "question": "Describe the computational resources used to train the model.", "answer": "Training for all experiments was parallelized across 8 devices (8 GPUs or 8 TPU v3 cores).", "title": "Reformer: The Efficient Transformer (2020)", "context": ["The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022", "In this section we present experimental results demonstrating the techniques described above. We analyze the techniques one-by-one to make clear which combinations have impact on performance. We start by showing that reversible layers and shared query-key spaces do not impact performance, then proceed to analyze hashing attention and finally the full Reformer model. We ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant of enwik8 that is chunked into subsequences of 216 = 64K tokens. We use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l2) attention. All experiments have dmodel = 1024, dff = 4096, nheads = 8, and a total batch size of 8 sequences. We used the Adafactor optimizer (Shazeer & Stern, 2018) for training these models. We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017). Training for all experiments\nwas parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made publicly available.2\nEffect of sharing QK. We first consider the effect of shared-QK attention on a regular Transformer model. Shared-QK attention sets kj =\nqj \u2016qj\u2016 and prevents tokens from attending to themselves\n(except when no other context is available). In the left part of Figure 3, we plot perplexity curves for both regular and shared-QK attention. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not sacrificing accuracy by switching to shared-QK attention. Effect of reversible layers. In the two plots on the right in Figure 3, we compare a regular Transformer per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same.", "We see that while regular attention becomes slower at longer sequence length, LSH attention speed remains flat. Large Reformer models. To verify that the Reformer can indeed fit large models on a single core and train fast on long sequences, we train up to 20-layer big Reformers on enwik8 and imagenet64. As can be seen in Figure 5, these models fit into memory and train. We were not able to train Transformer baselines in this case as they are too slow and memory-hungry, but we see clear improvement with the number of layers. A 12-layer model on enwik8 trained for 20K steps with a dropout rate of 0.1 achieves 1.19 bits/dim on the test set. We also trained a 12-layer Reformer model for longer with further tuning and improvements and we reached 1.05 bits/dim on the enwiki8 test set. 6 CONCLUSION. Reformer combines the modeling capacity of a Transformer with an architecture that can be executed efficiently on long sequences and with small memory use even for models with a large number of layers. We believe that this will help large, richly-parameterized Transformer models become more widespread and accessible. Also, the ability to handle long sequences opens the way for the use of the Reformer on many generative tasks. In addition to generating very long coherent text, the Reformer can bring the power of Transformer models to other domains like time-series forecasting, music, image and video generation. A MULTI-ROUND LSH ATTENTION. In this section we describe in more detail the multi-hash version of our LSH attention mechanism. We first repeat Equation (3) from the main text, which describes a general formulation of attention with sparsity: oi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) In the multi-round case, a query position i can attend to key positions Pi as defined in (6), which we also repeat here:\nPi = nrounds\u22c3\nr=1\nP(r)i where P (r) i = { j : h(r)(qi) = h (r)(qj) }\n(6)"]}
{"pkey": "reformer_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "Code for training our models is made publicly available.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["That is much more efficient, but how can we find the nearest neighbors among the keys? Locality sensitive hashing. The problem of finding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector x to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not. In our case, we actually only require that nearby vectors get the same hash with high probability and that hash-buckets are of similar size with high probability. We achieve this by employing random projections as follows (see Figure 1). To get b hashes, we first fix a random matrix R of size [dk, b/2]. We then define h(x) = argmax([xR;\u2212xR]) where [u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors. LSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We first rewrite the equation for normal attention, (1), for a single query position i at a time: oi = \u2211 j\u2208Pi exp (qi \u00b7 kj \u2212 z(i,Pi)) vj where Pi = {j : i \u2265 j} (2)\nWe introduce the notation Pi to represent the set that the query at position i attends to, and z to denote the partition function (i.e. the normalizing term in the softmax). For clarity, we also omit scaling by \u221a dk. For batching purposes we typically perform attention over a larger set P\u0303i = {0, 1, . . . , l} \u2287 Pi while masking out elements not in Pi:\noi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) Now we turn to LSH attention, which we can think of in terms of restricting the set Pi of target items a query position i can attend to, by only allowing attention within a single hash bucket. Pi = {j : h(qi) = h(kj)} (4)\nFigure 2(a-b) shows a schematic comparison of full-attention with a hashed variant.", "Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model\u2019s self-attention mechanism (Sukhbaatar et al., 2019a;b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al., 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al., 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory. 5 EXPERIMENTS.", "The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022"]}
{"pkey": "reformer_15", "question": "What is the pretraining objective of the model? ", "answer": "The paper authors also evaluate reversible layers in the context of an encoder-decoder Transformer model for machine translation from English to German.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022", "But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately, only calculating softmax( qiK T\n\u221a dk )V once in memory, and then re-computing it on the backward pass when needed for gradients. This way of computing attention may be less efficient but it only uses memory proportional to length. We use this memory-efficient implementation of attention to run the full-attention baselines presented in the experimental section. Where do Q, K, V come from? The multi-head attention described above operates on keys, queries and values, but usually we are only given a single tensor of activations A of the shape [batch size, length, dmodel] \u2013 e.g., coming from embedding the tokens in a sentence into vectors. To build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K and V with different parameters. For models with LSH attention, we want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V. We call a model that behaves like this a shared-QK Transformer. It turns out that sharing QK does not affect the performance of Transformer, even if we additionally normalize the length of the keys K, as we show in the experimental Section 5. Hashing attention. For the LSH attention, we start with two tensors, Q=K and V of the shape [batch size, length, dmodel]. We keep the multi-head mechanism intact and focus on the attention computation from Equation 1. As already mentioned, the main issue is the term QKT , which has the shape [batch size, length, length]. But note that we are actually only interested in softmax(QKT ). Since softmax is dominated by the largest elements, for each query qi we only need to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi we could only consider a small subset of, say, the 32 or 64 closest keys.", "That is much more efficient, but how can we find the nearest neighbors among the keys? Locality sensitive hashing. The problem of finding nearest neighbors quickly in high-dimensional spaces can be solved by locality-sensitive hashing (LSH). A hashing scheme that assigns each vector x to a hash h(x) is called locality-sensitive if nearby vectors get the same hash with high probability and distant ones do not. In our case, we actually only require that nearby vectors get the same hash with high probability and that hash-buckets are of similar size with high probability. We achieve this by employing random projections as follows (see Figure 1). To get b hashes, we first fix a random matrix R of size [dk, b/2]. We then define h(x) = argmax([xR;\u2212xR]) where [u; v] denotes the concatenation of two vectors. This method is a known LSH scheme (Andoni et al., 2015) and is easy to implement and apply to batches of vectors. LSH attention. Knowing our LSH scheme and the general idea of hashing attention, we will now formalize the LSH attention we use in this paper. We first rewrite the equation for normal attention, (1), for a single query position i at a time: oi = \u2211 j\u2208Pi exp (qi \u00b7 kj \u2212 z(i,Pi)) vj where Pi = {j : i \u2265 j} (2)\nWe introduce the notation Pi to represent the set that the query at position i attends to, and z to denote the partition function (i.e. the normalizing term in the softmax). For clarity, we also omit scaling by \u221a dk. For batching purposes we typically perform attention over a larger set P\u0303i = {0, 1, . . . , l} \u2287 Pi while masking out elements not in Pi:\noi = \u2211 j\u2208P\u0303i exp (qi \u00b7 kj \u2212m(j,Pi)\u2212 z(i,Pi)) vj where m(j,Pi) = { \u221e if j /\u2208 Pi 0 otherwise (3) Now we turn to LSH attention, which we can think of in terms of restricting the set Pi of target items a query position i can attend to, by only allowing attention within a single hash bucket. Pi = {j : h(qi) = h(kj)} (4)\nFigure 2(a-b) shows a schematic comparison of full-attention with a hashed variant."]}
{"pkey": "reformer_16", "question": "What is the loss function that is used to train the model?", "answer": "Not Specified in paper", "title": "Reformer: The Efficient Transformer (2020)", "context": ["The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022", "The above task can be solved perfectly (to accuracy 100% and loss 0) by a 1-layer Transformer model. Note though, that it requires non-local attention lookups, so it cannot be solved by any model relying on sparse attention with a limited span. To make it easy and fast to train but similar to models used in NLP, we use a 1-layer Transformer with dmodel = dff = 256 and 4 heads. We train it for 150K steps in 4 different settings: with full attention, LSH attention with nrounds = 1, nrounds = 2 and nrounds = 4.\nFrom the results summarized in Table 2 we see that a model trained with full attention can be immediately used with LSH attention, but at some loss of accuracy. When trained from scratch with LSH attention, the model trained with 4 hashes achieves almost perfect accuracy as well. Interestingly, the accuracy becomes perfect when evaluated with 8 hashes. It goes down when evaluated with 2 or 1 hashes. Models trained with less hashes show worse results but even the model trained with just 1 hash performs almost perfectly when evaluated with 8 hashes. 3 REVERSIBLE TRANSFORMER. As the above section shows, the complexity of attention can be reduced from square in length to linear, provided an approximation is acceptable. But it is clear from Table 1 that each field starts with a b \u00b7 nh \u00b7 l term: the b \u00b7 nh \u00b7 l \u00b7 dk, or alternatively b \u00b7 l \u00b7 dmodel cost cannot be avoided. Indeed, the activations before each layer are already of the size b \u00b7 l \u00b7dmodel, so the memory use of the whole model with nl layers is at least b \u00b7 l \u00b7 dmodel \u00b7 nl. Even worse: inside the feed-forward layers of Transformer this goes up to b \u00b7 l \u00b7 dff \u00b7 nl. In a big Transformer it is usual to set dff = 4K and nl = 16 so with l = 64K this again would use an impractical 16GB of memory In this section, we show how to reduce this cost by first dealing with the nl part of the term using reversible layers and then showing how chunking can allow us to handle the dff problem.", "In both cases we show that Reformer matches the results obtained with full Transformer but runs much faster, especially on the text task, and with orders of magnitude better memory efficiency. 2 LOCALITY-SENSITIVE HASHING ATTENTION. Dot-product attention. The standard attention used in the Transformer is the scaled dot-product attention (Vaswani et al., 2017). The input consists of queries and keys of dimension dk, and values of dimension dv . The dot products of the query with all keys are computed, scaled by \u221a dk, and a softmax function is applied to obtain the weights on the values. In practice, the attention function on a set of queries is computed simultaneously, packed together into a matrix Q. Assuming the keys and values are also packed together into matrices K and V , the matrix of outputs is defined as:\nAttention(Q,K, V ) = softmax( QKT\u221a\ndk )V (1)\nMulti-head attention. In the Transformer, instead of performing a single attention function with dmodel-dimensional keys, values and queries, one linearly projects the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. Attention is applied to each of these projected versions of queries, keys and values in parallel, yielding dvdimensional output values. These are concatenated and once again projected, resulting in the final values. This mechanism is known as multi-head attention. Memory-efficient attention. To calculate the memory use of the attention mechanism, let us focus on the attention computation from Equation 1. Let us assume that Q, K and V all have the shape [batch size, length, dmodel]. The main issue is the term QKT , which has the shape [batch size, length, length]. In the experimental section we train a model on sequences of length 64K \u2013 in this case, even at batch-size of 1, this is a 64K\u00d764K matrix, which in 32-bit floats would take 16GB of memory. This is impractical and has hindered the use of the Transformer for long sequences."]}
{"pkey": "reformer_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The paper authors use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l^2) attention. All experiments have d_model = 1024, d_ff = 4096, n_heads = 8, and a total batch size of 8 sequences.  The paper authors also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017)", "title": "Reformer: The Efficient Transformer (2020)", "context": ["The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022", "These results show that the memory savings in the reversible Transformer do not come at the expense of accuracy. Reversible layers in machine translation. We also evaluate reversible layers in the context of an encoder-decoder Transformer model for machine translation from English to German. We start by making both the encoder and the decoder fully reversible in the Transformer-base architecture, and\n2https://github.com/google/trax/tree/master/trax/models/reformer 3 BLEU+case.lc+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3 4 BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3\nsee that the resulting model performs comparably to Vaswani et al. (2017) when trained for 100K steps. We also evaluate training for a greater number of steps and with a larger model. Reformer models are very memory-efficient, so for the latter two experiments we do not need to save memory by sharing embedding and output projection weight matrices throughout the model. Results are shown in Table 4. We do not apply LSH attention in this setting because examples are single sentences, and sentences tend to be relatively short. Our typical LSH attention configuration uses chunks of 128 tokens after hashing and sorting, whereas the examples in the WMT14 test set are all shorter than 128 tokens. LSH attention in Transformer. LSH attention is an approximation for full attention that, as evidenced in Figure 4, becomes more accurate as the number of hashes increases. At nrounds = 8, it already almost matches full attention. The computational cost of a model grows with the number of hashes, so this hyperparameter can be adjusted depending on the available compute budget. Additionally, as in Table 2, the number of hashes can be increased at evaluation time to produce more accurate results. On the right half of Figure 5, we plot the speed of different attention types vs. the sequence length, while holding the total number of tokens fixed.", "Since the depth dff of intermediate feed-forward layers is often much larger than the depth dmodel of attention activations, it accounts for a large fraction of memory use. \u2022 Attention on sequences of length L is O(L2) in both computational and memory complexity, so even for a single sequence of 64K tokens can exhaust accelerator memory. \u2217Equal Contribution 1https://hackingsemantics.xyz/2019/leaderboards/\nar X\niv :2\n00 1.\n04 45\n1v 2\n[ cs\n.L G\n] 1\n8 Fe\nb 20\n20\nWe introduce the Reformer model which solves these problems using the following techniques:\n\u2022 Reversible layers, first introduced in Gomez et al. (2017), enable storing only a single copy of activations in the whole model, so the N factor disappears. \u2022 Splitting activations inside feed-forward layers and processing them in chunks removes the dff factor and saves memory inside feed-forward layers. \u2022 Approximate attention computation based on locality-sensitive hashing replaces the O(L2) factor in attention layers with O(L logL) and so allows operating on long sequences. We study these techniques and show that they have negligible impact on the training process compared to the standard Transformer. Splitting activations in fact only affects the implementation; it is numerically identical to the layers used in the Transformer. Applying reversible residuals instead of the standard ones does change the model but has a negligible effect on training in all configurations we experimented with. Finally, locality-sensitive hashing in attention is a more major change that can influence the training dynamics, depending on the number of concurrent hashes used. We study this parameter and find a value which is both efficient to use and yields results very close to full attention. We experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image generation task (imagenet-64 generation) with sequences of length 12K."]}
{"pkey": "reformer_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "we present experimental results demonstrating the techniques described above. The paper authors analyze the techniques one-by-one to make clear which combinations have impact on performance. The paper authors first consider the effect of shared-QK attention on a regular Transformer model. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, the paper authors are not sacrificing accuracy by switching to shared-QK attention. we compare a regular Transformer per Vaswani et al. (2017) with the reversible one. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same. These results show that the memory savings in the reversible Transformer do not come at the expense of accuracy. The paper authors also evaluate reversible layers in the context of an encoder-decoder Transformer model for machine translation from English to German.\nthe resulting model performs comparably to Vaswani et al. (2017) when trained for 100K steps. we plot the speed of different attention types vs. the sequence length, while holding the total number of tokens fixed. The paper authors see that while regular attention becomes slower at longer sequence length, LSH attention speed remains flat. To verify that the Reformer can indeed fit large models on a single core and train fast on long sequences, the paper authors train up to 20-layer big Reformers on enwik8 and imagenet64. A 12-layer model on enwik8 trained for 20K steps with a dropout rate of 0.1 achieves 1.19 bits/dim on the test set.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["These results show that the memory savings in the reversible Transformer do not come at the expense of accuracy. Reversible layers in machine translation. We also evaluate reversible layers in the context of an encoder-decoder Transformer model for machine translation from English to German. We start by making both the encoder and the decoder fully reversible in the Transformer-base architecture, and\n2https://github.com/google/trax/tree/master/trax/models/reformer 3 BLEU+case.lc+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3 4 BLEU+case.mixed+lang.en-de+numrefs.1+smooth.exp+test.wmt14/full+tok.intl+version.1.4.3\nsee that the resulting model performs comparably to Vaswani et al. (2017) when trained for 100K steps. We also evaluate training for a greater number of steps and with a larger model. Reformer models are very memory-efficient, so for the latter two experiments we do not need to save memory by sharing embedding and output projection weight matrices throughout the model. Results are shown in Table 4. We do not apply LSH attention in this setting because examples are single sentences, and sentences tend to be relatively short. Our typical LSH attention configuration uses chunks of 128 tokens after hashing and sorting, whereas the examples in the WMT14 test set are all shorter than 128 tokens. LSH attention in Transformer. LSH attention is an approximation for full attention that, as evidenced in Figure 4, becomes more accurate as the number of hashes increases. At nrounds = 8, it already almost matches full attention. The computational cost of a model grows with the number of hashes, so this hyperparameter can be adjusted depending on the available compute budget. Additionally, as in Table 2, the number of hashes can be increased at evaluation time to produce more accurate results. On the right half of Figure 5, we plot the speed of different attention types vs. the sequence length, while holding the total number of tokens fixed.", "In this section we present experimental results demonstrating the techniques described above. We analyze the techniques one-by-one to make clear which combinations have impact on performance. We start by showing that reversible layers and shared query-key spaces do not impact performance, then proceed to analyze hashing attention and finally the full Reformer model. We ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant of enwik8 that is chunked into subsequences of 216 = 64K tokens. We use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l2) attention. All experiments have dmodel = 1024, dff = 4096, nheads = 8, and a total batch size of 8 sequences. We used the Adafactor optimizer (Shazeer & Stern, 2018) for training these models. We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017). Training for all experiments\nwas parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made publicly available.2\nEffect of sharing QK. We first consider the effect of shared-QK attention on a regular Transformer model. Shared-QK attention sets kj =\nqj \u2016qj\u2016 and prevents tokens from attending to themselves\n(except when no other context is available). In the left part of Figure 3, we plot perplexity curves for both regular and shared-QK attention. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not sacrificing accuracy by switching to shared-QK attention. Effect of reversible layers. In the two plots on the right in Figure 3, we compare a regular Transformer per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same.", "The above task can be solved perfectly (to accuracy 100% and loss 0) by a 1-layer Transformer model. Note though, that it requires non-local attention lookups, so it cannot be solved by any model relying on sparse attention with a limited span. To make it easy and fast to train but similar to models used in NLP, we use a 1-layer Transformer with dmodel = dff = 256 and 4 heads. We train it for 150K steps in 4 different settings: with full attention, LSH attention with nrounds = 1, nrounds = 2 and nrounds = 4.\nFrom the results summarized in Table 2 we see that a model trained with full attention can be immediately used with LSH attention, but at some loss of accuracy. When trained from scratch with LSH attention, the model trained with 4 hashes achieves almost perfect accuracy as well. Interestingly, the accuracy becomes perfect when evaluated with 8 hashes. It goes down when evaluated with 2 or 1 hashes. Models trained with less hashes show worse results but even the model trained with just 1 hash performs almost perfectly when evaluated with 8 hashes. 3 REVERSIBLE TRANSFORMER. As the above section shows, the complexity of attention can be reduced from square in length to linear, provided an approximation is acceptable. But it is clear from Table 1 that each field starts with a b \u00b7 nh \u00b7 l term: the b \u00b7 nh \u00b7 l \u00b7 dk, or alternatively b \u00b7 l \u00b7 dmodel cost cannot be avoided. Indeed, the activations before each layer are already of the size b \u00b7 l \u00b7dmodel, so the memory use of the whole model with nl layers is at least b \u00b7 l \u00b7 dmodel \u00b7 nl. Even worse: inside the feed-forward layers of Transformer this goes up to b \u00b7 l \u00b7 dff \u00b7 nl. In a big Transformer it is usual to set dff = 4K and nl = 16 so with l = 64K this again would use an impractical 16GB of memory In this section, we show how to reduce this cost by first dealing with the nl part of the term using reversible layers and then showing how chunking can allow us to handle the dff problem."]}
{"pkey": "reformer_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "The paper authors use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l^2) attention.\nEffect of sharing QK\nEffect of reversible layers\nReversible layers in machine translation\nLSH attention in Transformer\nLarge Reformer models", "title": "Reformer: The Efficient Transformer (2020)", "context": ["The number of parameters exceeds 0.5B per layer in the largest configuration reported in (Shazeer et al., 2018) while the number of layers goes up to 64 in (Al-Rfou et al., 2018). Transformer models are also used on increasingly long sequences. Up to 11 thousand tokens of text in a single example were processed in (Liu et al., 2018) and when processing other modalities, like music (Huang et al., 2018) and images (Parmar et al., 2018), even longer sequences are commonplace. These large-scale long-sequence models yield great results but strain resources to the point where some argue that this trend is breaking NLP research1. Many large Transformer models can only realistically be trained in large industrial research laboratories and such models trained with model parallelism cannot even be fine-tuned on a single GPU as their memory requirements demand a multi-accelerator hardware setup even for a single training step. Do large Transformer models fundamentally require such huge resources or are they simply inefficient? Consider the following calculation: the 0.5B parameters used in the largest reported Transformer layer account for 2GB of memory. Activations for 64K tokens with embedding size 1024 and batch size 8 account for 64K \u00d7 1K \u00d7 8 = 0.5B floats, requiring another 2GB of memory. If our memory use was only per-layer, then we should fairly easily fit a large Transformer even on sequences of length 64K on a single accelerator. Further, the whole corpus used to train BERT only requires 17GB to store. Why is it then that we cannot even fine-tune these models on single machines? The above estimate includes only per-layer memory and input activations cost and does not take into account the following major sources of memory use in the Transformer. \u2022 Memory in a model with N layers is N -times larger than in a single-layer model due to the fact that activations need to be stored for back-propagation. \u2022", "In this section we present experimental results demonstrating the techniques described above. We analyze the techniques one-by-one to make clear which combinations have impact on performance. We start by showing that reversible layers and shared query-key spaces do not impact performance, then proceed to analyze hashing attention and finally the full Reformer model. We ran our experiments on the imagenet64 and enwik8-64K tasks, where the latter is a variant of enwik8 that is chunked into subsequences of 216 = 64K tokens. We use 3-layer models for our ablations so as to make it tractable to compare with the regular Transformer, which has high memory usage and performs full O(l2) attention. All experiments have dmodel = 1024, dff = 4096, nheads = 8, and a total batch size of 8 sequences. We used the Adafactor optimizer (Shazeer & Stern, 2018) for training these models. We also evaluate on the WMT 2014 English-to-German translation task, following the hyperparameters of Vaswani et al. (2017). Training for all experiments\nwas parallelized across 8 devices (8 GPUs or 8 TPU v3 cores). Code for training our models is made publicly available.2\nEffect of sharing QK. We first consider the effect of shared-QK attention on a regular Transformer model. Shared-QK attention sets kj =\nqj \u2016qj\u2016 and prevents tokens from attending to themselves\n(except when no other context is available). In the left part of Figure 3, we plot perplexity curves for both regular and shared-QK attention. A shared query-key space does not perform worse than regular attention; in fact, for enwik8 it appears to train slightly faster. In other words, we are not sacrificing accuracy by switching to shared-QK attention. Effect of reversible layers. In the two plots on the right in Figure 3, we compare a regular Transformer per Vaswani et al. (2017) with the reversible one describe in Section 3. The two models have identical parameter counts, and the learning curves likewise appear to be nearly the same.", "Since the depth dff of intermediate feed-forward layers is often much larger than the depth dmodel of attention activations, it accounts for a large fraction of memory use. \u2022 Attention on sequences of length L is O(L2) in both computational and memory complexity, so even for a single sequence of 64K tokens can exhaust accelerator memory. \u2217Equal Contribution 1https://hackingsemantics.xyz/2019/leaderboards/\nar X\niv :2\n00 1.\n04 45\n1v 2\n[ cs\n.L G\n] 1\n8 Fe\nb 20\n20\nWe introduce the Reformer model which solves these problems using the following techniques:\n\u2022 Reversible layers, first introduced in Gomez et al. (2017), enable storing only a single copy of activations in the whole model, so the N factor disappears. \u2022 Splitting activations inside feed-forward layers and processing them in chunks removes the dff factor and saves memory inside feed-forward layers. \u2022 Approximate attention computation based on locality-sensitive hashing replaces the O(L2) factor in attention layers with O(L logL) and so allows operating on long sequences. We study these techniques and show that they have negligible impact on the training process compared to the standard Transformer. Splitting activations in fact only affects the implementation; it is numerically identical to the layers used in the Transformer. Applying reversible residuals instead of the standard ones does change the model but has a negligible effect on training in all configurations we experimented with. Finally, locality-sensitive hashing in attention is a more major change that can influence the training dynamics, depending on the number of concurrent hashes used. We study this parameter and find a value which is both efficient to use and yields results very close to full attention. We experiment on a synthetic task, a text task (enwik8) with sequences of length 64K and an image generation task (imagenet-64 generation) with sequences of length 12K."]}
{"pkey": "reformer_20", "question": "List the future work mentioned in the paper.", "answer": "In addition to generating very long coherent text, the Reformer can bring the power of Transformer models to other domains like time-series forecasting, music, image and video generation.", "title": "Reformer: The Efficient Transformer (2020)", "context": ["Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model\u2019s self-attention mechanism (Sukhbaatar et al., 2019a;b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al., 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al., 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015; Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory. 5 EXPERIMENTS.", "The multi-round case essentially involves performing LSH attention nrounds times in parallel; the details of the procedure are described in in Appendix A.\nCausal masking for shared-QK attention. In a Transformer decoder, masking (denoted by m(j,Pi) in Equation 3) is used to prevent positions from attending into the future. To implement masking in LSH attention, we associate every query/key vector with a position index, re-order the position indices using the same permutations used to sort the query/key vectors, and then use a comparison operation to compute the mask. While attention to the future is not allowed, typical implementations of the Transformer do allow a position to attend to itself. Such behavior is undesirable in a shared-QK formulation because the dot-product of a query vector with itself will almost always be greater than the dot product of a query vector with a vector at another position. We therefore modify the masking to forbid a token from attending to itself, except in situations where a token has no other valid attention targets (e.g. the first token in a sequence). 2.1 ANALYSIS ON A SYNTHETIC TASK. To verify the performance of LSH attention and study its behavior, we start with the following synthetic task: duplicate a sequence of symbols. In this task, each training and testing example has the form 0w0w where w \u2208 {1, . . . , N}\u2217 is a sequence of symbols ranging from 1 to N (we use N = 127 in our experiments). An example with the word w of length 3 is given below. Example: 0 19 113 72 0 19 113 72\nTo study LSH attention, we train a language model on examples of the above form where each w is of length 511 (so the whole input 0w0w is of length 1024). As this is a language modeling task, we always predict the next symbol given all the previous ones, but we mask the loss and accuracy to only consider positions in the second half of the input, i.e., those that can actually be predicted.", "But it is important to note that the QKT matrix does not need to be fully materialized in memory. The attention can indeed be computed for each query qi separately, only calculating softmax( qiK T\n\u221a dk )V once in memory, and then re-computing it on the backward pass when needed for gradients. This way of computing attention may be less efficient but it only uses memory proportional to length. We use this memory-efficient implementation of attention to run the full-attention baselines presented in the experimental section. Where do Q, K, V come from? The multi-head attention described above operates on keys, queries and values, but usually we are only given a single tensor of activations A of the shape [batch size, length, dmodel] \u2013 e.g., coming from embedding the tokens in a sentence into vectors. To build Q, K and V from A, the Transformer uses 3 different linear layers projecting A into Q, K and V with different parameters. For models with LSH attention, we want queries and keys (Q and K) to be identical. This is easily achieved by using the same linear layer to go from A to Q and K, and a separate one for V. We call a model that behaves like this a shared-QK Transformer. It turns out that sharing QK does not affect the performance of Transformer, even if we additionally normalize the length of the keys K, as we show in the experimental Section 5. Hashing attention. For the LSH attention, we start with two tensors, Q=K and V of the shape [batch size, length, dmodel]. We keep the multi-head mechanism intact and focus on the attention computation from Equation 1. As already mentioned, the main issue is the term QKT , which has the shape [batch size, length, length]. But note that we are actually only interested in softmax(QKT ). Since softmax is dominated by the largest elements, for each query qi we only need to focus on the keys in K that are closest to qi. For example, if K is of length 64K, for each qi we could only consider a small subset of, say, the 32 or 64 closest keys."]}
{"pkey": "transformerxl_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Paper propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme.  method not only enables capturing longer-term dependency but also resolves the context fragmentation problem", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.", "As a result, TransformerXL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch1. 1 Introduction. Language modeling is among the important problems that require modeling long-term dependency, with successful applications such as unsupervised pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). However, it has been a challenge to equip neural networks with the capability to model long-term dependency in sequential data. Recurrent neural networks (RNNs), in particular Long Short\u2217Equal contribution. Order determined by swapping the one in Yang et al. (2017). 1https://github.com/kimiyoung/ transformer-xl\nTerm Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), have been a standard solution to language modeling and obtained strong results on multiple benchmarks. Despite the wide adaption, RNNs are difficult to optimize due to gradient vanishing and explosion (Hochreiter et al., 2001), and the introduction of gating in LSTMs and the gradient clipping technique (Graves, 2013) might not be sufficient to fully address this issue. Empirically, previous work has found that LSTM language models use 200 context words on average (Khandelwal et al., 2018), indicating room for further improvement."]}
{"pkey": "transformerxl_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "In generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. Different from them,  mentioned work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependency.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.", "1b, this procedure ensures that each prediction utilizes the longest possible context exposed during training, and also relieves context fragmentation issue encountered in training. However, this evaluation procedure is extremely expensive. We will show that our proposed architecture is able to substantially improve the evaluation speed. 3.2 Segment-Level Recurrence with State Reuse. To address the limitations of using a fixed-length context, we propose to introduce a recurrence mechanism to the Transformer architecture. During training, the hidden state sequence computed for the previous segment is fixed and cached to be reused as an extended context when the model processes the next new segment, as shown in Fig. 2a. Although the gradient still remains within a segment, this additional input allows the network to exploit information in the history, leading to an ability of modeling longer-term dependency and avoiding context fragmentation. Formally, let the two consecutive segments of length L be s\u03c4 = [x\u03c4,1, \u00b7 \u00b7 \u00b7 , x\u03c4,L] and s\u03c4+1 = [x\u03c4+1,1, \u00b7 \u00b7 \u00b7 , x\u03c4+1,L] respectively. Denoting the n-th layer hidden state sequence produced for the \u03c4 -th segment s\u03c4 by hn\u03c4 \u2208 RL\u00d7d, where d is the hidden dimension. Then, the n-th layer hidden state for segment s\u03c4+1 is produced (schematically) as follows, h\u0303n\u22121\u03c4+1 = [ SG(hn\u22121\u03c4 ) \u25e6 hn\u22121\u03c4+1 ] ,\nqn\u03c4+1,k n \u03c4+1,v n \u03c4+1 = h n\u22121 \u03c4+1W > q , h\u0303 n\u22121 \u03c4+1W > k , h\u0303 n\u22121 \u03c4+1W > v , hn\u03c4+1 = Transformer-Layer (q n \u03c4+1,k n \u03c4+1,v n \u03c4+1) . where the function SG(\u00b7) stands for stop-gradient, the notation [hu \u25e6 hv] indicates the concatenation of two hidden sequences along the length dimension, and W\u00b7 denotes model parameters. Compared to the standard Transformer, the critical difference lies in that the key kn\u03c4+1 and value v n \u03c4+1 are conditioned on the extended context h\u0303n\u22121\u03c4+1 and hence hn\u22121\u03c4 cached from the previous segment. We emphasize this particular design by the green paths in Fig. 2a."]}
{"pkey": "transformerxl_3", "question": "What are the main contributions of the paper?", "answer": "Paper proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem.Our main technical contributions include introducing the notion of recurrence in a purely selfattentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "More importantly, we show the necessity of using relative positional encodings rather than absolute ones, in order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training. Transformer-XL obtained strong results on five datasets, varying from word-level to characterlevel language modeling. Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens. Our main technical contributions include introducing the notion of recurrence in a purely selfattentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling. 2 Related Work. In the last few years, the field of language modeling has witnessed many significant advances, including but not limited to devising novel architectures to better encode the context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), improving regularization and optimization algorithms (Gal and Ghahramani, 2016) , speeding up the Softmax computation (Grave et al., 2016a) , and enriching the output distribution family (Yang et al., 2017). To capture the long-range context in language modeling, a line of work directly feeds a representation of the wider context into the network\nas an additional input.", "For the head 158 in the 16th layer (i.e. the last layer), each target location (corresponding to each row) has its own distinct sparse focus, differing from head 78 where target locations largely share the same attentive location in memory. Meanwhile, the pattern is also different from the case of head 8, where a few locations are clearly attended more than others. Finally, as we have discussed in section 3.3, the attention score can be decomposed into four intuitive terms. Here, we want to further investigate how these four terms contribute to the overall attention trend in Fig. 5. Since the term (c) represents the global content bias, i.e., the prior importance of each word regardless of the context, we will leave it out and focus on the terms (a), (b) and (d). So, for each term, we take the Softmax w.r.t. the memory span and average the resulted distribution of all tokens in the validation set. The results are visualized in Fig. 7:\n\u2022 Since term (a) is fully content-based addressing, when averaging over all target words, the result is essentially uniform over the entire context, except for a few very close words, which are likely to be semantically similar to the target word. \u2022 The overall trend of term (b) highly resembles that of the entire attention distribution in Fig. 5. It suggests that the global trend of focusing on the nearby context is largely contributed by this contentdependent positional bias. \u2022 The overall trend of term (d) is also focusing more on nearby words. However, compared to the trend of term (b), it is clearly flatter and biases towards a longer context. E Generated Text. In this section, we present some generated text from our best model trained the Wikitext-103 dataset. We seed the our Transformer-XL with a context of at most 512 consecutive tokens randomly sampled from the test set of Wikitext-103. Then, we run Transformer-XL to generate a pre-defined number of tokens (500 or 1,000 in our case)."]}
{"pkey": "transformerxl_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "Authors envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.", "In order to achieve this goal, we deliberately choose a dataset that does not require longterm dependency, so that any improvement from establishing the recurrence can be attributed to solving the context fragmentation. Specifically, we perform this controlled experiment on the One Billion Word dataset, which can only benefit from removing the context fragmentation. We train a 20-layer Transformer-XL with \u223c0.3B parameters for 400K steps. As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings is also superior to Shaw et al. (2018) on short sequences. 4.3 Relative Effective Context Length. Khandelwal et al. (2018) proposed a method to evaluate the Effective Context Length (ECL) of a sequence model. ECL is the longest length to which increasing the context span would lead to a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower perplexity using only a shorter context, and thus it is not suitable for fair comparison among multiple models. We instead propose a new metric\ncalled Relative Effective Context Length (RECL). RECL is defined on a model group instead of a single model, and the gain of a long context is measure by the relative improvement over the best short context model. As such, the model group shares the same baseline to enable fair comparison. RECL also has a parameter r, which means constraining the comparison on top-r hard examples. See Appedix C for more details about RECL. As shown in Table 8, Transformer-XL manages to model dependency of 900 words long on av-\nAttn Len How much Al-Rfou et al. (2018) is slower\nerage with r = 0.1. The RECL of TransformerXL is 80% and 450% longer than recurrent networks and Transformer respectively.", "For the head 158 in the 16th layer (i.e. the last layer), each target location (corresponding to each row) has its own distinct sparse focus, differing from head 78 where target locations largely share the same attentive location in memory. Meanwhile, the pattern is also different from the case of head 8, where a few locations are clearly attended more than others. Finally, as we have discussed in section 3.3, the attention score can be decomposed into four intuitive terms. Here, we want to further investigate how these four terms contribute to the overall attention trend in Fig. 5. Since the term (c) represents the global content bias, i.e., the prior importance of each word regardless of the context, we will leave it out and focus on the terms (a), (b) and (d). So, for each term, we take the Softmax w.r.t. the memory span and average the resulted distribution of all tokens in the validation set. The results are visualized in Fig. 7:\n\u2022 Since term (a) is fully content-based addressing, when averaging over all target words, the result is essentially uniform over the entire context, except for a few very close words, which are likely to be semantically similar to the target word. \u2022 The overall trend of term (b) highly resembles that of the entire attention distribution in Fig. 5. It suggests that the global trend of focusing on the nearby context is largely contributed by this contentdependent positional bias. \u2022 The overall trend of term (d) is also focusing more on nearby words. However, compared to the trend of term (b), it is clearly flatter and biases towards a longer context. E Generated Text. In this section, we present some generated text from our best model trained the Wikitext-103 dataset. We seed the our Transformer-XL with a context of at most 512 consecutive tokens randomly sampled from the test set of Wikitext-103. Then, we run Transformer-XL to generate a pre-defined number of tokens (500 or 1,000 in our case)."]}
{"pkey": "transformerxl_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors apply Transformer-XL to a variety of datasets on both word-level and character-level language modeling to have a comparison with state-of-theart systems, including WikiText-103 (Merity et al., 2016), enwik8 (LLC, 2009), text8 (LLC, 2009), One Billion Word (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012).", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018). Different from them, our work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependency. 3 Model. Given a corpus of tokens x = (x1, . . . , xT ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = \u220f t P (xt | x<t). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token. 3.1 Vanilla Transformer Language Models. In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation.", "In order to achieve this goal, we deliberately choose a dataset that does not require longterm dependency, so that any improvement from establishing the recurrence can be attributed to solving the context fragmentation. Specifically, we perform this controlled experiment on the One Billion Word dataset, which can only benefit from removing the context fragmentation. We train a 20-layer Transformer-XL with \u223c0.3B parameters for 400K steps. As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings is also superior to Shaw et al. (2018) on short sequences. 4.3 Relative Effective Context Length. Khandelwal et al. (2018) proposed a method to evaluate the Effective Context Length (ECL) of a sequence model. ECL is the longest length to which increasing the context span would lead to a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower perplexity using only a shorter context, and thus it is not suitable for fair comparison among multiple models. We instead propose a new metric\ncalled Relative Effective Context Length (RECL). RECL is defined on a model group instead of a single model, and the gain of a long context is measure by the relative improvement over the best short context model. As such, the model group shares the same baseline to enable fair comparison. RECL also has a parameter r, which means constraining the comparison on top-r hard examples. See Appedix C for more details about RECL. As shown in Table 8, Transformer-XL manages to model dependency of 900 words long on av-\nAttn Len How much Al-Rfou et al. (2018) is slower\nerage with r = 0.1. The RECL of TransformerXL is 80% and 450% longer than recurrent networks and Transformer respectively."]}
{"pkey": "transformerxl_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, the paper authors believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.", "For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "More importantly, we show the necessity of using relative positional encodings rather than absolute ones, in order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training. Transformer-XL obtained strong results on five datasets, varying from word-level to characterlevel language modeling. Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens. Our main technical contributions include introducing the notion of recurrence in a purely selfattentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling. 2 Related Work. In the last few years, the field of language modeling has witnessed many significant advances, including but not limited to devising novel architectures to better encode the context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), improving regularization and optimization algorithms (Gal and Ghahramani, 2016) , speeding up the Softmax computation (Grave et al., 2016a) , and enriching the output distribution family (Yang et al., 2017). To capture the long-range context in language modeling, a line of work directly feeds a representation of the wider context into the network\nas an additional input."]}
{"pkey": "transformerxl_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, the paper authors believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network. However, this is usually infeasible with the limited resource in practice. One feasible but crude approximation is to split the entire corpus into shorter segments of man-\nageable sizes, and only train the model within each segment, ignoring all contextual information from previous segments. This is the idea adopted by Al-Rfou et al. (2018). We call it the vanilla model and visualize it in Fig. 1a. Under this training paradigm, information never flows across segments in either the forward or backward pass. There are two critical limitations of using a fixedlength context. First, the largest possible dependency length is upper bounded by the segment length, which is a few hundred on character-level language modeling (Al-Rfou et al., 2018). Therefore, although the self-attention mechanism is less affected by the vanishing gradient problem compared to RNNs, the vanilla model is not able to fully exploit this optimization advantage. Second, though it is possible to use padding to respect the sentence or other semantic boundaries, in practice it has been standard practice to simply chunk long text into fixed-length segments due to improved efficiency (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). However, simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem as discussed in Section 1. During evaluation, at each step, the vanilla model also consumes a segment of the same length as in training, but only makes one prediction at the last position. Then, at the next step, the segment is shifted to the right by only one position, and the new segment has to be processed all from scratch. As shown in Fig.", "Both the recurrence mechanism and our positional encodings contribute to a longer RECL. This further substantiates our argument that Transformer-XL is able to model longer-term dependency. 4.4 Generated Text.\nTrained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples. 4.5 Evaluation Speed. Finally, we compare the evaluation speed of our model with the vanilla Transformer model (AlRfou et al., 2018). As shown in Table 9, due to the state reuse scheme, Transformer-XL achieves an up to 1,874 times speedup during evaluation. 5 Conclusions. Transformer-XL obtains strong perplexity results, models longer-term dependency than RNNs and Transformer, achieves substantial speedup during evaluation, and is able to generate coherent text articles. We envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling.\nAcknowledgments. ZD and YY were supported in part by National Science Foundation (NSF) under the grant IIS1546329 and by the DOE-Office of Science under the grant ASCR #KJ040201. ZY and RS were supported in part by the Office of Naval Research grant N000141812861, the NSF grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. A Ablation Study with Memory Constraints. Table 10 compares Transformer-XL with baseline under the same memory budget. Transformer-XL still outperforms the baseline even with a shorter backprop length. B Efficient Computation of the Attention with Relative Positional Embedding. As we discussed in section 3.3, the naive way of computing the Wk,RRi\u2212j for all pairs (i, j) is subject to a quadratic cost. Here, we present a simple method with only a linear cost."]}
{"pkey": "transformerxl_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors apply Transformer-XL to a variety of datasets on both word-level and character-level language modeling to have a comparison with state-of-theart systems, including WikiText-103 (Merity et al., 2016), enwik8 (LLC, 2009), text8 (LLC, 2009), One Billion Word (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012). WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling.The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["An\u03c4,i,j =q n \u03c4,i >kn\u03c4,j + q n \u03c4,i >Wnk,RRi\u2212j\n+ u>k\u03c4,j + v >Wnk,RRi\u2212j\nan\u03c4 =Masked-Softmax(A n \u03c4 )v n \u03c4\non\u03c4 =LayerNorm(Linear(a n \u03c4 ) + h n\u22121 \u03c4 ) hn\u03c4 = Positionwise-Feed-Forward(o n \u03c4 )\nwith h0\u03c4 := Es\u03c4 defined as the word embedding sequence. In addition, it is worth mentioning that a naive way to compute A requires computing Wnk,RRi\u2212j for all pairs (i, j), whose cost is quadratic w.r.t. the sequence length. However, noticing that the value of i \u2212 j only ranges from zero to the sequence length, we show a simple computation procedure in Appendix B, which reduces the cost to be linear w.r.t. the sequence length. 4 Experiments. 4.1 Main Results. We apply Transformer-XL to a variety of datasets on both word-level and character-level language\nmodeling to have a comparison with state-of-theart systems, including WikiText-103 (Merity et al., 2016), enwik8 (LLC, 2009), text8 (LLC, 2009), One Billion Word (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012). WikiText-103 is the largest available word-level language modeling benchmark with long-term dependency. It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling. We set the attention length to 384 during training and 1600 during evaluation. We adopted adaptive softmax and input representations (Baevski and Auli, 2018; Grave et al., 2016a). As shown in Table 1, Transformer-XL reduces the previous state-of-theart (SoTA) perplexity from 20.5 to 18.3, which demonstrates the superiority of the TransformerXL architecture. The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text. We compare our architecture with the previous results in Table 2. Under the model size constraint, the 12-layer Transformer-XL achieves a new SoTA result, outperforming the 12-layer vanilla Transformer from Al-Rfou et al. (2018) by 0.05, while both Trans-\nformer variants have a large margin over conventional RNN-based models.", "In order to achieve this goal, we deliberately choose a dataset that does not require longterm dependency, so that any improvement from establishing the recurrence can be attributed to solving the context fragmentation. Specifically, we perform this controlled experiment on the One Billion Word dataset, which can only benefit from removing the context fragmentation. We train a 20-layer Transformer-XL with \u223c0.3B parameters for 400K steps. As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings is also superior to Shaw et al. (2018) on short sequences. 4.3 Relative Effective Context Length. Khandelwal et al. (2018) proposed a method to evaluate the Effective Context Length (ECL) of a sequence model. ECL is the longest length to which increasing the context span would lead to a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower perplexity using only a shorter context, and thus it is not suitable for fair comparison among multiple models. We instead propose a new metric\ncalled Relative Effective Context Length (RECL). RECL is defined on a model group instead of a single model, and the gain of a long context is measure by the relative improvement over the best short context model. As such, the model group shares the same baseline to enable fair comparison. RECL also has a parameter r, which means constraining the comparison on top-r hard examples. See Appedix C for more details about RECL. As shown in Table 8, Transformer-XL manages to model dependency of 900 words long on av-\nAttn Len How much Al-Rfou et al. (2018) is slower\nerage with r = 0.1. The RECL of TransformerXL is 80% and 450% longer than recurrent networks and Transformer respectively.", "Notably, our 12-layer architecture achieves the same result as the 64- layer network from Al-Rfou et al. (2018) , using only 17% of the parameter budget. In order to see whether better performances can be obtained by increasing the model size, we train 18-layer and 24-layer Transformer-XLs with increased model sizes. With the attention length 784 during training and 3,800 during evaluation, we obtained a new SoTA result and our method is the first to break through 1.0 on widely-studied characterlevel benchmarks. Different from Al-Rfou et al. (2018), Transformer-XL does not need any auxiliary losses, and thus all benefits are credited to a better architecture. Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any character other than the 26 letters a through z, and space. Due to the similarity, we simply adapt the best model and the same hyper-parameters on enwik8 to text8 without further tuning. The comparison with previous methods is summarized in Table 3. Again, Transformer-XL achieves the new SoTA result with a clear margin. One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences. We also report the results on word-level Penn Treebank in Table 5. Similar to AWD-LSTM (Merity et al., 2017), we apply variational dropout and weight average to Transformer-XL."]}
{"pkey": "transformerxl_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors apply Transformer-XL to a variety of datasets on both word-level and character-level language.It contains 103M training tokens from 28K articles, with an average length of 3.6K tokens per article, which allows testing the ability of long-term dependency modeling.The dataset enwik8 contains 100M bytes of unprocessed Wikipedia text.Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any character other than the 26 letters a through z, and space. Due to the similarity, the paper authors simply adapt the best model and the same hyper-parameters on enwik8 to text8 without further tuning.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018). Different from them, our work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependency. 3 Model. Given a corpus of tokens x = (x1, . . . , xT ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = \u220f t P (xt | x<t). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token. 3.1 Vanilla Transformer Language Models. In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation.", "For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "Notably, our 12-layer architecture achieves the same result as the 64- layer network from Al-Rfou et al. (2018) , using only 17% of the parameter budget. In order to see whether better performances can be obtained by increasing the model size, we train 18-layer and 24-layer Transformer-XLs with increased model sizes. With the attention length 784 during training and 3,800 during evaluation, we obtained a new SoTA result and our method is the first to break through 1.0 on widely-studied characterlevel benchmarks. Different from Al-Rfou et al. (2018), Transformer-XL does not need any auxiliary losses, and thus all benefits are credited to a better architecture. Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any character other than the 26 letters a through z, and space. Due to the similarity, we simply adapt the best model and the same hyper-parameters on enwik8 to text8 without further tuning. The comparison with previous methods is summarized in Table 3. Again, Transformer-XL achieves the new SoTA result with a clear margin. One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences. We also report the results on word-level Penn Treebank in Table 5. Similar to AWD-LSTM (Merity et al., 2017), we apply variational dropout and weight average to Transformer-XL."]}
{"pkey": "transformerxl_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any character other than the 26 letters a through z, and space", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "In order to achieve this goal, we deliberately choose a dataset that does not require longterm dependency, so that any improvement from establishing the recurrence can be attributed to solving the context fragmentation. Specifically, we perform this controlled experiment on the One Billion Word dataset, which can only benefit from removing the context fragmentation. We train a 20-layer Transformer-XL with \u223c0.3B parameters for 400K steps. As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings is also superior to Shaw et al. (2018) on short sequences. 4.3 Relative Effective Context Length. Khandelwal et al. (2018) proposed a method to evaluate the Effective Context Length (ECL) of a sequence model. ECL is the longest length to which increasing the context span would lead to a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower perplexity using only a shorter context, and thus it is not suitable for fair comparison among multiple models. We instead propose a new metric\ncalled Relative Effective Context Length (RECL). RECL is defined on a model group instead of a single model, and the gain of a long context is measure by the relative improvement over the best short context model. As such, the model group shares the same baseline to enable fair comparison. RECL also has a parameter r, which means constraining the comparison on top-r hard examples. See Appedix C for more details about RECL. As shown in Table 8, Transformer-XL manages to model dependency of 900 words long on av-\nAttn Len How much Al-Rfou et al. (2018) is slower\nerage with r = 0.1. The RECL of TransformerXL is 80% and 450% longer than recurrent networks and Transformer respectively.", "Notably, our 12-layer architecture achieves the same result as the 64- layer network from Al-Rfou et al. (2018) , using only 17% of the parameter budget. In order to see whether better performances can be obtained by increasing the model size, we train 18-layer and 24-layer Transformer-XLs with increased model sizes. With the attention length 784 during training and 3,800 during evaluation, we obtained a new SoTA result and our method is the first to break through 1.0 on widely-studied characterlevel benchmarks. Different from Al-Rfou et al. (2018), Transformer-XL does not need any auxiliary losses, and thus all benefits are credited to a better architecture. Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any character other than the 26 letters a through z, and space. Due to the similarity, we simply adapt the best model and the same hyper-parameters on enwik8 to text8 without further tuning. The comparison with previous methods is summarized in Table 3. Again, Transformer-XL achieves the new SoTA result with a clear margin. One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences. We also report the results on word-level Penn Treebank in Table 5. Similar to AWD-LSTM (Merity et al., 2017), we apply variational dropout and weight average to Transformer-XL."]}
{"pkey": "transformerxl_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "Transformer-XL is an encode-decoder type of model. Multiple variants, with 12, 18, 20, 24 layer transformer-XL models are trained. The architecture used in the paper is Transformer architecture with a novel positional encoding scheme and a segment-level recurrence mechanism. Multiple models of varying sizes were trained and evaluated as paper is all about transformer improvement paper does not talk about hidden size, embedding size, layers. Smaller model has parameters: 128M parameter, medium model has parameters: 151 parameter, and large model has parameter: 257 parameters.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For the head 158 in the 16th layer (i.e. the last layer), each target location (corresponding to each row) has its own distinct sparse focus, differing from head 78 where target locations largely share the same attentive location in memory. Meanwhile, the pattern is also different from the case of head 8, where a few locations are clearly attended more than others. Finally, as we have discussed in section 3.3, the attention score can be decomposed into four intuitive terms. Here, we want to further investigate how these four terms contribute to the overall attention trend in Fig. 5. Since the term (c) represents the global content bias, i.e., the prior importance of each word regardless of the context, we will leave it out and focus on the terms (a), (b) and (d). So, for each term, we take the Softmax w.r.t. the memory span and average the resulted distribution of all tokens in the validation set. The results are visualized in Fig. 7:\n\u2022 Since term (a) is fully content-based addressing, when averaging over all target words, the result is essentially uniform over the entire context, except for a few very close words, which are likely to be semantically similar to the target word. \u2022 The overall trend of term (b) highly resembles that of the entire attention distribution in Fig. 5. It suggests that the global trend of focusing on the nearby context is largely contributed by this contentdependent positional bias. \u2022 The overall trend of term (d) is also focusing more on nearby words. However, compared to the trend of term (b), it is clearly flatter and biases towards a longer context. E Generated Text. In this section, we present some generated text from our best model trained the Wikitext-103 dataset. We seed the our Transformer-XL with a context of at most 512 consecutive tokens randomly sampled from the test set of Wikitext-103. Then, we run Transformer-XL to generate a pre-defined number of tokens (500 or 1,000 in our case).", "With this recurrence mechanism applied to every two consecutive segments of a corpus, it essentially creates a segment-level recurrence in the hidden states. As a result, the effective context being utilized can go way beyond just two segments. However, notice that the recurrent dependency between hn\u03c4+1 and h n\u22121 \u03c4 shifts one layer downwards\nper-segment, which differs from the same-layer recurrence in conventional RNN-LMs. Consequently, the largest possible dependency length grows linearly w.r.t. the number of layers as well as the segment length, i.e., O(N \u00d7 L), as visualized by the shaded area in Fig. 2b. This is analogous to truncated BPTT (Mikolov et al., 2010), a technique developed for training RNNLMs. However, different from truncated BPTT, our method caches a sequence of hidden states instead of the last one, and should be applied together with the relative positional encoding technique described in Section 3.3. Besides achieving extra long context and resolving fragmentation, another benefit that comes with the recurrence scheme is significantly faster evaluation. Specifically, during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model. In our experiments on enwiki8, Transformer-XL is up to 1,800+ times faster than the vanilla model during evaluation (see Section 4). Finally, notice that the recurrence scheme does not need to be restricted to only the previous segment. In theory, we can cache as many previous segments as the GPU memory allows, and reuse all of them as the extra context when processing the current segment. Thus, we can cache a predefined length-M old hidden states spanning (possibly) multiple segments, and refer to them as the memory mn\u03c4 \u2208 RM\u00d7d, due to a clear connection to the memory augmented neural networks (Graves et al., 2014; Weston et al., 2014).", "Notably, our 12-layer architecture achieves the same result as the 64- layer network from Al-Rfou et al. (2018) , using only 17% of the parameter budget. In order to see whether better performances can be obtained by increasing the model size, we train 18-layer and 24-layer Transformer-XLs with increased model sizes. With the attention length 784 during training and 3,800 during evaluation, we obtained a new SoTA result and our method is the first to break through 1.0 on widely-studied characterlevel benchmarks. Different from Al-Rfou et al. (2018), Transformer-XL does not need any auxiliary losses, and thus all benefits are credited to a better architecture. Similar to but different from enwik8, text8 contains 100M processed Wikipedia characters created by lowering case the text and removing any character other than the 26 letters a through z, and space. Due to the similarity, we simply adapt the best model and the same hyper-parameters on enwik8 to text8 without further tuning. The comparison with previous methods is summarized in Table 3. Again, Transformer-XL achieves the new SoTA result with a clear margin. One Billion Word does not preserve any longterm dependency because sentences have been shuffled. Consequently, this dataset mainly tests the ability of modeling only short-term dependency. The comparison between Transformer-XL and the other methods is shown in Table 4. Although Transformer-XL is mainly designed to better capture longer-term dependency, it dramatically improves the single-model SoTA from 23.7 to 21.8. Specifically, Transformer-XL significantly outperforms a contemporary method using vanilla Transformers (Baevski and Auli, 2018), suggesting the advantage of Transformer-XL is generalizable to modeling short sequences. We also report the results on word-level Penn Treebank in Table 5. Similar to AWD-LSTM (Merity et al., 2017), we apply variational dropout and weight average to Transformer-XL."]}
{"pkey": "transformerxl_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "Transformer-XL, 20-layer Transformer-XL with around 0.3B parameters for 400K steps.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network. However, this is usually infeasible with the limited resource in practice. One feasible but crude approximation is to split the entire corpus into shorter segments of man-\nageable sizes, and only train the model within each segment, ignoring all contextual information from previous segments. This is the idea adopted by Al-Rfou et al. (2018). We call it the vanilla model and visualize it in Fig. 1a. Under this training paradigm, information never flows across segments in either the forward or backward pass. There are two critical limitations of using a fixedlength context. First, the largest possible dependency length is upper bounded by the segment length, which is a few hundred on character-level language modeling (Al-Rfou et al., 2018). Therefore, although the self-attention mechanism is less affected by the vanishing gradient problem compared to RNNs, the vanilla model is not able to fully exploit this optimization advantage. Second, though it is possible to use padding to respect the sentence or other semantic boundaries, in practice it has been standard practice to simply chunk long text into fixed-length segments due to improved efficiency (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). However, simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem as discussed in Section 1. During evaluation, at each step, the vanilla model also consumes a segment of the same length as in training, but only makes one prediction at the last position. Then, at the next step, the segment is shifted to the right by only one position, and the new segment has to be processed all from scratch. As shown in Fig.", "On the other hand, the direct connections between long-distance word pairs baked in attention mechanisms might ease optimization and enable the learning of long-term dependency (Bahdanau et al., 2014; Vaswani et al., 2017). Recently, Al-Rfou et al. (2018) designed a set of auxiliary losses to train deep Transformer networks for character-level language modeling, which outperform LSTMs by a large margin. Despite the success, the LM training in Al-Rfou et al. (2018) is performed on separated fixed-length segments of a few hundred characters, without any information flow across segments. As a consequence of the fixed context length, the model cannot capture any longer-term dependency beyond the predefined context length. In addition, the fixed-length segments are created by selecting a consecutive chunk of symbols without respecting the sentence or any other semantic boundary. Hence, the model lacks necessary contextual information needed to well predict the first few symbols, leading to inefficient optimization and inferior performance. We refer to this problem as context fragmentation. To address the aforementioned limitations of fixed-length contexts, we propose a new architecture called Transformer-XL (meaning extra long). We introduce the notion of recurrence into our\nar X\niv :1\n90 1.\n02 86\n0v 3\n[ cs\n.L G\n] 2\nJ un\n2 01\n9\ndeep self-attention network. In particular, instead of computing the hidden states from scratch for each new segment, we reuse the hidden states obtained in previous segments. The reused hidden states serve as memory for the current segment, which builds up a recurrent connection between the segments. As a result, modeling very longterm dependency becomes possible because information can be propagated through the recurrent connections. Meanwhile, passing information from the previous segment can also resolve the problem of context fragmentation.", "Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018). Different from them, our work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependency. 3 Model. Given a corpus of tokens x = (x1, . . . , xT ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = \u220f t P (xt | x<t). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token. 3.1 Vanilla Transformer Language Models. In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation."]}
{"pkey": "transformerxl_13", "question": "Describe the computational resources used to train the model.", "answer": "The paper does not talk about computational resources used to tain but it is mentioned that GPU is used.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["Given infinite memory and computation, a simple solution would be to process the entire context sequence using an unconditional Transformer decoder, similar to a feed-forward neural network. However, this is usually infeasible with the limited resource in practice. One feasible but crude approximation is to split the entire corpus into shorter segments of man-\nageable sizes, and only train the model within each segment, ignoring all contextual information from previous segments. This is the idea adopted by Al-Rfou et al. (2018). We call it the vanilla model and visualize it in Fig. 1a. Under this training paradigm, information never flows across segments in either the forward or backward pass. There are two critical limitations of using a fixedlength context. First, the largest possible dependency length is upper bounded by the segment length, which is a few hundred on character-level language modeling (Al-Rfou et al., 2018). Therefore, although the self-attention mechanism is less affected by the vanishing gradient problem compared to RNNs, the vanilla model is not able to fully exploit this optimization advantage. Second, though it is possible to use padding to respect the sentence or other semantic boundaries, in practice it has been standard practice to simply chunk long text into fixed-length segments due to improved efficiency (Peters et al., 2018; Devlin et al., 2018; Al-Rfou et al., 2018). However, simply chunking a sequence into fixed-length segments will lead to the context fragmentation problem as discussed in Section 1. During evaluation, at each step, the vanilla model also consumes a segment of the same length as in training, but only makes one prediction at the last position. Then, at the next step, the segment is shifted to the right by only one position, and the new segment has to be processed all from scratch. As shown in Fig.", "With this recurrence mechanism applied to every two consecutive segments of a corpus, it essentially creates a segment-level recurrence in the hidden states. As a result, the effective context being utilized can go way beyond just two segments. However, notice that the recurrent dependency between hn\u03c4+1 and h n\u22121 \u03c4 shifts one layer downwards\nper-segment, which differs from the same-layer recurrence in conventional RNN-LMs. Consequently, the largest possible dependency length grows linearly w.r.t. the number of layers as well as the segment length, i.e., O(N \u00d7 L), as visualized by the shaded area in Fig. 2b. This is analogous to truncated BPTT (Mikolov et al., 2010), a technique developed for training RNNLMs. However, different from truncated BPTT, our method caches a sequence of hidden states instead of the last one, and should be applied together with the relative positional encoding technique described in Section 3.3. Besides achieving extra long context and resolving fragmentation, another benefit that comes with the recurrence scheme is significantly faster evaluation. Specifically, during evaluation, the representations from the previous segments can be reused instead of being computed from scratch as in the case of the vanilla model. In our experiments on enwiki8, Transformer-XL is up to 1,800+ times faster than the vanilla model during evaluation (see Section 4). Finally, notice that the recurrence scheme does not need to be restricted to only the previous segment. In theory, we can cache as many previous segments as the GPU memory allows, and reuse all of them as the extra context when processing the current segment. Thus, we can cache a predefined length-M old hidden states spanning (possibly) multiple segments, and refer to them as the memory mn\u03c4 \u2208 RM\u00d7d, due to a clear connection to the memory augmented neural networks (Graves et al., 2014; Weston et al., 2014).", "More importantly, we show the necessity of using relative positional encodings rather than absolute ones, in order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training. Transformer-XL obtained strong results on five datasets, varying from word-level to characterlevel language modeling. Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens. Our main technical contributions include introducing the notion of recurrence in a purely selfattentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling. 2 Related Work. In the last few years, the field of language modeling has witnessed many significant advances, including but not limited to devising novel architectures to better encode the context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), improving regularization and optimization algorithms (Gal and Ghahramani, 2016) , speeding up the Softmax computation (Grave et al., 2016a) , and enriching the output distribution family (Yang et al., 2017). To capture the long-range context in language modeling, a line of work directly feeds a representation of the wider context into the network\nas an additional input."]}
{"pkey": "transformerxl_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "No, certain details about the copmute resources are Not mentioned in the paper..", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.", "With proper regularization, Transformer-XL achieves a new SoTA result among models without two-step finetuning. Penn Treebank has only 1M training tokens, which implies that Transformer-XL also generalizes well even on small datasets. 4.2 Ablation Study. We conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme. The first study is performed on WikiText-103, which requires modeling long-term dependency. The results are reported in Table 6. Among the compared encoding schemes, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are absolute. \u201cFull\u201d and \u201chalf\u201d losses refer to applying a cross entropy loss to all or the recent half positions in the segment. We found\nthat absolute encodings only work well with half losses because half losses exclude positions with very short attention lengths during training for better generalization. Table 6 shows that both the recurrence mechanism and our encoding scheme are necessary to achieve the best performance, as well as generalizing to longer attention sequences during evaluation time. Although the backpropagation length during training is only 128, with the two techniques the attention length can be increased to 640 at test time. In the standard setting with 151M parameters, the perplexity decreases as the attention length increases. Since the recurrence mechanism costs additional memory, we also compare Transformer-XL with baselines under the same GPU memory constraints. As shown in Table 10 in Appendix A, despite using a shorter backpropagation length, Transformer-XL remains superior to the baselines. The second study targets at isolating the effects of resolving the context fragmentation problem from the benefit of capturing longer context length."]}
{"pkey": "transformerxl_15", "question": "What is the pretraining objective of the model? ", "answer": "Paper presents some generated text from our best model trained the Wikitext-103 dataset. Trained only on WikiText-103 which is medium sized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "As a result, TransformerXL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-ofthe-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch1. 1 Introduction. Language modeling is among the important problems that require modeling long-term dependency, with successful applications such as unsupervised pretraining (Dai and Le, 2015; Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018). However, it has been a challenge to equip neural networks with the capability to model long-term dependency in sequential data. Recurrent neural networks (RNNs), in particular Long Short\u2217Equal contribution. Order determined by swapping the one in Yang et al. (2017). 1https://github.com/kimiyoung/ transformer-xl\nTerm Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997), have been a standard solution to language modeling and obtained strong results on multiple benchmarks. Despite the wide adaption, RNNs are difficult to optimize due to gradient vanishing and explosion (Hochreiter et al., 2001), and the introduction of gating in LSTMs and the gradient clipping technique (Graves, 2013) might not be sufficient to fully address this issue. Empirically, previous work has found that LSTM language models use 200 context words on average (Khandelwal et al., 2018), indicating room for further improvement.", "In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner."]}
{"pkey": "transformerxl_16", "question": "What is the loss function that is used to train the model?", "answer": "Paper does not talk about cross-entropy loss function specifically, but\nLet M = {m1, m2, \u00b7 \u00b7 \u00b7 , mN } be a model group consisting of N models. Let li(c, t) denote the loss of model mi on the t-th token in the corpus with a context length c. Concretely, the loss can be written as\nli(c, t) = \u2212 log Pmi(xt|xt\u22121, \u00b7 \u00b7 \u00b7 , xt\u2212c)\nwhere Pmi is the probability distribution given by model mi, and xt is the t-th token in the corpus.The relative loss of mi w.r.t. the model group M is written as fi(c, c0) = 1 |T | Xt\u2208T min b(c, t), li(c0, t).The above equation uses the minimum loss of all models on the short length c as a baseline, and only losses smaller than the baseline will be effectively counted towards the relative loss", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.", "For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018). Different from them, our work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependency. 3 Model. Given a corpus of tokens x = (x1, . . . , xT ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = \u220f t P (xt | x<t). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token. 3.1 Vanilla Transformer Language Models. In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation."]}
{"pkey": "transformerxl_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "Transformer-XL is an encode-decoder type of model. Multiple variants, with 12, 18, 20, 24 layer transformer-XL models are trained. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme that enables learning dependency beyond a fixed length without disrupting temporal coherence.  Transformer-XL  learns dependencies that are 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018). Different from them, our work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependency. 3 Model. Given a corpus of tokens x = (x1, . . . , xT ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = \u220f t P (xt | x<t). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token. 3.1 Vanilla Transformer Language Models. In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation.", "For the head 158 in the 16th layer (i.e. the last layer), each target location (corresponding to each row) has its own distinct sparse focus, differing from head 78 where target locations largely share the same attentive location in memory. Meanwhile, the pattern is also different from the case of head 8, where a few locations are clearly attended more than others. Finally, as we have discussed in section 3.3, the attention score can be decomposed into four intuitive terms. Here, we want to further investigate how these four terms contribute to the overall attention trend in Fig. 5. Since the term (c) represents the global content bias, i.e., the prior importance of each word regardless of the context, we will leave it out and focus on the terms (a), (b) and (d). So, for each term, we take the Softmax w.r.t. the memory span and average the resulted distribution of all tokens in the validation set. The results are visualized in Fig. 7:\n\u2022 Since term (a) is fully content-based addressing, when averaging over all target words, the result is essentially uniform over the entire context, except for a few very close words, which are likely to be semantically similar to the target word. \u2022 The overall trend of term (b) highly resembles that of the entire attention distribution in Fig. 5. It suggests that the global trend of focusing on the nearby context is largely contributed by this contentdependent positional bias. \u2022 The overall trend of term (d) is also focusing more on nearby words. However, compared to the trend of term (b), it is clearly flatter and biases towards a longer context. E Generated Text. In this section, we present some generated text from our best model trained the Wikitext-103 dataset. We seed the our Transformer-XL with a context of at most 512 consecutive tokens randomly sampled from the test set of Wikitext-103. Then, we run Transformer-XL to generate a pre-defined number of tokens (500 or 1,000 in our case).", "For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings."]}
{"pkey": "transformerxl_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme. The first study is performed on WikiText-103, which requires modeling long-term dependency.The paper authors apply Transformer-XL to a variety of datasets on both word-level and character-level language.modeling to have a comparison with state-of-theart systems, including WikiText-103 (Merity et al., 2016), enwik8 (LLC, 2009), text8 (LLC, 2009), One Billion Word (Chelba et al., 2013), and Penn Treebank (Mikolov and Zweig, 2012).paper present some generated text from our best model trained the Wikitext-103 dataset. The paper authors seed the our Transformer-XL with a context of at most 512 consecutive tokens randomly sampled from the test set of Wikitext-103.Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner.", "For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "In order to achieve this goal, we deliberately choose a dataset that does not require longterm dependency, so that any improvement from establishing the recurrence can be attributed to solving the context fragmentation. Specifically, we perform this controlled experiment on the One Billion Word dataset, which can only benefit from removing the context fragmentation. We train a 20-layer Transformer-XL with \u223c0.3B parameters for 400K steps. As shown in Table 7, using segment-level recurrence substantially improves performance even when long-term dependency is not needed, which is consistent with our previous discussion that the recurrence mechanism resolves the context fragmentation problem. Moreover, our relative positional encodings is also superior to Shaw et al. (2018) on short sequences. 4.3 Relative Effective Context Length. Khandelwal et al. (2018) proposed a method to evaluate the Effective Context Length (ECL) of a sequence model. ECL is the longest length to which increasing the context span would lead to a gain more than a threshold. However, ECL ignores the fact that it is harder to get improvement when a model already achieves a lower perplexity using only a shorter context, and thus it is not suitable for fair comparison among multiple models. We instead propose a new metric\ncalled Relative Effective Context Length (RECL). RECL is defined on a model group instead of a single model, and the gain of a long context is measure by the relative improvement over the best short context model. As such, the model group shares the same baseline to enable fair comparison. RECL also has a parameter r, which means constraining the comparison on top-r hard examples. See Appedix C for more details about RECL. As shown in Table 8, Transformer-XL manages to model dependency of 900 words long on av-\nAttn Len How much Al-Rfou et al. (2018) is slower\nerage with r = 0.1. The RECL of TransformerXL is 80% and 450% longer than recurrent networks and Transformer respectively."]}
{"pkey": "transformerxl_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "The paper authors conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme. The first study is performed on WikiText-103, which requires modeling long-term dependency.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["With proper regularization, Transformer-XL achieves a new SoTA result among models without two-step finetuning. Penn Treebank has only 1M training tokens, which implies that Transformer-XL also generalizes well even on small datasets. 4.2 Ablation Study. We conduct two sets of ablation studies to examine the effects of two proposed techniques used in Transformer-XL: the recurrence mechanism and the new positional encoding scheme. The first study is performed on WikiText-103, which requires modeling long-term dependency. The results are reported in Table 6. Among the compared encoding schemes, Shaw et al. (2018) is relative, while Vaswani et al. (2017) and Al-Rfou et al. (2018) are absolute. \u201cFull\u201d and \u201chalf\u201d losses refer to applying a cross entropy loss to all or the recent half positions in the segment. We found\nthat absolute encodings only work well with half losses because half losses exclude positions with very short attention lengths during training for better generalization. Table 6 shows that both the recurrence mechanism and our encoding scheme are necessary to achieve the best performance, as well as generalizing to longer attention sequences during evaluation time. Although the backpropagation length during training is only 128, with the two techniques the attention length can be increased to 640 at test time. In the standard setting with 151M parameters, the perplexity decreases as the attention length increases. Since the recurrence mechanism costs additional memory, we also compare Transformer-XL with baselines under the same GPU memory constraints. As shown in Table 10 in Appendix A, despite using a shorter backpropagation length, Transformer-XL remains superior to the baselines. The second study targets at isolating the effects of resolving the context fragmentation problem from the benefit of capturing longer context length.", "Both the recurrence mechanism and our positional encodings contribute to a longer RECL. This further substantiates our argument that Transformer-XL is able to model longer-term dependency. 4.4 Generated Text.\nTrained only on WikiText-103 which is mediumsized, Transformer-XL is already able to generate relatively coherent articles with thousands of tokens without manual cherry picking, despite minor flaws. Please refer to Appendix E for samples. 4.5 Evaluation Speed. Finally, we compare the evaluation speed of our model with the vanilla Transformer model (AlRfou et al., 2018). As shown in Table 9, due to the state reuse scheme, Transformer-XL achieves an up to 1,874 times speedup during evaluation. 5 Conclusions. Transformer-XL obtains strong perplexity results, models longer-term dependency than RNNs and Transformer, achieves substantial speedup during evaluation, and is able to generate coherent text articles. We envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling.\nAcknowledgments. ZD and YY were supported in part by National Science Foundation (NSF) under the grant IIS1546329 and by the DOE-Office of Science under the grant ASCR #KJ040201. ZY and RS were supported in part by the Office of Naval Research grant N000141812861, the NSF grant IIS1763562, the Nvidia fellowship, and the Siebel scholarship. A Ablation Study with Memory Constraints. Table 10 compares Transformer-XL with baseline under the same memory budget. Transformer-XL still outperforms the baseline even with a shorter backprop length. B Efficient Computation of the Attention with Relative Positional Embedding. As we discussed in section 3.3, the naive way of computing the Wk,RRi\u2212j for all pairs (i, j) is subject to a quadratic cost. Here, we present a simple method with only a linear cost.", "In our experiments, we set M equal to the segment length during training, and increase it by multiple times during evaluation. 3.3 Relative Positional Encodings. While we found the idea presented in the previous subsection very appealing, there is a crucial technical challenge we haven\u2019t solved in or-\nder to reuse the hidden states. That is, how can we keep the positional information coherent when we reuse the states? Recall that, in the standard Transformer, the information of sequence order is provided by a set of positional encodings, denoted as U \u2208 RLmax\u00d7d, where the i-th row Ui corresponds to the i-th absolute position within a segment and Lmax prescribes the maximum possible length to be modeled. Then, the actual input to the Transformer is the element-wise addition of the word embeddings and the positional encodings. If we simply adapt this positional encoding to our recurrence mechanism, the hidden state sequence would be computed schematically by\nh\u03c4+1 = f(h\u03c4 ,Es\u03c4+1 +U1:L)\nh\u03c4 = f(h\u03c4\u22121,Es\u03c4 +U1:L),\nwhere Es\u03c4 \u2208 RL\u00d7d is the word embedding sequence of s\u03c4 , and f represents a transformation function. Notice that, both Es\u03c4 and Es\u03c4+1 are associated with the same positional encoding U1: L. As a result, the model has no information to distinguish the positional difference between x\u03c4,j and x\u03c4+1,j for any j = 1, . . . , L, resulting in a sheer performance loss. In order to avoid this failure mode, the fundamental idea is to only encode the relative positional information in the hidden states. Conceptually, the positional encoding gives the model a temporal clue or \u201cbias\u201d about how information should be gathered, i.e., where to attend. For the same purpose, instead of incorporating bias statically into the initial embedding, one can inject the same information into the attention score of each layer. More importantly, it is more intuitive and generalizable to define the temporal bias in a relative manner."]}
{"pkey": "transformerxl_20", "question": "List the future work mentioned in the paper.", "answer": "Paper presents an improved version of transformer that i.e transformer-XL. paper talks about Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. The paper authors envision interesting applications of Transformer-XL in the fields of text generation, unsupervised feature learning, image and speech modeling.", "title": "Transformer-XL: Attentive language models beyond a fixed-length context", "context": ["For each generation step, we first find the top-40 probabilities of the next-step distribution and sample from top-40 tokens based on the re-normalized distribution. To help reading, we detokenize the context, the generated text and the reference text. Three generated examples are shown in Tables 11, 12, and 13. Note that we do not perform any cherry picking and present the first three examples we generate in the paper. In the text, \u201c= text =\u201d, \u201c= = text = =\u201d and \u201c= = = text = = =\u201d denote the Wikipedia page tile, section title and subsection title, respectively, due to the original data preprocessing procedure of Wikitext-103 (Merity et al., 2016). As we can see, though only trained on 100M tokens, Transformer-XL is a strong model at generating long text articles, particularly in the following aspects: \u2022 Transformer-XL is able to structurally maintain the sectional arrangement of Wikipedia. \u2022 Transformer-XL manages to semantically stay on the same topic throughout the course of genera-\ntion. \u2022 Long-range references are common in the generated text. \u2022 Transformer-XL often generates novel content that is not present in the training data. For more detailed explanation of the interesting observations in each example, please refer to the corresponding caption. Despite the overall excellence of the generation quality, the model can only perceive the seed context and hallucinate what to generate based on the limited knowledge (100M tokens only) it is trained on. As a result, the generated text sometimes looks clearly relevant but not close enough or to the point compared to what human writer would do. That said, we believe this issue is mostly a problem of limited training data size and could be alleviated by using a larger training set. Context: Kershaw started the 2010 season by posting a 3.07 ERA in April, but did so by walking 22 batters in 29 innings.", "Existing works range from ones where context representations are manually defined (Mikolov and Zweig, 2012; Ji et al., 2015; Wang and Cho, 2015) to others that rely on document-level topics learned from data (Dieng et al., 2016; Wang et al., 2017). More broadly, in generic sequence modeling, how to capture long-term dependency has been a long-standing research problem. From this perspective, since the ubiquitous adaption of LSTM, many efforts have been spent on relieving the vanishing gradient problem, including better initialization (Le et al., 2015), additional loss signal (Trinh et al., 2018), augmented memory structure (Ke et al., 2018) and others that modify the internal architecture of RNNs to ease the optimization (Wu et al., 2016; Li et al., 2018). Different from them, our work is based on the Transformer architecture and shows that language modeling as a real-world task benefits from the ability to learn longer-term dependency. 3 Model. Given a corpus of tokens x = (x1, . . . , xT ), the task of language modeling is to estimate the joint probability P (x), which is often auto-regressively factorized as P (x) = \u220f t P (xt | x<t). With the factorization, the problem reduces to estimating each conditional factor. In this work, we stick to the standard neural approach to modeling the conditional probability. Specifically, a trainable neural network is used to encode the context x<t into a fixed size hidden state, which is multiplied with the word embeddings to obtain the logits. The logits are then fed into the Softmax function, yielding a categorical probability distribution over the next token. 3.1 Vanilla Transformer Language Models. In order to apply Transformer or self-attention to language modeling, the central problem is how to train a Transformer to effectively encode an arbitrarily long context into a fixed size representation.", "More importantly, we show the necessity of using relative positional encodings rather than absolute ones, in order to enable state reuse without causing temporal confusion. Hence, as an additional technical contribution, we introduce a simple but more effective relative positional encoding formulation that generalizes to attention lengths longer than the one observed during training. Transformer-XL obtained strong results on five datasets, varying from word-level to characterlevel language modeling. Transformer-XL is also able to generate relatively coherent long text articles with thousands of tokens (see Appendix E), trained on only 100M tokens. Our main technical contributions include introducing the notion of recurrence in a purely selfattentive model and deriving a novel positional encoding scheme. These two techniques form a complete set of solutions, as any one of them alone does not address the issue of fixed-length contexts. Transformer-XL is the first self-attention model that achieves substantially better results than RNNs on both character-level and word-level language modeling. 2 Related Work. In the last few years, the field of language modeling has witnessed many significant advances, including but not limited to devising novel architectures to better encode the context (Bengio et al., 2003; Mikolov et al., 2010; Merity et al., 2016; Al-Rfou et al., 2018), improving regularization and optimization algorithms (Gal and Ghahramani, 2016) , speeding up the Softmax computation (Grave et al., 2016a) , and enriching the output distribution family (Yang et al., 2017). To capture the long-range context in language modeling, a line of work directly feeds a representation of the wider context into the network\nas an additional input."]}
{"pkey": "bigbird_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism.  BIG BIRD, a sparse attention mechanism that reduces this quadratic dependency to linear.  The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. BIG BIRD satisfies all the known theoretical properties of full transformer. In particular, the paper authors show that adding extra tokens allows one to express all continuous sequence to sequence functions with only O(n)-inner products. Furthermore, the paper authors show that under standard assumptions regarding precision, BIG B IRD is Turing complete.", "title": "Big Bird: Transformers for Longer Sequences", "context": ["By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?", "In this paper, we address both the above questions and produce a sparse attention mechanism that improves performance on a multitude of tasks that require long contexts. We systematically develop BIGBIRD, an attention mechanism whose complexity is linear in the number of tokens (Sec. 2). We take inspiration from graph sparsification methods and understand where the proof for expressiveness of Transformers breaks down when full-attention is relaxed to form the proposed attention pattern. This understanding helped us develop BIGBIRD, which is theoretically as expressive and also empirically useful. In particular, our BIGBIRD consists of three main part:\n\u2022 A set of g global tokens attending on all parts of the sequence. \u2022 All tokens attending to a set of w local neighboring tokens. \u2022 All tokens attending to a set of r random tokens. This leads to a high performing attention mechanism scaling to much longer sequence lengths (8x). To summarize, our main contributions are:\n1. BIGBIRD satisfies all the known theoretical properties of full transformer (Sec. 3). In particular, we show that adding extra tokens allows one to express all continuous sequence to sequence functions with only O(n)-inner products. Furthermore, we show that under standard assumptions regarding precision, BIGBIRD is Turing complete. 2. Empirically, we show that the extended context modelled by BIGBIRD benefits variety of NLP tasks. We achieve state of the art results for question answering and document summarization on a number of different datasets. Summary of these results are presented in Sec. 4.\n3. Lastly, we introduce a novel application of attention based models where long contexts are beneficial: extracting contextual representations of genomics sequences like DNA. With longer masked LM pretraining, BIGBIRD improves performance on downstream tasks such as promoterregion and chromatin profile prediction (Sec. 5). 1.1 Related Work."]}
{"pkey": "bigbird_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. The paper authors note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement translates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA, document classification, etc.  Some other works happens like one of the Simplest methods in this category just employ sliding window, but in general most work fits in the following general paradigm: using some other mechanism select a smaller subset of relevant contexts to feed in the transformer and optionally iterate, i.e. call transformer block multiple time with different contexts each time. Most prominently, SpanBERT, ORQA, REALM, RAG have achieved strong performance for different tasks. However, it is worth noting that these methods often require significant engineering efforts (like back prop through large scale nearest neighbor search) and are hard to train. Authors work is closely related to and built on the work of Extended Transformers Construction. In this paper, the paper authors address both the above questions and produce a sparse attention mechanism that improves performance on a multitude of tasks that require long contexts. The paper authors systematically develop BIG BIRD, an attention mechanism whose complexity is linear in the number of tokens .", "title": "Big Bird: Transformers for Longer Sequences", "context": ["By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?", "In this paper, we address both the above questions and produce a sparse attention mechanism that improves performance on a multitude of tasks that require long contexts. We systematically develop BIGBIRD, an attention mechanism whose complexity is linear in the number of tokens (Sec. 2). We take inspiration from graph sparsification methods and understand where the proof for expressiveness of Transformers breaks down when full-attention is relaxed to form the proposed attention pattern. This understanding helped us develop BIGBIRD, which is theoretically as expressive and also empirically useful. In particular, our BIGBIRD consists of three main part:\n\u2022 A set of g global tokens attending on all parts of the sequence. \u2022 All tokens attending to a set of w local neighboring tokens. \u2022 All tokens attending to a set of r random tokens. This leads to a high performing attention mechanism scaling to much longer sequence lengths (8x). To summarize, our main contributions are:\n1. BIGBIRD satisfies all the known theoretical properties of full transformer (Sec. 3). In particular, we show that adding extra tokens allows one to express all continuous sequence to sequence functions with only O(n)-inner products. Furthermore, we show that under standard assumptions regarding precision, BIGBIRD is Turing complete. 2. Empirically, we show that the extended context modelled by BIGBIRD benefits variety of NLP tasks. We achieve state of the art results for question answering and document summarization on a number of different datasets. Summary of these results are presented in Sec. 4.\n3. Lastly, we introduce a novel application of attention based models where long contexts are beneficial: extracting contextual representations of genomics sequences like DNA. With longer masked LM pretraining, BIGBIRD improves performance on downstream tasks such as promoterregion and chromatin profile prediction (Sec. 5). 1.1 Related Work."]}
{"pkey": "bigbird_3", "question": "What are the main contributions of the paper?", "answer": "To summarize, our main contributions are:\n1. BIG BIRD satisfies all the known theoretical properties of full transformer. In particular, authors show that adding extra tokens allows one to express all continuous sequence to sequence functions with only O(n)-inner products. Furthermore, authors show that under standard assumptions regarding precision, BIG BIRD is Turing complete.\n2. Empirically, the paper authors show that the extended context modelled by BIG BIRD benefits variety of NLP tasks. The paper authors achieve state of the art results for question answering and document summarization on a number of different datasets.\n3. Lastly, the paper authors introduce a novel application of attention based models where long contexts are beneficial: extracting contextual representations of genomics sequences like DNA. With longer masked LM pretraining, BIGBIRD improves performance on downstream tasks such as promoter-region and chromatin profile prediction .", "title": "Big Bird: Transformers for Longer Sequences", "context": ["In this paper, we address both the above questions and produce a sparse attention mechanism that improves performance on a multitude of tasks that require long contexts. We systematically develop BIGBIRD, an attention mechanism whose complexity is linear in the number of tokens (Sec. 2). We take inspiration from graph sparsification methods and understand where the proof for expressiveness of Transformers breaks down when full-attention is relaxed to form the proposed attention pattern. This understanding helped us develop BIGBIRD, which is theoretically as expressive and also empirically useful. In particular, our BIGBIRD consists of three main part:\n\u2022 A set of g global tokens attending on all parts of the sequence. \u2022 All tokens attending to a set of w local neighboring tokens. \u2022 All tokens attending to a set of r random tokens. This leads to a high performing attention mechanism scaling to much longer sequence lengths (8x). To summarize, our main contributions are:\n1. BIGBIRD satisfies all the known theoretical properties of full transformer (Sec. 3). In particular, we show that adding extra tokens allows one to express all continuous sequence to sequence functions with only O(n)-inner products. Furthermore, we show that under standard assumptions regarding precision, BIGBIRD is Turing complete. 2. Empirically, we show that the extended context modelled by BIGBIRD benefits variety of NLP tasks. We achieve state of the art results for question answering and document summarization on a number of different datasets. Summary of these results are presented in Sec. 4.\n3. Lastly, we introduce a novel application of attention based models where long contexts are beneficial: extracting contextual representations of genomics sequences like DNA. With longer masked LM pretraining, BIGBIRD improves performance on downstream tasks such as promoterregion and chromatin profile prediction (Sec. 5). 1.1 Related Work.", "By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?"]}
{"pkey": "bigbird_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "Authors not only provided experiments on text data but also on applications of  genomics sequences like DNA. BIGBIRD satisfies all the known theoretical properties of full transformer. In particular, authors show that adding extra tokens allows one to express all continuous sequence to sequence functions with only O(n)-inner products. Furthermore, the paper authors show that under standard assumptions regarding precision, BIG B IRD is Turing complete. A novel application of attention based models where long contexts are beneficial: extracting contextual representations of genomics sequences like DNA. With longer masked LM pretraining, BIGBIRD improves performance on downstream tasks such as promoter- region and chromatin profile prediction", "title": "Big Bird: Transformers for Longer Sequences", "context": ["The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?", "Given 1 < p < \u221e and > 0, for any f \u2208 FCD, there exists a transformer with sparse-attention, g \u2208 T H,m,qD such that dp(f, g) \u2264 where D is any graph containing star graph S.\nTo prove the theorem, we will follow the standard proof structure outlined in [104]. Step 1: Approximate FCD by piece-wise constant functions. Since f is a continuous function with bounded domain [0, 1)n\u00d7d, we will approximate it with a suitable piece-wise constant function. This is accomplished by a suitable partition of the region [0, 1) into a grid of granularity \u03b4 to get a discrete set G\u03b4. Therefore, we can assume that we are dealing with a function f\u0304 : G\u03b4 \u2192 Rn\u00d7d, where dp(f, f\u0304) \u2264 3 . Step 2: Approximate piece-wise constant functions by modified transformers. This is the key step of the proof where the self-attention mechanism is used to generate a contextual-mapping of the input. Informally, a contextual mapping is a unique code for the pair consisting of a matrix (X,xi) and a column. Its uniqueness allows the Feed forward layers to use each code to map it to a unique output column. The main technical challenge is computing the contextual mapping using only sparse attention mechanism. This was done in [104] using a \u201cselective\u201d shift operator which shift up entries that are in a specific interval. Key to their proof was the fact that the shift, was exactly the range of the largest entry to the smallest entry. Creating a contextual mapping with a sparse attention mechanism is quite a challenge. In particular, because each query only attends to a few keys, it is not at all clear that sufficient information can be corralled to make a contextual embedding of the entire matrix. To get around this, we develop a sparse shift operator which shifts the entries of the matrices if they lie in a certain range. The exact amount of the shift is controlled by the directed sparse attention graphg D. The second key ingredient is the use of additional global token.", "There have been a number of interesting attempts, that were aimed at alleviating the quadratic dependency of Transformers, which can broadly categorized into two directions. First line of work embraces the length limitation and develops method around it. Simplest methods in this category just employ sliding window [93], but in general most work fits in the following general paradigm: using some other mechanism select a smaller subset of relevant contexts to feed in the transformer and optionally iterate, i.e. call transformer block multiple time with different contexts each time. Most prominently, SpanBERT [42], ORQA [54], REALM [34], RAG [57] have achieved strong performance for different tasks. However, it is worth noting that these methods often require significant engineering efforts (like back prop through large scale nearest neighbor search) and are hard to train. Second line of work questions if full attention is essential and have tried to come up with approaches that do not require full attention, thereby reducing the memory and computation requirements. Prominently, Dai et al. [21], Sukhbaatar et al. [82], Rae et al. [74] have proposed auto-regresive models that work well for left-to-right language modeling but suffer in tasks which require bidirectional context. Child et al. [16] proposed a sparse model that reduces the complexity to O(n \u221a n), Kitaev et al. [49] further reduced the complexity to O(n log(n)) by using LSH to compute nearest neighbors. Ye et al. [103] proposed binary partitions of the data where as Qiu et al. [73] reduced complexity by using block sparsity. Recently, Longformer [8] introduced a localized sliding window based mask with few global mask to reduce computation and extended BERT to longer sequence based tasks. Finally, our work is closely related to and built on the work of Extended Transformers Construction [4]. This work was designed to encode structure in text for transformers."]}
{"pkey": "bigbird_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "Datasets and tasks model evaluated upon:\n1. Encoder-Decoder Tasks\n    (i)   Arxiv\n    (ii)  PubMed\n    (iii) BigPatent\n2. Genomics\n    (i)   Promoter Region Prediction\n    (ii)  Chromatin-Profile Prediction \n3. MLM Pretraining\n    (i)   Books\n    (ii)  CC-News\n    (iii) Stories\n    (iv)  Wikipedia\n4.  Question Answering Tasks\n    (i)   HotpotQA-distractor \n    (ii)  Natural Questions \n    (iii) TriviaQA\n    (iv)  WikiHop\n5. Classification Tasks\n    (i)   IMDb\n    (ii)  Yelp-5\n    (iii) Arxiv\n    (iv)  Patents\n    (v)   Hyperpartisan\n6. Summarization Task\n    (i)   Arxiv\n    (ii)  PubMed\n    (iii) BigPatent\n    (iv)  BBC XSum (shorter documents)\n    (v)   CNN/DailyMail (shorter documents)", "title": "Big Bird: Transformers for Longer Sequences", "context": ["The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?", "By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BIGBIRD drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data. 1 Introduction. Models based on Transformers [91], such as BERT [22, 63], are wildly successful for a wide variety of Natural Language Processing (NLP) tasks and consequently are mainstay of modern NLP research. Their versatility and robustness are the primary drivers behind the wide-scale adoption of Transformers. The model is easily adapted for a diverse range of sequence based tasks \u2013 as a seq2seq model for translation [91], summarization [66], generation [15], etc. or as a standalone encoders for sentiment analysis [83], POS tagging [65], machine reading comprehension [93], etc. \u2013 and it is known to vastly outperform previous sequence models like LSTM [37]. The key innovation in Transformers is the introduction of a self-attention mechanism, which can be evaluated in parallel for each token of the input sequence, eliminating the sequential dependency in recurrent neural networks, like LSTM. This parallelism enables Transformers to leverage the full power of modern SIMD hardware accelerators like GPUs/TPUs, thereby facilitating training of NLP models on datasets of unprecedented size. This ability to train on large scale data has led to surfacing of models like BERT [22] and T5 [75], which pretrain transformers on large general purpose corpora and transfer the knowledge to down-stream task. The pretraining has led to significant improvement in low data regime downstream tasks [51] as well as tasks with sufficient data [101] and thus have been a major force behind the ubiquity of transformers in contemporary NLP."]}
{"pkey": "bigbird_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not Specified in Paper", "title": "Big Bird: Transformers for Longer Sequences", "context": ["By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?", "Classification We experiment on datasets of different lengths and contents, specifically various document classification and GLUE tasks. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there is improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. 4.1 Encoder-Decoder Tasks. For an encoder-decoder setup, one can easily see that both suffer from quadratic complexity due to the full self attention. We focus on introducing the sparse attention mechanism of BIGBIRD only at the encoder side. This is because, in practical generative applications, the length of output sequence is typically small as compared to the input. For example for text summarization, we see in realistic scenarios (c.f. App. E.5 Tab. 18) that the median output sequence length is \u223c 200 where as the input\nsequence\u2019s median length is > 3000. For such applications, it is more efficient to use sparse attention mechanism for the encoder and full self-attention for the decoder. Summarization Document summarization is a task of creating a short and accurate summary of a text document. We used three long document datasets for testing our model details of which are mention in Tab. 18. In this paper we focus on abstractive summarization of long documents where using a longer contextual encoder should improve performance."]}
{"pkey": "bigbird_7", "question": "List the limitations of the model discussed in the paper.", "answer": "The limitation of the proposed model is that Sparse attention mechanisms can not universally replace dense attention mechanisms, i.e. there is no free lunch. Authors demonstrate a natural task which can be solved by the full attention mechanism in O(1)-layers. However, under standard complexity theoretic assumptions, authors show that this problem will require \u03a9\u0303(n)-layers for any sparse attention layers with \u00d5(n) edges (not just BIG BIRD). (Authors use the standard notation \u03a9\u0303(n) to hide the dependence on poly-logarithmic\nfactors.)", "title": "Big Bird: Transformers for Longer Sequences", "context": ["In all experiments we use a learning rate warmup over the first 10% steps, and linear decay of the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better quantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length at which the document are often truncated. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. GLUE The General Language Understanding Evaluation (GLUE) benchmark [92], test language models on 8 different natural language understanding tasks. We used the same training parameters as mentioned in https://github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md. Our model parameters are b = 64, g = 2 \u00d7 b, w = 3\u00d7 b, r = 3\u00d7 b ( we used the BIGBIRD-ITC base model pretrained on MLM task). We compare the performance of BIGBIRD to BERT, XLNet [101] and RoBERTa in Tab. 16. We find that even on task that have a much smaller context, our performance is competitive to full attention models. E.5 Summarization. As discussed in Sec. 4.1, given the small length of output sequence, we used sparse BIGBIRD attention only for encoder, while keeping the full attention for decoder. The number of hidden layers, number of heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed in Tab. 17. We summarize our result in Tab. 20.", "In this section our goal is to showcase benefits of modeling longer input sequence for NLP tasks, for which we select three representative tasks. We begin with basic masked language modeling (MLM; Devlin et al. 22) to check if better contextual representations can be learnt by utilizing longer contiguous sequences. Next, we consider QA with supporting evidence, for which capability to handle longer sequence would allow us to retrieve more evidence using crude systems like TF-IDF/BM25. Finally, we tackle long document classification where discriminating information may not be located in first 512 tokens. Below we summarize the results for BIGBIRD using sequence length 40961, while we defer all other setup details including computational resources, batch size, step size, to App. E.\nPretraining and MLM We follow [22, 63] to create base and large versions of BIGBIRD and pretrain it using MLM objective. This task involves predicting a random subset of tokens which have been masked out. We use four standard data-sets for pretraining (listed in App. E.1, Tab. 9), warm-starting from the public RoBERTa checkpoint2. We compare performance in predicting the masked out tokens in terms of bits per character, following [8]. As seen in App. E.1, Tab. 10, both BIGBIRD and Longformer perform better than limited length RoBERTa, with BIGBIRD-ETC performing the best. We note that we trained our models on a reasonable 16GB memory/chip with batch size of 32-64. Our memory efficiency is due to efficient blocking and sparsity structure of the sparse attention mechanism described in Sec. 2.\nQuestion Answering (QA) We considered following four challenging datasets:\n1. Natural Questions [52]: For the given question, find a short span of answer (SA) from the given evidences as well highlight the paragraph from the given evidences containing information about the correct answer (LA). 2. HotpotQA-distractor", "This view of self-attention as a fully connected graph allows us to exploit existing graph theory to help reduce its complexity. The problem of reducing the quadratic complexity of self-attention can now be seen as a graph sparsification problem. It is well-known that random graphs are expanders and can approximate complete graphs in a number of different contexts including in their spectral properties [80, 38]. We believe sparse random graph for attention mechanism should have two desiderata: small average path length between nodes and a notion of locality, each of which we discuss below. Let us consider the simplest random graph construction, known as Erdo\u030bs-R\u00e9nyi model, where each edge is independently chosen with a fixed probability. In such a random graph with just \u0398\u0303(n) edges, the shortest path between any two nodes is logarithmic in the number of nodes [17, 43]. As a consequence, such a random graph approximates the complete graph spectrally and its second eigenvalue (of the adjacency matrix) is quite far from the first eigenvalue [9, 10, 6]. This property leads to a rapid mixing time for random walks in the grpah, which informally suggests that information can flow fast between any pair of nodes. Thus, we propose a sparse attention where each query attends over r random number of keys i.e. A(i, \u00b7) = 1 for r randomly chosen keys (see Fig. 1a). The second viewpoint which inspired the creation of BIGBIRD is that most contexts within NLP and computational biology have data which displays a great deal of locality of reference. In this phenomenon, a great deal of information about a token can be derived from its neighboring tokens. Most pertinently, Clark et al. [19] investigated self-attention models in NLP tasks and concluded that that neighboring inner-products are extremely important. The concept of locality, proximity of tokens in linguistic structure, also forms the basis of various linguistic theories such as transformationalgenerative grammar."]}
{"pkey": "bigbird_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "There are many datasets mentioned  in this paper for each specific task.\n1- For MLM Pretraining, four publicly available datasets Books, CC-News, Stories and Wikipedia to pretrain BIGBIRD. The paper authors borrow the sentencepiece vocabulary as RoBERTa (which is in turn borrowed from GPT2). The paper authors split any document longer than 4096 into multiple documents and the paper authors join documents that were much smaller than 4096. Following the original BERT training, the paper authors mask 15% of tokens in these four datasets, and train to predict the mask. The paper authors warm start from RoBERTa\u2019s checkpoint. We train two different models: BIG BIRD- ITC -base and B IG BIRD-ETC -base. \n2- For Question Answering task, above same datasets for pretraining Books, CC-News, Stories and Wikipedia are used. For training, HotpotQA-distractor, Natural Questions, TriviaQA, and WikiHop datasets are used.  In HotpotQA dataset, the data consists of each question with multiple evidence paragraphs. The paper authors filtered 16 QA where the answer was not in the given evidences. For BIG BIRD-ITC , the paper authors use first 128 global tokens. For B IG B IRD-ETC, the paper authors have one global token for each question token, one for each evidence paragraph, and one for each sentence within the paragraph, for a maximum of 256 global token.   For Natural Questions dataset,  the data consists of question with supporting evidence, but in form of a single, potentially long, document and not multiple paragraphs.  For documents, that are longer than 4096, a sliding window approach is used with stride of 2048. The paper authors use CLS token at the beginning, followed by the question followed by a separator token followed by the document as input.  For TriviaQA, The data consists of question-answer pairs with Wikipedia articles as the \u201cnoisy\u201d supporting evidence. The paper authors call them noisy because the given Wikipedia articles may or may not contain the answer. Moreover, the answer entities is not annotated to appropriate span in the article, rather all occurrences found using fuzzy string matching are listed. The paper authors use CLS token at the beginning, followed by the question followed by a separator token followed by the document as input. For each question in WikiHop, the paper authors are given upto 79 candidates, and 63 supporting paragraphs. In our BIG BIRD - ITC model, the paper authors concatenate the answer and the question with special tokens, [q] Question [/q] [ans] Ans1 [/ans] . . . [ans] AnsN [/ans] along with the context. As the start of the text, always contains questions followed by answers, the paper authors make the first 128 token attend globally. \n3- For Document Classification Task, IMDb and Yelp-5 datasets are used for sentiment analysis and for topic assignment task Arxiv, Patents, and Hyperpartisan datasets are used\n4- For Summarization task, Arxiv, PubMed, and BigPatent datasets are used. For shorter Summarization, Summarization and CNN/DailyMai are used", "title": "Big Bird: Transformers for Longer Sequences", "context": ["Classification We experiment on datasets of different lengths and contents, specifically various document classification and GLUE tasks. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there is improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. 4.1 Encoder-Decoder Tasks. For an encoder-decoder setup, one can easily see that both suffer from quadratic complexity due to the full self attention. We focus on introducing the sparse attention mechanism of BIGBIRD only at the encoder side. This is because, in practical generative applications, the length of output sequence is typically small as compared to the input. For example for text summarization, we see in realistic scenarios (c.f. App. E.5 Tab. 18) that the median output sequence length is \u223c 200 where as the input\nsequence\u2019s median length is > 3000. For such applications, it is more efficient to use sparse attention mechanism for the encoder and full self-attention for the decoder. Summarization Document summarization is a task of creating a short and accurate summary of a text document. We used three long document datasets for testing our model details of which are mention in Tab. 18. In this paper we focus on abstractive summarization of long documents where using a longer contextual encoder should improve performance.", "In all experiments we use a learning rate warmup over the first 10% steps, and linear decay of the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better quantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length at which the document are often truncated. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. GLUE The General Language Understanding Evaluation (GLUE) benchmark [92], test language models on 8 different natural language understanding tasks. We used the same training parameters as mentioned in https://github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md. Our model parameters are b = 64, g = 2 \u00d7 b, w = 3\u00d7 b, r = 3\u00d7 b ( we used the BIGBIRD-ITC base model pretrained on MLM task). We compare the performance of BIGBIRD to BERT, XLNet [101] and RoBERTa in Tab. 16. We find that even on task that have a much smaller context, our performance is competitive to full attention models. E.5 Summarization. As discussed in Sec. 4.1, given the small length of output sequence, we used sparse BIGBIRD attention only for encoder, while keeping the full attention for decoder. The number of hidden layers, number of heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed in Tab. 17. We summarize our result in Tab. 20.", "Lastly, we use a dense layer that takes in the output vector corresponding to a candidate answer, and predicts a score for the current candidate to be the correct answer. We apply this dense layer to each candidate independently and the candidate with the best score is picked as our final answer. It is worthwhile to note that explicitly designed attention connection in ETC works slightly better, the random connection based ITC is pretty competative. E.3 Relationship to Contemporary Work. Longformer Child et al. [16] introduced localized sliding window to reduce computation. A more recent version, which includes localized sliding windows and global tokens was introduced independently by Longofrmer[8]. Although BIGBIRD contains additional random tokens, there are also differences in the way global and local tokens are realized. In particular even when there is no random token, as used to get SoTA in question answering, there are two key differences between Longformer and BIGBIRD-etc (see [4]):\n1. We use global-local attention with relative position encodings enables it to better handle structured inputs\n2. Unlike Longformer, we train the global tokens using CPC loss and learn their use during finetuning. E.4 Classification. We try two types of classification task. Document classification We experiment on datasets of different lengths and contents, as listed in Tab. 15. In particular, we look at sentiment analysis (IMDb [64] and Yelp-5 [108]) task and topic\nassignment (Arxiv [35], Patents [53], and Hyperpartisan [47]) task. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token from the BIGBIRD encoder consuming 4096 tokens. We report the results of document classification experiments in Tab. 15. We compare against state-of-the-art (SoTA) methods for each dataset and plain RoBERTa model with 512 tokens truncation."]}
{"pkey": "bigbird_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "As mentioned in paper, for Genomics experiment, they build a byte-pair encoding  for the DNA sequence of size 32K, with each token representing 8.78 base pairs on average.", "title": "Big Bird: Transformers for Longer Sequences", "context": ["b\u00d7 (g + w + r)b, which can be reshaped to obtain all the attention scores according to the BigBird pattern. E NLP experiments details. E.1 MLM Pretraining. We use four publicly available datasets Books [110], CC-News [34], Stories [89] and Wikipedia to pretrain BIGBIRD. We borrow the sentencepiece vocabulary as RoBERTa (which is in turn borrowed from GPT2). We split any document longer than 4096 into multiple documents and we join documents that were much smaller than 4096. Following the original BERT training, we mask 15% of tokens in these four datasets, and train to predict the mask. We warm start from RoBERTa\u2019s checkpoint. We train two different models: BIGBIRD-ITC-base and BIGBIRD-ETC-base. The hyper-parameters for these two models are given in Tab. 8. In all experiments we use a learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. Similar to the norm, we trained a large version of model as well, which has 24 layers with 16 heads and hidden dimension of 1024. Following the observation from RoBERTa, we pretrain on a larger batch size of 2048 for this size. For BIGBIRD-ITC the block length was kept same as base size, but for BIGBIRD-ETC the block length was almost doubled to 169. All the remaining parameters were the same. E.2 Question Answering. The detailed statistics of the four datasets used are given in Tab. 11. All the hyperparameters for BIGBIRD, used for creating Tab. 2 are shown in Tab. 12 and those submitted to get Tab. 3 are shown in Tab. 13. We use two types of regularization in training:\n\u2022 We used a variant of contrastive predictive coding [70] as a dual encoder model. \u2022 We use position embedding for ITC and relative position encoding [79] for ETC. Next, we will mention the dataset/task specific part of the model. Model Base Large\nRoBERTa (sqln: 512) 1.846 1.496 Longformer (sqln: 4096) 1.705 1.358 BIGBIRD-ITC (sqln: 4096) 1.678 1.456 BIGBIRD-ETC (sqln: 4096) 1.611 1.274\nTable 10: MLM performance on held-out set. HotpotQA", "In this section our goal is to showcase benefits of modeling longer input sequence for NLP tasks, for which we select three representative tasks. We begin with basic masked language modeling (MLM; Devlin et al. 22) to check if better contextual representations can be learnt by utilizing longer contiguous sequences. Next, we consider QA with supporting evidence, for which capability to handle longer sequence would allow us to retrieve more evidence using crude systems like TF-IDF/BM25. Finally, we tackle long document classification where discriminating information may not be located in first 512 tokens. Below we summarize the results for BIGBIRD using sequence length 40961, while we defer all other setup details including computational resources, batch size, step size, to App. E.\nPretraining and MLM We follow [22, 63] to create base and large versions of BIGBIRD and pretrain it using MLM objective. This task involves predicting a random subset of tokens which have been masked out. We use four standard data-sets for pretraining (listed in App. E.1, Tab. 9), warm-starting from the public RoBERTa checkpoint2. We compare performance in predicting the masked out tokens in terms of bits per character, following [8]. As seen in App. E.1, Tab. 10, both BIGBIRD and Longformer perform better than limited length RoBERTa, with BIGBIRD-ETC performing the best. We note that we trained our models on a reasonable 16GB memory/chip with batch size of 32-64. Our memory efficiency is due to efficient blocking and sparsity structure of the sparse attention mechanism described in Sec. 2.\nQuestion Answering (QA) We considered following four challenging datasets:\n1. Natural Questions [52]: For the given question, find a short span of answer (SA) from the given evidences as well highlight the paragraph from the given evidences containing information about the correct answer (LA). 2. HotpotQA-distractor", "In all experiments we use a learning rate warmup over the first 10% steps, and linear decay of the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better quantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length at which the document are often truncated. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. GLUE The General Language Understanding Evaluation (GLUE) benchmark [92], test language models on 8 different natural language understanding tasks. We used the same training parameters as mentioned in https://github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md. Our model parameters are b = 64, g = 2 \u00d7 b, w = 3\u00d7 b, r = 3\u00d7 b ( we used the BIGBIRD-ITC base model pretrained on MLM task). We compare the performance of BIGBIRD to BERT, XLNet [101] and RoBERTa in Tab. 16. We find that even on task that have a much smaller context, our performance is competitive to full attention models. E.5 Summarization. As discussed in Sec. 4.1, given the small length of output sequence, we used sparse BIGBIRD attention only for encoder, while keeping the full attention for decoder. The number of hidden layers, number of heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed in Tab. 17. We summarize our result in Tab. 20."]}
{"pkey": "bigbird_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "Not Specified in paper", "title": "Big Bird: Transformers for Longer Sequences", "context": ["In all experiments we use a learning rate warmup over the first 10% steps, and linear decay of the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better quantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length at which the document are often truncated. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. GLUE The General Language Understanding Evaluation (GLUE) benchmark [92], test language models on 8 different natural language understanding tasks. We used the same training parameters as mentioned in https://github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md. Our model parameters are b = 64, g = 2 \u00d7 b, w = 3\u00d7 b, r = 3\u00d7 b ( we used the BIGBIRD-ITC base model pretrained on MLM task). We compare the performance of BIGBIRD to BERT, XLNet [101] and RoBERTa in Tab. 16. We find that even on task that have a much smaller context, our performance is competitive to full attention models. E.5 Summarization. As discussed in Sec. 4.1, given the small length of output sequence, we used sparse BIGBIRD attention only for encoder, while keeping the full attention for decoder. The number of hidden layers, number of heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed in Tab. 17. We summarize our result in Tab. 20.", "Lastly, we use a dense layer that takes in the output vector corresponding to a candidate answer, and predicts a score for the current candidate to be the correct answer. We apply this dense layer to each candidate independently and the candidate with the best score is picked as our final answer. It is worthwhile to note that explicitly designed attention connection in ETC works slightly better, the random connection based ITC is pretty competative. E.3 Relationship to Contemporary Work. Longformer Child et al. [16] introduced localized sliding window to reduce computation. A more recent version, which includes localized sliding windows and global tokens was introduced independently by Longofrmer[8]. Although BIGBIRD contains additional random tokens, there are also differences in the way global and local tokens are realized. In particular even when there is no random token, as used to get SoTA in question answering, there are two key differences between Longformer and BIGBIRD-etc (see [4]):\n1. We use global-local attention with relative position encodings enables it to better handle structured inputs\n2. Unlike Longformer, we train the global tokens using CPC loss and learn their use during finetuning. E.4 Classification. We try two types of classification task. Document classification We experiment on datasets of different lengths and contents, as listed in Tab. 15. In particular, we look at sentiment analysis (IMDb [64] and Yelp-5 [108]) task and topic\nassignment (Arxiv [35], Patents [53], and Hyperpartisan [47]) task. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token from the BIGBIRD encoder consuming 4096 tokens. We report the results of document classification experiments in Tab. 15. We compare against state-of-the-art (SoTA) methods for each dataset and plain RoBERTa model with 512 tokens truncation.", "In this section our goal is to showcase benefits of modeling longer input sequence for NLP tasks, for which we select three representative tasks. We begin with basic masked language modeling (MLM; Devlin et al. 22) to check if better contextual representations can be learnt by utilizing longer contiguous sequences. Next, we consider QA with supporting evidence, for which capability to handle longer sequence would allow us to retrieve more evidence using crude systems like TF-IDF/BM25. Finally, we tackle long document classification where discriminating information may not be located in first 512 tokens. Below we summarize the results for BIGBIRD using sequence length 40961, while we defer all other setup details including computational resources, batch size, step size, to App. E.\nPretraining and MLM We follow [22, 63] to create base and large versions of BIGBIRD and pretrain it using MLM objective. This task involves predicting a random subset of tokens which have been masked out. We use four standard data-sets for pretraining (listed in App. E.1, Tab. 9), warm-starting from the public RoBERTa checkpoint2. We compare performance in predicting the masked out tokens in terms of bits per character, following [8]. As seen in App. E.1, Tab. 10, both BIGBIRD and Longformer perform better than limited length RoBERTa, with BIGBIRD-ETC performing the best. We note that we trained our models on a reasonable 16GB memory/chip with batch size of 32-64. Our memory efficiency is due to efficient blocking and sparsity structure of the sparse attention mechanism described in Sec. 2.\nQuestion Answering (QA) We considered following four challenging datasets:\n1. Natural Questions [52]: For the given question, find a short span of answer (SA) from the given evidences as well highlight the paragraph from the given evidences containing information about the correct answer (LA). 2. HotpotQA-distractor"]}
{"pkey": "bigbird_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of cross-attention is added between the self-attention layers.\n\nThe final piece of B IG B IRD is inspired from our theoretical analysis, which is critical\nfor empirical performance. More specifically, our theory utilizes the importance of \u201cglobal tokens\u201d\n(tokens that attend to all tokens in the sequence and to whom all tokens attend to. These\nglobal tokens can be defined in two ways:\n\u2022 BIG B IRD-ITC: In internal transformer construction (ITC), the paper authors make some existing tokens \u201cglobal\u201d,\nwhich attend over the entire sequence. Concretely, the paper authors choose a subset G of indices (with\ng := |G|), such that A(i, :) = 1 and A(:, i) = 1 for all i \u2208 G.\n\u2022 B IG B IRD - ETC: In extended transformer construction ( ETC), the paper authors include additional \u201cglobal\u201d\ntokens such as CLS. Concretely, the paper authors add g global tokens that attend to all existing tokens. In\nour notation, this corresponds to creating a new matrix B \u2208 [0, 1](N +g)\u00d7(N +g) by adding\ng rows to matrix A, such that B(i, :) = 1, and B(:, i) = 1 for all i \u2208 {1, 2, . . . g}, and\nB(g + i, g + j) = A(i, j)\u2200 i, j \u2208 {1, . . . , N }. This adds extra location to store context  which improves performance.\n\nThe final attention mechanism for BIG B IRD has all three of these properties: queries attend\nto r random keys, each query attends to w/2 tokens to the left of its location and w/2 to the right of\nits location and they contain g global tokens (The global tokens can be from existing tokens or extra\nadded tokens). \n\n\nbase BIG B IRD model used for Question Answering\n\n        Parameter                              HotpotQA          NaturalQ          TriviaQA          WikiHop         \n        Global token location             ITC  ETC          ITC  ETC          ITC  ETC          ITC  ETC \n-------------------------------------------------------------------------------------------------------------------------------------------------        \n        # of global token, g                128  256          128  230          128  320          128  430         \n        Window length, w                  192  252          192  252          192  252          192  252         \n        # of random token, r               192  0            192  0              192  0               192  0         \n        Max. sequence length          4096 4096      4096 4096       4096 4096       4096 4096         \n        # of heads                                12  12             12  12              12  12              12  12         \n        # of hidden layers                     12  12             12  12              12  12              12  12         \n        Hidden layer size                   768  768          768  768          768  768          768  768         \n        Batch size                                 32  32          128  128             32  32              64  64         \n        Loss                                  cross-entropy     cross-entropy    cross-entropy   cross-entropy  \n                                                  golden spans     golden spans    noisy spans      ans choices\n                                                     \n        Compute resources          4 \u00d7 2 TPUv3          4 \u00d7 8 TPUv3   4 \u00d7 2 TPUv3          4 \u00d7 4 TPUv3 \n\n\ntwo BIGBIRD base models for MLM:\n\nParameter                       |  BIG B IRD-ITC            |  BIGB IRD-ETC\n-------------------------------------------------------------------------------------------\nBlock length, b                 |       64                          |      84 \n# of global token, g          |           2 \u00d7 b                  |      256 \nWindow length, w            |      3 \u00d7 b                       |     3 \u00d7 b \n# of random token, r        |           3 \u00d7 b                  |       0 \nMax. sequence length     |           4096                  |     4096 \n# of heads                       |       12                           |      12 \n# of hidden layers           |       12                            |      12 \nHidden layer size            |       768                          |      768 \nBatch size                       |       256                          |      256 \nLoss                                |       MLM                        |      MLM \nActivation layer               |       gelu                         |      gelu \nDropout prob                   |       0.1                          |      0.1 \nAttention dropout prob     |    0.1                             |      0.1 \nOptimizer                         |    Adam                         |      Adam \nLearning rate                   |    10\u22124                           |      10\u22124 \nCompute resources          |    8 \u00d7 8 TPUv3              |        8 \u00d7 8 TPUv3", "title": "Big Bird: Transformers for Longer Sequences", "context": ["In all experiments we use a learning rate warmup over the first 10% steps, and linear decay of the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better quantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length at which the document are often truncated. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. GLUE The General Language Understanding Evaluation (GLUE) benchmark [92], test language models on 8 different natural language understanding tasks. We used the same training parameters as mentioned in https://github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md. Our model parameters are b = 64, g = 2 \u00d7 b, w = 3\u00d7 b, r = 3\u00d7 b ( we used the BIGBIRD-ITC base model pretrained on MLM task). We compare the performance of BIGBIRD to BERT, XLNet [101] and RoBERTa in Tab. 16. We find that even on task that have a much smaller context, our performance is competitive to full attention models. E.5 Summarization. As discussed in Sec. 4.1, given the small length of output sequence, we used sparse BIGBIRD attention only for encoder, while keeping the full attention for decoder. The number of hidden layers, number of heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed in Tab. 17. We summarize our result in Tab. 20.", "[100]: Similar to natural questions, it requires finding the answer (Ans) as well as the supporting facts (Sup) over different documents needed for multi-hop reasoning from the given evidences. 3. TriviaQA-wiki [41]: We need to provide an answer for the given question using provided Wikipedia evidence, however, the answer might not be present in the given evidence. On a\n1code available at http://goo.gle/bigbird-transformer 2https://github.com/pytorch/fairseq/tree/master/examples/roberta\nsmaller verified subset of question, the given evidence is guaranteed to contain the answer. Nevertheless, we model the answer as span selection problem in this case as well. 4. WikiHop [95]: Chose correct option from multiple-choice questions (MCQ), by aggregating information spread across multiple documents given in the evidences. As these tasks are very competitive, multiple highly engineered systems have been designed specific each dataset confirming to respective output formats. For a fair comparison, we had to use some additional regularization for training BIGBIRD, details of which are provided in App. E.2 along with exact architecture description. We experiment using the base sized model and select the best configuration on the development set for each dataset (as reported in Tab. 2). We can see that BIGBIRD-ETC, with expanded global tokens consistently outperforms all other models. Thus, we chose this configuration to train a large sized model to be used for evaluation on the hidden test set. In Tab. 3, we compare BIGBIRD-ETC model to top-3 entries from the leaderboard excluding BIGBIRD. One can clearly see the importance of using longer context as both Longformer and BIGBIRD outperform models with smaller contexts. Also, it is worth noting that BIGBIRD submission is a single model, whereas the other top-3 entries for Natural Questions are ensembles, which might explain the slightly lower accuracy in exact answer phrase selection.", "The idea of global tokens was used extensively by them to achieve their goals. Our theoretical work can be seen as providing a justification for the success of these models as well. It is important to note that most of the aforementioned methods are heuristic based and empirically are not as versatile and robust as the original transformer, i.e. the same architecture do not attain SoTA on multiple standard benchmarks. (There is one exception of Longformer which we include in all our comparisons, see App. E.3 for a more detailed comparison). Moreover, these approximations do not come with theoretical guarantees. 2 BIGBIRD Architecture. In this section, we describe the BIGBIRD model using the generalised attention mechanism that is used in each layer of transformer operating on an input sequence X = (x1, ...,xn) \u2208 Rn\u00d7d. The generalized attention mechanism is described by a directed graph D whose vertex set is [n] = {1, . . . , n}. The set of arcs (directed edges) represent the set of inner products that the attention mechanism will consider. Let N(i) denote the out-neighbors set of node i in D, then the ith output vector of the generalized attention mechanism is defined as\nATTND(X)i = xi + H\u2211\nh=1\n\u03c3 ( Qh(xi)Kh(XN(i)) T ) \u00b7 Vh(XN(i)) (AT)\nwhere Qh,Kh : Rd \u2192 Rm are query and key functions respectively, Vh : Rd \u2192 Rd is a value function, \u03c3 is a scoring function (e.g. softmax or hardmax) and H denotes the number of heads. Also note XN(i) corresponds to the matrix formed by only stacking {xj : j \u2208 N(i)} and not all the inputs. If D is the complete digraph, we recover the full quadratic attention mechanism of Vaswani et al. [91]. To simplify our exposition, we will operate on the adjacency matrix A of the graph D even though the underlying graph maybe sparse. To elaborate, A \u2208 [0, 1]n\u00d7n with A(i, j) = 1 if query i attends to key j and is zero otherwise. For example, when A is the ones matrix (as in BERT), it leads to quadratic complexity, since all tokens attend on every other token."]}
{"pkey": "bigbird_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "1- Hyperparameters for the two BIGBIRD base models for MLM:\nParameter                       |  BIG B IRD-ITC            |  BIGB IRD-ETC\n-------------------------------------------------------------------------------------------\nBlock length, b                 |       64                          |      84 \n# of global token, g          |           2 \u00d7 b                  |      256 \nWindow length, w            |      3 \u00d7 b                       |     3 \u00d7 b \n# of random token, r        |           3 \u00d7 b                  |       0 \nMax. sequence length     |           4096                  |     4096 \n# of heads                       |       12                           |      12 \n# of hidden layers           |       12                            |      12 \nHidden layer size            |       768                          |      768 \nBatch size                       |       256                          |      256 \nLoss                                |       MLM                        |      MLM \nActivation layer               |       gelu                         |      gelu \nDropout prob                   |       0.1                          |      0.1 \nAttention dropout prob     |    0.1                             |      0.1 \nOptimizer                         |    Adam                         |      Adam \nLearning rate                   |    10\u22124                           |      10\u22124 \nCompute resources          |    8 \u00d7 8 TPUv3              |        8 \u00d7 8 TPUv3\n2- Hyperparameters of base BIG B IRD model used for Question Answering\n        Parameter                              HotpotQA          NaturalQ          TriviaQA          WikiHop         \n        Global token location             ITC  ETC          ITC  ETC          ITC  ETC          ITC  ETC \n-------------------------------------------------------------------------------------------------------------------------------------------------        \n        # of global token, g                128  256          128  230          128  320          128  430         \n        Window length, w                  192  252          192  252          192  252          192  252         \n        # of random token, r               192  0            192  0              192  0               192  0         \n        Max. sequence length          4096 4096      4096 4096       4096 4096       4096 4096         \n        # of heads                                12  12             12  12              12  12              12  12         \n        # of hidden layers                     12  12             12  12              12  12              12  12         \n        Hidden layer size                   768  768          768  768          768  768          768  768         \n        Batch size                                 32  32          128  128             32  32              64  64         \n        Loss                                  cross-entropy     cross-entropy    cross-entropy   cross-entropy  \n                                                  golden spans     golden spans    noisy spans      ans choices\n        Compute resources          4 \u00d7 2 TPUv3          4 \u00d7 8 TPUv3   4 \u00d7 2 TPUv3          4 \u00d7 4 TPUv3         \n hyperparameters for Computational biology (Chromatin-Profile Prediction)                                \n        Parameter                      |   Pretraining      |        Promoter Region  |        Chromatin-Profile \n-------------------------------------|------------------------|-------------------------------|---------------------        \n    Block length, b                  |      64                  |       64                          |       64 \n        Global token location    |      ITC                |      ITC                          |      ITC \n        # of global token, g       |     2 \u00d7 b               |       2 \u00d7 b                       |       2 \u00d7 b \n        Window length, w          |     3 \u00d7 b              |       3 \u00d7 b                        |       3 \u00d7 b \n        # of random token, r     |     3 \u00d7 b               |       3 \u00d7 b                        |       3 \u00d7 b \n        Max. Sequence Length |     4096              |       4096                         |       4096 \n        # of heads                      |       12                |        12                            |       12 \n        # of hidden layers          |       12                |        12                            |       12 \n        Hidden layer size           |       768              |        768                          |       768 \n        Batch Size                     |       256               |        256                          |       256 \n        Vocab Size                    |       32000           |       32000                        |       32000 \n        Loss                              |       MLM+NSP    |               BCE                 |      919 x +ve upweighted BCE \n        Dropout prob                 |       0.1                 |        0.1                          |       0.1 \n        Optimizer                       |       Adam            |        Adam                       |       Adam \n        Learning rate                 |       0.0001           |       0.0001                      |       0.0001 \n        # of steps                       |       1000000        |              711                    |       500000 \n        Compute Resources      |    8 \u00d7 8 TPUv3     |            8 \u00d7 8 TPUv3         |       8 \u00d7 8 TPUv3", "title": "Big Bird: Transformers for Longer Sequences", "context": ["During training the model gets as input pairs of sequences separated by [SEP] token along with a [CLS] token at the start. Overall the input pattern is: [CLS] sequence A [SEP] sequence B [SEP]. For 50% of the time the second sequence comes from true sequence after the first one. Remaining 50% of the time it is a a random sequence from the full dataset. The model is then required to predict this relationship using the output corresponding to the [CLS] token, which is fed into a simple binary classification layer. 7https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39\nThe sequence of steps is visually elaborated in Fig. 9. The model is trained with both MLM and NSP together. Training hyperparameter is provided in second columns of Tab. 21. In all experiments we use a learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We additionally performed a simple ablation study to validate the hypothesis, that similar to NLP, having a larger context improves performance. We use MLM task described above to test how BIGBIRD performed with sequences of different length. Accuracy on MLM task with increasing sequence length is shown in Fig. 8. Not only longer context improves final accuracy, it also leads to faster learning, as we have now more opportunities for masking. F.2 Promoter Region Prediction. The promoter region plays an important role in transcription initiation and thus its recognition is an important area of interest in the field of bioinformatics. Following Oubounyt et al. [71], we use datasets from Eukaryotic Promoter Database (EPDnew) [24], which contains 29,597 promoter region in the human genome. Around the transcription start site (TSS), we extract a sequence of 8000 bp (-5000 +3000 bp) from the human reference genome GRCh37. Since EPDnew uses newer GRCh38, we convert to GRCh37 coordinates using LiftOver [44]. Following Oubounyt et al.", "b\u00d7 (g + w + r)b, which can be reshaped to obtain all the attention scores according to the BigBird pattern. E NLP experiments details. E.1 MLM Pretraining. We use four publicly available datasets Books [110], CC-News [34], Stories [89] and Wikipedia to pretrain BIGBIRD. We borrow the sentencepiece vocabulary as RoBERTa (which is in turn borrowed from GPT2). We split any document longer than 4096 into multiple documents and we join documents that were much smaller than 4096. Following the original BERT training, we mask 15% of tokens in these four datasets, and train to predict the mask. We warm start from RoBERTa\u2019s checkpoint. We train two different models: BIGBIRD-ITC-base and BIGBIRD-ETC-base. The hyper-parameters for these two models are given in Tab. 8. In all experiments we use a learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. Similar to the norm, we trained a large version of model as well, which has 24 layers with 16 heads and hidden dimension of 1024. Following the observation from RoBERTa, we pretrain on a larger batch size of 2048 for this size. For BIGBIRD-ITC the block length was kept same as base size, but for BIGBIRD-ETC the block length was almost doubled to 169. All the remaining parameters were the same. E.2 Question Answering. The detailed statistics of the four datasets used are given in Tab. 11. All the hyperparameters for BIGBIRD, used for creating Tab. 2 are shown in Tab. 12 and those submitted to get Tab. 3 are shown in Tab. 13. We use two types of regularization in training:\n\u2022 We used a variant of contrastive predictive coding [70] as a dual encoder model. \u2022 We use position embedding for ITC and relative position encoding [79] for ETC. Next, we will mention the dataset/task specific part of the model. Model Base Large\nRoBERTa (sqln: 512) 1.846 1.496 Longformer (sqln: 4096) 1.705 1.358 BIGBIRD-ITC (sqln: 4096) 1.678 1.456 BIGBIRD-ETC (sqln: 4096) 1.611 1.274\nTable 10: MLM performance on held-out set. HotpotQA", "In all experiments we use a learning rate warmup over the first 10% steps, and linear decay of the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better quantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length at which the document are often truncated. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. GLUE The General Language Understanding Evaluation (GLUE) benchmark [92], test language models on 8 different natural language understanding tasks. We used the same training parameters as mentioned in https://github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md. Our model parameters are b = 64, g = 2 \u00d7 b, w = 3\u00d7 b, r = 3\u00d7 b ( we used the BIGBIRD-ITC base model pretrained on MLM task). We compare the performance of BIGBIRD to BERT, XLNet [101] and RoBERTa in Tab. 16. We find that even on task that have a much smaller context, our performance is competitive to full attention models. E.5 Summarization. As discussed in Sec. 4.1, given the small length of output sequence, we used sparse BIGBIRD attention only for encoder, while keeping the full attention for decoder. The number of hidden layers, number of heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed in Tab. 17. We summarize our result in Tab. 20."]}
{"pkey": "bigbird_13", "question": "Describe the computational resources used to train the model.", "answer": "For different Experiments, different type of resources are used:\n1- For MLM Pretraining\n    Two BIG BIRD base models for MLM\n    (i)   BIG BIRD-ITC: 8 \u00d7 8 TPUv3\n    (ii)  BIGB IRD-ETC: 8 \u00d7 8 TPUv3\n2- Question Answering\n    Specific coomputer resources used for each Dataset for \n    training BIG BIRD model:\n    (i)   HotpotQA: 4 \u00d7 2 TPUv3\n    (ii)  NaturalQ: 4 \u00d7 8 TPUv3\n    (iii) TriviaQA: 4 \u00d7 2 TPUv3\n    (iv)  WikiHop:  4 \u00d7 4 TPUv3\n3- Document Classiification\n    Specific coomputer resources used for each Dataset for training BIG BIRD model:\n    (i)   IMDb          : 4 \u00d7 4 TPUv3 \n    (ii)  Arxiv         : 4 \u00d7 4 TPUv3 \n    (iii) Patents       : 4 \u00d7 4 TPUv3 \n    (iv)  Hyperpartisan : 4 \u00d7 2 TPUv3 \n    (v)   Yelp-5        : 4 \u00d7 8 TPUv3 \n4- Summarization\n    Two BIG BIRD base models for Summarization\n    (i)   Base: BIGBIRD -RoBERTa - 4 \u00d7 4 TPUv3\n    (iii) Large:BIGBIRD -Pegasus - 4 \u00d7 8 TPUv3", "title": "Big Bird: Transformers for Longer Sequences", "context": ["In this section our goal is to showcase benefits of modeling longer input sequence for NLP tasks, for which we select three representative tasks. We begin with basic masked language modeling (MLM; Devlin et al. 22) to check if better contextual representations can be learnt by utilizing longer contiguous sequences. Next, we consider QA with supporting evidence, for which capability to handle longer sequence would allow us to retrieve more evidence using crude systems like TF-IDF/BM25. Finally, we tackle long document classification where discriminating information may not be located in first 512 tokens. Below we summarize the results for BIGBIRD using sequence length 40961, while we defer all other setup details including computational resources, batch size, step size, to App. E.\nPretraining and MLM We follow [22, 63] to create base and large versions of BIGBIRD and pretrain it using MLM objective. This task involves predicting a random subset of tokens which have been masked out. We use four standard data-sets for pretraining (listed in App. E.1, Tab. 9), warm-starting from the public RoBERTa checkpoint2. We compare performance in predicting the masked out tokens in terms of bits per character, following [8]. As seen in App. E.1, Tab. 10, both BIGBIRD and Longformer perform better than limited length RoBERTa, with BIGBIRD-ETC performing the best. We note that we trained our models on a reasonable 16GB memory/chip with batch size of 32-64. Our memory efficiency is due to efficient blocking and sparsity structure of the sparse attention mechanism described in Sec. 2.\nQuestion Answering (QA) We considered following four challenging datasets:\n1. Natural Questions [52]: For the given question, find a short span of answer (SA) from the given evidences as well highlight the paragraph from the given evidences containing information about the correct answer (LA). 2. HotpotQA-distractor", "[71] for each promoter region example, a negative example (non-promoter sequences) with the same size of the positive one is constructed as follow: The positive sequence is divided into 20 subsequences. Then, 12 subsequences are picked randomly and substituted randomly. The remaining 8 subsequences are conserved. This process is illustrated in Figure 1 of [71]. Applying this process to the positive set results in new non-promoter sequences with conserved parts from promoter sequences (the unchanged subsequences, 8 subsequences out of 20). These parameters enable generating a negative set that has 32 and 40% of its sequences containing conserved portions of promoter sequences. We prefix and append each example with [CLS] and [SEP] token respectively. The output corresponding to the [CLS] token from BIGBIRD transformer encoder is fed to a simple binary classification layer. We fine-tune the pretrained BIGBIRD from App. F.1 using hyper-parameters described in Tab. 21. We note that high performance is not surprising due to the overlap in the nature of negative example generation and MLM pretraining. F.3 Chromatin-Profile Prediction. The first step of sequence-based algorithmic framework for predicting non-coding effects is to build a model to predict, large scale chromatic profile [109]. In this paper, we use the dataset provided in\nZhou and Troyanskaya [109]8, to train BIGBIRD to predict the chromatic profile. Each training sample consists of a 8,000-bp sequence from the human GRCh37 reference genome centered on each 200-bp bin and is paired with a label vector for 919 chromatin features. As before, we prefix and append each example with [CLS] and [SEP] token respectively. The output corresponding to the [CLS] token from BIGBIRD transformer encoder is fed to a linear layer with 919 heads. Thus we jointly predict the 919 independent binary classification problems. We fine-tune the pretrained BIGBIRD from App. F.1 using hyper-parameters described in Tab. 21.", "During training the model gets as input pairs of sequences separated by [SEP] token along with a [CLS] token at the start. Overall the input pattern is: [CLS] sequence A [SEP] sequence B [SEP]. For 50% of the time the second sequence comes from true sequence after the first one. Remaining 50% of the time it is a a random sequence from the full dataset. The model is then required to predict this relationship using the output corresponding to the [CLS] token, which is fed into a simple binary classification layer. 7https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39\nThe sequence of steps is visually elaborated in Fig. 9. The model is trained with both MLM and NSP together. Training hyperparameter is provided in second columns of Tab. 21. In all experiments we use a learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We additionally performed a simple ablation study to validate the hypothesis, that similar to NLP, having a larger context improves performance. We use MLM task described above to test how BIGBIRD performed with sequences of different length. Accuracy on MLM task with increasing sequence length is shown in Fig. 8. Not only longer context improves final accuracy, it also leads to faster learning, as we have now more opportunities for masking. F.2 Promoter Region Prediction. The promoter region plays an important role in transcription initiation and thus its recognition is an important area of interest in the field of bioinformatics. Following Oubounyt et al. [71], we use datasets from Eukaryotic Promoter Database (EPDnew) [24], which contains 29,597 promoter region in the human genome. Around the transcription start site (TSS), we extract a sequence of 8000 bp (-5000 +3000 bp) from the human reference genome GRCh37. Since EPDnew uses newer GRCh38, we convert to GRCh37 coordinates using LiftOver [44]. Following Oubounyt et al."]}
{"pkey": "bigbird_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "The paper has provided all details of proposed architecture, pretraining, training setup,  and evaluation metrics used, hyperparameters tuning along with the results obtained  except the preprocessing step for dataset. Code is available on github and link is mentioned in paper:  https://github.com/google-research/bigbird/tree/master/bigbird/core", "title": "Big Bird: Transformers for Longer Sequences", "context": ["By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?", "Classification We experiment on datasets of different lengths and contents, specifically various document classification and GLUE tasks. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there is improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. 4.1 Encoder-Decoder Tasks. For an encoder-decoder setup, one can easily see that both suffer from quadratic complexity due to the full self attention. We focus on introducing the sparse attention mechanism of BIGBIRD only at the encoder side. This is because, in practical generative applications, the length of output sequence is typically small as compared to the input. For example for text summarization, we see in realistic scenarios (c.f. App. E.5 Tab. 18) that the median output sequence length is \u223c 200 where as the input\nsequence\u2019s median length is > 3000. For such applications, it is more efficient to use sparse attention mechanism for the encoder and full self-attention for the decoder. Summarization Document summarization is a task of creating a short and accurate summary of a text document. We used three long document datasets for testing our model details of which are mention in Tab. 18. In this paper we focus on abstractive summarization of long documents where using a longer contextual encoder should improve performance."]}
{"pkey": "bigbird_15", "question": "What is the pretraining objective of the model? ", "answer": "The paper has mentioned two pretraining tasks:\n1- Masked Language Model (MLM):  In order to train a deep bidirectional representation, BERT training introduces the MLM task, where the paper authors simply mask out 15% of the input tokens at random, and then predict those masked tokens.\n2- Next Sentence Prediction (NSP): In order to understand relationship between two sequences, BERT training introduces the NSP task, where the paper authors predict if a given pair of sequences are contiguous or not. During training the model gets as input pairs of sequences separated by [SEP] token along with a [CLS] token at the start.", "title": "Big Bird: Transformers for Longer Sequences", "context": ["The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?", "As explored in Liang [58], instead of operating on base pairs, we propose to first segment DNA into tokens so as to further increase the context length (App. F, Fig. 7). In particular, we build a byte-pair encoding [50] table for the DNA sequence of size 32K, with each token representing 8.78 base pairs on average. We learn contextual representation of these token on the human reference genome (GRCh37)3 using MLM objective. We then report the bits per character (BPC) on a held-out set in Tab. 5. We find that attention\nbased contextual representation of DNA does improve BPC, which is further improved by using longer context. Promoter Region Prediction Promoter is a DNA region typically located upstream of the gene, which is the site of transcription initiation. Multiple methods have been proposed to identify the promoter regions in a given DNA sequence [99, 59, 11, 98, 71], as it is an important first step in understanding gene regulation. The corresponding machine learning task is to classify a given DNA fragment as promoter or non-promoter sequence. We use the dataset compiled by Oubounyt et al. [71] which was built from Eukaryotic Promoter Database (EPDnew) [24] 4. We finetuned\nthe pretrained BIGBIRD model from above, using the training data and report F1 on test dataset. We compare our results to the previously reported best method in Tab. 6. We see that BIGBIRD achieve nearly perfect accuracy with a 5% jump from the previous best reported accuracy. 3https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.13/ 4https://epd.epfl.ch/human/human_database.php?db=human\nChromatin-Profile Prediction Non-coding regions of DNA do not code for proteins. Majority of diseases and other trait associated single-nucleotide polymorphism are correlated to non-coding genomic variations [109, 46]. Thus, understanding the functional effects of non-coding regions of DNA is a very important task.", "By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al."]}
{"pkey": "bigbird_16", "question": "What is the loss function that is used to train the model?", "answer": "The paper didn't mentioned anything about loss function. But it mentioned  about pretraining tasks : Masked Language Model (MLM) and Next Sentence Prediction (NSP)", "title": "Big Bird: Transformers for Longer Sequences", "context": ["The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?", "By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "As explored in Liang [58], instead of operating on base pairs, we propose to first segment DNA into tokens so as to further increase the context length (App. F, Fig. 7). In particular, we build a byte-pair encoding [50] table for the DNA sequence of size 32K, with each token representing 8.78 base pairs on average. We learn contextual representation of these token on the human reference genome (GRCh37)3 using MLM objective. We then report the bits per character (BPC) on a held-out set in Tab. 5. We find that attention\nbased contextual representation of DNA does improve BPC, which is further improved by using longer context. Promoter Region Prediction Promoter is a DNA region typically located upstream of the gene, which is the site of transcription initiation. Multiple methods have been proposed to identify the promoter regions in a given DNA sequence [99, 59, 11, 98, 71], as it is an important first step in understanding gene regulation. The corresponding machine learning task is to classify a given DNA fragment as promoter or non-promoter sequence. We use the dataset compiled by Oubounyt et al. [71] which was built from Eukaryotic Promoter Database (EPDnew) [24] 4. We finetuned\nthe pretrained BIGBIRD model from above, using the training data and report F1 on test dataset. We compare our results to the previously reported best method in Tab. 6. We see that BIGBIRD achieve nearly perfect accuracy with a 5% jump from the previous best reported accuracy. 3https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.13/ 4https://epd.epfl.ch/human/human_database.php?db=human\nChromatin-Profile Prediction Non-coding regions of DNA do not code for proteins. Majority of diseases and other trait associated single-nucleotide polymorphism are correlated to non-coding genomic variations [109, 46]. Thus, understanding the functional effects of non-coding regions of DNA is a very important task."]}
{"pkey": "bigbird_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The BIG BIRD model using the generalised attention mechanism that is used in each layer of transformer operating on an input sequence. The generalized attention mechanism is described by a directed graph. If D is the complete digraph, the paper authors recover the full quadratic attention mechanism. This view of self-attention as a fully connected graph allows us to exploit existing graph theory to help reduce its complexity. The problem of reducing the quadratic complexity of self-attention can now be seen as a graph\nsparsification problem. It is well-known that random graphs are expanders and can approximate complete graphs in a number of different contexts including in their spectral properties. We believe sparse random graph for attention mechanism should have two desiderata: small average path length between nodes and a notion of locality The three different building components of BIGBIRD are defined on the block matrix. In particular the three different components are:\n1. Random attention: Each query block attends to r random key blocks. In Fig. 3a, r = 1 with block size 2. This implies that each query block of size 2 randomly attends to a key block of size 2.\n2. Window local attention: While creating the block, the paper authors ensure that the number of query blocks and the number of key blocks are the same. This helps us in defining the block window attention. Every query block with index j attends to key block with index j \u2212 (w \u2212 1)/2 to j + (w \u2212 1)/2, including key block j. In Fig. 3b, w = 3 with block size 2. It means that each query block j (size 2 queries) attends to key block j \u2212 1, j, j + 1.\n3. Global attention: Global attention remains the same as defined in Sec. 2, but the paper authors compute it in terms of blocks. In Fig. 3c, g = 1 with block size 2. For BIG BIRD -ITC this implies that one query and key block, attend to everyone. The final piece of BIG B IRD is inspired from our theoretical analysis, which is critical for empirical performance. More specifically, our theory utilizes the importance of \u201cglobal tokens\u201d (tokens that attend to all tokens in the sequence and to whom all tokens attend to. The final attention mechanism for BIG BIRD has all three of these properties: queries attend to r random keys, each query attends to w/2 tokens to the left of its location and w/2 to the right of its location and they contain g global tokens (The global tokens can be from existing tokens or extra added tokens).", "title": "Big Bird: Transformers for Longer Sequences", "context": ["The idea of global tokens was used extensively by them to achieve their goals. Our theoretical work can be seen as providing a justification for the success of these models as well. It is important to note that most of the aforementioned methods are heuristic based and empirically are not as versatile and robust as the original transformer, i.e. the same architecture do not attain SoTA on multiple standard benchmarks. (There is one exception of Longformer which we include in all our comparisons, see App. E.3 for a more detailed comparison). Moreover, these approximations do not come with theoretical guarantees. 2 BIGBIRD Architecture. In this section, we describe the BIGBIRD model using the generalised attention mechanism that is used in each layer of transformer operating on an input sequence X = (x1, ...,xn) \u2208 Rn\u00d7d. The generalized attention mechanism is described by a directed graph D whose vertex set is [n] = {1, . . . , n}. The set of arcs (directed edges) represent the set of inner products that the attention mechanism will consider. Let N(i) denote the out-neighbors set of node i in D, then the ith output vector of the generalized attention mechanism is defined as\nATTND(X)i = xi + H\u2211\nh=1\n\u03c3 ( Qh(xi)Kh(XN(i)) T ) \u00b7 Vh(XN(i)) (AT)\nwhere Qh,Kh : Rd \u2192 Rm are query and key functions respectively, Vh : Rd \u2192 Rd is a value function, \u03c3 is a scoring function (e.g. softmax or hardmax) and H denotes the number of heads. Also note XN(i) corresponds to the matrix formed by only stacking {xj : j \u2208 N(i)} and not all the inputs. If D is the complete digraph, we recover the full quadratic attention mechanism of Vaswani et al. [91]. To simplify our exposition, we will operate on the adjacency matrix A of the graph D even though the underlying graph maybe sparse. To elaborate, A \u2208 [0, 1]n\u00d7n with A(i, j) = 1 if query i attends to key j and is zero otherwise. For example, when A is the ones matrix (as in BERT), it leads to quadratic complexity, since all tokens attend on every other token.", "By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "An important step in this process, as defined by Zhou and Troyanskaya [109], is to predict large-scale chromatin-profiling from\nnon-coding genomic sequence. To this effect, DeepSea [109], compiled 919 chromatin-profile of 2.4M non-coding variants from Encyclopedia of DNA Elements (ENCODE)5 and Roadmap Epigenomics projects6. The corresponding ML task is to predict, for a given non-coding region of DNA, these 919 chromatin-profile including 690 transcription factors (TF) binding profiles for 160 different TFs, 125 DNase I sensitivity (DHS) profiles and 104 histone-mark (HM) profiles. We jointly learn 919 binary classifiers to predict these functional effects from sequence of DNA fragments. On held-out chromosomes, we compare AUC with the baselines in Tab. 7 and see that we significantly improve on performance on the harder task HM, which is known to have longer-range correlations [27] than others. 6 Conclusion. We propose BIGBIRD: a sparse attention mechanism that is linear in the number of tokens. BIGBIRD satisfies a number of theoretical results: it is a universal approximator of sequence to sequence functions and is also Turing complete. Theoretically, we use the power of extra global tokens preserve the expressive powers of the model. We complement these results by showing that moving to sparse attention mechanism do incur a cost. Empirically, BIGBIRD gives state-of-the-art performance on a number of NLP tasks such as question answering and long document classification. We further introduce attention based contextual language model for DNA and fine-tune it for down stream tasks such as promoter region prediction and predicting effects of non-coding variants. Big Bird: Transformers for Longer Sequences \u2013 Appendix. A Universal Approximators. A.1 Notation. We begin by setting up some notations following P\u00e9rez et al. [72] to formally describe the complete architecture of Transformers."]}
{"pkey": "bigbird_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "For Natural Language Processing tasks,Authors select three representative tasks. They begin with basic masked language modeling. to check if better  contextual representations can be learnt by utilizing longer  contiguous sequences. Next, the paper authors consider QA with supporting evidence,  for which capability to  handle longer sequence would allow us to retrieve more evidence using crude systems like TF-IDF/BM25. The authors conducted experiment on QA task and published QA Dev  results using Base size models. The paper authors report accuracy for WikiHop  and F1 for HotpotQA, Natural Questions, and TriviaQA. Fine-tuning performed on BIGBIRD and their results on Test set  for QA tasks. The Test results (F1 for HotpotQA, Natural Questions, TriviaQA, and Accuracy for WikiHop) have been picked from their respective leaderboard. For each task the top-3 leaders  were picked not including BIG BIRD-etc. For Natural Questions Long Answer (LA), TriviaQA, and WikiHop, BIG BIRD-ETC is the new  state-of-the-art. On HotpotQA BIGBIRD is on third in the leaderboard  by F1 and second by Exact Match (EM). For classification task, the authors did  experiment on datasets  of different lengths and contents, specifically various document  classification and GLUE tasks. Following BERT, the paper authors used one layer with cross entropy loss on top of the first [CLS] token. The paper authors see  that gains of using B IG B IRD are more significant when the paper authors have  longer documents and fewer training examples. For instance, using  base sized model, BIG BIRD improves state-of-the-art for Arxiv dataset  by about 5% points. On Patents dataset, there is improvement over using  simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant.  Note that this performance gain is not seen for much smaller IMDb dataset. For an encoder-decoder, the authors focused  on introducing the  sparse attention mechanism of BIG B IRD only at the encoder side.  This is because, in practical generative applications, the length  of output sequence is typically small as compared to the input.  For example for text summarization. The experiment shows that  the median output sequence length is \u223c 200 where as the input sequence\u2019s median length is > 3000. For such applications, it is  more efficient to use sparse attention mechanism for the encoder  and full self-attention for the decoder. For Summarization task, the authors used three long document  datasets for testing BIG BIRD. Here they  focus on abstractive  summarization of long documents where using a longer contextual  encoder should improve performance. The results clearly show modeling  longer context brings significant improvement among various models such as RoBERTa, Pegasus etc. The authors also fine-tuned the model on the CNN/Daily Mail dataset to generate summaries of news articles.  The input to the model is a news article, and the expected output  is a summary. The evaluation metric used is ROUGE.", "title": "Big Bird: Transformers for Longer Sequences", "context": ["[100]: Similar to natural questions, it requires finding the answer (Ans) as well as the supporting facts (Sup) over different documents needed for multi-hop reasoning from the given evidences. 3. TriviaQA-wiki [41]: We need to provide an answer for the given question using provided Wikipedia evidence, however, the answer might not be present in the given evidence. On a\n1code available at http://goo.gle/bigbird-transformer 2https://github.com/pytorch/fairseq/tree/master/examples/roberta\nsmaller verified subset of question, the given evidence is guaranteed to contain the answer. Nevertheless, we model the answer as span selection problem in this case as well. 4. WikiHop [95]: Chose correct option from multiple-choice questions (MCQ), by aggregating information spread across multiple documents given in the evidences. As these tasks are very competitive, multiple highly engineered systems have been designed specific each dataset confirming to respective output formats. For a fair comparison, we had to use some additional regularization for training BIGBIRD, details of which are provided in App. E.2 along with exact architecture description. We experiment using the base sized model and select the best configuration on the development set for each dataset (as reported in Tab. 2). We can see that BIGBIRD-ETC, with expanded global tokens consistently outperforms all other models. Thus, we chose this configuration to train a large sized model to be used for evaluation on the hidden test set. In Tab. 3, we compare BIGBIRD-ETC model to top-3 entries from the leaderboard excluding BIGBIRD. One can clearly see the importance of using longer context as both Longformer and BIGBIRD outperform models with smaller contexts. Also, it is worth noting that BIGBIRD submission is a single model, whereas the other top-3 entries for Natural Questions are ensembles, which might explain the slightly lower accuracy in exact answer phrase selection.", "By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?"]}
{"pkey": "bigbird_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "The authors performed a simple ablation study to validate the hypothesis, that similar to NLP, having a larger context improves performance. They use MLM task described above to test how BIGBIRD  performed with sequences of different length. Accuracy on MLM task  with increasing sequence length. Not only longer context improves  final accuracy, it also leads to faster learning, as the paper authors have now \nmore opportunities for masking.", "title": "Big Bird: Transformers for Longer Sequences", "context": ["During training the model gets as input pairs of sequences separated by [SEP] token along with a [CLS] token at the start. Overall the input pattern is: [CLS] sequence A [SEP] sequence B [SEP]. For 50% of the time the second sequence comes from true sequence after the first one. Remaining 50% of the time it is a a random sequence from the full dataset. The model is then required to predict this relationship using the output corresponding to the [CLS] token, which is fed into a simple binary classification layer. 7https://www.ncbi.nlm.nih.gov/assembly/GCF_000001405.39\nThe sequence of steps is visually elaborated in Fig. 9. The model is trained with both MLM and NSP together. Training hyperparameter is provided in second columns of Tab. 21. In all experiments we use a learning rate warmup over the first 10,000 steps, and linear decay of the learning rate. We additionally performed a simple ablation study to validate the hypothesis, that similar to NLP, having a larger context improves performance. We use MLM task described above to test how BIGBIRD performed with sequences of different length. Accuracy on MLM task with increasing sequence length is shown in Fig. 8. Not only longer context improves final accuracy, it also leads to faster learning, as we have now more opportunities for masking. F.2 Promoter Region Prediction. The promoter region plays an important role in transcription initiation and thus its recognition is an important area of interest in the field of bioinformatics. Following Oubounyt et al. [71], we use datasets from Eukaryotic Promoter Database (EPDnew) [24], which contains 29,597 promoter region in the human genome. Around the transcription start site (TSS), we extract a sequence of 8000 bp (-5000 +3000 bp) from the human reference genome GRCh37. Since EPDnew uses newer GRCh38, we convert to GRCh37 coordinates using LiftOver [44]. Following Oubounyt et al.", "By carefully applying the operator to a set of chosen ranges, we will show that each column will contain a unique mapping of the full mapping. Therefore, we can augment the loss of inner-products in the self attention mechanism by using multiple layers and an auxiliary global token. Step 3: Approximate modified transformers by original Transformers: The final step is to approximate the modified transformers by the original transformer which uses ReLU and softmax. We provide the full details in App. A.\n3.3 Turing Completeness. Transformers are a very general class. In the original paper of Vaswani et al. [91], they were used in both an encoder and a decoder. While the previous section outlined how powerful just the encoders were, another natural question is to ask what the additional power of both a decoder along with an encoder is? P\u00e9rez et al. [72] showed that the full transformer based on a quadratic attention mechanism is Turing Complete. This result makes one unrealistic assumption, which is that the model works on arbitrary precision model. Of course, this is necessary as otherwise, Transformers are bounded finite state machines and cannot be Turing Complete. It is natural to ask if the full attention mechanism is necessary. Or can a sparse attention mechanism also be used to simulate any Turing Machine? We show that this is indeed the case: we can use a sparse encoder and sparse decoder to simulate any Turing Machine. To use the sparse attention mechanism in the transformer architecture, we need to define a suitable modification where each token only reacts to previous tokens. Unlike the case for BERT, where the entire attention mechanism is applied once, in full transformers, the sparse attention mechanism at decoder side is used token by token. Secondly the work of P\u00e9rez et al. [72], uses each token as a representation of the tape history and uses the full attention to move and retrieve the correct tape symbol. Most of the construction of P\u00e9rez et al.", "The self-attention mechanism overcomes constraints of RNNs (namely the sequential nature of RNN) by allowing each token in the input sequence to attend independently to every other token in the sequence. This design choice has several interesting repercussions. In particular, the full self-attention have computational and memory requirement that is quadratic in the sequence length. We note that while the corpus can be large, the sequence length, which provides the context in many applications is very limited. Using commonly available current hardware and model sizes, this requirement\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\nar X\niv :2\n00 7.\n14 06\n2v 2\n[ cs\n.L G\n] 8\nJ an\n2 02\ntranslates to roughly being able to handle input sequences of length 512 tokens. This reduces its direct applicability to tasks that require larger context, like QA [60], document classification, etc. However, while we know that self-attention and Transformers are useful, our theoretical understanding is rudimentary. What aspects of the self-attention model are necessary for its performance? What can we say about the expressivity of Transformers and similar models? Apriori, it was not even clear from the design if the proposed self-attention mechanism was as effective as RNNs. For example, the self-attention does not even obey sequence order as it is permutation equivariant. This concern has been partially resolved, as Yun et al. [104] showed that transformers are expressive enough to capture all continuous sequence to sequence functions with a compact domain. Meanwhile, P\u00e9rez et al. [72] showed that the full transformer is Turing Complete (i.e. can simulate a full Turing machine). Two natural questions arise: Can we achieve the empirical benefits of a fully quadratic self-attention scheme using fewer inner-products? Do these sparse attention mechanisms preserve the expressivity and flexibility of the original network?"]}
{"pkey": "bigbird_20", "question": "List the future work mentioned in the paper.", "answer": "The paper doesn't mentioned any such future works", "title": "Big Bird: Transformers for Longer Sequences", "context": ["In all experiments we use a learning rate warmup over the first 10% steps, and linear decay of the learning rate and detail list of remaining hyperparameters are provided in Tab. 14. For better quantitative evaluation, we compute the fraction of the dataset that exceeds 512 tokens, i.e. the length at which the document are often truncated. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there\nis improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. GLUE The General Language Understanding Evaluation (GLUE) benchmark [92], test language models on 8 different natural language understanding tasks. We used the same training parameters as mentioned in https://github.com/pytorch/fairseq/blob/master/ examples/roberta/README.glue.md. Our model parameters are b = 64, g = 2 \u00d7 b, w = 3\u00d7 b, r = 3\u00d7 b ( we used the BIGBIRD-ITC base model pretrained on MLM task). We compare the performance of BIGBIRD to BERT, XLNet [101] and RoBERTa in Tab. 16. We find that even on task that have a much smaller context, our performance is competitive to full attention models. E.5 Summarization. As discussed in Sec. 4.1, given the small length of output sequence, we used sparse BIGBIRD attention only for encoder, while keeping the full attention for decoder. The number of hidden layers, number of heads, and hidden dimension is same for encoder and decoder. The hyperparameters are detailed in Tab. 17. We summarize our result in Tab. 20.", "Lastly, we use a dense layer that takes in the output vector corresponding to a candidate answer, and predicts a score for the current candidate to be the correct answer. We apply this dense layer to each candidate independently and the candidate with the best score is picked as our final answer. It is worthwhile to note that explicitly designed attention connection in ETC works slightly better, the random connection based ITC is pretty competative. E.3 Relationship to Contemporary Work. Longformer Child et al. [16] introduced localized sliding window to reduce computation. A more recent version, which includes localized sliding windows and global tokens was introduced independently by Longofrmer[8]. Although BIGBIRD contains additional random tokens, there are also differences in the way global and local tokens are realized. In particular even when there is no random token, as used to get SoTA in question answering, there are two key differences between Longformer and BIGBIRD-etc (see [4]):\n1. We use global-local attention with relative position encodings enables it to better handle structured inputs\n2. Unlike Longformer, we train the global tokens using CPC loss and learn their use during finetuning. E.4 Classification. We try two types of classification task. Document classification We experiment on datasets of different lengths and contents, as listed in Tab. 15. In particular, we look at sentiment analysis (IMDb [64] and Yelp-5 [108]) task and topic\nassignment (Arxiv [35], Patents [53], and Hyperpartisan [47]) task. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token from the BIGBIRD encoder consuming 4096 tokens. We report the results of document classification experiments in Tab. 15. We compare against state-of-the-art (SoTA) methods for each dataset and plain RoBERTa model with 512 tokens truncation.", "Classification We experiment on datasets of different lengths and contents, specifically various document classification and GLUE tasks. Following BERT, we used one layer with cross entropy loss on top of the first [CLS] token. We see that gains of using BIGBIRD are more significant when we have longer documents and fewer training examples. For instance, using base sized model, BIGBIRD improves state-of-the-art for Arxiv dataset by about 5% points. On Patents dataset, there is improvement over using simple BERT/RoBERTa, but given the large size of training data the improvement over SoTA (which is not BERT based) is not significant. Note that this performance gain is not seen for much smaller IMDb dataset. Along with experimental setup detail, we present detailed results in App. E.4 which show competitive performance. 4.1 Encoder-Decoder Tasks. For an encoder-decoder setup, one can easily see that both suffer from quadratic complexity due to the full self attention. We focus on introducing the sparse attention mechanism of BIGBIRD only at the encoder side. This is because, in practical generative applications, the length of output sequence is typically small as compared to the input. For example for text summarization, we see in realistic scenarios (c.f. App. E.5 Tab. 18) that the median output sequence length is \u223c 200 where as the input\nsequence\u2019s median length is > 3000. For such applications, it is more efficient to use sparse attention mechanism for the encoder and full self-attention for the decoder. Summarization Document summarization is a task of creating a short and accurate summary of a text document. We used three long document datasets for testing our model details of which are mention in Tab. 18. In this paper we focus on abstractive summarization of long documents where using a longer contextual encoder should improve performance."]}
{"pkey": "mobilebert_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "The problem here is to use BERT like model in  resource-limited devices like mobiles with maintaining the accuracy as high as BERT Large.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["As shown in Table 1, the hidden dimension of each building block is only 128. On the other hand, we introduce two linear transformations for each building block to adjust its input and output dimensions to 512. Following the terminology in (He et al., 2016), we refer to such an architecture as bottleneck. It is challenging to train such a deep and thin network. To overcome the training issue, we first construct a teacher network and train it until convergence, and then conduct knowledge transfer from this teacher network to MobileBERT. We find that this is much better than directly training MobileBERT from scratch. Various training strategies will be discussed in a later section. Here, we introduce the architecture design of the teacher network which is illustrated in Figure 1(b). In fact, the teacher network is just BERTLARGE while augmented with inverted-bottleneck structures (Sandler et al., 2018) to adjust its feature map size to 512. In what follows, we refer to the teacher network as IB-BERTLARGE. Note that IB-BERT and MobileBERT have the same feature map size which is 512. Thus, we can directly compare the layerwise output difference between IB-BERT and MobileBERT. Such a direct comparison is needed in our knowledge transfer strategy. It is worth pointing out that the simultaneously introduced bottleneck and inverted-bottleneck structures result in a fairly flexible architecture design. One may either only use the bottlenecks for MobileBERT (correspondingly the\nteacher becomes BERTLARGE) or only the invertedbottlenecks for IB-BERT (then there is no bottleneck in MobileBERT) to align their feature maps. However, when using both of them, we can allow IB-BERTLARGE to preserve the performance of BERTLARGE while having MobileBERT sufficiently compact. 3.2 Stacked Feed-Forward Networks. A problem introduced by the bottleneck structure of MobileBERT is that the balance between the Multi-Head Attention (MHA) module and the FeedForward Network (FFN) module is broken.", "MobileBERT can\nenable various NLP applications7 to be easily deployed on mobile devices. In this paper, we show that 1) it is crucial to keep MobileBERT deep and thin, 2) bottleneck/invertedbottleneck structures enable effective layer-wise knowledge transfer, and 3) progressive knowledge transfer can efficiently train MobileBERT. We believe our findings are generic and can be applied to other model compression problems. Appendix for \u201cMobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\u201d. A Extra Related Work on Knowledge Transfer. Exploiting knowledge transfer to compress model size was first proposed by Bucilu et al. (2006). The idea was then adopted in knowledge distillation (Hinton et al., 2015), which requires the smaller student network to mimic the class distribution output of the larger teacher network. Fitnets (Romero et al., 2014) make the student mimic the intermediate hidden layers of the teacher to train narrow\nand deep networks. Luo et al. (2016) show that the knowledge of the teacher can also be obtained from the neurons in the top hidden layer. Similar to our proposed progressive knowledge transfer scheme, Yeo et al. (2018) proposed a sequential knowledge transfer scheme to distill knowledge from a deep teacher into a shallow student in a sequential way. Zagoruyko and Komodakis (2016) proposed to transfer the attention maps of the teacher on images. Li et al. (2019) proposed to transfer the similarity of hidden states and word alignment from an autoregressive Transformer teacher to a non-autoregressive student. B Extra Related Work on Compact Architecture Design. While much recent research has focused on improving efficient Convolutional Neural Networks (CNN) for mobile vision applications (Iandola et al., 2016; Howard et al., 2017; Zhang et al., 2017, 2018; Sandler et al., 2018; Tan et al., 2019; Howard et al., 2019), they are usually tailored for CNN.", "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE). MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning."]}
{"pkey": "mobilebert_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "1)BERT suffers from the heavy model size and high latency, making it impractical for resource-limited mobile devices to deploy the power of BERT in mobile-based machine translation, dialogue modeling, and the like.\n2) There is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a model that can be generically fine-tuned on different downstream NLP tasks as the original BERT does.\n3)In this paper, the paper authors propose MobileBERT to fill this gap.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["Turc et al. (2019) propose to pre-train the smaller BERT models to improve task-specific knowledge distillation. Tang et al. (2019) distill BERT into an extremely small LSTM model. Tsai et al. (2019) distill a multilingual BERT into smaller BERT models on sequence labeling tasks. Clark et al. (2019b) use several single-task BERT\n1The code and pre-trained models are available at https://github.com/google-research/ google-research/tree/master/mobilebert.\nmodels to teach a multi-task BERT. Liu et al. (2019a) distill knowledge from an ensemble of BERT models into a single BERT. Concurrently to our work, Sun et al. (2019) distill BERT into shallower students through knowledge distillation and an additional knowledge transfer of hidden states on multiple intermediate layers. Jiao et al. (2019) propose TinyBERT, which also uses a layer-wise distillation strategy for BERT but in both pre-training and fine-tuning stages. Sanh et al. (2019) propose DistilBERT, which successfully halves the depth of BERT model by knowledge distillation in the pre-training stage and an optional fine-tuning stage. In contrast to these existing literature, we only use knowledge transfer in the pre-training stage and do not require a fine-tuned teacher or data augmentation (Wu et al., 2019) in the down-stream tasks. Another key difference is that these previous work try to compress BERT by reducing its depth, while we focus on compressing BERT by reducing its width, which has been shown to be more effective (Turc et al., 2019). 3 MobileBERT. In this section, we present the detailed architecture design of MobileBERT and training strategies to efficiently train MobileBERT. The specific model settings are summarized in Table 1. These settings are obtained by extensive architecture search experiments which will be presented in Section 4.1.\n3.1 Bottleneck and Inverted-Bottleneck. The architecture of MobileBERT is illustrated in Figure 1(c). It is as deep as BERTLARGE, but each building block is made much smaller.", "From Table 7, we can see that both NoNorm and relu are very effective in reducing the latency of MobileBERT, while the two operational optimizations do not reduce FLOPS. This reveals the gap between the real-world inference latency and the theoretical computation overhead (i.e., FLOPS). 4.6.2 Training Strategies. We also study how the choice of training strategy, i.e., auxiliary knowledge transfer, joint knowledge transfer, and progressive knowledge transfer, can affect the performance of MobileBERT. As shown\nin Table 8, progressive knowledge transfer consistently outperforms the other two strategies. We notice that there is a significant performance gap between auxiliary knowledge transfer and the other two strategies. We think the reason is that the intermediate layer-wise knowledge (i.e., attention maps and feature maps) from the teacher may not be optimal for the student, so the student needs an additional pre-training distillation stage to fine-tune its parameters. 4.6.3 Training Objectives. We finally conduct a set of ablation experiments with regard to Attention Transfer (AT), Feature Map Transfer (FMT) and Pre-training Distillation (PD). The operational OPTimizations (OPT) are removed in these experiments to make a fair comparison between MobileBERT and the original BERT. The results are listed in Table 9. We can see that the proposed Feature Map Transfer contributes most to the performance improvement of MobileBERT, while Attention Transfer and Pre-training Distillation also play positive roles. We can also find that our IB-BERTLARGE teacher is as powerful as the original IB-BERTLARGE while MobileBERT degrades greatly when compared to its teacher. So we believe that there is still a big room in the improvement of MobileBERT. 5 Conclusion. We have presented MobileBERT which is a taskagnostic compact variant of BERT. Empirical results on popular NLP benchmarks show that MobileBERT is comparable with BERTBASE while being much smaller and faster.", "Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE). 1 Introduction. The NLP community has witnessed a revolution of pre-training self-supervised models. These models usually have hundreds of millions of parameters (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)\n\u2217This work was done when the first author was an intern at Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP, BERT suffers from the heavy model size and high latency, making it impractical for resource-limited mobile devices to deploy the power of BERT in mobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models (Turc et al., 2019; Tang et al., 2019; Sun et al., 2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a model that can be generically fine-tuned on different downstream NLP tasks as the original BERT does. In this paper, we propose MobileBERT to fill this gap. In practice, task-agnostic compression of BERT is desirable."]}
{"pkey": "mobilebert_3", "question": "What are the main contributions of the paper?", "answer": "1)The proposal of MobileBERT, task-agnostic and a compact and efficient version of BERT that is specifically designed for resource-limited devices.\n2)They used knowledge transfer in the pre-training stage and do not require a fine-tuned teacher or data augmentation  in the down-stream tasks.\n3) Another key difference is that these previous work try to compress BERT by reducing its depth, while we focus on compressing BERT by reducing its width.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["From Table 7, we can see that both NoNorm and relu are very effective in reducing the latency of MobileBERT, while the two operational optimizations do not reduce FLOPS. This reveals the gap between the real-world inference latency and the theoretical computation overhead (i.e., FLOPS). 4.6.2 Training Strategies. We also study how the choice of training strategy, i.e., auxiliary knowledge transfer, joint knowledge transfer, and progressive knowledge transfer, can affect the performance of MobileBERT. As shown\nin Table 8, progressive knowledge transfer consistently outperforms the other two strategies. We notice that there is a significant performance gap between auxiliary knowledge transfer and the other two strategies. We think the reason is that the intermediate layer-wise knowledge (i.e., attention maps and feature maps) from the teacher may not be optimal for the student, so the student needs an additional pre-training distillation stage to fine-tune its parameters. 4.6.3 Training Objectives. We finally conduct a set of ablation experiments with regard to Attention Transfer (AT), Feature Map Transfer (FMT) and Pre-training Distillation (PD). The operational OPTimizations (OPT) are removed in these experiments to make a fair comparison between MobileBERT and the original BERT. The results are listed in Table 9. We can see that the proposed Feature Map Transfer contributes most to the performance improvement of MobileBERT, while Attention Transfer and Pre-training Distillation also play positive roles. We can also find that our IB-BERTLARGE teacher is as powerful as the original IB-BERTLARGE while MobileBERT degrades greatly when compared to its teacher. So we believe that there is still a big room in the improvement of MobileBERT. 5 Conclusion. We have presented MobileBERT which is a taskagnostic compact variant of BERT. Empirical results on popular NLP benchmarks show that MobileBERT is comparable with BERTBASE while being much smaller and faster.", "In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark.", "As shown in Table 1, the hidden dimension of each building block is only 128. On the other hand, we introduce two linear transformations for each building block to adjust its input and output dimensions to 512. Following the terminology in (He et al., 2016), we refer to such an architecture as bottleneck. It is challenging to train such a deep and thin network. To overcome the training issue, we first construct a teacher network and train it until convergence, and then conduct knowledge transfer from this teacher network to MobileBERT. We find that this is much better than directly training MobileBERT from scratch. Various training strategies will be discussed in a later section. Here, we introduce the architecture design of the teacher network which is illustrated in Figure 1(b). In fact, the teacher network is just BERTLARGE while augmented with inverted-bottleneck structures (Sandler et al., 2018) to adjust its feature map size to 512. In what follows, we refer to the teacher network as IB-BERTLARGE. Note that IB-BERT and MobileBERT have the same feature map size which is 512. Thus, we can directly compare the layerwise output difference between IB-BERT and MobileBERT. Such a direct comparison is needed in our knowledge transfer strategy. It is worth pointing out that the simultaneously introduced bottleneck and inverted-bottleneck structures result in a fairly flexible architecture design. One may either only use the bottlenecks for MobileBERT (correspondingly the\nteacher becomes BERTLARGE) or only the invertedbottlenecks for IB-BERT (then there is no bottleneck in MobileBERT) to align their feature maps. However, when using both of them, we can allow IB-BERTLARGE to preserve the performance of BERTLARGE while having MobileBERT sufficiently compact. 3.2 Stacked Feed-Forward Networks. A problem introduced by the bottleneck structure of MobileBERT is that the balance between the Multi-Head Attention (MHA) module and the FeedForward Network (FFN) module is broken."]}
{"pkey": "mobilebert_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. It is not specifically designed for any particular domain, and can be used for various natural language processing tasks. In the paper they didn't explicitly mention the extension of the model to other domains.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["Turc et al. (2019) propose to pre-train the smaller BERT models to improve task-specific knowledge distillation. Tang et al. (2019) distill BERT into an extremely small LSTM model. Tsai et al. (2019) distill a multilingual BERT into smaller BERT models on sequence labeling tasks. Clark et al. (2019b) use several single-task BERT\n1The code and pre-trained models are available at https://github.com/google-research/ google-research/tree/master/mobilebert.\nmodels to teach a multi-task BERT. Liu et al. (2019a) distill knowledge from an ensemble of BERT models into a single BERT. Concurrently to our work, Sun et al. (2019) distill BERT into shallower students through knowledge distillation and an additional knowledge transfer of hidden states on multiple intermediate layers. Jiao et al. (2019) propose TinyBERT, which also uses a layer-wise distillation strategy for BERT but in both pre-training and fine-tuning stages. Sanh et al. (2019) propose DistilBERT, which successfully halves the depth of BERT model by knowledge distillation in the pre-training stage and an optional fine-tuning stage. In contrast to these existing literature, we only use knowledge transfer in the pre-training stage and do not require a fine-tuned teacher or data augmentation (Wu et al., 2019) in the down-stream tasks. Another key difference is that these previous work try to compress BERT by reducing its depth, while we focus on compressing BERT by reducing its width, which has been shown to be more effective (Turc et al., 2019). 3 MobileBERT. In this section, we present the detailed architecture design of MobileBERT and training strategies to efficiently train MobileBERT. The specific model settings are summarized in Table 1. These settings are obtained by extensive architecture search experiments which will be presented in Section 4.1.\n3.1 Bottleneck and Inverted-Bottleneck. The architecture of MobileBERT is illustrated in Figure 1(c). It is as deep as BERTLARGE, but each building block is made much smaller.", "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE). MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning.", "Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE). 1 Introduction. The NLP community has witnessed a revolution of pre-training self-supervised models. These models usually have hundreds of millions of parameters (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)\n\u2217This work was done when the first author was an intern at Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP, BERT suffers from the heavy model size and high latency, making it impractical for resource-limited mobile devices to deploy the power of BERT in mobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models (Turc et al., 2019; Tang et al., 2019; Sun et al., 2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a model that can be generically fine-tuned on different downstream NLP tasks as the original BERT does. In this paper, we propose MobileBERT to fill this gap. In practice, task-agnostic compression of BERT is desirable."]}
{"pkey": "mobilebert_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE).", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["MNLI The Multi-Genre Natural Language Inference Corpus (Williams et al., 2018) is a collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment ), contradicts the hypothesis (contradiction), or neither (neutral) and is evaluated by accuracy on both matched (indomain) and mismatched (cross-domain) sections of the test data. QNLI The Question-answering NLI dataset is converted from the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). The task is to determine whether the context sentence contains the answer to the question and is evaluated by the test accuracy. RTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges (Bentivogli et al., 2009). The task is to predict whether sentences in a sentence pair are entailment and is evaluated by accuracy. WNLI The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun\n8https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs\nfrom a list of choices. We follow Devlin et al. (2018) to skip this task in our experiments, because few previous works do better than predicting the majority class for this task.", "We use a lighter MHA structure for MobileBERTTINY. As illustrated in Figure 4, in stead of using hidden states from the inter-block feature maps as inputs to MHA, we use the reduced intra-block feature maps as key, query, and values in MHA for MobileBERTTINY. This can effectively reduce the parameters in MHA modules, but might harm the model capacity. F GLUE Dataset. In this section, we provide a brief description of the tasks in the GLUE benchmark (Wang et al., 2018). CoLA The Corpus of Linguistic Acceptability (Warstadt et al., 2018) is a collection of English ac-\nceptability judgments drawn from books and journal articles on linguistic theory. The task is to predict whether an example is a grammatical English sentence and is evaluated by Matthews correlation coefficient (Matthews, 1975). SST-2 The Stanford Sentiment Treebank (Socher et al., 2013) is a collection of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence and is evaluated by accuracy. MRPC The Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) is a collection of sentence pairs automatically extracted from online news sources. They are labeled by human annotations for whether the sentences in the pair are semantically equivalent. The performance is evaluated by both accuracy and F1 score. STS-B The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5. The task is to predict these scores and is evaluated by Pearson and Spearman correlation coefficients. QQP The Quora Question Pairs8 (Chen et al., 2018) dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent and is evaluated by both accuracy and F1 score.", "In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark."]}
{"pkey": "mobilebert_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "The paper does not mention any bias or prejudice observed in the MobileBERT model", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE). 1 Introduction. The NLP community has witnessed a revolution of pre-training self-supervised models. These models usually have hundreds of millions of parameters (Peters et al., 2018; Radford et al., 2018; Devlin et al., 2018; Radford et al., 2019; Yang et al., 2019). Among these models, BERT (Devlin et al., 2018)\n\u2217This work was done when the first author was an intern at Google Brain. shows substantial accuracy improvements. However, as one of the largest models ever in NLP, BERT suffers from the heavy model size and high latency, making it impractical for resource-limited mobile devices to deploy the power of BERT in mobile-based machine translation, dialogue modeling, and the like. There have been some efforts that taskspecifically distill BERT into compact models (Turc et al., 2019; Tang et al., 2019; Sun et al., 2019; Tsai et al., 2019). To the best of our knowledge, there is not yet any work for building a taskagnostic lightweight pre-trained model, that is, a model that can be generically fine-tuned on different downstream NLP tasks as the original BERT does. In this paper, we propose MobileBERT to fill this gap. In practice, task-agnostic compression of BERT is desirable.", "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE). MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning.", "In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark."]}
{"pkey": "mobilebert_7", "question": "List the limitations of the model discussed in the paper.", "answer": "\"Our proposed MobileBERT model is task-agnostic, and it can be further fine-tuned on specific tasks and domains to achieve even better performance.\"Our MobileBERT model achieves better performance on some tasks, it may not perform as well as the full-size BERT model on other tasks. The input length of the MobileBERT model is limited to small value compared ot BERT. we used the BooksCorpus and English Wikipedia as our pre-training data.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["We use a linear combination of the original masked language modeling (MLM) loss, next sentence prediction (NSP) loss, and the new MLM Knowledge Distillation (KD) loss as our pretraining distillation loss:\nLPD = \u03b1LMLM + (1\u2212 \u03b1)LKD + LNSP , (4)\nwhere \u03b1 is a hyperparameter in (0, 1). 3.6 Training Strategies. Given the objectives defined above, there can be various combination strategies in training. We discuss three strategies in this paper. Auxiliary Knowledge Transfer In this strategy, we regard intermediate knowledge transfer as an auxiliary task for knowledge distillation. We use a single loss, which is a linear combination of knowledge transfer losses from all layers as well as the pre-training distillation loss. Joint Knowledge Transfer However, the intermediate knowledge of the IB-BERT teacher (i.e. attention maps and feature maps) may not be an optimal solution for the MobileBERT student. Therefore, we propose to separate these two loss terms, where we first train MobileBERT with all layerwise knowledge transfer losses jointly, and then further train it by pre-training distillation. Progressive Knowledge Transfer One may also concern that if MobileBERT cannot perfectly mimic the IB-BERT teacher, the errors from the lower layers may affect the knowledge transfer in the higher layers. Therefore, we propose to progressively train each layer in the knowledge transfer. The progressive knowledge transfer is divided into L stages, where L is the number of layers. Diagram of three strategies Figure 2 illustrates the diagram of the three strategies. For joint knowledge transfer and progressive knowledge transfer, there is no knowledge transfer for the beginning embedding layer and the final classifier in the layerwise knowledge transfer stage. They are copied from the IB-BERT teacher to the MobileBERT student. Moreover, for progressive knowledge transfer, when we train the `th layer, we freeze all the trainable parameters in the layers below.", "In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark.", "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning. Basically, MobileBERT is a thin version of BERTLARGE, while equipped with bottleneck structures and a carefully designed balance between self-attentions and feed-forward networks. To train MobileBERT, we first train a specially designed teacher model, an invertedbottleneck incorporated BERTLARGE model. Then, we conduct knowledge transfer from this teacher to MobileBERT. Empirical studies show that MobileBERT is 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE while achieving competitive results on well-known benchmarks. On the natural language inference tasks of GLUE, MobileBERT achieves a GLUE score of 77.7 (0.6 lower than BERTBASE), and 62 ms latency on a Pixel 4 phone. On the SQuAD v1.1/v2.0 question answering task, MobileBERT achieves a dev F1 score of 90.0/79.2 (1.5/2.1 higher than BERTBASE). MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices. Natural Language Processing (NLP) has recently achieved great success by using huge pre-trained models with hundreds of millions of parameters. However, these models suffer from heavy model sizes and high latency such that they cannot be deployed to resourcelimited mobile devices. In this paper, we propose MobileBERT for compressing and accelerating the popular BERT model. Like the original BERT, MobileBERT is task-agnostic, that is, it can be generically applied to various downstream NLP tasks via simple fine-tuning."]}
{"pkey": "mobilebert_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "Our MobileBERT model is pre-trained on the same corpus as BERT, including the BooksCorpus and English Wikipedia", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["We use a lighter MHA structure for MobileBERTTINY. As illustrated in Figure 4, in stead of using hidden states from the inter-block feature maps as inputs to MHA, we use the reduced intra-block feature maps as key, query, and values in MHA for MobileBERTTINY. This can effectively reduce the parameters in MHA modules, but might harm the model capacity. F GLUE Dataset. In this section, we provide a brief description of the tasks in the GLUE benchmark (Wang et al., 2018). CoLA The Corpus of Linguistic Acceptability (Warstadt et al., 2018) is a collection of English ac-\nceptability judgments drawn from books and journal articles on linguistic theory. The task is to predict whether an example is a grammatical English sentence and is evaluated by Matthews correlation coefficient (Matthews, 1975). SST-2 The Stanford Sentiment Treebank (Socher et al., 2013) is a collection of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence and is evaluated by accuracy. MRPC The Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) is a collection of sentence pairs automatically extracted from online news sources. They are labeled by human annotations for whether the sentences in the pair are semantically equivalent. The performance is evaluated by both accuracy and F1 score. STS-B The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5. The task is to predict these scores and is evaluated by Pearson and Spearman correlation coefficients. QQP The Quora Question Pairs8 (Chen et al., 2018) dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent and is evaluated by both accuracy and F1 score.", "MNLI The Multi-Genre Natural Language Inference Corpus (Williams et al., 2018) is a collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment ), contradicts the hypothesis (contradiction), or neither (neutral) and is evaluated by accuracy on both matched (indomain) and mismatched (cross-domain) sections of the test data. QNLI The Question-answering NLI dataset is converted from the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). The task is to determine whether the context sentence contains the answer to the question and is evaluated by the test accuracy. RTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges (Bentivogli et al., 2009). The task is to predict whether sentences in a sentence pair are entailment and is evaluated by accuracy. WNLI The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun\n8https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs\nfrom a list of choices. We follow Devlin et al. (2018) to skip this task in our experiments, because few previous works do better than predicting the majority class for this task.", "MobileBERT achieves an overall GLUE score of 77.7, which is only 0.6 lower than BERTBASE, while be-\n4The detailed model setting of MobileBERTTINY can be found in Table 1 and in the appendix. 5https://www.tensorflow.org/lite 6We follow Devlin et al. (2018) to skip the WNLI task. ing 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE. Moreover, It outperforms the strong OpenAI GPT baseline by 0.8 GLUE score with 4.3\u00d7 smaller model size. It also outperforms all the other compressed BERT models with smaller or similar model sizes. Finally, we find that the introduced operational optimizations hurt the model performance a bit. Without these optimizations, MobileBERT can even outperforms BERTBASE by 0.2 GLUE score. 4.4 Results on SQuAD. SQuAD is a large-scale reading comprehension datasets. SQuAD1.1 (Rajpurkar et al., 2016) only contains questions that always have an answer in the given context, while SQuAD2.0 (Rajpurkar et al., 2018) contains unanswerable questions. We evaluate MobileBERT only on the SQuAD dev datasets, as there is nearly no single model submission on SQuAD test leaderboard. We compare our MobileBERT with BERTBASE, DistilBERT, and a strong baseline DocQA (Clark and Gardner, 2017). As shown in Table 5, MobileBERT outperforms a large margin over all the other models with smaller or similar model sizes. 4.5 Quantization. We apply the standard post-training quantization in TensorFlow Lite to MobileBERT. The results are shown in Table 6. We find that while quantization can further compress MobileBERT by 4\u00d7, there is nearly no performance degradation from it. This indicates that there is still a big room in the compression of MobileBERT.\n4.6 Ablation Studies. 4.6.1 Operational Optimizations. We evaluate the effectiveness of the two operational optimizations introduced in Section 3.3, i.e., replacing layer normalization (LayerNorm) with NoNorm and replacing gelu activation with relu activation. We report the inference latencies using the same experimental setting as in Section 4.6.1."]}
{"pkey": "mobilebert_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Construct a MobileBERT tokenizer. Based on WordPiece.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["MHA and FFN play different roles in the Transformer architecture: The former allows the model to jointly attend to information from different subspaces, while the latter increases the non-linearity of the model. In original BERT, the ratio of the parameter numbers in MHA and FFN is always 1:2. But in the bottleneck structure, the inputs to the MHA are from wider feature maps (of inter-block size), while the inputs to the FFN are from narrower bottlenecks (of intra-block size). This results in that the MHA modules in MobileBERT relatively contain more parameters. To fix this issue, we propose to use stacked feedforward networks in MobileBERT to re-balance the relative size between MHA and FFN. As illustrated in Figure 1(c), each MobileBERT layer contains one MHA but several stacked FFN. In MobileBERT, we use 4 stacked FFN after each MHA. 3.3 Operational Optimizations. By model latency analysis2, we find that layer normalization (Ba et al., 2016) and gelu activation (Hendrycks and Gimpel, 2016) accounted for a considerable proportion of total latency. Therefore, we propose to replace them with new operations in our MobileBERT. Remove layer normalization We replace the layer normalization of a n-channel hidden state h with an element-wise linear transformation:\nNoNorm(h) = \u03b3 \u25e6 h+ \u03b2, (1)\nwhere \u03b3,\u03b2 \u2208 Rn and \u25e6 denotes the Hadamard product. Please note that NoNorm has different properties from LayerNorm even in test mode since the original layer normalization is not a linear operation for a batch of vectors. Use relu activation We replace the gelu activation with simpler relu activation (Nair and Hinton, 2010). 3.4 Embedding Factorization. The embedding table in BERT models accounts for a substantial proportion of model size. To compress the embedding layer, as shown in Table 1, we reduce the embedding dimension to 128 in MobileBERT. Then, we apply a 1D convolution with kernel size 3 on the raw token embedding to produce a 512 dimensional output. 3.5 Training Objectives.", "Popular lightweight operations such as depth-wise convolution (Howard et al., 2017) cannot be directly applied to Transformer or BERT. In the NLP literature, the most relevant work can be group LSTMs (Kuchaiev and Ginsburg, 2017; Gao et al., 2018), which employs the idea of group convolution (Zhang et al., 2017, 2018) into Recurrent Neural Networks (RNN). C Visualization of Attention Distributions\nWe visualize the attention distributions of the 1st and the 12th layers of a few models in the ablation study for further investigation. They are shown in Figure 3. We find that the proposed attention transfer can help the student mimic the attention distributions of the teacher very well. Surprisingly, we find that the attention distributions in the attention heads of \u201dMobileBERT(bare)+PD+FMT\u201d are exactly a re-order of those of \u201dMobileBERT(bare)+PD+FMT+AT\u201d (also the teacher model), even if it has not been trained by the attention transfer objective. This phenomenon indicates that multi-head attention is a crucial and unique part of the non-linearity of BERT. Moreover, it can explain the minor improvements of Attention Transfer in the ablation study table, since the alignment of feature maps lead to the alignment of attention distributions. D Extra Experimental Settings. For a fair comparison with original BERT, we follow the same pre-processing scheme as BERT, where we mask 15% of all WordPiece (Kudo and Richardson, 2018) tokens in each sequence at random and use next sentence prediction. Please note that MobileBERT can be potentially further improved by several training techniques recently introduced, such as span prediction (Joshi et al., 2019) or removing next sentence prediction objective (Liu et al., 2019b). We leave it for future work. In pre-training distillation, the hyperparameter \u03b1 is used to balance the original masked language modeling loss and the distillation loss. Following (Kim and Rush, 2016), we set \u03b1 to 0.5. E Architecture of MobileBERTTINY.", "In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark."]}
{"pkey": "mobilebert_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "The paper does not mention any specific preprocessing techniques used on the datasets.\n\nDuring pre-training and fine-tuning, the paper authors set the maximum input sequence length to 512, and truncate any sequence that exceeds this length.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["MNLI The Multi-Genre Natural Language Inference Corpus (Williams et al., 2018) is a collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment ), contradicts the hypothesis (contradiction), or neither (neutral) and is evaluated by accuracy on both matched (indomain) and mismatched (cross-domain) sections of the test data. QNLI The Question-answering NLI dataset is converted from the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). The task is to determine whether the context sentence contains the answer to the question and is evaluated by the test accuracy. RTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges (Bentivogli et al., 2009). The task is to predict whether sentences in a sentence pair are entailment and is evaluated by accuracy. WNLI The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun\n8https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs\nfrom a list of choices. We follow Devlin et al. (2018) to skip this task in our experiments, because few previous works do better than predicting the majority class for this task.", "MobileBERT achieves an overall GLUE score of 77.7, which is only 0.6 lower than BERTBASE, while be-\n4The detailed model setting of MobileBERTTINY can be found in Table 1 and in the appendix. 5https://www.tensorflow.org/lite 6We follow Devlin et al. (2018) to skip the WNLI task. ing 4.3\u00d7 smaller and 5.5\u00d7 faster than BERTBASE. Moreover, It outperforms the strong OpenAI GPT baseline by 0.8 GLUE score with 4.3\u00d7 smaller model size. It also outperforms all the other compressed BERT models with smaller or similar model sizes. Finally, we find that the introduced operational optimizations hurt the model performance a bit. Without these optimizations, MobileBERT can even outperforms BERTBASE by 0.2 GLUE score. 4.4 Results on SQuAD. SQuAD is a large-scale reading comprehension datasets. SQuAD1.1 (Rajpurkar et al., 2016) only contains questions that always have an answer in the given context, while SQuAD2.0 (Rajpurkar et al., 2018) contains unanswerable questions. We evaluate MobileBERT only on the SQuAD dev datasets, as there is nearly no single model submission on SQuAD test leaderboard. We compare our MobileBERT with BERTBASE, DistilBERT, and a strong baseline DocQA (Clark and Gardner, 2017). As shown in Table 5, MobileBERT outperforms a large margin over all the other models with smaller or similar model sizes. 4.5 Quantization. We apply the standard post-training quantization in TensorFlow Lite to MobileBERT. The results are shown in Table 6. We find that while quantization can further compress MobileBERT by 4\u00d7, there is nearly no performance degradation from it. This indicates that there is still a big room in the compression of MobileBERT.\n4.6 Ablation Studies. 4.6.1 Operational Optimizations. We evaluate the effectiveness of the two operational optimizations introduced in Section 3.3, i.e., replacing layer normalization (LayerNorm) with NoNorm and replacing gelu activation with relu activation. We report the inference latencies using the same experimental setting as in Section 4.6.1.", "We use a lighter MHA structure for MobileBERTTINY. As illustrated in Figure 4, in stead of using hidden states from the inter-block feature maps as inputs to MHA, we use the reduced intra-block feature maps as key, query, and values in MHA for MobileBERTTINY. This can effectively reduce the parameters in MHA modules, but might harm the model capacity. F GLUE Dataset. In this section, we provide a brief description of the tasks in the GLUE benchmark (Wang et al., 2018). CoLA The Corpus of Linguistic Acceptability (Warstadt et al., 2018) is a collection of English ac-\nceptability judgments drawn from books and journal articles on linguistic theory. The task is to predict whether an example is a grammatical English sentence and is evaluated by Matthews correlation coefficient (Matthews, 1975). SST-2 The Stanford Sentiment Treebank (Socher et al., 2013) is a collection of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence and is evaluated by accuracy. MRPC The Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) is a collection of sentence pairs automatically extracted from online news sources. They are labeled by human annotations for whether the sentences in the pair are semantically equivalent. The performance is evaluated by both accuracy and F1 score. STS-B The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5. The task is to predict these scores and is evaluated by Pearson and Spearman correlation coefficients. QQP The Quora Question Pairs8 (Chen et al., 2018) dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent and is evaluated by both accuracy and F1 score."]}
{"pkey": "mobilebert_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "Answer: MobileBERT is an encoder-only model with the same number of layers.It has 4 times feed-forward dimensions as the original BERT-base in oreder to maintain 1:2 ration between th parameters of MHA and FFn layers. It has four attention heads, and its embedding dimension is 512. The model also introduces bottleneck structures in the feed-forward network to reduce the computation cost. MobileBERT has several variants with different model sizes, i.e., MobileBERT, MobileBERT-Tiny.The architecture details for these models are as follows:\nMobileBERT: Embedding size: 512, The Number of Heads: 4, Number of Parameters:25.3M \nMobileBERT-TINY: Embedding size: 512, The Number of Heads: 4, Number of Parameters:15.1M", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["MHA and FFN play different roles in the Transformer architecture: The former allows the model to jointly attend to information from different subspaces, while the latter increases the non-linearity of the model. In original BERT, the ratio of the parameter numbers in MHA and FFN is always 1:2. But in the bottleneck structure, the inputs to the MHA are from wider feature maps (of inter-block size), while the inputs to the FFN are from narrower bottlenecks (of intra-block size). This results in that the MHA modules in MobileBERT relatively contain more parameters. To fix this issue, we propose to use stacked feedforward networks in MobileBERT to re-balance the relative size between MHA and FFN. As illustrated in Figure 1(c), each MobileBERT layer contains one MHA but several stacked FFN. In MobileBERT, we use 4 stacked FFN after each MHA. 3.3 Operational Optimizations. By model latency analysis2, we find that layer normalization (Ba et al., 2016) and gelu activation (Hendrycks and Gimpel, 2016) accounted for a considerable proportion of total latency. Therefore, we propose to replace them with new operations in our MobileBERT. Remove layer normalization We replace the layer normalization of a n-channel hidden state h with an element-wise linear transformation:\nNoNorm(h) = \u03b3 \u25e6 h+ \u03b2, (1)\nwhere \u03b3,\u03b2 \u2208 Rn and \u25e6 denotes the Hadamard product. Please note that NoNorm has different properties from LayerNorm even in test mode since the original layer normalization is not a linear operation for a batch of vectors. Use relu activation We replace the gelu activation with simpler relu activation (Nair and Hinton, 2010). 3.4 Embedding Factorization. The embedding table in BERT models accounts for a substantial proportion of model size. To compress the embedding layer, as shown in Table 1, we reduce the embedding dimension to 128 in MobileBERT. Then, we apply a 1D convolution with kernel size 3 on the raw token embedding to produce a 512 dimensional output. 3.5 Training Objectives.", "Architecture Search for MobileBERT We seek a compression ratio of 4\u00d7 for BERTBASE, so we design a set of MobileBERT models all with approximately 25M parameters but different ratios of the parameter numbers in MHA and FFN to select a good MobileBERT student model. Table 3 shows our experimental results. They have different balances between MHA and FFN. From the table, we can see that the model performance reaches the peak when the ratio of parameters in MHA and FFN is 0.4 \u223c 0.6. This may justify why the original Transformer chooses the parameter ratio of MHA and FFN to 0.5. We choose the architecture with 128 intra-block hidden size and 4 stacked FFNs as the MobileBERT student model in consideration of model accuracy and training efficiency. We also accordingly set the number of attention heads in the teacher model to 4 in preparation for the layer-wise knowledge transfer. Table 1 demonstrates the model settings of our IB-BERTLARGE teacher and MobileBERT student. One may wonder whether reducing the number of heads will harm the performance of the teacher model. By comparing (a) and (f) in Table 2, we can see that reducing the number of heads from 16 to 4\ndoes not affect the performance of IB-BERTLARGE. 4.2 Implementation Details. Following BERT (Devlin et al., 2018), we use the BooksCorpus (Zhu et al., 2015) and English Wikipedia as our pre-training data. To make the IB-BERTLARGE teacher reach the same accuracy as original BERTLARGE, we train IB-BERTLARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer (You et al., 2019). For a fair comparison with the original BERT, we do not use training tricks in other BERT variants (Liu et al., 2019b; Joshi et al., 2019). For MobileBERT, we use the same training schedule in the pre-training distillation stage. Additionally, we use progressive knowledge transfer to train MobileBERT, which takes additional 240k steps over 24 layers.", "Turc et al. (2019) propose to pre-train the smaller BERT models to improve task-specific knowledge distillation. Tang et al. (2019) distill BERT into an extremely small LSTM model. Tsai et al. (2019) distill a multilingual BERT into smaller BERT models on sequence labeling tasks. Clark et al. (2019b) use several single-task BERT\n1The code and pre-trained models are available at https://github.com/google-research/ google-research/tree/master/mobilebert.\nmodels to teach a multi-task BERT. Liu et al. (2019a) distill knowledge from an ensemble of BERT models into a single BERT. Concurrently to our work, Sun et al. (2019) distill BERT into shallower students through knowledge distillation and an additional knowledge transfer of hidden states on multiple intermediate layers. Jiao et al. (2019) propose TinyBERT, which also uses a layer-wise distillation strategy for BERT but in both pre-training and fine-tuning stages. Sanh et al. (2019) propose DistilBERT, which successfully halves the depth of BERT model by knowledge distillation in the pre-training stage and an optional fine-tuning stage. In contrast to these existing literature, we only use knowledge transfer in the pre-training stage and do not require a fine-tuned teacher or data augmentation (Wu et al., 2019) in the down-stream tasks. Another key difference is that these previous work try to compress BERT by reducing its depth, while we focus on compressing BERT by reducing its width, which has been shown to be more effective (Turc et al., 2019). 3 MobileBERT. In this section, we present the detailed architecture design of MobileBERT and training strategies to efficiently train MobileBERT. The specific model settings are summarized in Table 1. These settings are obtained by extensive architecture search experiments which will be presented in Section 4.1.\n3.1 Bottleneck and Inverted-Bottleneck. The architecture of MobileBERT is illustrated in Figure 1(c). It is as deep as BERTLARGE, but each building block is made much smaller."]}
{"pkey": "mobilebert_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "we train IB-BERTLARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer. For MobileBERT, the paper authors use the same training schedule in the pre-training distillation stage. Additionally, the paper authors use progressive knowledge transfer to train MobileBERT, which takes additional 240k steps over 24 layers. simply fine-tuning MobileBERT just like what the original BERT does. we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2-10).", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark.", "In practice, we can soften the training process as follows. When training a layer, we further tune the lower layers with a small learning rate rather than entirely freezing them. 4 Experiments. In this section, we first present our architecture search experiments which lead to the model settings in Table 1, and then present the empirical\nresults on benchmarks from MobileBERT and various baselines. 4.1 Model Settings. We conduct extensive experiments to search good model settings for the IB-BERT teacher and the MobileBERT student. We start with SQuAD v1.1 dev F1 score as the performance metric in the search of model settings. In this section, we only train each model for 125k steps with 2048 batch size, which halves the training schedule of original BERT (Devlin et al., 2018; You et al., 2019). Architecture Search for IB-BERT Our design philosophy for the teacher model is to use as small inter-block hidden size (feature map size) as possible, as long as there is no accuracy loss. Under this guideline, we design experiments to manipulate the inter-block size of a BERTLARGE-sized IB-BERT, and the results are shown in Table 2 with labels (a)-(e). We can see that reducing the interblock hidden size doesn\u2019t damage the performance\nof BERT until it is smaller than 512. Hence, we choose IB-BERTLARGE with its inter-block hidden size being 512 as the teacher model. One may wonder whether we can also shrink the intra-block hidden size of the teacher. We conduct experiments and the results are shown in Table 2 with labels (f)-(i). We can see that when the intra-block hidden size is reduced, the model performance is dramatically worse. This means that the intra-block hidden size, which represents the representation power of non-linear modules, plays a crucial role in BERT. Therefore, unlike the interblock hidden size, we do not shrink the intra-block hidden size of our teacher model.", "Architecture Search for MobileBERT We seek a compression ratio of 4\u00d7 for BERTBASE, so we design a set of MobileBERT models all with approximately 25M parameters but different ratios of the parameter numbers in MHA and FFN to select a good MobileBERT student model. Table 3 shows our experimental results. They have different balances between MHA and FFN. From the table, we can see that the model performance reaches the peak when the ratio of parameters in MHA and FFN is 0.4 \u223c 0.6. This may justify why the original Transformer chooses the parameter ratio of MHA and FFN to 0.5. We choose the architecture with 128 intra-block hidden size and 4 stacked FFNs as the MobileBERT student model in consideration of model accuracy and training efficiency. We also accordingly set the number of attention heads in the teacher model to 4 in preparation for the layer-wise knowledge transfer. Table 1 demonstrates the model settings of our IB-BERTLARGE teacher and MobileBERT student. One may wonder whether reducing the number of heads will harm the performance of the teacher model. By comparing (a) and (f) in Table 2, we can see that reducing the number of heads from 16 to 4\ndoes not affect the performance of IB-BERTLARGE. 4.2 Implementation Details. Following BERT (Devlin et al., 2018), we use the BooksCorpus (Zhu et al., 2015) and English Wikipedia as our pre-training data. To make the IB-BERTLARGE teacher reach the same accuracy as original BERTLARGE, we train IB-BERTLARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer (You et al., 2019). For a fair comparison with the original BERT, we do not use training tricks in other BERT variants (Liu et al., 2019b; Joshi et al., 2019). For MobileBERT, we use the same training schedule in the pre-training distillation stage. Additionally, we use progressive knowledge transfer to train MobileBERT, which takes additional 240k steps over 24 layers."]}
{"pkey": "mobilebert_13", "question": "Describe the computational resources used to train the model.", "answer": "we train IB-BERTLARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer.Apart from this nothing is discussed about the computional resources used in training.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["From Table 7, we can see that both NoNorm and relu are very effective in reducing the latency of MobileBERT, while the two operational optimizations do not reduce FLOPS. This reveals the gap between the real-world inference latency and the theoretical computation overhead (i.e., FLOPS). 4.6.2 Training Strategies. We also study how the choice of training strategy, i.e., auxiliary knowledge transfer, joint knowledge transfer, and progressive knowledge transfer, can affect the performance of MobileBERT. As shown\nin Table 8, progressive knowledge transfer consistently outperforms the other two strategies. We notice that there is a significant performance gap between auxiliary knowledge transfer and the other two strategies. We think the reason is that the intermediate layer-wise knowledge (i.e., attention maps and feature maps) from the teacher may not be optimal for the student, so the student needs an additional pre-training distillation stage to fine-tune its parameters. 4.6.3 Training Objectives. We finally conduct a set of ablation experiments with regard to Attention Transfer (AT), Feature Map Transfer (FMT) and Pre-training Distillation (PD). The operational OPTimizations (OPT) are removed in these experiments to make a fair comparison between MobileBERT and the original BERT. The results are listed in Table 9. We can see that the proposed Feature Map Transfer contributes most to the performance improvement of MobileBERT, while Attention Transfer and Pre-training Distillation also play positive roles. We can also find that our IB-BERTLARGE teacher is as powerful as the original IB-BERTLARGE while MobileBERT degrades greatly when compared to its teacher. So we believe that there is still a big room in the improvement of MobileBERT. 5 Conclusion. We have presented MobileBERT which is a taskagnostic compact variant of BERT. Empirical results on popular NLP benchmarks show that MobileBERT is comparable with BERTBASE while being much smaller and faster.", "Architecture Search for MobileBERT We seek a compression ratio of 4\u00d7 for BERTBASE, so we design a set of MobileBERT models all with approximately 25M parameters but different ratios of the parameter numbers in MHA and FFN to select a good MobileBERT student model. Table 3 shows our experimental results. They have different balances between MHA and FFN. From the table, we can see that the model performance reaches the peak when the ratio of parameters in MHA and FFN is 0.4 \u223c 0.6. This may justify why the original Transformer chooses the parameter ratio of MHA and FFN to 0.5. We choose the architecture with 128 intra-block hidden size and 4 stacked FFNs as the MobileBERT student model in consideration of model accuracy and training efficiency. We also accordingly set the number of attention heads in the teacher model to 4 in preparation for the layer-wise knowledge transfer. Table 1 demonstrates the model settings of our IB-BERTLARGE teacher and MobileBERT student. One may wonder whether reducing the number of heads will harm the performance of the teacher model. By comparing (a) and (f) in Table 2, we can see that reducing the number of heads from 16 to 4\ndoes not affect the performance of IB-BERTLARGE. 4.2 Implementation Details. Following BERT (Devlin et al., 2018), we use the BooksCorpus (Zhu et al., 2015) and English Wikipedia as our pre-training data. To make the IB-BERTLARGE teacher reach the same accuracy as original BERTLARGE, we train IB-BERTLARGE on 256 TPU v3 chips for 500k steps with a batch size of 4096 and LAMB optimizer (You et al., 2019). For a fair comparison with the original BERT, we do not use training tricks in other BERT variants (Liu et al., 2019b; Joshi et al., 2019). For MobileBERT, we use the same training schedule in the pre-training distillation stage. Additionally, we use progressive knowledge transfer to train MobileBERT, which takes additional 240k steps over 24 layers.", "Turc et al. (2019) propose to pre-train the smaller BERT models to improve task-specific knowledge distillation. Tang et al. (2019) distill BERT into an extremely small LSTM model. Tsai et al. (2019) distill a multilingual BERT into smaller BERT models on sequence labeling tasks. Clark et al. (2019b) use several single-task BERT\n1The code and pre-trained models are available at https://github.com/google-research/ google-research/tree/master/mobilebert.\nmodels to teach a multi-task BERT. Liu et al. (2019a) distill knowledge from an ensemble of BERT models into a single BERT. Concurrently to our work, Sun et al. (2019) distill BERT into shallower students through knowledge distillation and an additional knowledge transfer of hidden states on multiple intermediate layers. Jiao et al. (2019) propose TinyBERT, which also uses a layer-wise distillation strategy for BERT but in both pre-training and fine-tuning stages. Sanh et al. (2019) propose DistilBERT, which successfully halves the depth of BERT model by knowledge distillation in the pre-training stage and an optional fine-tuning stage. In contrast to these existing literature, we only use knowledge transfer in the pre-training stage and do not require a fine-tuned teacher or data augmentation (Wu et al., 2019) in the down-stream tasks. Another key difference is that these previous work try to compress BERT by reducing its depth, while we focus on compressing BERT by reducing its width, which has been shown to be more effective (Turc et al., 2019). 3 MobileBERT. In this section, we present the detailed architecture design of MobileBERT and training strategies to efficiently train MobileBERT. The specific model settings are summarized in Table 1. These settings are obtained by extensive architecture search experiments which will be presented in Section 4.1.\n3.1 Bottleneck and Inverted-Bottleneck. The architecture of MobileBERT is illustrated in Figure 1(c). It is as deep as BERTLARGE, but each building block is made much smaller."]}
{"pkey": "mobilebert_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "The paper provides detailed information on the model architecture, training setup, and evaluation results, which can be used to reproduce the experiments.It also provided a link that contains all the details for implementation neeeded to reproduce the results that are provided in the paper.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["We use a linear combination of the original masked language modeling (MLM) loss, next sentence prediction (NSP) loss, and the new MLM Knowledge Distillation (KD) loss as our pretraining distillation loss:\nLPD = \u03b1LMLM + (1\u2212 \u03b1)LKD + LNSP , (4)\nwhere \u03b1 is a hyperparameter in (0, 1). 3.6 Training Strategies. Given the objectives defined above, there can be various combination strategies in training. We discuss three strategies in this paper. Auxiliary Knowledge Transfer In this strategy, we regard intermediate knowledge transfer as an auxiliary task for knowledge distillation. We use a single loss, which is a linear combination of knowledge transfer losses from all layers as well as the pre-training distillation loss. Joint Knowledge Transfer However, the intermediate knowledge of the IB-BERT teacher (i.e. attention maps and feature maps) may not be an optimal solution for the MobileBERT student. Therefore, we propose to separate these two loss terms, where we first train MobileBERT with all layerwise knowledge transfer losses jointly, and then further train it by pre-training distillation. Progressive Knowledge Transfer One may also concern that if MobileBERT cannot perfectly mimic the IB-BERT teacher, the errors from the lower layers may affect the knowledge transfer in the higher layers. Therefore, we propose to progressively train each layer in the knowledge transfer. The progressive knowledge transfer is divided into L stages, where L is the number of layers. Diagram of three strategies Figure 2 illustrates the diagram of the three strategies. For joint knowledge transfer and progressive knowledge transfer, there is no knowledge transfer for the beginning embedding layer and the final classifier in the layerwise knowledge transfer stage. They are copied from the IB-BERT teacher to the MobileBERT student. Moreover, for progressive knowledge transfer, when we train the `th layer, we freeze all the trainable parameters in the layers below.", "We use a lighter MHA structure for MobileBERTTINY. As illustrated in Figure 4, in stead of using hidden states from the inter-block feature maps as inputs to MHA, we use the reduced intra-block feature maps as key, query, and values in MHA for MobileBERTTINY. This can effectively reduce the parameters in MHA modules, but might harm the model capacity. F GLUE Dataset. In this section, we provide a brief description of the tasks in the GLUE benchmark (Wang et al., 2018). CoLA The Corpus of Linguistic Acceptability (Warstadt et al., 2018) is a collection of English ac-\nceptability judgments drawn from books and journal articles on linguistic theory. The task is to predict whether an example is a grammatical English sentence and is evaluated by Matthews correlation coefficient (Matthews, 1975). SST-2 The Stanford Sentiment Treebank (Socher et al., 2013) is a collection of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence and is evaluated by accuracy. MRPC The Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) is a collection of sentence pairs automatically extracted from online news sources. They are labeled by human annotations for whether the sentences in the pair are semantically equivalent. The performance is evaluated by both accuracy and F1 score. STS-B The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5. The task is to predict these scores and is evaluated by Pearson and Spearman correlation coefficients. QQP The Quora Question Pairs8 (Chen et al., 2018) dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent and is evaluated by both accuracy and F1 score.", "MobileBERT can\nenable various NLP applications7 to be easily deployed on mobile devices. In this paper, we show that 1) it is crucial to keep MobileBERT deep and thin, 2) bottleneck/invertedbottleneck structures enable effective layer-wise knowledge transfer, and 3) progressive knowledge transfer can efficiently train MobileBERT. We believe our findings are generic and can be applied to other model compression problems. Appendix for \u201cMobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\u201d. A Extra Related Work on Knowledge Transfer. Exploiting knowledge transfer to compress model size was first proposed by Bucilu et al. (2006). The idea was then adopted in knowledge distillation (Hinton et al., 2015), which requires the smaller student network to mimic the class distribution output of the larger teacher network. Fitnets (Romero et al., 2014) make the student mimic the intermediate hidden layers of the teacher to train narrow\nand deep networks. Luo et al. (2016) show that the knowledge of the teacher can also be obtained from the neurons in the top hidden layer. Similar to our proposed progressive knowledge transfer scheme, Yeo et al. (2018) proposed a sequential knowledge transfer scheme to distill knowledge from a deep teacher into a shallow student in a sequential way. Zagoruyko and Komodakis (2016) proposed to transfer the attention maps of the teacher on images. Li et al. (2019) proposed to transfer the similarity of hidden states and word alignment from an autoregressive Transformer teacher to a non-autoregressive student. B Extra Related Work on Compact Architecture Design. While much recent research has focused on improving efficient Convolutional Neural Networks (CNN) for mobile vision applications (Iandola et al., 2016; Howard et al., 2017; Zhang et al., 2017, 2018; Sandler et al., 2018; Tan et al., 2019; Howard et al., 2019), they are usually tailored for CNN."]}
{"pkey": "mobilebert_15", "question": "What is the pretraining objective of the model? ", "answer": "The pre-training objectives for MobileBERT are the same as the original BERT model, namely masked language modeling (MLM) and next sentence prediction (NSP) and to make it more efficient for resource-limited devices, the authors introduced use of knowledge distillation, the introduction of two training stages", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark.", "We use a linear combination of the original masked language modeling (MLM) loss, next sentence prediction (NSP) loss, and the new MLM Knowledge Distillation (KD) loss as our pretraining distillation loss:\nLPD = \u03b1LMLM + (1\u2212 \u03b1)LKD + LNSP , (4)\nwhere \u03b1 is a hyperparameter in (0, 1). 3.6 Training Strategies. Given the objectives defined above, there can be various combination strategies in training. We discuss three strategies in this paper. Auxiliary Knowledge Transfer In this strategy, we regard intermediate knowledge transfer as an auxiliary task for knowledge distillation. We use a single loss, which is a linear combination of knowledge transfer losses from all layers as well as the pre-training distillation loss. Joint Knowledge Transfer However, the intermediate knowledge of the IB-BERT teacher (i.e. attention maps and feature maps) may not be an optimal solution for the MobileBERT student. Therefore, we propose to separate these two loss terms, where we first train MobileBERT with all layerwise knowledge transfer losses jointly, and then further train it by pre-training distillation. Progressive Knowledge Transfer One may also concern that if MobileBERT cannot perfectly mimic the IB-BERT teacher, the errors from the lower layers may affect the knowledge transfer in the higher layers. Therefore, we propose to progressively train each layer in the knowledge transfer. The progressive knowledge transfer is divided into L stages, where L is the number of layers. Diagram of three strategies Figure 2 illustrates the diagram of the three strategies. For joint knowledge transfer and progressive knowledge transfer, there is no knowledge transfer for the beginning embedding layer and the final classifier in the layerwise knowledge transfer stage. They are copied from the IB-BERT teacher to the MobileBERT student. Moreover, for progressive knowledge transfer, when we train the `th layer, we freeze all the trainable parameters in the layers below.", "As shown in Table 1, the hidden dimension of each building block is only 128. On the other hand, we introduce two linear transformations for each building block to adjust its input and output dimensions to 512. Following the terminology in (He et al., 2016), we refer to such an architecture as bottleneck. It is challenging to train such a deep and thin network. To overcome the training issue, we first construct a teacher network and train it until convergence, and then conduct knowledge transfer from this teacher network to MobileBERT. We find that this is much better than directly training MobileBERT from scratch. Various training strategies will be discussed in a later section. Here, we introduce the architecture design of the teacher network which is illustrated in Figure 1(b). In fact, the teacher network is just BERTLARGE while augmented with inverted-bottleneck structures (Sandler et al., 2018) to adjust its feature map size to 512. In what follows, we refer to the teacher network as IB-BERTLARGE. Note that IB-BERT and MobileBERT have the same feature map size which is 512. Thus, we can directly compare the layerwise output difference between IB-BERT and MobileBERT. Such a direct comparison is needed in our knowledge transfer strategy. It is worth pointing out that the simultaneously introduced bottleneck and inverted-bottleneck structures result in a fairly flexible architecture design. One may either only use the bottlenecks for MobileBERT (correspondingly the\nteacher becomes BERTLARGE) or only the invertedbottlenecks for IB-BERT (then there is no bottleneck in MobileBERT) to align their feature maps. However, when using both of them, we can allow IB-BERTLARGE to preserve the performance of BERTLARGE while having MobileBERT sufficiently compact. 3.2 Stacked Feed-Forward Networks. A problem introduced by the bottleneck structure of MobileBERT is that the balance between the Multi-Head Attention (MHA) module and the FeedForward Network (FFN) module is broken."]}
{"pkey": "mobilebert_16", "question": "What is the loss function that is used to train the model?", "answer": "In the paper the paper authors used: MLM loss, NSP Loss, Knowledge Distillation (KD) loss and  l layer-wise knowledge transfer loss LKT.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["As shown in Table 1, the hidden dimension of each building block is only 128. On the other hand, we introduce two linear transformations for each building block to adjust its input and output dimensions to 512. Following the terminology in (He et al., 2016), we refer to such an architecture as bottleneck. It is challenging to train such a deep and thin network. To overcome the training issue, we first construct a teacher network and train it until convergence, and then conduct knowledge transfer from this teacher network to MobileBERT. We find that this is much better than directly training MobileBERT from scratch. Various training strategies will be discussed in a later section. Here, we introduce the architecture design of the teacher network which is illustrated in Figure 1(b). In fact, the teacher network is just BERTLARGE while augmented with inverted-bottleneck structures (Sandler et al., 2018) to adjust its feature map size to 512. In what follows, we refer to the teacher network as IB-BERTLARGE. Note that IB-BERT and MobileBERT have the same feature map size which is 512. Thus, we can directly compare the layerwise output difference between IB-BERT and MobileBERT. Such a direct comparison is needed in our knowledge transfer strategy. It is worth pointing out that the simultaneously introduced bottleneck and inverted-bottleneck structures result in a fairly flexible architecture design. One may either only use the bottlenecks for MobileBERT (correspondingly the\nteacher becomes BERTLARGE) or only the invertedbottlenecks for IB-BERT (then there is no bottleneck in MobileBERT) to align their feature maps. However, when using both of them, we can allow IB-BERTLARGE to preserve the performance of BERTLARGE while having MobileBERT sufficiently compact. 3.2 Stacked Feed-Forward Networks. A problem introduced by the bottleneck structure of MobileBERT is that the balance between the Multi-Head Attention (MHA) module and the FeedForward Network (FFN) module is broken.", "We use a linear combination of the original masked language modeling (MLM) loss, next sentence prediction (NSP) loss, and the new MLM Knowledge Distillation (KD) loss as our pretraining distillation loss:\nLPD = \u03b1LMLM + (1\u2212 \u03b1)LKD + LNSP , (4)\nwhere \u03b1 is a hyperparameter in (0, 1). 3.6 Training Strategies. Given the objectives defined above, there can be various combination strategies in training. We discuss three strategies in this paper. Auxiliary Knowledge Transfer In this strategy, we regard intermediate knowledge transfer as an auxiliary task for knowledge distillation. We use a single loss, which is a linear combination of knowledge transfer losses from all layers as well as the pre-training distillation loss. Joint Knowledge Transfer However, the intermediate knowledge of the IB-BERT teacher (i.e. attention maps and feature maps) may not be an optimal solution for the MobileBERT student. Therefore, we propose to separate these two loss terms, where we first train MobileBERT with all layerwise knowledge transfer losses jointly, and then further train it by pre-training distillation. Progressive Knowledge Transfer One may also concern that if MobileBERT cannot perfectly mimic the IB-BERT teacher, the errors from the lower layers may affect the knowledge transfer in the higher layers. Therefore, we propose to progressively train each layer in the knowledge transfer. The progressive knowledge transfer is divided into L stages, where L is the number of layers. Diagram of three strategies Figure 2 illustrates the diagram of the three strategies. For joint knowledge transfer and progressive knowledge transfer, there is no knowledge transfer for the beginning embedding layer and the final classifier in the layerwise knowledge transfer stage. They are copied from the IB-BERT teacher to the MobileBERT student. Moreover, for progressive knowledge transfer, when we train the `th layer, we freeze all the trainable parameters in the layers below.", "In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark."]}
{"pkey": "mobilebert_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "Authors proposed the MobileBERT which is thin, light-weight BERT model. They reduced the number of layers and hidden units in each layer to reduce model size and computational complexity.Then they introduce a bottleneck block and a mobile inverted residual block to reduce model size and improve efficiency. Finally adopt knowledge distillation from a larger teacher BERT model to improve the accuracy of the MobileBERT model", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["Popular lightweight operations such as depth-wise convolution (Howard et al., 2017) cannot be directly applied to Transformer or BERT. In the NLP literature, the most relevant work can be group LSTMs (Kuchaiev and Ginsburg, 2017; Gao et al., 2018), which employs the idea of group convolution (Zhang et al., 2017, 2018) into Recurrent Neural Networks (RNN). C Visualization of Attention Distributions\nWe visualize the attention distributions of the 1st and the 12th layers of a few models in the ablation study for further investigation. They are shown in Figure 3. We find that the proposed attention transfer can help the student mimic the attention distributions of the teacher very well. Surprisingly, we find that the attention distributions in the attention heads of \u201dMobileBERT(bare)+PD+FMT\u201d are exactly a re-order of those of \u201dMobileBERT(bare)+PD+FMT+AT\u201d (also the teacher model), even if it has not been trained by the attention transfer objective. This phenomenon indicates that multi-head attention is a crucial and unique part of the non-linearity of BERT. Moreover, it can explain the minor improvements of Attention Transfer in the ablation study table, since the alignment of feature maps lead to the alignment of attention distributions. D Extra Experimental Settings. For a fair comparison with original BERT, we follow the same pre-processing scheme as BERT, where we mask 15% of all WordPiece (Kudo and Richardson, 2018) tokens in each sequence at random and use next sentence prediction. Please note that MobileBERT can be potentially further improved by several training techniques recently introduced, such as span prediction (Joshi et al., 2019) or removing next sentence prediction objective (Liu et al., 2019b). We leave it for future work. In pre-training distillation, the hyperparameter \u03b1 is used to balance the original masked language modeling loss and the distillation loss. Following (Kim and Rush, 2016), we set \u03b1 to 0.5. E Architecture of MobileBERTTINY.", "In practice, we can soften the training process as follows. When training a layer, we further tune the lower layers with a small learning rate rather than entirely freezing them. 4 Experiments. In this section, we first present our architecture search experiments which lead to the model settings in Table 1, and then present the empirical\nresults on benchmarks from MobileBERT and various baselines. 4.1 Model Settings. We conduct extensive experiments to search good model settings for the IB-BERT teacher and the MobileBERT student. We start with SQuAD v1.1 dev F1 score as the performance metric in the search of model settings. In this section, we only train each model for 125k steps with 2048 batch size, which halves the training schedule of original BERT (Devlin et al., 2018; You et al., 2019). Architecture Search for IB-BERT Our design philosophy for the teacher model is to use as small inter-block hidden size (feature map size) as possible, as long as there is no accuracy loss. Under this guideline, we design experiments to manipulate the inter-block size of a BERTLARGE-sized IB-BERT, and the results are shown in Table 2 with labels (a)-(e). We can see that reducing the interblock hidden size doesn\u2019t damage the performance\nof BERT until it is smaller than 512. Hence, we choose IB-BERTLARGE with its inter-block hidden size being 512 as the teacher model. One may wonder whether we can also shrink the intra-block hidden size of the teacher. We conduct experiments and the results are shown in Table 2 with labels (f)-(i). We can see that when the intra-block hidden size is reduced, the model performance is dramatically worse. This means that the intra-block hidden size, which represents the representation power of non-linear modules, plays a crucial role in BERT. Therefore, unlike the interblock hidden size, we do not shrink the intra-block hidden size of our teacher model.", "Turc et al. (2019) propose to pre-train the smaller BERT models to improve task-specific knowledge distillation. Tang et al. (2019) distill BERT into an extremely small LSTM model. Tsai et al. (2019) distill a multilingual BERT into smaller BERT models on sequence labeling tasks. Clark et al. (2019b) use several single-task BERT\n1The code and pre-trained models are available at https://github.com/google-research/ google-research/tree/master/mobilebert.\nmodels to teach a multi-task BERT. Liu et al. (2019a) distill knowledge from an ensemble of BERT models into a single BERT. Concurrently to our work, Sun et al. (2019) distill BERT into shallower students through knowledge distillation and an additional knowledge transfer of hidden states on multiple intermediate layers. Jiao et al. (2019) propose TinyBERT, which also uses a layer-wise distillation strategy for BERT but in both pre-training and fine-tuning stages. Sanh et al. (2019) propose DistilBERT, which successfully halves the depth of BERT model by knowledge distillation in the pre-training stage and an optional fine-tuning stage. In contrast to these existing literature, we only use knowledge transfer in the pre-training stage and do not require a fine-tuned teacher or data augmentation (Wu et al., 2019) in the down-stream tasks. Another key difference is that these previous work try to compress BERT by reducing its depth, while we focus on compressing BERT by reducing its width, which has been shown to be more effective (Turc et al., 2019). 3 MobileBERT. In this section, we present the detailed architecture design of MobileBERT and training strategies to efficiently train MobileBERT. The specific model settings are summarized in Table 1. These settings are obtained by extensive architecture search experiments which will be presented in Section 4.1.\n3.1 Bottleneck and Inverted-Bottleneck. The architecture of MobileBERT is illustrated in Figure 1(c). It is as deep as BERTLARGE, but each building block is made much smaller."]}
{"pkey": "mobilebert_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "\"we concducted many experiments: 1)Experiments on SQuAD v1.1 dev F1 score in search of good model settings for the IB-BERTLARGE teacher. The number of layers is set tto 24 for all models. 2)Experiments on SQuAD v1.1 dev F1 score in search of good model settings for the MobileBERT student. The number of layers is set to 24 and the inter-block hidden size is set to 512 for all models. 3)conducted evaluation tests on the GLUE benchmark. 4)evaluation expereiments on the SQuAD dev datasets. 5) conducted evaluation of MobileBERT on GLUE dev accuracy and SQuAD v1.1 dev F1 score with 8-bit Quantization. 6) conducted experiments The see effectiveness of operational optimizations on real-world inference latency for MobileBERT. etc\" with \"parameters are\"", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["We use a lighter MHA structure for MobileBERTTINY. As illustrated in Figure 4, in stead of using hidden states from the inter-block feature maps as inputs to MHA, we use the reduced intra-block feature maps as key, query, and values in MHA for MobileBERTTINY. This can effectively reduce the parameters in MHA modules, but might harm the model capacity. F GLUE Dataset. In this section, we provide a brief description of the tasks in the GLUE benchmark (Wang et al., 2018). CoLA The Corpus of Linguistic Acceptability (Warstadt et al., 2018) is a collection of English ac-\nceptability judgments drawn from books and journal articles on linguistic theory. The task is to predict whether an example is a grammatical English sentence and is evaluated by Matthews correlation coefficient (Matthews, 1975). SST-2 The Stanford Sentiment Treebank (Socher et al., 2013) is a collection of sentences from movie reviews and human annotations of their sentiment. The task is to predict the sentiment of a given sentence and is evaluated by accuracy. MRPC The Microsoft Research Paraphrase Corpus (Dolan and Brockett, 2005) is a collection of sentence pairs automatically extracted from online news sources. They are labeled by human annotations for whether the sentences in the pair are semantically equivalent. The performance is evaluated by both accuracy and F1 score. STS-B The Semantic Textual Similarity Benchmark (Cer et al., 2017) is a collection of sentence pairs drawn from news headlines, video and image captions, and natural language inference data. Each pair is human-annotated with a similarity score from 1 to 5. The task is to predict these scores and is evaluated by Pearson and Spearman correlation coefficients. QQP The Quora Question Pairs8 (Chen et al., 2018) dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent and is evaluated by both accuracy and F1 score.", "In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark.", "In practice, we can soften the training process as follows. When training a layer, we further tune the lower layers with a small learning rate rather than entirely freezing them. 4 Experiments. In this section, we first present our architecture search experiments which lead to the model settings in Table 1, and then present the empirical\nresults on benchmarks from MobileBERT and various baselines. 4.1 Model Settings. We conduct extensive experiments to search good model settings for the IB-BERT teacher and the MobileBERT student. We start with SQuAD v1.1 dev F1 score as the performance metric in the search of model settings. In this section, we only train each model for 125k steps with 2048 batch size, which halves the training schedule of original BERT (Devlin et al., 2018; You et al., 2019). Architecture Search for IB-BERT Our design philosophy for the teacher model is to use as small inter-block hidden size (feature map size) as possible, as long as there is no accuracy loss. Under this guideline, we design experiments to manipulate the inter-block size of a BERTLARGE-sized IB-BERT, and the results are shown in Table 2 with labels (a)-(e). We can see that reducing the interblock hidden size doesn\u2019t damage the performance\nof BERT until it is smaller than 512. Hence, we choose IB-BERTLARGE with its inter-block hidden size being 512 as the teacher model. One may wonder whether we can also shrink the intra-block hidden size of the teacher. We conduct experiments and the results are shown in Table 2 with labels (f)-(i). We can see that when the intra-block hidden size is reduced, the model performance is dramatically worse. This means that the intra-block hidden size, which represents the representation power of non-linear modules, plays a crucial role in BERT. Therefore, unlike the interblock hidden size, we do not shrink the intra-block hidden size of our teacher model."]}
{"pkey": "mobilebert_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "In paper authors conducted three ablation studies:\n1) The effectiveness of operational optimizations on real-world inference latency for MobileBERT.\n2) : Ablation study of MobileBERT on GLUE dev accuracy and SQuAD v1.1 dev F1 score with Auxiliary Knowledge Transfer (AKT), Joint Knowledge Transfer (JKT), and Progressive Knowledge Transfer (PKT)\n3)Ablation on the dev sets of GLUE benchmark. BERTBASE and the bare MobileBERT (i.e., w/o PD, FMT, AT, FMT & OPT) use the standard BERT pretraining scheme. PD, AT, FMT, and OPT denote Pretraining Distillation, Attention Transfer, Feature Map Transfer, and operational OPTimizations respectively. \u2020marks our runs with the official code.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["In ablation studies, we halve the pretraining distillation schedule of MobileBERT to accelerate experiments. Moreover, in the ablation study of knowledge transfer strategies, for a fair comparison, joint knowledge transfer and auxiliary knowledge transfer also take additional 240k steps. For the downstream tasks, all reported results are obtained by simply fine-tuning MobileBERT just like what the original BERT does. To finetune the pre-trained models, we search the optimization hyperparameters in a search space including different batch sizes (16/32/48), learning rates ((1-10) * e-5), and the number of epochs (2- 10). The search space is different from the original BERT because we find that MobileBERT usually needs a larger learning rate and more training epochs in fine-tuning. We select the model for testing according to their performance on the development (dev) set. 4.3 Results on GLUE. The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018) is a collection of 9 natural language understanding tasks. We compare MobileBERT with BERTBASE and a few state-of-the-art pre-BERT models on the GLUE leaderboard3: OpenAI GPT (Radford et al., 2018) and ELMo (Peters et al., 2018). We also compare with three recently proposed compressed BERT models: BERT-PKD (Sun et al., 2019), and DistilBERT (Sanh et al., 2019). To further show the advantage of MobileBERT over recent small BERT models, we also evaluate a smaller variant of our\n3https://gluebenchmark.com/leaderboard\nmodel with approximately 15M parameters called MobileBERTTINY4, which reduces the number of FFNs in each layer and uses a lighter MHA structure. Besides, to verify the performance of MobileBERT on real-world mobile devices, we export the models with TensorFlow Lite5 APIs and measure the inference latencies on a 4-thread Pixel 4 phone with a fixed sequence length of 128. The results are listed in Table 4. 6\nFrom the table, we can see that MobileBERT is very competitive on the GLUE benchmark.", "Popular lightweight operations such as depth-wise convolution (Howard et al., 2017) cannot be directly applied to Transformer or BERT. In the NLP literature, the most relevant work can be group LSTMs (Kuchaiev and Ginsburg, 2017; Gao et al., 2018), which employs the idea of group convolution (Zhang et al., 2017, 2018) into Recurrent Neural Networks (RNN). C Visualization of Attention Distributions\nWe visualize the attention distributions of the 1st and the 12th layers of a few models in the ablation study for further investigation. They are shown in Figure 3. We find that the proposed attention transfer can help the student mimic the attention distributions of the teacher very well. Surprisingly, we find that the attention distributions in the attention heads of \u201dMobileBERT(bare)+PD+FMT\u201d are exactly a re-order of those of \u201dMobileBERT(bare)+PD+FMT+AT\u201d (also the teacher model), even if it has not been trained by the attention transfer objective. This phenomenon indicates that multi-head attention is a crucial and unique part of the non-linearity of BERT. Moreover, it can explain the minor improvements of Attention Transfer in the ablation study table, since the alignment of feature maps lead to the alignment of attention distributions. D Extra Experimental Settings. For a fair comparison with original BERT, we follow the same pre-processing scheme as BERT, where we mask 15% of all WordPiece (Kudo and Richardson, 2018) tokens in each sequence at random and use next sentence prediction. Please note that MobileBERT can be potentially further improved by several training techniques recently introduced, such as span prediction (Joshi et al., 2019) or removing next sentence prediction objective (Liu et al., 2019b). We leave it for future work. In pre-training distillation, the hyperparameter \u03b1 is used to balance the original masked language modeling loss and the distillation loss. Following (Kim and Rush, 2016), we set \u03b1 to 0.5. E Architecture of MobileBERTTINY.", "From Table 7, we can see that both NoNorm and relu are very effective in reducing the latency of MobileBERT, while the two operational optimizations do not reduce FLOPS. This reveals the gap between the real-world inference latency and the theoretical computation overhead (i.e., FLOPS). 4.6.2 Training Strategies. We also study how the choice of training strategy, i.e., auxiliary knowledge transfer, joint knowledge transfer, and progressive knowledge transfer, can affect the performance of MobileBERT. As shown\nin Table 8, progressive knowledge transfer consistently outperforms the other two strategies. We notice that there is a significant performance gap between auxiliary knowledge transfer and the other two strategies. We think the reason is that the intermediate layer-wise knowledge (i.e., attention maps and feature maps) from the teacher may not be optimal for the student, so the student needs an additional pre-training distillation stage to fine-tune its parameters. 4.6.3 Training Objectives. We finally conduct a set of ablation experiments with regard to Attention Transfer (AT), Feature Map Transfer (FMT) and Pre-training Distillation (PD). The operational OPTimizations (OPT) are removed in these experiments to make a fair comparison between MobileBERT and the original BERT. The results are listed in Table 9. We can see that the proposed Feature Map Transfer contributes most to the performance improvement of MobileBERT, while Attention Transfer and Pre-training Distillation also play positive roles. We can also find that our IB-BERTLARGE teacher is as powerful as the original IB-BERTLARGE while MobileBERT degrades greatly when compared to its teacher. So we believe that there is still a big room in the improvement of MobileBERT. 5 Conclusion. We have presented MobileBERT which is a taskagnostic compact variant of BERT. Empirical results on popular NLP benchmarks show that MobileBERT is comparable with BERTBASE while being much smaller and faster."]}
{"pkey": "mobilebert_20", "question": "List the future work mentioned in the paper.", "answer": "The paper authors can also find that our IB-BERTLARGE teacher is as powerful as the original IB-BERTLARGE while MobileBERT degrades greatly when compared to its teacher. So the paper authors believe that there is still a big room in the improvement of MobileBERT. Please note that MobileBERT can be potentially further improved by several training techniques recently introduced, such as span prediction (Joshi et al., 2019)or removing next sentence prediction objective (Liu et al., 2019b). The paper authors leave it for future work.", "title": "MobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices", "context": ["Popular lightweight operations such as depth-wise convolution (Howard et al., 2017) cannot be directly applied to Transformer or BERT. In the NLP literature, the most relevant work can be group LSTMs (Kuchaiev and Ginsburg, 2017; Gao et al., 2018), which employs the idea of group convolution (Zhang et al., 2017, 2018) into Recurrent Neural Networks (RNN). C Visualization of Attention Distributions\nWe visualize the attention distributions of the 1st and the 12th layers of a few models in the ablation study for further investigation. They are shown in Figure 3. We find that the proposed attention transfer can help the student mimic the attention distributions of the teacher very well. Surprisingly, we find that the attention distributions in the attention heads of \u201dMobileBERT(bare)+PD+FMT\u201d are exactly a re-order of those of \u201dMobileBERT(bare)+PD+FMT+AT\u201d (also the teacher model), even if it has not been trained by the attention transfer objective. This phenomenon indicates that multi-head attention is a crucial and unique part of the non-linearity of BERT. Moreover, it can explain the minor improvements of Attention Transfer in the ablation study table, since the alignment of feature maps lead to the alignment of attention distributions. D Extra Experimental Settings. For a fair comparison with original BERT, we follow the same pre-processing scheme as BERT, where we mask 15% of all WordPiece (Kudo and Richardson, 2018) tokens in each sequence at random and use next sentence prediction. Please note that MobileBERT can be potentially further improved by several training techniques recently introduced, such as span prediction (Joshi et al., 2019) or removing next sentence prediction objective (Liu et al., 2019b). We leave it for future work. In pre-training distillation, the hyperparameter \u03b1 is used to balance the original masked language modeling loss and the distillation loss. Following (Kim and Rush, 2016), we set \u03b1 to 0.5. E Architecture of MobileBERTTINY.", "MNLI The Multi-Genre Natural Language Inference Corpus (Williams et al., 2018) is a collection of sentence pairs with textual entailment annotations. Given a premise sentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis (entailment ), contradicts the hypothesis (contradiction), or neither (neutral) and is evaluated by accuracy on both matched (indomain) and mismatched (cross-domain) sections of the test data. QNLI The Question-answering NLI dataset is converted from the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016). The task is to determine whether the context sentence contains the answer to the question and is evaluated by the test accuracy. RTE The Recognizing Textual Entailment (RTE) datasets come from a series of annual textual entailment challenges (Bentivogli et al., 2009). The task is to predict whether sentences in a sentence pair are entailment and is evaluated by accuracy. WNLI The Winograd Schema Challenge (Levesque et al., 2011) is a reading comprehension task in which a system must read a sentence with a pronoun and select the referent of that pronoun\n8https://data.quora.com/ First-Quora-Dataset-Release-Question-Pairs\nfrom a list of choices. We follow Devlin et al. (2018) to skip this task in our experiments, because few previous works do better than predicting the majority class for this task.", "MobileBERT can\nenable various NLP applications7 to be easily deployed on mobile devices. In this paper, we show that 1) it is crucial to keep MobileBERT deep and thin, 2) bottleneck/invertedbottleneck structures enable effective layer-wise knowledge transfer, and 3) progressive knowledge transfer can efficiently train MobileBERT. We believe our findings are generic and can be applied to other model compression problems. Appendix for \u201cMobileBERT: a Compact Task-Agnostic BERT for Resource-Limited Devices\u201d. A Extra Related Work on Knowledge Transfer. Exploiting knowledge transfer to compress model size was first proposed by Bucilu et al. (2006). The idea was then adopted in knowledge distillation (Hinton et al., 2015), which requires the smaller student network to mimic the class distribution output of the larger teacher network. Fitnets (Romero et al., 2014) make the student mimic the intermediate hidden layers of the teacher to train narrow\nand deep networks. Luo et al. (2016) show that the knowledge of the teacher can also be obtained from the neurons in the top hidden layer. Similar to our proposed progressive knowledge transfer scheme, Yeo et al. (2018) proposed a sequential knowledge transfer scheme to distill knowledge from a deep teacher into a shallow student in a sequential way. Zagoruyko and Komodakis (2016) proposed to transfer the attention maps of the teacher on images. Li et al. (2019) proposed to transfer the similarity of hidden states and word alignment from an autoregressive Transformer teacher to a non-autoregressive student. B Extra Related Work on Compact Architecture Design. While much recent research has focused on improving efficient Convolutional Neural Networks (CNN) for mobile vision applications (Iandola et al., 2016; Howard et al., 2017; Zhang et al., 2017, 2018; Sandler et al., 2018; Tan et al., 2019; Howard et al., 2019), they are usually tailored for CNN."]}
{"pkey": "structbert_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "The authors extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, the paper authors pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering.", "At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21].", "As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.\n1 Introduction. A pre-trained language model (LM) is a key component in many natural language understanding (NLU) tasks such as semantic textual similarity [4], question answering [21] and sentiment classification [25]. In order to obtain reliable language representations, neural language models are designed to define the joint probability function of sequences of words in text with self-supervised learning. Different from traditional word-specific embedding in which each token is assigned a global representation, recent work, such as Cove [16], ELMo [18], GPT [20] and BERT [6], derives contextualized word vectors from a language model trained on a large text corpus. These models have been shown effective for many downstream NLU tasks. Among the context-sensitive language models, BERT (and its robustly optimized version RoBERTa [15]) has taken the NLP world by storm. It is designed to pre-train bidirectional representations by jointly conditioning on both left and right context in all layers and model the representations by predicting masked words only through the contexts. However, it does not make the most of underlying language structures. According to Elman [8]\u2019s study, the recurrent neural networks was shown to be sensitive to regularities in word order in simple sentences. Since language fluency is determined by the ordering of words and sentences, finding the best permutation of a set of words and sentences is an essential problem in many NLP tasks, such as machine translation and NLU [9]."]}
{"pkey": "structbert_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "In this paper, the paper authors introduce a new type of contextual representation, StructBERT, which incorporates language structures into BERT pre-training by proposing two novel linearization strategies. Specifically, in addition to the existing masking strategy, StructBERT extends BERT by leveraging the structural information: word-level ordering and sentence-level ordering. The paper authors augment model pre-training with two new structural objectives on the inner-sentence and inter-sentence structures, respectively. In this way, the linguistic aspects (Elman, 1990) are explicitly captured during the pre-training procedure. With structural pre-training, StructBERT encodes dependency between words as well as sentences in the contextualized representation, which provides the model with better generalizability and adaptability. StructBERT significantly advances the state-of-the-art results on a variety of NLU tasks, including the GLUE benchmark (Wang et al., 2018), the SNLI dataset (Bowman et al., 2015) and the SQuAD v1.1 question answering task (Rajpurkar et al., 2016). All of these experimental results clearly demonstrate StructBERT\u2019s exceptional effectiveness and generalization capability in language understanding.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering.", "At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21].", "The new word objective is jointly learned together with the masked LM objective in a unified pre-trained model with equal weights. 2.3.2 Sentence Structural Objective. The next sentence prediction task is considered easy for the original BERT model (the prediction accuracy of BERT can easily achieve 97%-98% in this task [6]). We, therefore, extend the sentence prediction task by predicting both the next sentence and the previous sentence, to make the pre-trained language model aware of the sequential order of the sentences in a bidirectional manner. As illustrated in Figure 1b, given a pair of sentences (S1, S2) as input, we predict whether S2 is the next sentence that follows S1, or the previous sentence that precedes S1, or a random sentence from a different document. Specifically, for the sentence S1, 13 of the time we choose the text span that follows S1 as the second sentence S2, 1 3 of the time the previous sentence ahead of S1 is selected, and 13 of the time a sentence randomly sampled from the other documents is used as S2. The two sentences are concatenated together into an input sequence with the separator token [SEP] in\nbetween, as done in BERT. We pool the model output by taking the hidden state corresponding to the first token [CLS], and feed the encoding vector of [CLS] into a softmax classifier to make a three-class prediction. 2.4 Pre-training Setup. The training objective function is a linear combination of the word structural objective and the sentence structural objective. For the masked LM objective, we followed the same masking rate and settings as in BERT [6]. 5% of trigrams are selected for random shuffling. We used documents from English Wikipedia (2,500M words) and BookCorpus [35] as pre-training data, following the preprocessing and the WordPiece tokenization from [6]. The maximum length of input sequence was set to 512."]}
{"pkey": "structbert_3", "question": "What are the main contributions of the paper?", "answer": "The paper authors make the following major contributions:\n\u2022 The paper authors propose novel structural pre-training that extends BERT by incorporating the word structural objective and the sentence structural objective to leverage language structures in contextualized representation. This enables the StructBERT to explicitly model language structures by forcing it to reconstruct the right order of words and sentences for correct prediction.\n\u2022 StructBERT significantly outperforms all published state-of-the-art models on a wide range of NLU tasks at the time of model submission. This model extends the superiority of BERT, and boosts the performance in many language understanding applications such as semantic textual similarity, sentiment analysis, textual entailment, and question answering.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering.", "Recently, word ordering was treated as LM-based linearization solely based on language models [24]. Schmaltz showed that recurrent neural network language models [17] with long short-term memory [11] cells work effectively for word ordering even without any explicit syntactic information. In this paper, we introduce a new type of contextual representation, StructBERT, which incorporates language structures into BERT pre-training by proposing two novel linearization strategies. Specifically, in addition to the existing masking strategy, StructBERT extends BERT by leveraging the structural information: word-level ordering and sentence-level ordering. We augment model pre-training with two new structural objectives on the inner-sentence and inter-sentence structures, respectively. In this way, the linguistic aspects [8] are explicitly captured during the pre-training procedure. ar X\niv :1\n90 8.\n04 57\n7v 3\n[ cs\n.C L\n] 2\n7 Se\np 20\nWith structural pre-training, StructBERT encodes dependency between words as well as sentences in the contextualized representation, which provides the model with better generalizability and adaptability.\nStructBERT significantly advances the state-of-the-art results on a variety of NLU tasks, including the GLUE benchmark [27], the SNLI dataset [3] and the SQuAD v1.1 question answering task [21]. All of these experimental results clearly demonstrate StructBERT\u2019s exceptional effectiveness and generalization capability in language understanding. We make the following major contributions:\n\u2022 We propose novel structural pre-training that extends BERT by incorporating the word structural objective and the sentence structural objective to leverage language structures in contextualized representation. This enables the StructBERT to explicitly model language structures by forcing it to reconstruct the right order of words and sentences for correct prediction. \u2022 StructBERT significantly outperforms all published state-of-the-art models on a wide range of NLU tasks.", "The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4."]}
{"pkey": "structbert_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman (Elman, 1990), we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, the paper authors pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["The new word objective is jointly learned together with the masked LM objective in a unified pre-trained model with equal weights. 2.3.2 Sentence Structural Objective. The next sentence prediction task is considered easy for the original BERT model (the prediction accuracy of BERT can easily achieve 97%-98% in this task [6]). We, therefore, extend the sentence prediction task by predicting both the next sentence and the previous sentence, to make the pre-trained language model aware of the sequential order of the sentences in a bidirectional manner. As illustrated in Figure 1b, given a pair of sentences (S1, S2) as input, we predict whether S2 is the next sentence that follows S1, or the previous sentence that precedes S1, or a random sentence from a different document. Specifically, for the sentence S1, 13 of the time we choose the text span that follows S1 as the second sentence S2, 1 3 of the time the previous sentence ahead of S1 is selected, and 13 of the time a sentence randomly sampled from the other documents is used as S2. The two sentences are concatenated together into an input sequence with the separator token [SEP] in\nbetween, as done in BERT. We pool the model output by taking the hidden state corresponding to the first token [CLS], and feed the encoding vector of [CLS] into a softmax classifier to make a three-class prediction. 2.4 Pre-training Setup. The training objective function is a linear combination of the word structural objective and the sentence structural objective. For the masked LM objective, we followed the same masking rate and settings as in BERT [6]. 5% of trigrams are selected for random shuffling. We used documents from English Wikipedia (2,500M words) and BookCorpus [35] as pre-training data, following the preprocessing and the WordPiece tokenization from [6]. The maximum length of input sequence was set to 512.", "STRUCTBERT: INCORPORATING LANGUAGE STRUCTURES. Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7. STRUCTBERT: INCORPORATING LANGUAGE STRUCTURES. Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively.", "This model extends the superiority of BERT, and boosts the performance in many language understanding applications such as semantic textual similarity, sentiment analysis, textual entailment, and question answering. 2 StructBERT Model Pre-training.\nStructBERT builds upon the BERT architecture, which uses a multi-layer bidirectional Transformer network [26]. Given a single text sentence or a pair of text sentences, BERT packs them in one token sequence and learns a contextualized vector representation for each token. Every input token is represented based on the word, the position, and the text segment it belongs to. Next, the input vectors are fed into a stack of multi-layer bidirectional Transformer blocks, which uses self-attention to compute the text representations by considering the entire input sequence. The original BERT introduces two unsupervised prediction tasks to pre-train the model: i.e., a masked LM task and a next sentence prediction task. Different from original BERT, our StructBERT amplifies the ability of the masked LM task by shuffling certain number of tokens after word masking and predicting the right order. Moreover, to better understand the relationship between sentences, StructBERT randomly swaps the sentence order and predicts the next sentence and the previous sentence as a new sentence prediction task. In this way, the new model not only explicitly captures the fine-grained word structure in every sentence, but also properly models the inter-sentence structure in a bidirectional manner. Once the StructBERT language model is pre-trained with these two auxiliary tasks, we can fine-tune it on task-specific data for a wide range of downstream tasks. 2.1 Input Representation. Every input x is a sequence of word tokens, which can be either a single sentence or a pair of sentences packed together. The input representation follows that used in BERT [6]."]}
{"pkey": "structbert_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "In this section, the paper authors report results of StructBERT on a variety of downstream tasks including General Language Understanding Evaluation (GLUE benchmark), Standford Natural Language inference (SNLI corpus) and extractive question answering (SQuAD v1.1).", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21].", "The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4.", "We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering."]}
{"pkey": "structbert_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Different from traditional word-specific embedding in which each token is assigned a global representation, recent work, such as Cove (McCann et al., 2017), ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2018), derives contextualized word vectors from a language model trained on a large text corpus. These models have been shown effective for many downstream NLU tasks.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["Recently, word ordering was treated as LM-based linearization solely based on language models [24]. Schmaltz showed that recurrent neural network language models [17] with long short-term memory [11] cells work effectively for word ordering even without any explicit syntactic information. In this paper, we introduce a new type of contextual representation, StructBERT, which incorporates language structures into BERT pre-training by proposing two novel linearization strategies. Specifically, in addition to the existing masking strategy, StructBERT extends BERT by leveraging the structural information: word-level ordering and sentence-level ordering. We augment model pre-training with two new structural objectives on the inner-sentence and inter-sentence structures, respectively. In this way, the linguistic aspects [8] are explicitly captured during the pre-training procedure. ar X\niv :1\n90 8.\n04 57\n7v 3\n[ cs\n.C L\n] 2\n7 Se\np 20\nWith structural pre-training, StructBERT encodes dependency between words as well as sentences in the contextualized representation, which provides the model with better generalizability and adaptability.\nStructBERT significantly advances the state-of-the-art results on a variety of NLU tasks, including the GLUE benchmark [27], the SNLI dataset [3] and the SQuAD v1.1 question answering task [21]. All of these experimental results clearly demonstrate StructBERT\u2019s exceptional effectiveness and generalization capability in language understanding. We make the following major contributions:\n\u2022 We propose novel structural pre-training that extends BERT by incorporating the word structural objective and the sentence structural objective to leverage language structures in contextualized representation. This enables the StructBERT to explicitly model language structures by forcing it to reconstruct the right order of words and sentences for correct prediction. \u2022 StructBERT significantly outperforms all published state-of-the-art models on a wide range of NLU tasks.", "At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21].", "From the table, we can see that: (1) the two structural objectives were both critical to most of the downstream tasks, except for the word structural objective in the SNLI task. Removing any word or sentence objective from pre-training always led to degraded performance in the downstream tasks. The StructBERT model with structural pre-training consistently outperformed the original BERT model, which shows the effectiveness of the proposed structural objectives. (2) For the sentence-pair tasks such as MNLI, SNLI, QQP and SQuAD, incorporating the sentence structural objective significantly improved the performance. It demonstrates the effect of inter-sentence structures learned by pre-training in understanding the relationship between sentences for downstream tasks. (3) For the single-sentence tasks such as CoLA and SST-2, the word structural objective played the most important role. Especially in the CoLA task, which is related to the grammatical error correction, the improvement was over 5%. The ability of reconstructing the order of words in pre-training helped the model better judge the acceptability of a single sentence. We also studied the effect of both structural objectives during self-supervised pre-training. Figure 2 illustrates the loss and accuracy of word and sentence prediction over the number of pre-training steps for StructBERTBase and BERTBase. From the two sub-figures on top, it is observed that compared with BERT, the augmented shuffled token prediction in StructBERT\u2019s word structural objective had little effect on the loss and accuracy of masked token prediction. On the other hand, the integration of the simpler task of shuffled token prediction (lower loss and higher accuracy) provides StructBERT with the capability of word reordering. In contrast, the new sentence structural objective in StructBERT leads to a more challenging prediction task than that in BERT, as shown in the two figures at the bottom."]}
{"pkey": "structbert_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Our StructBERTLarge ensemble suppressed all published models (excluding RoBERTa ensemble and XLNet ensemble) on the average score. In the SST-2 task, our model improved over BERT while performed worse than MT-DNN did, which indicates that sentiment analysis based on single sentences benefits less from the word structural objective and sentence structural objective. With pre-training on large corpus, XLNet ensemble and RoBERTa ensemble outperformed all published models including our StructBERTLarge ensemble. To take advantage of the large data which RoBERTa is trained on, the paper authors continued pre-training with our two new objectives from the released RoBERTa model, named StructBERTRoBERTa. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["This new pre-training objective enables StructBERT to exploit inter-sentence structures, which benefits sentence-pair downstream tasks. 4 Related Work.\n4.1 Contextualized Language Representation. A word can have different semantics depending on the its context. Contextualized word representation is considered to be an important part of modern NLP research, with various pre-trained language models [16, 18, 20, 6] emerging recently. ELMo [18] learns two unidirectional LMs based on long short-term memory networks (LSTMs). A forward LM reads the text from left to right, and a backward LM encodes the text from right to left. Following the similar idea of ELMo, OpenAI GPT [20] expands the unsupervised language model to a much larger scale by training on a giant collection of free text corpora. Different from ELMo, it builds upon a multi-layer Transformer [26] decoder, and uses a left-to-right Transformer to predict a text sequence word-by-word. In contrast, BERT [6] (as well as its robustly optimized version RoBERTa [15]) employs a bidirectional Transformer encoder to fuse both the left and the right context, and introduces two novel pre-training tasks for better language understanding. We base our LM on the architecture of BERT, and further extend it by introducing word and sentence structures into pre-training tasks for deep language understanding. 4.2 Word & Sentence Ordering. The task of linearization aims to recover the original order of a shuffled sentence [24]. Part of larger discussion as to whether LSTMs are capturing syntactic phenomena linearization, is standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation [34] models. Recently, Transformers have emerged as a powerful architecture for learning the latent structure of language. For example, Bidirectional Transformers (BERT) has reduced the perplexity for language modeling task.", "The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4.", "At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21]."]}
{"pkey": "structbert_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors used documents from English Wikipedia (2,500M words) and BookCorpus (Zhu et al., 2015) as pre-training data.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21].", "We ran Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warm-up over the first 10% of the total steps, and linear decay of the learning rate. We set a dropout probability of 0.1 for every layer. The gelu activation [10] was used as done in GPT [20]. We denote the number of Transformer block layers as L, the size of hidden vectors asH , and the number of self-attention heads as A. Following the practice of BERT, We primarily report experimental results on the two model sizes:\nStructBERTBase: L = 12, H = 768, A = 12, Number of parameters= 110M\nStructBERTLarge: L = 24, H = 1024, A = 16, Number of parameters= 340M\nPre-training of StructBERT was performed on a distributed computing cluster consisting of 64 Telsa V100 GPU cards. For the StructBERTBase, we ran the pre-training procedure for 40 epochs, which took about 38 hours, and the training of StructBERTLarge took about 7 days to complete. 3 Experiments. In this section, we report results of StructBERT on a variety of downstream tasks including General Language Understanding Evaluation (GLUE benchmark), Standford Natural Language inference (SNLI corpus) and extractive question answering (SQuAD v1.1). Following BERT\u2019s practice, during fine-tuning on downstream tasks, we performed a grid search or an exhaustive search (depending on the data size) on the following sets of parameters and chose the model that performed the best on the dev set. All the other parameters remain the same as those in pre-training:\nBatch size: 16, 24, 32; Learning rate: 2e-5, 3e-5, 5e-5; Number of epochs: 2, 3; Dropout rate: 0.05, 0.1\n3.1 General Language Understanding. 3.1.1 GLUE benchmark. The General Language Understanding Evaluation (GLUE) benchmark [27] is a collection of nine NLU tasks, covering textual entailment (RTE [1] and MNLI [29]), question-answer entailment (QNLI", "The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4."]}
{"pkey": "structbert_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Texts are tokenized to subword units by WordPiece (Wu et al., 2016).", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["We ran Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warm-up over the first 10% of the total steps, and linear decay of the learning rate. We set a dropout probability of 0.1 for every layer. The gelu activation [10] was used as done in GPT [20]. We denote the number of Transformer block layers as L, the size of hidden vectors asH , and the number of self-attention heads as A. Following the practice of BERT, We primarily report experimental results on the two model sizes:\nStructBERTBase: L = 12, H = 768, A = 12, Number of parameters= 110M\nStructBERTLarge: L = 24, H = 1024, A = 16, Number of parameters= 340M\nPre-training of StructBERT was performed on a distributed computing cluster consisting of 64 Telsa V100 GPU cards. For the StructBERTBase, we ran the pre-training procedure for 40 epochs, which took about 38 hours, and the training of StructBERTLarge took about 7 days to complete. 3 Experiments. In this section, we report results of StructBERT on a variety of downstream tasks including General Language Understanding Evaluation (GLUE benchmark), Standford Natural Language inference (SNLI corpus) and extractive question answering (SQuAD v1.1). Following BERT\u2019s practice, during fine-tuning on downstream tasks, we performed a grid search or an exhaustive search (depending on the data size) on the following sets of parameters and chose the model that performed the best on the dev set. All the other parameters remain the same as those in pre-training:\nBatch size: 16, 24, 32; Learning rate: 2e-5, 3e-5, 5e-5; Number of epochs: 2, 3; Dropout rate: 0.05, 0.1\n3.1 General Language Understanding. 3.1.1 GLUE benchmark. The General Language Understanding Evaluation (GLUE) benchmark [27] is a collection of nine NLU tasks, covering textual entailment (RTE [1] and MNLI [29]), question-answer entailment (QNLI", "For each input token ti, its vector representation xi is computed by summing the corresponding token embedding, positional embedding, and segment embedding. We always add a special classification embedding ([CLS]) as the first token of every sequence, and a special end-of-sequence ([SEP]) token to the end of each segment. Texts are tokenized to subword units by WordPiece [30] and absolute positional embeddings are learned with supported sequence lengths up to 512 tokens. In addition, the segment embeddings are used to differentiate a pair of sentences as in BERT. 2.2 Transformer Encoder. We use a multi-layer bidirectional Transformer encoder [26] to encode contextual information for input representation. Given the input vectors X = {xi}Ni=1, an L-layer Transformer is used to encode the input as:\nHl = Transformerl(Hl\u22121) (1)\nwhere l \u2208 [1, L], H0 = X and HL = [hL1 , \u00b7 \u00b7 \u00b7 ,h L N ]. We use the hidden vector h L i as the contextualized representation of the input token ti.\n2.3 Pre-training Objectives. To make full use of the rich inner-sentence and inter-sentence structures in language, we extend the pre-training objectives of original BERT in two ways: 1\u00a9 word structural objective (mainly for the single-sentence task), and 2\u00a9 sentence structural objective (mainly for the sentence-pair task). We pre-train these two auxiliary objectives together with the original masked LM objective in a unified model to exploit inherent language structures. 2.3.1 Word Structural Objective. Despite its success in various NLU tasks, original BERT is unable to explicitly model the sequential order and high-order dependency of words in natural language. Given a set of words in random order from a sentence, ideally a good language model should be able to recover this sentence by reconstructing the correct order of these words.", "This model extends the superiority of BERT, and boosts the performance in many language understanding applications such as semantic textual similarity, sentiment analysis, textual entailment, and question answering. 2 StructBERT Model Pre-training.\nStructBERT builds upon the BERT architecture, which uses a multi-layer bidirectional Transformer network [26]. Given a single text sentence or a pair of text sentences, BERT packs them in one token sequence and learns a contextualized vector representation for each token. Every input token is represented based on the word, the position, and the text segment it belongs to. Next, the input vectors are fed into a stack of multi-layer bidirectional Transformer blocks, which uses self-attention to compute the text representations by considering the entire input sequence. The original BERT introduces two unsupervised prediction tasks to pre-train the model: i.e., a masked LM task and a next sentence prediction task. Different from original BERT, our StructBERT amplifies the ability of the masked LM task by shuffling certain number of tokens after word masking and predicting the right order. Moreover, to better understand the relationship between sentences, StructBERT randomly swaps the sentence order and predicts the next sentence and the previous sentence as a new sentence prediction task. In this way, the new model not only explicitly captures the fine-grained word structure in every sentence, but also properly models the inter-sentence structure in a bidirectional manner. Once the StructBERT language model is pre-trained with these two auxiliary tasks, we can fine-tune it on task-specific data for a wide range of downstream tasks. 2.1 Input Representation. Every input x is a sequence of word tokens, which can be either a single sentence or a pair of sentences packed together. The input representation follows that used in BERT [6]."]}
{"pkey": "structbert_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "To make full use of the rich inner-sentence and inter-sentence structures in language, the paper authors extend the pre-training objectives of original BERT in two ways: \n1. word structural objective (mainly for the single-sentence task), and  \n2. sentence structural objective (mainly for the sentence-pair task). We pre-train these two auxiliary objectives together with the original masked LM objective in a unified model to exploit inherent language structures.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21].", "The new word objective is jointly learned together with the masked LM objective in a unified pre-trained model with equal weights. 2.3.2 Sentence Structural Objective. The next sentence prediction task is considered easy for the original BERT model (the prediction accuracy of BERT can easily achieve 97%-98% in this task [6]). We, therefore, extend the sentence prediction task by predicting both the next sentence and the previous sentence, to make the pre-trained language model aware of the sequential order of the sentences in a bidirectional manner. As illustrated in Figure 1b, given a pair of sentences (S1, S2) as input, we predict whether S2 is the next sentence that follows S1, or the previous sentence that precedes S1, or a random sentence from a different document. Specifically, for the sentence S1, 13 of the time we choose the text span that follows S1 as the second sentence S2, 1 3 of the time the previous sentence ahead of S1 is selected, and 13 of the time a sentence randomly sampled from the other documents is used as S2. The two sentences are concatenated together into an input sequence with the separator token [SEP] in\nbetween, as done in BERT. We pool the model output by taking the hidden state corresponding to the first token [CLS], and feed the encoding vector of [CLS] into a softmax classifier to make a three-class prediction. 2.4 Pre-training Setup. The training objective function is a linear combination of the word structural objective and the sentence structural objective. For the masked LM objective, we followed the same masking rate and settings as in BERT [6]. 5% of trigrams are selected for random shuffling. We used documents from English Wikipedia (2,500M words) and BookCorpus [35] as pre-training data, following the preprocessing and the WordPiece tokenization from [6]. The maximum length of input sequence was set to 512.", "The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4."]}
{"pkey": "structbert_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "The paper authors denote the number of Transformer block layers as L, the size of hidden vectors as H, and the number of self-attention heads as A. Following the practice of BERT, The paper authors primarily report experimental results on the two model sizes:\nStructBERTBase: L = 12, H = 768, A = 12, Number of parameters= 110M\nStructBERTLarge: L = 24, H = 1024, A = 16, Number of parameters= 340M", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["We ran Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warm-up over the first 10% of the total steps, and linear decay of the learning rate. We set a dropout probability of 0.1 for every layer. The gelu activation [10] was used as done in GPT [20]. We denote the number of Transformer block layers as L, the size of hidden vectors asH , and the number of self-attention heads as A. Following the practice of BERT, We primarily report experimental results on the two model sizes:\nStructBERTBase: L = 12, H = 768, A = 12, Number of parameters= 110M\nStructBERTLarge: L = 24, H = 1024, A = 16, Number of parameters= 340M\nPre-training of StructBERT was performed on a distributed computing cluster consisting of 64 Telsa V100 GPU cards. For the StructBERTBase, we ran the pre-training procedure for 40 epochs, which took about 38 hours, and the training of StructBERTLarge took about 7 days to complete. 3 Experiments. In this section, we report results of StructBERT on a variety of downstream tasks including General Language Understanding Evaluation (GLUE benchmark), Standford Natural Language inference (SNLI corpus) and extractive question answering (SQuAD v1.1). Following BERT\u2019s practice, during fine-tuning on downstream tasks, we performed a grid search or an exhaustive search (depending on the data size) on the following sets of parameters and chose the model that performed the best on the dev set. All the other parameters remain the same as those in pre-training:\nBatch size: 16, 24, 32; Learning rate: 2e-5, 3e-5, 5e-5; Number of epochs: 2, 3; Dropout rate: 0.05, 0.1\n3.1 General Language Understanding. 3.1.1 GLUE benchmark. The General Language Understanding Evaluation (GLUE) benchmark [27] is a collection of nine NLU tasks, covering textual entailment (RTE [1] and MNLI [29]), question-answer entailment (QNLI", "This new pre-training objective enables StructBERT to exploit inter-sentence structures, which benefits sentence-pair downstream tasks. 4 Related Work.\n4.1 Contextualized Language Representation. A word can have different semantics depending on the its context. Contextualized word representation is considered to be an important part of modern NLP research, with various pre-trained language models [16, 18, 20, 6] emerging recently. ELMo [18] learns two unidirectional LMs based on long short-term memory networks (LSTMs). A forward LM reads the text from left to right, and a backward LM encodes the text from right to left. Following the similar idea of ELMo, OpenAI GPT [20] expands the unsupervised language model to a much larger scale by training on a giant collection of free text corpora. Different from ELMo, it builds upon a multi-layer Transformer [26] decoder, and uses a left-to-right Transformer to predict a text sequence word-by-word. In contrast, BERT [6] (as well as its robustly optimized version RoBERTa [15]) employs a bidirectional Transformer encoder to fuse both the left and the right context, and introduces two novel pre-training tasks for better language understanding. We base our LM on the architecture of BERT, and further extend it by introducing word and sentence structures into pre-training tasks for deep language understanding. 4.2 Word & Sentence Ordering. The task of linearization aims to recover the original order of a shuffled sentence [24]. Part of larger discussion as to whether LSTMs are capturing syntactic phenomena linearization, is standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation [34] models. Recently, Transformers have emerged as a powerful architecture for learning the latent structure of language. For example, Bidirectional Transformers (BERT) has reduced the perplexity for language modeling task.", "As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.\n1 Introduction. A pre-trained language model (LM) is a key component in many natural language understanding (NLU) tasks such as semantic textual similarity [4], question answering [21] and sentiment classification [25]. In order to obtain reliable language representations, neural language models are designed to define the joint probability function of sequences of words in text with self-supervised learning. Different from traditional word-specific embedding in which each token is assigned a global representation, recent work, such as Cove [16], ELMo [18], GPT [20] and BERT [6], derives contextualized word vectors from a language model trained on a large text corpus. These models have been shown effective for many downstream NLU tasks. Among the context-sensitive language models, BERT (and its robustly optimized version RoBERTa [15]) has taken the NLP world by storm. It is designed to pre-train bidirectional representations by jointly conditioning on both left and right context in all layers and model the representations by predicting masked words only through the contexts. However, it does not make the most of underlying language structures. According to Elman [8]\u2019s study, the recurrent neural networks was shown to be sensitive to regularities in word order in simple sentences. Since language fluency is determined by the ordering of words and sentences, finding the best permutation of a set of words and sentences is an essential problem in many NLP tasks, such as machine translation and NLU [9]."]}
{"pkey": "structbert_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The maximum length of input sequence was set to 512. The paper authors ran Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warm-up over the first 10% of the total steps, and linear decay of the learning rate. The paper authors set a dropout probability of 0.1 for every layer. The gelu activation (Hendrycks & Gimpel, 2016) was used as done in GPT (Radford et al., 2018). Pre-training of StructBERT was performed on a distributed computing cluster consisting of 64 Telsa V100 GPU cards. For the StructBERTBase, the paper authors ran the pre-training procedure for 40 epochs.  Batch size: 16, 24, 32; Learning rate: 2e-5, 3e-5, 5e-5; Number of epochs: 2, 3; Dropout rate: 0.05, 0.1", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["We ran Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warm-up over the first 10% of the total steps, and linear decay of the learning rate. We set a dropout probability of 0.1 for every layer. The gelu activation [10] was used as done in GPT [20]. We denote the number of Transformer block layers as L, the size of hidden vectors asH , and the number of self-attention heads as A. Following the practice of BERT, We primarily report experimental results on the two model sizes:\nStructBERTBase: L = 12, H = 768, A = 12, Number of parameters= 110M\nStructBERTLarge: L = 24, H = 1024, A = 16, Number of parameters= 340M\nPre-training of StructBERT was performed on a distributed computing cluster consisting of 64 Telsa V100 GPU cards. For the StructBERTBase, we ran the pre-training procedure for 40 epochs, which took about 38 hours, and the training of StructBERTLarge took about 7 days to complete. 3 Experiments. In this section, we report results of StructBERT on a variety of downstream tasks including General Language Understanding Evaluation (GLUE benchmark), Standford Natural Language inference (SNLI corpus) and extractive question answering (SQuAD v1.1). Following BERT\u2019s practice, during fine-tuning on downstream tasks, we performed a grid search or an exhaustive search (depending on the data size) on the following sets of parameters and chose the model that performed the best on the dev set. All the other parameters remain the same as those in pre-training:\nBatch size: 16, 24, 32; Learning rate: 2e-5, 3e-5, 5e-5; Number of epochs: 2, 3; Dropout rate: 0.05, 0.1\n3.1 General Language Understanding. 3.1.1 GLUE benchmark. The General Language Understanding Evaluation (GLUE) benchmark [27] is a collection of nine NLU tasks, covering textual entailment (RTE [1] and MNLI [29]), question-answer entailment (QNLI", "The new word objective is jointly learned together with the masked LM objective in a unified pre-trained model with equal weights. 2.3.2 Sentence Structural Objective. The next sentence prediction task is considered easy for the original BERT model (the prediction accuracy of BERT can easily achieve 97%-98% in this task [6]). We, therefore, extend the sentence prediction task by predicting both the next sentence and the previous sentence, to make the pre-trained language model aware of the sequential order of the sentences in a bidirectional manner. As illustrated in Figure 1b, given a pair of sentences (S1, S2) as input, we predict whether S2 is the next sentence that follows S1, or the previous sentence that precedes S1, or a random sentence from a different document. Specifically, for the sentence S1, 13 of the time we choose the text span that follows S1 as the second sentence S2, 1 3 of the time the previous sentence ahead of S1 is selected, and 13 of the time a sentence randomly sampled from the other documents is used as S2. The two sentences are concatenated together into an input sequence with the separator token [SEP] in\nbetween, as done in BERT. We pool the model output by taking the hidden state corresponding to the first token [CLS], and feed the encoding vector of [CLS] into a softmax classifier to make a three-class prediction. 2.4 Pre-training Setup. The training objective function is a linear combination of the word structural objective and the sentence structural objective. For the masked LM objective, we followed the same masking rate and settings as in BERT [6]. 5% of trigrams are selected for random shuffling. We used documents from English Wikipedia (2,500M words) and BookCorpus [35] as pre-training data, following the preprocessing and the WordPiece tokenization from [6]. The maximum length of input sequence was set to 512.", "STRUCTBERT: INCORPORATING LANGUAGE STRUCTURES. Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7. STRUCTBERT: INCORPORATING LANGUAGE STRUCTURES. Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively."]}
{"pkey": "structbert_13", "question": "Describe the computational resources used to train the model.", "answer": "Pre-training of StructBERT was performed on a distributed computing cluster consisting of 64 Telsa V100 GPU cards. For the StructBERTBase, the paper authors ran the pre-training procedure for 40 epochs, which took about 38 hours, and the training of StructBERTLarge took about 7 days to complete.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["To implement this idea in StructBERT, we supplement BERT\u2019s training objectives with a new word structural objective which endows the model with the ability to reconstruct the right order of certain number of intentionally shuffled word tokens. This new word objective is jointly trained together with the original masked LM objective from BERT. Figure 1a illustrates the procedure of jointly training the new word objective and the masked LM objective. In every input sequence, we first mask 15% of all tokens at random, as done in BERT [6]. The corresponding output vectors hLi of the masked tokens computed by the bidirectional Transformer encoder are fed into a softmax classifier to predict the original tokens. Next, the new word objective comes into play to take word order into consideration. Given the randomicity of token shuffling, the word objective is equivalent to maximizing the likelihood of placing every shuffled token in its correct position. More formally, this objective can be formulated as:\nargmax \u03b8\n\u2211 logP (pos1 = t1, pos2 = t2, . . . , posK = tK |t1, t2, . . . , tK , \u03b8), (2)\nwhere \u03b8 represents the set of trainable parameters of StructBERT, and K indicates the length of every shuffled subsequence. Technically, a larger K would force the model to be able to reconstruct longer sequences while injecting more disturbed input. On the contrary, when K is smaller, the model gets more undisturbed sequences while less capable of recovering long sequences. We decide to use trigrams (i.e., K = 3) for subsequence shuffling to balance language reconstructability and robustness of the model. Specifically, as shown in Figure 1a, we randomly choose some percentage of trigrams from unmasked tokens, and shuffle the three words (e.g., t2, t3, and t4 in the figure) within each of the trigrams. The output vectors of the shuffled tokens computed by the bidirectional Transformer encoder are then fed into a softmax classifier to predict the original tokens.", "This model extends the superiority of BERT, and boosts the performance in many language understanding applications such as semantic textual similarity, sentiment analysis, textual entailment, and question answering. 2 StructBERT Model Pre-training.\nStructBERT builds upon the BERT architecture, which uses a multi-layer bidirectional Transformer network [26]. Given a single text sentence or a pair of text sentences, BERT packs them in one token sequence and learns a contextualized vector representation for each token. Every input token is represented based on the word, the position, and the text segment it belongs to. Next, the input vectors are fed into a stack of multi-layer bidirectional Transformer blocks, which uses self-attention to compute the text representations by considering the entire input sequence. The original BERT introduces two unsupervised prediction tasks to pre-train the model: i.e., a masked LM task and a next sentence prediction task. Different from original BERT, our StructBERT amplifies the ability of the masked LM task by shuffling certain number of tokens after word masking and predicting the right order. Moreover, to better understand the relationship between sentences, StructBERT randomly swaps the sentence order and predicts the next sentence and the previous sentence as a new sentence prediction task. In this way, the new model not only explicitly captures the fine-grained word structure in every sentence, but also properly models the inter-sentence structure in a bidirectional manner. Once the StructBERT language model is pre-trained with these two auxiliary tasks, we can fine-tune it on task-specific data for a wide range of downstream tasks. 2.1 Input Representation. Every input x is a sequence of word tokens, which can be either a single sentence or a pair of sentences packed together. The input representation follows that used in BERT [6].", "For each input token ti, its vector representation xi is computed by summing the corresponding token embedding, positional embedding, and segment embedding. We always add a special classification embedding ([CLS]) as the first token of every sequence, and a special end-of-sequence ([SEP]) token to the end of each segment. Texts are tokenized to subword units by WordPiece [30] and absolute positional embeddings are learned with supported sequence lengths up to 512 tokens. In addition, the segment embeddings are used to differentiate a pair of sentences as in BERT. 2.2 Transformer Encoder. We use a multi-layer bidirectional Transformer encoder [26] to encode contextual information for input representation. Given the input vectors X = {xi}Ni=1, an L-layer Transformer is used to encode the input as:\nHl = Transformerl(Hl\u22121) (1)\nwhere l \u2208 [1, L], H0 = X and HL = [hL1 , \u00b7 \u00b7 \u00b7 ,h L N ]. We use the hidden vector h L i as the contextualized representation of the input token ti.\n2.3 Pre-training Objectives. To make full use of the rich inner-sentence and inter-sentence structures in language, we extend the pre-training objectives of original BERT in two ways: 1\u00a9 word structural objective (mainly for the single-sentence task), and 2\u00a9 sentence structural objective (mainly for the sentence-pair task). We pre-train these two auxiliary objectives together with the original masked LM objective in a unified model to exploit inherent language structures. 2.3.1 Word Structural Objective. Despite its success in various NLU tasks, original BERT is unable to explicitly model the sequential order and high-order dependency of words in natural language. Given a set of words in random order from a sentence, ideally a good language model should be able to recover this sentence by reconstructing the correct order of these words."]}
{"pkey": "structbert_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "The training objective function is a linear combination of the word structural objective and the sentence structural objective. For the masked LM objective, the paper authors followed the same masking rate and settings as in BERT (Devlin et al., 2018). 5% of trigrams are selected for random shuffling. The paper authors used documents from English Wikipedia (2,500M words) and BookCorpus (Zhu et al., 2015) as pre-training data, following the preprocessing and the WordPiece tokenization from (Devlin et al., 2018). The maximum length of input sequence was set to 512. The paper authors ran Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warm-up over the first 10% of the total steps, and linear decay of the learning rate. The paper authors set a dropout probability of 0.1 for every layer. The gelu activation (Hendrycks & Gimpel, 2016) was used as done in GPT (Radford et al., 2018).\nThe paper authors denote the number of Transformer block layers as L, the size of hidden vectors as H, and the number of self-attention heads as A. Following the practice of BERT, The paper authors primarily report experimental results on the two model sizes:\nStructBERTBase: L = 12, H = 768, A = 12, Number of parameters= 110M\nStructBERTLarge: L = 24, H = 1024, A = 16, Number of parameters= 340M \nPre-training of StructBERT was performed on a distributed computing cluster consisting of 64 Telsa V100 GPU cards. For the StructBERTBase, the paper authors ran the pre-training procedure for 40 epochs, which took about 38 hours, and the training of StructBERTLarge took about 7 days to complete.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["Recently, word ordering was treated as LM-based linearization solely based on language models [24]. Schmaltz showed that recurrent neural network language models [17] with long short-term memory [11] cells work effectively for word ordering even without any explicit syntactic information. In this paper, we introduce a new type of contextual representation, StructBERT, which incorporates language structures into BERT pre-training by proposing two novel linearization strategies. Specifically, in addition to the existing masking strategy, StructBERT extends BERT by leveraging the structural information: word-level ordering and sentence-level ordering. We augment model pre-training with two new structural objectives on the inner-sentence and inter-sentence structures, respectively. In this way, the linguistic aspects [8] are explicitly captured during the pre-training procedure. ar X\niv :1\n90 8.\n04 57\n7v 3\n[ cs\n.C L\n] 2\n7 Se\np 20\nWith structural pre-training, StructBERT encodes dependency between words as well as sentences in the contextualized representation, which provides the model with better generalizability and adaptability.\nStructBERT significantly advances the state-of-the-art results on a variety of NLU tasks, including the GLUE benchmark [27], the SNLI dataset [3] and the SQuAD v1.1 question answering task [21]. All of these experimental results clearly demonstrate StructBERT\u2019s exceptional effectiveness and generalization capability in language understanding. We make the following major contributions:\n\u2022 We propose novel structural pre-training that extends BERT by incorporating the word structural objective and the sentence structural objective to leverage language structures in contextualized representation. This enables the StructBERT to explicitly model language structures by forcing it to reconstruct the right order of words and sentences for correct prediction. \u2022 StructBERT significantly outperforms all published state-of-the-art models on a wide range of NLU tasks.", "The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4.", "At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21]."]}
{"pkey": "structbert_15", "question": "What is the pretraining objective of the model? ", "answer": "To make full use of the rich inner-sentence and inter-sentence structures in language, the paper authors extend the pre-training objectives of original BERT in two ways: \n1. word structural objective (mainly for the single-sentence task), and  \n2. sentence structural objective (mainly for the sentence-pair task). We pre-train these two auxiliary objectives together with the original masked LM objective in a unified model to exploit inherent language structures.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering.", "The new word objective is jointly learned together with the masked LM objective in a unified pre-trained model with equal weights. 2.3.2 Sentence Structural Objective. The next sentence prediction task is considered easy for the original BERT model (the prediction accuracy of BERT can easily achieve 97%-98% in this task [6]). We, therefore, extend the sentence prediction task by predicting both the next sentence and the previous sentence, to make the pre-trained language model aware of the sequential order of the sentences in a bidirectional manner. As illustrated in Figure 1b, given a pair of sentences (S1, S2) as input, we predict whether S2 is the next sentence that follows S1, or the previous sentence that precedes S1, or a random sentence from a different document. Specifically, for the sentence S1, 13 of the time we choose the text span that follows S1 as the second sentence S2, 1 3 of the time the previous sentence ahead of S1 is selected, and 13 of the time a sentence randomly sampled from the other documents is used as S2. The two sentences are concatenated together into an input sequence with the separator token [SEP] in\nbetween, as done in BERT. We pool the model output by taking the hidden state corresponding to the first token [CLS], and feed the encoding vector of [CLS] into a softmax classifier to make a three-class prediction. 2.4 Pre-training Setup. The training objective function is a linear combination of the word structural objective and the sentence structural objective. For the masked LM objective, we followed the same masking rate and settings as in BERT [6]. 5% of trigrams are selected for random shuffling. We used documents from English Wikipedia (2,500M words) and BookCorpus [35] as pre-training data, following the preprocessing and the WordPiece tokenization from [6]. The maximum length of input sequence was set to 512.", "At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21]."]}
{"pkey": "structbert_16", "question": "What is the loss function that is used to train the model?", "answer": "The paper authors pre-train these two auxiliary objectives together with the original masked LM objective in a unified model to exploit inherent language structures.  To implement this idea in StructBERT, the paper authors supplement BERT\u2019s training objectives with a new word structural objective which endows the model with the ability to reconstruct the right order of certain number of intentionally shuffled word tokens. This new word objective is jointly trained together with the original masked LM objective from BERT. The new word objective is jointly learned together with the masked LM objective in a unified pre-trained model with equal weights.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["From the table, we can see that: (1) the two structural objectives were both critical to most of the downstream tasks, except for the word structural objective in the SNLI task. Removing any word or sentence objective from pre-training always led to degraded performance in the downstream tasks. The StructBERT model with structural pre-training consistently outperformed the original BERT model, which shows the effectiveness of the proposed structural objectives. (2) For the sentence-pair tasks such as MNLI, SNLI, QQP and SQuAD, incorporating the sentence structural objective significantly improved the performance. It demonstrates the effect of inter-sentence structures learned by pre-training in understanding the relationship between sentences for downstream tasks. (3) For the single-sentence tasks such as CoLA and SST-2, the word structural objective played the most important role. Especially in the CoLA task, which is related to the grammatical error correction, the improvement was over 5%. The ability of reconstructing the order of words in pre-training helped the model better judge the acceptability of a single sentence. We also studied the effect of both structural objectives during self-supervised pre-training. Figure 2 illustrates the loss and accuracy of word and sentence prediction over the number of pre-training steps for StructBERTBase and BERTBase. From the two sub-figures on top, it is observed that compared with BERT, the augmented shuffled token prediction in StructBERT\u2019s word structural objective had little effect on the loss and accuracy of masked token prediction. On the other hand, the integration of the simpler task of shuffled token prediction (lower loss and higher accuracy) provides StructBERT with the capability of word reordering. In contrast, the new sentence structural objective in StructBERT leads to a more challenging prediction task than that in BERT, as shown in the two figures at the bottom.", "The new word objective is jointly learned together with the masked LM objective in a unified pre-trained model with equal weights. 2.3.2 Sentence Structural Objective. The next sentence prediction task is considered easy for the original BERT model (the prediction accuracy of BERT can easily achieve 97%-98% in this task [6]). We, therefore, extend the sentence prediction task by predicting both the next sentence and the previous sentence, to make the pre-trained language model aware of the sequential order of the sentences in a bidirectional manner. As illustrated in Figure 1b, given a pair of sentences (S1, S2) as input, we predict whether S2 is the next sentence that follows S1, or the previous sentence that precedes S1, or a random sentence from a different document. Specifically, for the sentence S1, 13 of the time we choose the text span that follows S1 as the second sentence S2, 1 3 of the time the previous sentence ahead of S1 is selected, and 13 of the time a sentence randomly sampled from the other documents is used as S2. The two sentences are concatenated together into an input sequence with the separator token [SEP] in\nbetween, as done in BERT. We pool the model output by taking the hidden state corresponding to the first token [CLS], and feed the encoding vector of [CLS] into a softmax classifier to make a three-class prediction. 2.4 Pre-training Setup. The training objective function is a linear combination of the word structural objective and the sentence structural objective. For the masked LM objective, we followed the same masking rate and settings as in BERT [6]. 5% of trigrams are selected for random shuffling. We used documents from English Wikipedia (2,500M words) and BookCorpus [35] as pre-training data, following the preprocessing and the WordPiece tokenization from [6]. The maximum length of input sequence was set to 512.", "We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering."]}
{"pkey": "structbert_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The paper authors use a multi-layer bidirectional Transformer encoder [26] to encode contextual information for input representation. In this paper, the paper authors introduce a new type of contextual representation, StructBERT, which incorporates language structures into BERT pre-training by proposing two novel linearization strategies. Specifically, in addition to the existing masking strategy, StructBERT extends BERT by leveraging the structural information: word-level ordering and sentence-level ordering. The paper authors augment model pre-training with two new structural objectives on the inner-sentence and inter-sentence structures, respectively. In this way, the linguistic aspects (Elman, 1990) are explicitly captured during the pre-training procedure. With structural pre-training, StructBERT encodes dependency between words as well as sentences in the contextualized representation, which provides the model with better generalizability and adaptability.\nThe paper authors propose novel structural pre-training that extends BERT by incorporating the word structural objective and the sentence structural objective to leverage language structures in contextualized representation. This enables the StructBERT to explicitly model language structures by forcing it to reconstruct the right order of words and sentences for correct prediction.\nStructBERT significantly outperforms all published state-of-the-art models on a wide range of NLU tasks at the time of model submission. This model extends the superiority of BERT, and boosts the performance in many language understanding applications such as semantic textual similarity, sentiment analysis, textual entailment, and question answering.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["This new pre-training objective enables StructBERT to exploit inter-sentence structures, which benefits sentence-pair downstream tasks. 4 Related Work.\n4.1 Contextualized Language Representation. A word can have different semantics depending on the its context. Contextualized word representation is considered to be an important part of modern NLP research, with various pre-trained language models [16, 18, 20, 6] emerging recently. ELMo [18] learns two unidirectional LMs based on long short-term memory networks (LSTMs). A forward LM reads the text from left to right, and a backward LM encodes the text from right to left. Following the similar idea of ELMo, OpenAI GPT [20] expands the unsupervised language model to a much larger scale by training on a giant collection of free text corpora. Different from ELMo, it builds upon a multi-layer Transformer [26] decoder, and uses a left-to-right Transformer to predict a text sequence word-by-word. In contrast, BERT [6] (as well as its robustly optimized version RoBERTa [15]) employs a bidirectional Transformer encoder to fuse both the left and the right context, and introduces two novel pre-training tasks for better language understanding. We base our LM on the architecture of BERT, and further extend it by introducing word and sentence structures into pre-training tasks for deep language understanding. 4.2 Word & Sentence Ordering. The task of linearization aims to recover the original order of a shuffled sentence [24]. Part of larger discussion as to whether LSTMs are capturing syntactic phenomena linearization, is standardized in a recent line of research as a method useful for isolating the performance of text-to-text generation [34] models. Recently, Transformers have emerged as a powerful architecture for learning the latent structure of language. For example, Bidirectional Transformers (BERT) has reduced the perplexity for language modeling task.", "This model extends the superiority of BERT, and boosts the performance in many language understanding applications such as semantic textual similarity, sentiment analysis, textual entailment, and question answering. 2 StructBERT Model Pre-training.\nStructBERT builds upon the BERT architecture, which uses a multi-layer bidirectional Transformer network [26]. Given a single text sentence or a pair of text sentences, BERT packs them in one token sequence and learns a contextualized vector representation for each token. Every input token is represented based on the word, the position, and the text segment it belongs to. Next, the input vectors are fed into a stack of multi-layer bidirectional Transformer blocks, which uses self-attention to compute the text representations by considering the entire input sequence. The original BERT introduces two unsupervised prediction tasks to pre-train the model: i.e., a masked LM task and a next sentence prediction task. Different from original BERT, our StructBERT amplifies the ability of the masked LM task by shuffling certain number of tokens after word masking and predicting the right order. Moreover, to better understand the relationship between sentences, StructBERT randomly swaps the sentence order and predicts the next sentence and the previous sentence as a new sentence prediction task. In this way, the new model not only explicitly captures the fine-grained word structure in every sentence, but also properly models the inter-sentence structure in a bidirectional manner. Once the StructBERT language model is pre-trained with these two auxiliary tasks, we can fine-tune it on task-specific data for a wide range of downstream tasks. 2.1 Input Representation. Every input x is a sequence of word tokens, which can be either a single sentence or a pair of sentences packed together. The input representation follows that used in BERT [6].", "[27]), paraphrase (MRPC [7]), question\nparaphrase (QQP 1), textual similarity (STS-B [4]), sentiment (SST-2 [25]), linguistic acceptability (CoLA [28]), and Winograd Schema (WNLI [13]). On the GLUE benchmark, given the similarity of MRPC/RTE/STS-B to MNLI, we fine-tuned StructBERT on MNLI before training on MRPC/RTE/STS-B data for the respective tasks. This follows the two-stage transfer learning STILTs introduced in [19]. For all the other tasks (i.e., RTE, QNLI, QQP, SST-2, CoLA and MNLI), we fine-tuned StructBERT for each single task only on its in-domain data. Table 1 presents the results of published models on the GLUE test set obtained from the official benchmark evaluation server. Our StructBERTLarge ensemble suppressed all published models (excluding RoBERTa ensemble and XLNet ensemble) on the average score, and performed the best among these models in six of the nine tasks. In the most popular MNLI task, our StructBERTLarge single model improved the best result by 0.3%/0.5%, since we fine-tuned MNLI only on its in-domain data, this improvement is entirely attributed to our new training objectives. The most significant improvement over BERT was observed on CoLA (4.8%), which may be due to the strong correlation between the word order task and the grammatical error correction task. In the SST-2 task, our model improved over BERT while performed worse than MT-DNN did, which indicates that sentiment analysis based on single sentences benefits less from the word structural objective and sentence structural objective. With pre-training on large corpus, XLNet ensemble and RoBERTa ensemble outperformed all published models including our StructBERTLarge ensemble. To take advantage of the large data which RoBERTa is trained on, we continued pre-training with our two new objectives from the released RoBERTa model, named StructBERTRoBERTa."]}
{"pkey": "structbert_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "In this section, the paper authors report results of StructBERT on a variety of downstream tasks including the General Language Understanding Evaluation (GLUE benchmark), Stanford Natural Language Inference (SNLI corpus), and extractive question answering (SQuAD v1.1). Following BERT\u2019s practice, during fine-tuning on downstream tasks, the paper authors performed a grid search or an exhaustive search (depending on the data size) on the following sets of parameters and chose the model that performed the best on the dev set. All other parameters remain the same as those in pre-training. The paper authors have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, the paper authors conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4.", "We ran Adam with learning rate of 1e-4, \u03b21 = 0.9, \u03b22 = 0.999, L2 weight decay of 0.01, learning rate warm-up over the first 10% of the total steps, and linear decay of the learning rate. We set a dropout probability of 0.1 for every layer. The gelu activation [10] was used as done in GPT [20]. We denote the number of Transformer block layers as L, the size of hidden vectors asH , and the number of self-attention heads as A. Following the practice of BERT, We primarily report experimental results on the two model sizes:\nStructBERTBase: L = 12, H = 768, A = 12, Number of parameters= 110M\nStructBERTLarge: L = 24, H = 1024, A = 16, Number of parameters= 340M\nPre-training of StructBERT was performed on a distributed computing cluster consisting of 64 Telsa V100 GPU cards. For the StructBERTBase, we ran the pre-training procedure for 40 epochs, which took about 38 hours, and the training of StructBERTLarge took about 7 days to complete. 3 Experiments. In this section, we report results of StructBERT on a variety of downstream tasks including General Language Understanding Evaluation (GLUE benchmark), Standford Natural Language inference (SNLI corpus) and extractive question answering (SQuAD v1.1). Following BERT\u2019s practice, during fine-tuning on downstream tasks, we performed a grid search or an exhaustive search (depending on the data size) on the following sets of parameters and chose the model that performed the best on the dev set. All the other parameters remain the same as those in pre-training:\nBatch size: 16, 24, 32; Learning rate: 2e-5, 3e-5, 5e-5; Number of epochs: 2, 3; Dropout rate: 0.05, 0.1\n3.1 General Language Understanding. 3.1.1 GLUE benchmark. The General Language Understanding Evaluation (GLUE) benchmark [27] is a collection of nine NLU tasks, covering textual entailment (RTE [1] and MNLI [29]), question-answer entailment (QNLI", "We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering."]}
{"pkey": "structbert_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, the paper authors conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks.  Ablation over the pre-training objectives using StructBERTBase architecture. Every result is the average score of 8 runs with different random seeds (the MNLI accuracy is the average score of the matched and mis-matched settings).", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4.", "We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering.", "At the time of paper submission, our StructBERTRoBERTa ensemble, which was submitted under a different name ALICE, achieved the best performance among all published models including RoBERTa on the leaderboard, creating a new state-of-the-art result of 89.0% on the average GLUE score. It demonstrates that the proposed objectives are able to improve language models in addition to BERT. 3.1.2 SNLI. Natural Language Inference (NLI) is one of the important tasks in natural language understanding. The goal of this task is to test the ability of the model to reason the semantic relationship between two sentences. In order to perform well on an NLI task, a model needs to capture the semantics of sentences, and thus to infer the relationship between a pair of sentences: entailment, contradiction or neutral. We evaluated our model on the most widely used NLI dataset: The Stanford Natural Language Inference (SNLI) Corpus [3], which consists of 549,367/9,842/9,824 premise-hypothesis pairs in train/dev/test sets and target labels indicating their relations. We performed a grid search on the sets of parameters, and chose the model that performed best on the dev set. Table 2 shows the results on the SNLI dataset of our model with other published models. StructBERT outperformed all existing systems on SNLI, creating new state-of-the-art results 91.7%, which amounts to 0.4% absolute improvement over the previous state-of-the-art model SJRC and 0.9% absolute improvement over BERT. Since the network architecture of our model is identical to that of BERT, this improvement is entirely attributed to the new pre-training objectives, which justifies the effectiveness of the proposed tasks of word prediction and sentence prediction. 3.2 Extractive Question Answering. SQuAD v1.1 is a popular machine reading comprehension dataset consisting of 100,000+ questions created by crowd workers on 536 Wikipedia articles [21]."]}
{"pkey": "structbert_20", "question": "List the future work mentioned in the paper.", "answer": "No future directions for extending the work are specified in paper.", "title": "StructBert: INCORPORATING LANGUAGE STRUCTURES INTO PRE-TRAINING FOR DEEP LANGUAGE UNDERSTANDING", "context": ["We revisit Elman\u2019s question by applying BERT to the word-ordering task, without any explicit syntactic approaches, and find that pre-trained language models are effective for various downstream tasks with linearization. Many important downstream tasks such as STS and NLI [27] are based on understanding the relationship between two text sentences, which is not directly captured by language modeling. While BERT [6] pre-trains a binarized next sentence prediction task to understand sentence relationships, we take one step further and treat it as a sentence ordering task. The goal of sentence ordering is to arrange a set of sentences into a coherent text in a clear and consistent manner, which can be viewed as a ranking problem [5]. The task is general and yet challenging, and once is especially important for natural language generation [23]. Text should be organized according to the following properties: rhetorical\ncoherence, topical relevancy, chronological sequence, and cause-effect. In this work, we focus on what is arguably the most basic characteristics of a sequence: their order. Most of prior work on sentence ordering was part of the study of downstream tasks, such as multi-document summarization [2]. We revisit this problem in the context of language modeling as a new sentence prediction task. 5 Conclusion. In this paper, we propose novel structural pre-training which incorporates word and sentence structures into BERT pre-training. A word structural objective and a sentence structural objective are introduced as two new pre-training tasks for deep understanding of natural language in different granularities. Experimental results demonstrate that the new StructBERT model can obtain new state-of-the-art results in a variety of downstream tasks, including the popular GLUE benchmark, the SNLI Corpus and the SQuAD v1.1 question answering.", "Recently, word ordering was treated as LM-based linearization solely based on language models [24]. Schmaltz showed that recurrent neural network language models [17] with long short-term memory [11] cells work effectively for word ordering even without any explicit syntactic information. In this paper, we introduce a new type of contextual representation, StructBERT, which incorporates language structures into BERT pre-training by proposing two novel linearization strategies. Specifically, in addition to the existing masking strategy, StructBERT extends BERT by leveraging the structural information: word-level ordering and sentence-level ordering. We augment model pre-training with two new structural objectives on the inner-sentence and inter-sentence structures, respectively. In this way, the linguistic aspects [8] are explicitly captured during the pre-training procedure. ar X\niv :1\n90 8.\n04 57\n7v 3\n[ cs\n.C L\n] 2\n7 Se\np 20\nWith structural pre-training, StructBERT encodes dependency between words as well as sentences in the contextualized representation, which provides the model with better generalizability and adaptability.\nStructBERT significantly advances the state-of-the-art results on a variety of NLU tasks, including the GLUE benchmark [27], the SNLI dataset [3] and the SQuAD v1.1 question answering task [21]. All of these experimental results clearly demonstrate StructBERT\u2019s exceptional effectiveness and generalization capability in language understanding. We make the following major contributions:\n\u2022 We propose novel structural pre-training that extends BERT by incorporating the word structural objective and the sentence structural objective to leverage language structures in contextualized representation. This enables the StructBERT to explicitly model language structures by forcing it to reconstruct the right order of words and sentences for correct prediction. \u2022 StructBERT significantly outperforms all published state-of-the-art models on a wide range of NLU tasks.", "The goal of the task is to extract the right answer span from the corresponding paragraph given a question. We fine-tuned our StructBERT language model on the SQuAD dataset for 3 epochs, and compared the result against the state-of-the-art methods on the official leaderboard 2, as shown in Table 3. We can see that even without any\n1https://data.quora.com/First-Quora-Dataset-Release-Question-Pairs 2https://rajpurkar.github.io/SQuAD-explorer/\nadditional data augmentation (DA) techniques, the proposed StructBERT model was superior to all published models except XLNet+DA on the dev set. 3. With data augmentation and large corpus used during pre-training, XLNet+DA outperformed our StructBERT which did not use data augmentation or large pre-training corpus. It demonstrates the effectiveness of the proposed pre-trained StructBERT in modeling the question-paragraph relationship for extractive question answering. Incorporating the word and sentence structures significantly improves the understanding ability in this fine-grained answer extraction task. 3We have submitted the model under the name of ALICE to the SQuAD v1.1 CodaLab for evaluation on the test set. However, due to crash of the Codalab evaluation server, we have not got our test result back yet at the time of paper submission. We will update the result once it is announced. 3.3 Effect of Different Structural Objectives. We have demonstrated the strong empirical results of the proposed model on a variety of downstream tasks. In the StructBERT pre-training, the two new structural prediction tasks are the most important components. Therefore, we conducted an ablation study by removing one structural objective from pre-training at a time to examine how the two structural objectives influence the performance on various downstream tasks. Results are presented in Table 4."]}
{"pkey": "scibert_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "The paper authors release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": [". Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/. . Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\nar X\niv :1\n90 3.\n10 67\n6v 3\n[ cs\n.C L\n] 1\n0 Se\np 20\n19\ntasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data.", "We did not perform extensive hyperparameter search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across most tasks and BERT variants. 4 Results. Table 1 summarizes the experimental results. We observe that SCIBERT outperforms BERT-Base on scientific tasks (+2.11 F1 with finetuning and +2.43 F1 without)8. We also achieve new SOTA results on many of these tasks using SCIBERT. 4.1 Biomedical Domain. We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018). SCIBERT performs slightly worse than SOTA on 3 datasets. The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018). The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use. In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on\n7The SOTA paper did not report a single score. We compute the average of the reported results for each class weighted by number of examples in each class. 8For rest of this paper, all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS.\nBC5CDR and ChemProt, and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus. 4.2 Computer Science Domain. We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with finetuning and +1.13 F1 without).", "In addition, SCIBERT achieves new SOTA results on ACLARC (Cohan et al., 2019), and the NER part of SciERC (Luan et al., 2018). For relations in SciERC, our results are not comparable with those in Luan et al. (2018) because we are performing relation classification given gold entities, while they perform joint entity and relation extraction.\n4.3 Multiple Domains. We observe that SCIBERT outperforms BERTBase on the multidomain tasks (+0.49 F1 with finetuning and +0.93 F1 without). In addition, SCIBERT outperforms the SOTA on Sci-\nCite (Cohan et al., 2019). No prior published SOTA results exist for the Paper Field dataset. 5 Discussion. 5.1 Effect of Finetuning. We observe improved results via BERT finetuning rather than task-specific architectures atop frozen embeddings (+3.25 F1 with SCIBERT and +3.58 with BERT-Base, on average). For each scientific domain, we observe the largest effects of finetuning on the computer science (+5.59 F1 with SCIBERT and +3.17 F1 with BERT-Base) and biomedical tasks (+2.94 F1 with SCIBERT and +4.61 F1 with BERT-Base), and the smallest effect on multidomain tasks (+0.7 F1 with SCIBERT and +1.14 F1 with BERT-Base). On every dataset except BC5CDR and SciCite, BERT-Base with finetuning outperforms (or performs similarly to) a model using frozen SCIBERT embeddings. 5.2 Effect of SCIVOCAB. We assess the importance of an in-domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB. We find the optimal hyperparameters for SCIBERTBASEVOCAB often coincide with those of SCIBERT-SCIVOCAB. Averaged across datasets, we observe +0.60 F1 when using SCIVOCAB. For each scientific do-\nmain, we observe +0.76 F1 for biomedical tasks, +0.61 F1 for computer science tasks, and +0.11 F1 for multidomain tasks. Given the disjoint vocabularies (Section 2) and the magnitude of improvement over BERT-Base (Section 4), we suspect that while an in-domain vocabulary is helpful, SCIBERT benefits most from the scientific corpus pretraining."]}
{"pkey": "scibert_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "(i) The paper authors release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain. SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) The paper authors perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary. (iii) The paper authors evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": [". Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/. . Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\nar X\niv :1\n90 3.\n10 67\n6v 3\n[ cs\n.C L\n] 1\n0 Se\np 20\n19\ntasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data.", "We did not perform extensive hyperparameter search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across most tasks and BERT variants. 4 Results. Table 1 summarizes the experimental results. We observe that SCIBERT outperforms BERT-Base on scientific tasks (+2.11 F1 with finetuning and +2.43 F1 without)8. We also achieve new SOTA results on many of these tasks using SCIBERT. 4.1 Biomedical Domain. We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018). SCIBERT performs slightly worse than SOTA on 3 datasets. The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018). The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use. In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on\n7The SOTA paper did not report a single score. We compute the average of the reported results for each class weighted by number of examples in each class. 8For rest of this paper, all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS.\nBC5CDR and ChemProt, and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus. 4.2 Computer Science Domain. We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with finetuning and +1.13 F1 without).", "We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB."]}
{"pkey": "scibert_3", "question": "What are the main contributions of the paper?", "answer": "In this work, the paper authors make the following contributions: (i) The paper authors release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain. SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) The paper authors perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary. (iii) The paper authors evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. The paper authors demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["In addition, SCIBERT achieves new SOTA results on ACLARC (Cohan et al., 2019), and the NER part of SciERC (Luan et al., 2018). For relations in SciERC, our results are not comparable with those in Luan et al. (2018) because we are performing relation classification given gold entities, while they perform joint entity and relation extraction.\n4.3 Multiple Domains. We observe that SCIBERT outperforms BERTBase on the multidomain tasks (+0.49 F1 with finetuning and +0.93 F1 without). In addition, SCIBERT outperforms the SOTA on Sci-\nCite (Cohan et al., 2019). No prior published SOTA results exist for the Paper Field dataset. 5 Discussion. 5.1 Effect of Finetuning. We observe improved results via BERT finetuning rather than task-specific architectures atop frozen embeddings (+3.25 F1 with SCIBERT and +3.58 with BERT-Base, on average). For each scientific domain, we observe the largest effects of finetuning on the computer science (+5.59 F1 with SCIBERT and +3.17 F1 with BERT-Base) and biomedical tasks (+2.94 F1 with SCIBERT and +4.61 F1 with BERT-Base), and the smallest effect on multidomain tasks (+0.7 F1 with SCIBERT and +1.14 F1 with BERT-Base). On every dataset except BC5CDR and SciCite, BERT-Base with finetuning outperforms (or performs similarly to) a model using frozen SCIBERT embeddings. 5.2 Effect of SCIVOCAB. We assess the importance of an in-domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB. We find the optimal hyperparameters for SCIBERTBASEVOCAB often coincide with those of SCIBERT-SCIVOCAB. Averaged across datasets, we observe +0.60 F1 when using SCIVOCAB. For each scientific do-\nmain, we observe +0.76 F1 for biomedical tasks, +0.61 F1 for computer science tasks, and +0.11 F1 for multidomain tasks. Given the disjoint vocabularies (Section 2) and the magnitude of improvement over BERT-Base (Section 4), we suspect that while an in-domain vocabulary is helpful, SCIBERT benefits most from the scientific corpus pretraining.", "We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB.", "We did not perform extensive hyperparameter search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across most tasks and BERT variants. 4 Results. Table 1 summarizes the experimental results. We observe that SCIBERT outperforms BERT-Base on scientific tasks (+2.11 F1 with finetuning and +2.43 F1 without)8. We also achieve new SOTA results on many of these tasks using SCIBERT. 4.1 Biomedical Domain. We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018). SCIBERT performs slightly worse than SOTA on 3 datasets. The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018). The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use. In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on\n7The SOTA paper did not report a single score. We compute the average of the reported results for each class weighted by number of examples in each class. 8For rest of this paper, all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS.\nBC5CDR and ChemProt, and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus. 4.2 Computer Science Domain. We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with finetuning and +1.13 F1 without)."]}
{"pkey": "scibert_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "The paper authors release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain. SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB.", "SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\n1 Introduction. The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these documents. Recent progress in NLP has been driven by the adoption of deep neural models, but training such models often requires large amounts of labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation. As shown through ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks. These models return contextualized embeddings for each token which can be passed\ninto minimal task-specific neural architectures. Leveraging the success of unsupervised pretraining has become especially important especially when task-specific annotations are difficult to obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu-\ntions:\n(i) We release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain.", "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained."]}
{"pkey": "scibert_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) \n2. PICO Extraction (PICO) \n3. Text Classification (CLS)\n4. Relation Classification (REL)\n5. Dependency Parsing (DEP)\nDatasets used are BC5CDR (Li et al., 2016) ,JNLPBA (Collier and Kim, 2004) ,NCBI-disease, EBM-NLP (Nye et al., 2018) ,GENIA , ChemProt (Kringelum et al., 2016),SciERC (Luan et al., 2018), ACL-ARC (Jurgens et al., 2018), Paper Field SciCite.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": [". Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/. . Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\nar X\niv :1\n90 3.\n10 67\n6v 3\n[ cs\n.C L\n] 1\n0 Se\np 20\n19\ntasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data.", "SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\n1 Introduction. The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these documents. Recent progress in NLP has been driven by the adoption of deep neural models, but training such models often requires large amounts of labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation. As shown through ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks. These models return contextualized embeddings for each token which can be passed\ninto minimal task-specific neural architectures. Leveraging the success of unsupervised pretraining has become especially important especially when task-specific annotations are difficult to obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu-\ntions:\n(i) We release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain.", "6 Related Work. Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al., 2019). BIOBERT is trained on PubMed abstracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the MIMIC-III database (Johnson et al., 2016). In contrast, SCIBERT is trained on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus (Ammar et al., 2018). Furthermore, SCIBERT uses an in-domain vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB). 7 Conclusion and Future Work. We released SCIBERT, a pretrained language model for scientific text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from scientific domains. SCIBERT significantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to some reported BIOBERT (Lee et al., 2019) results on biomedical tasks. For future work, we will release a version of SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from each domain. Because these language models are costly to train, we aim to build a single resource that\u2019s useful across multiple domains. Acknowledgment. We thank the anonymous reviewers for their comments and suggestions. We also thank Waleed Ammar, Noah Smith, Yoav Goldberg, Daniel King, Doug Downey, and Dan Weld for their helpful discussions and feedback. All experiments were performed on beaker.org and supported in part by credits from Google Cloud."]}
{"pkey": "scibert_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not specified in the paper.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["We did not perform extensive hyperparameter search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across most tasks and BERT variants. 4 Results. Table 1 summarizes the experimental results. We observe that SCIBERT outperforms BERT-Base on scientific tasks (+2.11 F1 with finetuning and +2.43 F1 without)8. We also achieve new SOTA results on many of these tasks using SCIBERT. 4.1 Biomedical Domain. We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018). SCIBERT performs slightly worse than SOTA on 3 datasets. The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018). The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use. In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on\n7The SOTA paper did not report a single score. We compute the average of the reported results for each class weighted by number of examples in each class. 8For rest of this paper, all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS.\nBC5CDR and ChemProt, and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus. 4.2 Computer Science Domain. We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with finetuning and +1.13 F1 without).", "The two models that use BASEVOCAB are finetuned from the corresponding BERT-Base models. The other two models that use the new SCIVOCAB are trained from scratch. Pretraining BERT for long sentences can be slow. Following the original BERT code, we set a maximum sentence length of 128 tokens, and train the model until the training loss stops decreasing. We then continue training the model allowing sentence lengths up to 512 tokens. We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week5 (5 days with max length 128, then 2 days with max length 512). The BASEVOCAB models take 2 fewer days of training because they aren\u2019t trained from scratch. All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017). Casing We follow Devlin et al. (2019) in using the cased models for NER and the uncased models\n3 https://academic.microsoft.com/ 4 https://github.com/google-research/bert\n5BERT\u2019s largest model was trained on 16 Cloud TPUs for 4 days. Expected 40-70 days (Dettmers, 2019) on an 8-GPU machine. 6 https://github.com/huggingface/pytorch-transformers\nfor all other tasks. We also use the cased models for parsing. Some light experimentation showed that the uncased models perform slightly better (even sometimes on NER) than cased models. 3.4 Finetuning BERT. We mostly follow the same architecture, optimization, and hyperparameter choices used in Devlin et al. (2019). For text classification (i.e. CLS and REL), we feed the final BERT vector for the [CLS] token into a linear classification layer. For sequence labeling (i.e. NER and PICO), we feed the final BERT vector for each token into a linear classification layer with softmax output. We differ slightly in using an additional conditional random field, which made evaluation easier by guaranteeing well-formed entities.", "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained."]}
{"pkey": "scibert_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Not specified in the paper.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["6 Related Work. Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al., 2019). BIOBERT is trained on PubMed abstracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the MIMIC-III database (Johnson et al., 2016). In contrast, SCIBERT is trained on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus (Ammar et al., 2018). Furthermore, SCIBERT uses an in-domain vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB). 7 Conclusion and Future Work. We released SCIBERT, a pretrained language model for scientific text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from scientific domains. SCIBERT significantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to some reported BIOBERT (Lee et al., 2019) results on biomedical tasks. For future work, we will release a version of SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from each domain. Because these language models are costly to train, we aim to build a single resource that\u2019s useful across multiple domains. Acknowledgment. We thank the anonymous reviewers for their comments and suggestions. We also thank Waleed Ammar, Noah Smith, Yoav Goldberg, Daniel King, Doug Downey, and Dan Weld for their helpful discussions and feedback. All experiments were performed on beaker.org and supported in part by credits from Google Cloud.", "In addition, SCIBERT achieves new SOTA results on ACLARC (Cohan et al., 2019), and the NER part of SciERC (Luan et al., 2018). For relations in SciERC, our results are not comparable with those in Luan et al. (2018) because we are performing relation classification given gold entities, while they perform joint entity and relation extraction.\n4.3 Multiple Domains. We observe that SCIBERT outperforms BERTBase on the multidomain tasks (+0.49 F1 with finetuning and +0.93 F1 without). In addition, SCIBERT outperforms the SOTA on Sci-\nCite (Cohan et al., 2019). No prior published SOTA results exist for the Paper Field dataset. 5 Discussion. 5.1 Effect of Finetuning. We observe improved results via BERT finetuning rather than task-specific architectures atop frozen embeddings (+3.25 F1 with SCIBERT and +3.58 with BERT-Base, on average). For each scientific domain, we observe the largest effects of finetuning on the computer science (+5.59 F1 with SCIBERT and +3.17 F1 with BERT-Base) and biomedical tasks (+2.94 F1 with SCIBERT and +4.61 F1 with BERT-Base), and the smallest effect on multidomain tasks (+0.7 F1 with SCIBERT and +1.14 F1 with BERT-Base). On every dataset except BC5CDR and SciCite, BERT-Base with finetuning outperforms (or performs similarly to) a model using frozen SCIBERT embeddings. 5.2 Effect of SCIVOCAB. We assess the importance of an in-domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB. We find the optimal hyperparameters for SCIBERTBASEVOCAB often coincide with those of SCIBERT-SCIVOCAB. Averaged across datasets, we observe +0.60 F1 when using SCIVOCAB. For each scientific do-\nmain, we observe +0.76 F1 for biomedical tasks, +0.61 F1 for computer science tasks, and +0.11 F1 for multidomain tasks. Given the disjoint vocabularies (Section 2) and the magnitude of improvement over BERT-Base (Section 4), we suspect that while an in-domain vocabulary is helpful, SCIBERT benefits most from the scientific corpus pretraining.", "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained."]}
{"pkey": "scibert_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. The paper authors use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained. The paper authors split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained.", "For DEP, we use the model from Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs. In all settings, we apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). For each dataset and BERT variant, we pick the best learning rate and number of epochs on the development set and report the corresponding test results. We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e-5. While task-dependent, optimal hyperparameters for each task are often the same across BERT variants. 3.5 Frozen BERT Embeddings. We also explore the usage of BERT as pretrained contextualized word embeddings, like ELMo (Peters et al., 2018), by training simple task-specific models atop frozen BERT embeddings. For text classification, we feed each sentence of BERT vectors into a 2-layer BiLSTM of size 200 and apply a multilayer perceptron (with hidden size 200) on the concatenated first and last BiLSTM vectors. For sequence labeling, we use the same BiLSTM layers and use a conditional random field to guarantee well-formed predictions. For DEP, we use the full model from\nDozat and Manning (2017) with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks. We did not find changing the depth or size of the BiLSTMs to significantly impact results (Reimers and Gurevych, 2017). We optimize cross entropy loss using Adam, but holding BERT weights frozen and applying a dropout of 0.5. We train with early stopping on the development set (patience of 10) using a batch size of 32 and a learning rate of 0.001.", "We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB."]}
{"pkey": "scibert_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece library. The paper authors produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained.", "For DEP, we use the model from Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs. In all settings, we apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). For each dataset and BERT variant, we pick the best learning rate and number of epochs on the development set and report the corresponding test results. We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e-5. While task-dependent, optimal hyperparameters for each task are often the same across BERT variants. 3.5 Frozen BERT Embeddings. We also explore the usage of BERT as pretrained contextualized word embeddings, like ELMo (Peters et al., 2018), by training simple task-specific models atop frozen BERT embeddings. For text classification, we feed each sentence of BERT vectors into a 2-layer BiLSTM of size 200 and apply a multilayer perceptron (with hidden size 200) on the concatenated first and last BiLSTM vectors. For sequence labeling, we use the same BiLSTM layers and use a conditional random field to guarantee well-formed predictions. For DEP, we use the full model from\nDozat and Manning (2017) with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks. We did not find changing the depth or size of the BiLSTMs to significantly impact results (Reimers and Gurevych, 2017). We optimize cross entropy loss using Adam, but holding BERT weights frozen and applying a dropout of 0.5. We train with early stopping on the development set (patience of 10) using a batch size of 32 and a learning rate of 0.001.", "We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB."]}
{"pkey": "scibert_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "The paper authors construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece library The paper authors split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. The paper authors train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["We did not perform extensive hyperparameter search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across most tasks and BERT variants. 4 Results. Table 1 summarizes the experimental results. We observe that SCIBERT outperforms BERT-Base on scientific tasks (+2.11 F1 with finetuning and +2.43 F1 without)8. We also achieve new SOTA results on many of these tasks using SCIBERT. 4.1 Biomedical Domain. We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018). SCIBERT performs slightly worse than SOTA on 3 datasets. The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018). The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use. In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on\n7The SOTA paper did not report a single score. We compute the average of the reported results for each class weighted by number of examples in each class. 8For rest of this paper, all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS.\nBC5CDR and ChemProt, and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus. 4.2 Computer Science Domain. We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with finetuning and +1.13 F1 without).", "In addition, SCIBERT achieves new SOTA results on ACLARC (Cohan et al., 2019), and the NER part of SciERC (Luan et al., 2018). For relations in SciERC, our results are not comparable with those in Luan et al. (2018) because we are performing relation classification given gold entities, while they perform joint entity and relation extraction.\n4.3 Multiple Domains. We observe that SCIBERT outperforms BERTBase on the multidomain tasks (+0.49 F1 with finetuning and +0.93 F1 without). In addition, SCIBERT outperforms the SOTA on Sci-\nCite (Cohan et al., 2019). No prior published SOTA results exist for the Paper Field dataset. 5 Discussion. 5.1 Effect of Finetuning. We observe improved results via BERT finetuning rather than task-specific architectures atop frozen embeddings (+3.25 F1 with SCIBERT and +3.58 with BERT-Base, on average). For each scientific domain, we observe the largest effects of finetuning on the computer science (+5.59 F1 with SCIBERT and +3.17 F1 with BERT-Base) and biomedical tasks (+2.94 F1 with SCIBERT and +4.61 F1 with BERT-Base), and the smallest effect on multidomain tasks (+0.7 F1 with SCIBERT and +1.14 F1 with BERT-Base). On every dataset except BC5CDR and SciCite, BERT-Base with finetuning outperforms (or performs similarly to) a model using frozen SCIBERT embeddings. 5.2 Effect of SCIVOCAB. We assess the importance of an in-domain scientific vocabulary by repeating the finetuning experiments for SCIBERT with BASEVOCAB. We find the optimal hyperparameters for SCIBERTBASEVOCAB often coincide with those of SCIBERT-SCIVOCAB. Averaged across datasets, we observe +0.60 F1 when using SCIVOCAB. For each scientific do-\nmain, we observe +0.76 F1 for biomedical tasks, +0.61 F1 for computer science tasks, and +0.11 F1 for multidomain tasks. Given the disjoint vocabularies (Section 2) and the magnitude of improvement over BERT-Base (Section 4), we suspect that while an in-domain vocabulary is helpful, SCIBERT benefits most from the scientific corpus pretraining.", "For DEP, we use the model from Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs. In all settings, we apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). For each dataset and BERT variant, we pick the best learning rate and number of epochs on the development set and report the corresponding test results. We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e-5. While task-dependent, optimal hyperparameters for each task are often the same across BERT variants. 3.5 Frozen BERT Embeddings. We also explore the usage of BERT as pretrained contextualized word embeddings, like ELMo (Peters et al., 2018), by training simple task-specific models atop frozen BERT embeddings. For text classification, we feed each sentence of BERT vectors into a 2-layer BiLSTM of size 200 and apply a multilayer perceptron (with hidden size 200) on the concatenated first and last BiLSTM vectors. For sequence labeling, we use the same BiLSTM layers and use a conditional random field to guarantee well-formed predictions. For DEP, we use the full model from\nDozat and Manning (2017) with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks. We did not find changing the depth or size of the BiLSTMs to significantly impact results (Reimers and Gurevych, 2017). We optimize cross entropy loss using Adam, but holding BERT weights frozen and applying a dropout of 0.5. We train with early stopping on the development set (patience of 10) using a batch size of 32 and a learning rate of 0.001."]}
{"pkey": "scibert_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. The paper authors use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB.", "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained.", "For DEP, we use the model from Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs. In all settings, we apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). For each dataset and BERT variant, we pick the best learning rate and number of epochs on the development set and report the corresponding test results. We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e-5. While task-dependent, optimal hyperparameters for each task are often the same across BERT variants. 3.5 Frozen BERT Embeddings. We also explore the usage of BERT as pretrained contextualized word embeddings, like ELMo (Peters et al., 2018), by training simple task-specific models atop frozen BERT embeddings. For text classification, we feed each sentence of BERT vectors into a 2-layer BiLSTM of size 200 and apply a multilayer perceptron (with hidden size 200) on the concatenated first and last BiLSTM vectors. For sequence labeling, we use the same BiLSTM layers and use a conditional random field to guarantee well-formed predictions. For DEP, we use the full model from\nDozat and Manning (2017) with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks. We did not find changing the depth or size of the BiLSTMs to significantly impact results (Reimers and Gurevych, 2017). We optimize cross entropy loss using Adam, but holding BERT weights frozen and applying a dropout of 0.5. We train with early stopping on the development set (patience of 10) using a batch size of 32 and a learning rate of 0.001."]}
{"pkey": "scibert_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The  paper authors apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). The paper authors finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). The paper authors found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e-5. The paper authors use a single TPU v3 with 8 cores.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["For DEP, we use the model from Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs. In all settings, we apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). For each dataset and BERT variant, we pick the best learning rate and number of epochs on the development set and report the corresponding test results. We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e-5. While task-dependent, optimal hyperparameters for each task are often the same across BERT variants. 3.5 Frozen BERT Embeddings. We also explore the usage of BERT as pretrained contextualized word embeddings, like ELMo (Peters et al., 2018), by training simple task-specific models atop frozen BERT embeddings. For text classification, we feed each sentence of BERT vectors into a 2-layer BiLSTM of size 200 and apply a multilayer perceptron (with hidden size 200) on the concatenated first and last BiLSTM vectors. For sequence labeling, we use the same BiLSTM layers and use a conditional random field to guarantee well-formed predictions. For DEP, we use the full model from\nDozat and Manning (2017) with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks. We did not find changing the depth or size of the BiLSTMs to significantly impact results (Reimers and Gurevych, 2017). We optimize cross entropy loss using Adam, but holding BERT weights frozen and applying a dropout of 0.5. We train with early stopping on the development set (patience of 10) using a batch size of 32 and a learning rate of 0.001.", "We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB.", "The two models that use BASEVOCAB are finetuned from the corresponding BERT-Base models. The other two models that use the new SCIVOCAB are trained from scratch. Pretraining BERT for long sentences can be slow. Following the original BERT code, we set a maximum sentence length of 128 tokens, and train the model until the training loss stops decreasing. We then continue training the model allowing sentence lengths up to 512 tokens. We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week5 (5 days with max length 128, then 2 days with max length 512). The BASEVOCAB models take 2 fewer days of training because they aren\u2019t trained from scratch. All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017). Casing We follow Devlin et al. (2019) in using the cased models for NER and the uncased models\n3 https://academic.microsoft.com/ 4 https://github.com/google-research/bert\n5BERT\u2019s largest model was trained on 16 Cloud TPUs for 4 days. Expected 40-70 days (Dettmers, 2019) on an 8-GPU machine. 6 https://github.com/huggingface/pytorch-transformers\nfor all other tasks. We also use the cased models for parsing. Some light experimentation showed that the uncased models perform slightly better (even sometimes on NER) than cased models. 3.4 Finetuning BERT. We mostly follow the same architecture, optimization, and hyperparameter choices used in Devlin et al. (2019). For text classification (i.e. CLS and REL), we feed the final BERT vector for the [CLS] token into a linear classification layer. For sequence labeling (i.e. NER and PICO), we feed the final BERT vector for each token into a linear classification layer with softmax output. We differ slightly in using an additional conditional random field, which made evaluation easier by guaranteeing well-formed entities."]}
{"pkey": "scibert_13", "question": "Describe the computational resources used to train the model.", "answer": "The paper authors use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week (5 days with max length 128, then 2 days with max length 512).", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB.", "6 Related Work. Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al., 2019). BIOBERT is trained on PubMed abstracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the MIMIC-III database (Johnson et al., 2016). In contrast, SCIBERT is trained on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus (Ammar et al., 2018). Furthermore, SCIBERT uses an in-domain vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB). 7 Conclusion and Future Work. We released SCIBERT, a pretrained language model for scientific text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from scientific domains. SCIBERT significantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to some reported BIOBERT (Lee et al., 2019) results on biomedical tasks. For future work, we will release a version of SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from each domain. Because these language models are costly to train, we aim to build a single resource that\u2019s useful across multiple domains. Acknowledgment. We thank the anonymous reviewers for their comments and suggestions. We also thank Waleed Ammar, Noah Smith, Yoav Goldberg, Daniel King, Doug Downey, and Dan Weld for their helpful discussions and feedback. All experiments were performed on beaker.org and supported in part by credits from Google Cloud.", "SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\n1 Introduction. The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these documents. Recent progress in NLP has been driven by the adoption of deep neural models, but training such models often requires large amounts of labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation. As shown through ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks. These models return contextualized embeddings for each token which can be passed\ninto minimal task-specific neural architectures. Leveraging the success of unsupervised pretraining has become especially important especially when task-specific annotations are difficult to obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu-\ntions:\n(i) We release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain."]}
{"pkey": "scibert_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "The code and pretrained models are available at https://github.com/allenai/scibert/ . The paper authors use a single TPU v3 with 8 cores. In all settings, the paper authors apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). The paper authors finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). For each dataset and BERT variant, the paper authors pick the best learning rate and number of epochs on the development set and report the corresponding test results.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["We did not perform extensive hyperparameter search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across most tasks and BERT variants. 4 Results. Table 1 summarizes the experimental results. We observe that SCIBERT outperforms BERT-Base on scientific tasks (+2.11 F1 with finetuning and +2.43 F1 without)8. We also achieve new SOTA results on many of these tasks using SCIBERT. 4.1 Biomedical Domain. We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018). SCIBERT performs slightly worse than SOTA on 3 datasets. The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018). The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use. In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on\n7The SOTA paper did not report a single score. We compute the average of the reported results for each class weighted by number of examples in each class. 8For rest of this paper, all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS.\nBC5CDR and ChemProt, and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus. 4.2 Computer Science Domain. We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with finetuning and +1.13 F1 without).", "We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB.", "6 Related Work. Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al., 2019). BIOBERT is trained on PubMed abstracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the MIMIC-III database (Johnson et al., 2016). In contrast, SCIBERT is trained on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus (Ammar et al., 2018). Furthermore, SCIBERT uses an in-domain vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB). 7 Conclusion and Future Work. We released SCIBERT, a pretrained language model for scientific text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from scientific domains. SCIBERT significantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to some reported BIOBERT (Lee et al., 2019) results on biomedical tasks. For future work, we will release a version of SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from each domain. Because these language models are costly to train, we aim to build a single resource that\u2019s useful across multiple domains. Acknowledgment. We thank the anonymous reviewers for their comments and suggestions. We also thank Waleed Ammar, Noah Smith, Yoav Goldberg, Daniel King, Doug Downey, and Dan Weld for their helpful discussions and feedback. All experiments were performed on beaker.org and supported in part by credits from Google Cloud."]}
{"pkey": "scibert_15", "question": "What is the pretraining objective of the model? ", "answer": "Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained.", ". Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/. . Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data. SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\nar X\niv :1\n90 3.\n10 67\n6v 3\n[ cs\n.C L\n] 1\n0 Se\np 20\n19\ntasks in the scientific domain is challenging and expensive. We release SCIBERT, a pretrained language model based on BERT (Devlin et al., 2019) to address the lack of high-quality, large-scale labeled scientific data.", "SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\n1 Introduction. The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these documents. Recent progress in NLP has been driven by the adoption of deep neural models, but training such models often requires large amounts of labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation. As shown through ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks. These models return contextualized embeddings for each token which can be passed\ninto minimal task-specific neural architectures. Leveraging the success of unsupervised pretraining has become especially important especially when task-specific annotations are difficult to obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu-\ntions:\n(i) We release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain."]}
{"pkey": "scibert_16", "question": "What is the loss function that is used to train the model?", "answer": "The paper authors apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015).", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["For DEP, we use the model from Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs. In all settings, we apply a dropout of 0.1 and optimize cross entropy loss using Adam (Kingma and Ba, 2015). We finetune for 2 to 5 epochs using a batch size of 32 and a learning rate of 5e-6, 1e-5, 2e-5, or 5e-5 with a slanted triangular schedule (Howard and Ruder, 2018) which is equivalent to the linear warmup followed by linear decay (Devlin et al., 2019). For each dataset and BERT variant, we pick the best learning rate and number of epochs on the development set and report the corresponding test results. We found the setting that works best across most datasets and models is 2 or 4 epochs and a learning rate of 2e-5. While task-dependent, optimal hyperparameters for each task are often the same across BERT variants. 3.5 Frozen BERT Embeddings. We also explore the usage of BERT as pretrained contextualized word embeddings, like ELMo (Peters et al., 2018), by training simple task-specific models atop frozen BERT embeddings. For text classification, we feed each sentence of BERT vectors into a 2-layer BiLSTM of size 200 and apply a multilayer perceptron (with hidden size 200) on the concatenated first and last BiLSTM vectors. For sequence labeling, we use the same BiLSTM layers and use a conditional random field to guarantee well-formed predictions. For DEP, we use the full model from\nDozat and Manning (2017) with dependency tag and arc embeddings of size 100 and the same BiLSTM setup as other tasks. We did not find changing the depth or size of the BiLSTMs to significantly impact results (Reimers and Gurevych, 2017). We optimize cross entropy loss using Adam, but holding BERT weights frozen and applying a dropout of 0.5. We train with early stopping on the development set (patience of 10) using a batch size of 32 and a learning rate of 0.001.", "The two models that use BASEVOCAB are finetuned from the corresponding BERT-Base models. The other two models that use the new SCIVOCAB are trained from scratch. Pretraining BERT for long sentences can be slow. Following the original BERT code, we set a maximum sentence length of 128 tokens, and train the model until the training loss stops decreasing. We then continue training the model allowing sentence lengths up to 512 tokens. We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week5 (5 days with max length 128, then 2 days with max length 512). The BASEVOCAB models take 2 fewer days of training because they aren\u2019t trained from scratch. All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017). Casing We follow Devlin et al. (2019) in using the cased models for NER and the uncased models\n3 https://academic.microsoft.com/ 4 https://github.com/google-research/bert\n5BERT\u2019s largest model was trained on 16 Cloud TPUs for 4 days. Expected 40-70 days (Dettmers, 2019) on an 8-GPU machine. 6 https://github.com/huggingface/pytorch-transformers\nfor all other tasks. We also use the cased models for parsing. Some light experimentation showed that the uncased models perform slightly better (even sometimes on NER) than cased models. 3.4 Finetuning BERT. We mostly follow the same architecture, optimization, and hyperparameter choices used in Devlin et al. (2019). For text classification (i.e. CLS and REL), we feed the final BERT vector for the [CLS] token into a linear classification layer. For sequence labeling (i.e. NER and PICO), we feed the final BERT vector for each token into a linear classification layer with softmax output. We differ slightly in using an additional conditional random field, which made evaluation easier by guaranteeing well-formed entities.", "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained."]}
{"pkey": "scibert_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. The paper authors construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece library. For text classification (i.e.CLS and REL), the paper authors feed the final BERT vector for the [CLS] token into a linear classification layer. For sequence labeling (i.e. NER and PICO), we feed the final BERT vector for each token into a linear classification layer with softmax output.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained.", "SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\n1 Introduction. The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these documents. Recent progress in NLP has been driven by the adoption of deep neural models, but training such models often requires large amounts of labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation. As shown through ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks. These models return contextualized embeddings for each token which can be passed\ninto minimal task-specific neural architectures. Leveraging the success of unsupervised pretraining has become especially important especially when task-specific annotations are difficult to obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu-\ntions:\n(i) We release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain.", "We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB."]}
{"pkey": "scibert_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER)\n2. PICO Extraction (PICO)\n3. Text Classification (CLS)\n4. Relation Classification (REL)\n5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. The paper authors mostly follow the same architecture, optimization, and hyperparameter choices used in Devlin et al. (2019). For text classification (i.e. CLS and REL), the paper authors feed the final BERT vector for the [CLS] token into a linear classification layer. For sequence labeling (i.e. NER and PICO), we feed the final BERT vector for each token into a linear classification layer with softmax output. The paper authors differ slightly in using an additional conditional random field, which made evaluation easier by guaranteeing well-formed entities. For DEP, we use the model from Dozat and Manning (2017) with dependency tag and arc embeddings of size 100 and biaffine matrix attention over BERT vectors instead of stacked BiLSTMs.\nKeeping with past work, the paper authors report macro F1 scores for NER (span-level), macro F1 scores for REL and CLS (sentence-level), and macro F1 for PICO (token-level), and micro F1 for ChemProt specifically. For DEP, the paper authors report labeled (LAS) and unlabeled (UAS) attachment scores (excluding punctuation) for the same model with hyperparameters tuned for LAS.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["6 Related Work. Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al., 2019). BIOBERT is trained on PubMed abstracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the MIMIC-III database (Johnson et al., 2016). In contrast, SCIBERT is trained on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus (Ammar et al., 2018). Furthermore, SCIBERT uses an in-domain vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB). 7 Conclusion and Future Work. We released SCIBERT, a pretrained language model for scientific text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from scientific domains. SCIBERT significantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to some reported BIOBERT (Lee et al., 2019) results on biomedical tasks. For future work, we will release a version of SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from each domain. Because these language models are costly to train, we aim to build a single resource that\u2019s useful across multiple domains. Acknowledgment. We thank the anonymous reviewers for their comments and suggestions. We also thank Waleed Ammar, Noah Smith, Yoav Goldberg, Daniel King, Doug Downey, and Dan Weld for their helpful discussions and feedback. All experiments were performed on beaker.org and supported in part by credits from Google Cloud.", "The two models that use BASEVOCAB are finetuned from the corresponding BERT-Base models. The other two models that use the new SCIVOCAB are trained from scratch. Pretraining BERT for long sentences can be slow. Following the original BERT code, we set a maximum sentence length of 128 tokens, and train the model until the training loss stops decreasing. We then continue training the model allowing sentence lengths up to 512 tokens. We use a single TPU v3 with 8 cores. Training the SCIVOCAB models from scratch on our corpus takes 1 week5 (5 days with max length 128, then 2 days with max length 512). The BASEVOCAB models take 2 fewer days of training because they aren\u2019t trained from scratch. All pretrained BERT models are converted to be compatible with PyTorch using the pytorchtransformers library.6 All our models (Sections 3.4 and 3.5) are implemented in PyTorch using AllenNLP (Gardner et al., 2017). Casing We follow Devlin et al. (2019) in using the cased models for NER and the uncased models\n3 https://academic.microsoft.com/ 4 https://github.com/google-research/bert\n5BERT\u2019s largest model was trained on 16 Cloud TPUs for 4 days. Expected 40-70 days (Dettmers, 2019) on an 8-GPU machine. 6 https://github.com/huggingface/pytorch-transformers\nfor all other tasks. We also use the cased models for parsing. Some light experimentation showed that the uncased models perform slightly better (even sometimes on NER) than cased models. 3.4 Finetuning BERT. We mostly follow the same architecture, optimization, and hyperparameter choices used in Devlin et al. (2019). For text classification (i.e. CLS and REL), we feed the final BERT vector for the [CLS] token into a linear classification layer. For sequence labeling (i.e. NER and PICO), we feed the final BERT vector for each token into a linear classification layer with softmax output. We differ slightly in using an additional conditional random field, which made evaluation easier by guaranteeing well-formed entities.", "SCIBERT is a pretrained language model based on BERT but trained on a large corpus of scientific text. (ii) We perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary.\n(iii) We evaluate SCIBERT on a suite of tasks in the scientific domain, and achieve new state-ofthe-art (SOTA) results on many of these tasks. 2 Methods. Background The BERT model architecture (Devlin et al., 2019) is based on a multilayer bidirectional Transformer (Vaswani et al., 2017). Instead of the traditional left-to-right language modeling objective, BERT is trained on two tasks: predicting randomly masked tokens and predicting whether two sentences follow each other. SCIBERT follows the same architecture as BERT but is instead pretrained on scientific text. Vocabulary BERT uses WordPiece (Wu et al., 2016) for unsupervised tokenization of the input text. The vocabulary is built such that it contains the most frequently used words or subword units. We refer to the original vocabulary released with\nBERT as BASEVOCAB. We construct SCIVOCAB, a new WordPiece vocabulary on our scientific corpus using the SentencePiece1 library. We produce both cased and uncased vocabularies and set the vocabulary size to 30K to match the size of BASEVOCAB. The resulting token overlap between BASEVOCAB and SCIVOCAB is 42%, illustrating a substantial difference in frequently used words between scientific and general domain texts. Corpus We train SCIBERT on a random sample of 1.14M papers from Semantic Scholar (Ammar et al., 2018). This corpus consists of 18% papers from the computer science domain and 82% from the broad biomedical domain. We use the full text of the papers, not just the abstracts. The average paper length is 154 sentences (2,769 tokens) resulting in a corpus size of 3.17B tokens, similar to the 3.3B tokens on which BERT was trained."]}
{"pkey": "scibert_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "The paper authors perform extensive experimentation to investigate the performance of finetuning versus task-specific architectures atop frozen embeddings, and the effect of having an in-domain vocabulary. The paper authors train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB. The two models that use BASEVOCAB are finetuned from the corresponding BERT-Base models", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB.", "We did not perform extensive hyperparameter search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across most tasks and BERT variants. 4 Results. Table 1 summarizes the experimental results. We observe that SCIBERT outperforms BERT-Base on scientific tasks (+2.11 F1 with finetuning and +2.43 F1 without)8. We also achieve new SOTA results on many of these tasks using SCIBERT. 4.1 Biomedical Domain. We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018). SCIBERT performs slightly worse than SOTA on 3 datasets. The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018). The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use. In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on\n7The SOTA paper did not report a single score. We compute the average of the reported results for each class weighted by number of examples in each class. 8For rest of this paper, all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS.\nBC5CDR and ChemProt, and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus. 4.2 Computer Science Domain. We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with finetuning and +1.13 F1 without).", "SCIBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-theart results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.\n1 Introduction. The exponential increase in the volume of scientific publications in the past decades has made NLP an essential tool for large-scale knowledge extraction and machine reading of these documents. Recent progress in NLP has been driven by the adoption of deep neural models, but training such models often requires large amounts of labeled data. In general domains, large-scale training data is often possible to obtain through crowdsourcing, but in scientific domains, annotated data is difficult and expensive to collect due to the expertise required for quality annotation. As shown through ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2019), unsupervised pretraining of language models on large corpora significantly improves performance on many NLP tasks. These models return contextualized embeddings for each token which can be passed\ninto minimal task-specific neural architectures. Leveraging the success of unsupervised pretraining has become especially important especially when task-specific annotations are difficult to obtain, like in scientific NLP. Yet while both BERT and ELMo have released pretrained models, they are still trained on general domain corpora such as news articles and Wikipedia. In this work, we make the following contribu-\ntions:\n(i) We release SCIBERT, a new resource demonstrated to improve performance on a range of NLP tasks in the scientific domain."]}
{"pkey": "scibert_20", "question": "List the future work mentioned in the paper.", "answer": "For future work, the paper authors will release a version of SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from each domain. Because these language models are costly to train, the paper authors aim to build a single resource that\u2019s useful across multiple domains.", "title": "SciBERT: A Pretrained Language Model for Scientific Text", "context": ["6 Related Work. Recent work on domain adaptation of BERT includes BIOBERT (Lee et al., 2019) and CLINICALBERT (Alsentzer et al., 2019; Huang et al., 2019). BIOBERT is trained on PubMed abstracts and PMC full text articles, and CLINICALBERT is trained on clinical text from the MIMIC-III database (Johnson et al., 2016). In contrast, SCIBERT is trained on the full text of 1.14M biomedical and computer science papers from the Semantic Scholar corpus (Ammar et al., 2018). Furthermore, SCIBERT uses an in-domain vocabulary (SCIVOCAB) while the other abovementioned models use the original BERT vocabulary (BASEVOCAB). 7 Conclusion and Future Work. We released SCIBERT, a pretrained language model for scientific text based on BERT. We evaluated SCIBERT on a suite of tasks and datasets from scientific domains. SCIBERT significantly outperformed BERT-Base and achieves new SOTA results on several of these tasks, even compared to some reported BIOBERT (Lee et al., 2019) results on biomedical tasks. For future work, we will release a version of SCIBERT analogous to BERT-Large, as well as experiment with different proportions of papers from each domain. Because these language models are costly to train, we aim to build a single resource that\u2019s useful across multiple domains. Acknowledgment. We thank the anonymous reviewers for their comments and suggestions. We also thank Waleed Ammar, Noah Smith, Yoav Goldberg, Daniel King, Doug Downey, and Dan Weld for their helpful discussions and feedback. All experiments were performed on beaker.org and supported in part by credits from Google Cloud.", "We did not perform extensive hyperparameter search, but while optimal hyperparameters are going to be task-dependent, some light experimentation showed these settings work fairly well across most tasks and BERT variants. 4 Results. Table 1 summarizes the experimental results. We observe that SCIBERT outperforms BERT-Base on scientific tasks (+2.11 F1 with finetuning and +2.43 F1 without)8. We also achieve new SOTA results on many of these tasks using SCIBERT. 4.1 Biomedical Domain. We observe that SCIBERT outperforms BERTBase on biomedical tasks (+1.92 F1 with finetuning and +3.59 F1 without). In addition, SCIBERT achieves new SOTA results on BC5CDR and ChemProt (Lee et al., 2019), and EBMNLP (Nye et al., 2018). SCIBERT performs slightly worse than SOTA on 3 datasets. The SOTA model for JNLPBA is a BiLSTM-CRF ensemble trained on multiple NER datasets not just JNLPBA (Yoon et al., 2018). The SOTA model for NCBI-disease is BIOBERT (Lee et al., 2019), which is BERTBase finetuned on 18B tokens from biomedical papers. The SOTA result for GENIA is in Nguyen and Verspoor (2019) which uses the model from Dozat and Manning (2017) with partof-speech (POS) features, which we do not use. In Table 2, we compare SCIBERT results with reported BIOBERT results on the subset of datasets included in (Lee et al., 2019). Interesting, SCIBERT outperforms BIOBERT results on\n7The SOTA paper did not report a single score. We compute the average of the reported results for each class weighted by number of examples in each class. 8For rest of this paper, all results reported in this manner are averaged over datasets excluding UAS for DEP since we already include LAS.\nBC5CDR and ChemProt, and performs similarly on JNLPBA despite being trained on a substantially smaller biomedical corpus. 4.2 Computer Science Domain. We observe that SCIBERT outperforms BERTBase on computer science tasks (+3.55 F1 with finetuning and +1.13 F1 without).", "We split sentences using ScispaCy (Neumann et al., 2019),2 which is optimized for scientific text. 3 Experimental Setup.\n3.1 Tasks. We experiment on the following core NLP tasks:\n1. Named Entity Recognition (NER) 2. PICO Extraction (PICO) 3. Text Classification (CLS) 4. Relation Classification (REL) 5. Dependency Parsing (DEP)\nPICO, like NER, is a sequence labeling task where the model extracts spans describing the Participants, Interventions, Comparisons, and Outcomes in a clinical trial paper (Kim et al., 2011). REL is a special case of text classification where the model predicts the type of relation expressed between two entities, which are encapsulated in the sentence by inserted special tokens. 3.2 Datasets. For brevity, we only describe the newer datasets here, and refer the reader to the references in Table 1 for the older datasets. EBM-NLP (Nye et al., 2018) annotates PICO spans in clinical trial abstracts. SciERC (Luan et al., 2018) annotates entities and relations from computer science ab-\n1https://github.com/google/sentencepiece 2 https://github.com/allenai/SciSpaCy\nstracts. ACL-ARC (Jurgens et al., 2018) and SciCite (Cohan et al., 2019) assign intent labels (e.g. Comparison, Extension, etc.) to sentences from scientific papers that cite other papers. The Paper Field dataset is built from the Microsoft Academic Graph (Sinha et al., 2015)3 and maps paper titles to one of 7 fields of study. Each field of study (i.e. geography, politics, economics, business, sociology, medicine, and psychology) has approximately 12K training examples.\n3.3 Pretrained BERT Variants. BERT-Base We use the pretrained weights for BERT-Base (Devlin et al., 2019) released with the original BERT code.4 The vocabulary is BASEVOCAB. We evaluate both cased and uncased versions of this model. SCIBERT We use the original BERT code to train SCIBERT on our corpus with the same configuration and size as BERT-Base. We train 4 different versions of SCIBERT: (i) cased or uncased and (ii) BASEVOCAB or SCIVOCAB."]}
{"pkey": "transformer_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "In this work, the paper authors propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs", "title": "Attention Is All You Need", "context": ["We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion. In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.", "Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", "We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting."]}
{"pkey": "transformer_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [20], ByteNet [15] and ConvS2S [8], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.", "title": "Attention Is All You Need", "context": ["MultiHead(Q,K, V ) = Concat(head1, ...,headh)W O where headi = Attention(QW Q i ,KW K i , V W V i ) Where the projections are parameter matrices WQi \u2208 Rdmodel\u00d7dk , WKi \u2208 Rdmodel\u00d7dk , WVi \u2208 Rdmodel\u00d7dv and WO \u2208 Rhdv\u00d7dmodel . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model. The Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. \u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. \u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks. In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\nFFN(x)", "We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", ". The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations."]}
{"pkey": "transformer_3", "question": "What are the main contributions of the paper?", "answer": "The paper authors propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.", "title": "Attention Is All You Need", "context": ["Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", ". The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.", "Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. \u2020Work performed while at Google Brain. \u2021Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. ar X iv :1 70 6. 03 76 2v 7 [ cs .C L ] 2 A ug 2 02 3 . The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work."]}
{"pkey": "transformer_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "Yes. The paper authors are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video.", "title": "Attention Is All You Need", "context": ["We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion. In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor.", "We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background. The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2."]}
{"pkey": "transformer_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, the paper authors achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.", "title": "Attention Is All You Need", "context": ["In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training. This section describes the training regime for our models. 5.1 Training Data and Batching. We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.", ". The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.", "Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation."]}
{"pkey": "transformer_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "The Transformer achieves better BLEU scores than previous state-of-the-art models on the English-to-German and English-to-French newstest2014 tests at a fraction of the training cost. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. To evaluate the importance of different components of the Transformer, the paper authors varied our base model in different ways, measuring the change in performance on English-to-German translation on the development set, newstest2013. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, the paper authors achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles.", "title": "Attention Is All You Need", "context": ["In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training. This section describes the training regime for our models. 5.1 Training Data and Batching. We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.", "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention. In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208 Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations.", ". The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations."]}
{"pkey": "transformer_7", "question": "List the limitations of the model discussed in the paper.", "answer": "As side benefit, self-attention could yield more interpretable models. The paper authors inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [31] and byte-pair [25] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). The paper authors plan to investigate this approach further in future work.", "title": "Attention Is All You Need", "context": [". The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.", "Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", "Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. \u2020Work performed while at Google Brain. \u2021Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\nar X\niv :1\n70 6.\n03 76\n2v 7\n1 Introduction. Recurrent neural networks, long short-term memory [13] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [35, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [38, 24, 15].\nRecurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht\u22121 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples."]}
{"pkey": "transformer_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.For English-French, the paper authors used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences.", "title": "Attention Is All You Need", "context": ["Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training. This section describes the training regime for our models. 5.1 Training Data and Batching. We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.", "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.\n6.2 Model Variations. To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\ndevelopment set, newstest2013."]}
{"pkey": "transformer_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "WMT 2014 English-German dataset sentences were encoded using byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens.WMT 2014 English-French dataset split tokens into a 32000 word-piece vocabulary.", "title": "Attention Is All You Need", "context": ["We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", "= max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048. 3.4 Embeddings and Softmax. Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221a dmodel. 3.5 Positional Encoding. Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/10000 2i/dmodel)\nPE(pos,2i+1) = cos(pos/10000 2i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos."]}
{"pkey": "transformer_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "Similarly to other sequence transduction models, the paper authors use learned embeddings to convert the input tokens and output tokens to vectors of dimension d_model\u200b. Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, the paper authors must inject some information about the relative or absolute position of the tokens in the sequence. To this end, the paper authors add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension  d_model as the embeddings, so that the two can be summed.\nSentences were encoded using byte-pair encoding [3], which has a shared source-target vocabulary of about 37,000 tokens. For English-French, the paper authors used the significantly larger WMT 2014 English-French dataset consisting of 36 million sentences and split tokens into a 32,000 word-piece vocabulary [31]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25,000 source tokens and 25,000 target tokens.", "title": "Attention Is All You Need", "context": ["Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.\n6.2 Model Variations. To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\ndevelopment set, newstest2013.", "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training. This section describes the training regime for our models. 5.1 Training Data and Batching. We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs."]}
{"pkey": "transformer_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "The encoder is composed of a stack of N = 6 identical layers. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512. The decoder is also composed of a stack of N = 6 identical layers. In this work the paper authors employ h = 8 parallel attention layers or heads. For each of these, the paper authors use dk = dv = dmodel/h = 64.  Table 3: Variations on the Transformer architecture. (All the details regarding base transformer and big transformer are provided in this table 3).", "title": "Attention Is All You Need", "context": ["We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "MultiHead(Q,K, V ) = Concat(head1, ...,headh)W O where headi = Attention(QW Q i ,KW K i , V W V i ) Where the projections are parameter matrices WQi \u2208 Rdmodel\u00d7dk , WKi \u2208 Rdmodel\u00d7dk , WVi \u2208 Rdmodel\u00d7dv and WO \u2208 Rhdv\u00d7dmodel . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model. The Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. \u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. \u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks. In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\nFFN(x)", "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion. In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor."]}
{"pkey": "transformer_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors used the Adam optimizer [17] with \u03b21 = 0.9, \u03b22 = 0.98 and epsilon = 10\u22129. The paper authors varied the learning rate over the course of training, according to the formula: lrate = d^\u22120.5 *  min(step_num\u22120.5, step_num \u00b7 warmup_steps\u22121.5).  s. The paper authors trained the base models for a total of 100,000 steps or 12 hours.", "title": "Attention Is All You Need", "context": ["Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", "On the WMT 2014 English-to-German translation task, the big transformer model (Transformer (big) in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0 BLEU, establishing a new state-of-the-art BLEU score of 28.4. The configuration of this model is listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. On the WMT 2014 English-to-French translation task, our big model achieves a BLEU score of 41.0, outperforming all of the previously published single models, at less than 1/4 the training cost of the previous state-of-the-art model. The Transformer (big) model trained for English-to-French used dropout rate Pdrop = 0.1, instead of 0.3. For the base models, we used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. For the big models, we averaged the last 20 checkpoints. We used beam search with a beam size of 4 and length penalty \u03b1 = 0.6 [38]. These hyperparameters were chosen after experimentation on the development set. We set the maximum output length during inference to input length + 50, but terminate early when possible [38]. Table 2 summarizes our results and compares our translation quality and training costs to other model architectures from the literature. We estimate the number of floating point operations used to train a model by multiplying the training time, the number of GPUs used, and an estimate of the sustained single-precision floating-point capacity of each GPU 5.\n6.2 Model Variations. To evaluate the importance of different components of the Transformer, we varied our base model in different ways, measuring the change in performance on English-to-German translation on the\n5We used values of 2.8, 3.7, 6.0 and 9.5 TFLOPS for K80, K40, M40 and P100, respectively.\ndevelopment set, newstest2013.", "We compute the matrix of outputs as:\nAttention(Q,K, V ) = softmax( QKT\u221a\ndk )V (1) The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1\u221a\ndk . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1\u221a\ndk . 3.2.2 Multi-Head Attention. Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \u00b7 k = \u2211dk i=1 qiki, has mean 0 and variance dk. output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this."]}
{"pkey": "transformer_13", "question": "Describe the computational resources used to train the model.", "answer": "The paper authors trained our models on one machine with 8 NVIDIA P100 GPUs.", "title": "Attention Is All You Need", "context": ["Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", "We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "Recent work has achieved significant improvements in computational efficiency through factorization tricks [21] and conditional computation [32], while also improving model performance in case of the latter. The fundamental constraint of sequential computation, however, remains. Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 19]. In all but a few cases [27], however, such attention mechanisms are used in conjunction with a recurrent network. In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. 2 Background. The goal of reducing sequential computation also forms the foundation of the Extended Neural GPU [16], ByteNet [18] and ConvS2S [9], all of which use convolutional neural networks as basic building block, computing hidden representations in parallel for all input and output positions. In these models, the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions [12]. In the Transformer this is reduced to a constant number of operations, albeit at the cost of reduced effective resolution due to averaging attention-weighted positions, an effect we counteract with Multi-Head Attention as described in section 3.2."]}
{"pkey": "transformer_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "Yes all the steps to reproduce the model are provided in the documentation which is availabe at the github likn. The code the paper authors used to train and evaluate our models is available at the Github url : https://github.com/tensorflow/tensor2tensor", "title": "Attention Is All You Need", "context": ["Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", ". The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations.", "Noam proposed scaled dot-product attention, multi-head attention and the parameter-free position representation and became the other person involved in nearly every detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating our research. \u2020Work performed while at Google Brain. \u2021Work performed while at Google Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA. ar X iv :1 70 6. 03 76 2v 7 [ cs .C L ] 2 A ug 2 02 3\nThe dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 Englishto-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. \u2217Equal contribution."]}
{"pkey": "transformer_15", "question": "What is the pretraining objective of the model? ", "answer": "Not specified in the paper.", "title": "Attention Is All You Need", "context": ["Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations [4, 27, 28, 22].\nEnd-to-end memory networks are based on a recurrent attention mechanism instead of sequencealigned recurrence and have been shown to perform well on simple-language question answering and language modeling tasks [34]. To the best of our knowledge, however, the Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution. In the following sections, we will describe the Transformer, motivate self-attention and discuss its advantages over models such as [17, 18] and [9].\n3 Model Architecture. Most competitive neural sequence transduction models have an encoder-decoder structure [5, 2, 35]. Here, the encoder maps an input sequence of symbol representations (x1, ..., xn) to a sequence of continuous representations z = (z1, ..., zn). Given z, the decoder then generates an output sequence (y1, ..., ym) of symbols one element at a time. At each step the model is auto-regressive [10], consuming the previously generated symbols as additional input when generating the next. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.\n3.1 Encoder and Decoder Stacks. Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, positionwise fully connected feed-forward network.", "= max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048. 3.4 Embeddings and Softmax. Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221a dmodel. 3.5 Positional Encoding. Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/10000 2i/dmodel)\nPE(pos,2i+1) = cos(pos/10000 2i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.", "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training. This section describes the training regime for our models. 5.1 Training Data and Batching. We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs."]}
{"pkey": "transformer_16", "question": "What is the loss function that is used to train the model?", "answer": "Not specified in the paper.", "title": "Attention Is All You Need", "context": ["= max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048. 3.4 Embeddings and Softmax. Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221a dmodel. 3.5 Positional Encoding. Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/10000 2i/dmodel)\nPE(pos,2i+1) = cos(pos/10000 2i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos.", "We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "We compute the matrix of outputs as:\nAttention(Q,K, V ) = softmax( QKT\u221a\ndk )V (1) The two most commonly used attention functions are additive attention [2], and dot-product (multiplicative) attention. Dot-product attention is identical to our algorithm, except for the scaling factor of 1\u221a\ndk . Additive attention computes the compatibility function using a feed-forward network with a single hidden layer. While the two are similar in theoretical complexity, dot-product attention is much faster and more space-efficient in practice, since it can be implemented using highly optimized matrix multiplication code. While for small values of dk the two mechanisms perform similarly, additive attention outperforms dot product attention without scaling for larger values of dk [3]. We suspect that for large values of dk, the dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients 4. To counteract this effect, we scale the dot products by 1\u221a\ndk . 3.2.2 Multi-Head Attention. Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively. On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding dv-dimensional\n4To illustrate why the dot products get large, assume that the components of q and k are independent random variables with mean 0 and variance 1. Then their dot product, q \u00b7 k = \u2211dk i=1 qiki, has mean 0 and variance dk. output values. These are concatenated and once again projected, resulting in the final values, as depicted in Figure 2.\nMulti-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this."]}
{"pkey": "transformer_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The transformer model architecture proposed in the paper is the base architecture for the encoder-decoder models. In this work, the paper authors presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention.  In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically.", "title": "Attention Is All You Need", "context": ["In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training. This section describes the training regime for our models. 5.1 Training Data and Batching. We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.", "We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "We also experimented with using learned positional embeddings [9] instead, and found that the two versions produced nearly identical results (see Table 3 row (E)). We chose the sinusoidal version because it may allow the model to extrapolate to sequence lengths longer than the ones encountered during training. 4 Why Self-Attention. In this section we compare various aspects of self-attention layers to the recurrent and convolutional layers commonly used for mapping one variable-length sequence of symbol representations (x1, ..., xn) to another sequence of equal length (z1, ..., zn), with xi, zi \u2208 Rd, such as a hidden layer in a typical sequence transduction encoder or decoder. Motivating our use of self-attention we consider three desiderata. One is the total computational complexity per layer. Another is the amount of computation that can be parallelized, as measured by the minimum number of sequential operations required. The third is the path length between long-range dependencies in the network. Learning long-range dependencies is a key challenge in many sequence transduction tasks. One key factor affecting the ability to learn such dependencies is the length of the paths forward and backward signals have to traverse in the network. The shorter these paths between any combination of positions in the input and output sequences, the easier it is to learn long-range dependencies [12]. Hence we also compare the maximum path length between any two input and output positions in networks composed of the different layer types. As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially executed operations, whereas a recurrent layer requires O(n) sequential operations."]}
{"pkey": "transformer_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "Our model achieves 28.4 BLEU on the WMT 2014 English_x0002_to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.", "title": "Attention Is All You Need", "context": ["We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "We employ a residual connection [11] around each of the two sub-layers, followed by layer normalization [1]. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension dmodel = 512.\nDecoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.\n3.2 Attention. An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is computed as a weighted sum\nof the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key. 3.2.1 Scaled Dot-Product Attention. We call our particular attention \"Scaled Dot-Product Attention\" (Figure 2). The input consists of queries and keys of dimension dk, and values of dimension dv . We compute the dot products of the query with all keys, divide each by \u221a dk, and apply a softmax function to obtain the weights on the values. In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. The keys and values are also packed together into matrices K and V .", "Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation."]}
{"pkey": "transformer_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "Variations on the Transformer architecture. Unlisted values are identical to those of the base model. All metrics are on the English-to-German translation development set, newstest2013. Listed perplexities are per-wordpiece, according to our byte-pair encoding, and should not be compared to per-word perplexities.In Table 3 rows (B), the paper authors observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. The paper authors further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) the paper authors replace our sinusoidal positional encoding with learned positional embeddings [8], and observe nearly identical results to the base model.", "title": "Attention Is All You Need", "context": ["MultiHead(Q,K, V ) = Concat(head1, ...,headh)W O where headi = Attention(QW Q i ,KW K i , V W V i ) Where the projections are parameter matrices WQi \u2208 Rdmodel\u00d7dk , WKi \u2208 Rdmodel\u00d7dk , WVi \u2208 Rdmodel\u00d7dv and WO \u2208 Rhdv\u00d7dmodel . In this work we employ h = 8 parallel attention layers, or heads. For each of these we use dk = dv = dmodel/h = 64. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 3.2.3 Applications of Attention in our Model. The Transformer uses multi-head attention in three different ways:\n\u2022 In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. \u2022 The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. \u2022 Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position. We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to \u2212\u221e) all values in the input of the softmax which correspond to illegal connections. See Figure 2.\n3.3 Position-wise Feed-Forward Networks. In addition to attention sub-layers, each of the layers in our encoder and decoder contains a fully connected feed-forward network, which is applied to each position separately and identically. This consists of two linear transformations with a ReLU activation in between.\nFFN(x)", "We used beam search as described in the previous section, but no checkpoint averaging. We present these results in Table 3. In Table 3 rows (A), we vary the number of attention heads and the attention key and value dimensions, keeping the amount of computation constant, as described in Section 3.2.2. While single-head attention is 0.9 BLEU worse than the best setting, quality also drops off with too many heads. In Table 3 rows (B), we observe that reducing the attention key size dk hurts model quality. This suggests that determining compatibility is not easy and that a more sophisticated compatibility function than dot product may be beneficial. We further observe in rows (C) and (D) that, as expected, bigger models are better, and dropout is very helpful in avoiding over-fitting. In row (E) we replace our sinusoidal positional encoding with learned positional embeddings [9], and observe nearly identical results to the base model. 6.3 English Constituency Parsing. To evaluate if the Transformer can generalize to other tasks we performed experiments on English constituency parsing. This task presents specific challenges: the output is subject to strong structural constraints and is significantly longer than the input. Furthermore, RNN sequence-to-sequence models have not been able to attain state-of-the-art results in small-data regimes [37]. We trained a 4-layer transformer with dmodel = 1024 on the Wall Street Journal (WSJ) portion of the Penn Treebank [25], about 40K training sentences. We also trained it in a semi-supervised setting, using the larger high-confidence and BerkleyParser corpora from with approximately 17M sentences [37]. We used a vocabulary of 16K tokens for the WSJ only setting and a vocabulary of 32K tokens for the semi-supervised setting.", "= max(0, xW1 + b1)W2 + b2 (2) While the linear transformations are the same across different positions, they use different parameters from layer to layer. Another way of describing this is as two convolutions with kernel size 1. The dimensionality of input and output is dmodel = 512, and the inner-layer has dimensionality dff = 2048. 3.4 Embeddings and Softmax. Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension dmodel. We also use the usual learned linear transformation and softmax function to convert the decoder output to predicted next-token probabilities. In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation, similar to [30]. In the embedding layers, we multiply those weights by \u221a dmodel. 3.5 Positional Encoding. Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add \"positional encodings\" to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed. There are many choices of positional encodings, learned and fixed [9]. In this work, we use sine and cosine functions of different frequencies:\nPE(pos,2i) = sin(pos/10000 2i/dmodel)\nPE(pos,2i+1) = cos(pos/10000 2i/dmodel)\nwhere pos is the position and i is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0. We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PEpos+k can be represented as a linear function of PEpos."]}
{"pkey": "transformer_20", "question": "List the future work mentioned in the paper.", "answer": "1) To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum\npath length to O(n/r). The paper authors plan to investigate this approach further in future work.                                                                                                            2)The paper authors are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goal of the paper.", "title": "Attention Is All You Need", "context": ["Sentences were encoded using byte-pair encoding [3], which has a shared sourcetarget vocabulary of about 37000 tokens. For English-French, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences and split tokens into a 32000 word-piece vocabulary [38]. Sentence pairs were batched together by approximate sequence length. Each training batch contained a set of sentence pairs containing approximately 25000 source tokens and 25000 target tokens. 5.2 Hardware and Schedule. We trained our models on one machine with 8 NVIDIA P100 GPUs. For our base models using the hyperparameters described throughout the paper, each training step took about 0.4 seconds. We trained the base models for a total of 100,000 steps or 12 hours. For our big models,(described on the bottom line of table 3), step time was 1.0 seconds. The big models were trained for 300,000 steps (3.5 days). 5.3 Optimizer. We used the Adam optimizer [20] with \u03b21 = 0.9, \u03b22 = 0.98 and \u03f5 = 10\u22129. We varied the learning rate over the course of training, according to the formula:\nlrate = d\u22120.5model \u00b7min(step_num \u22120.5, step_num \u00b7 warmup_steps\u22121.5) (3) This corresponds to increasing the learning rate linearly for the first warmup_steps training steps, and decreasing it thereafter proportionally to the inverse square root of the step number. We used warmup_steps = 4000.\n5.4 Regularization. We employ three types of regularization during training:\nResidual Dropout We apply dropout [33] to the output of each sub-layer, before it is added to the sub-layer input and normalized. In addition, we apply dropout to the sums of the embeddings and the positional encodings in both the encoder and decoder stacks. For the base model, we use a rate of Pdrop = 0.1.\nLabel Smoothing During training, we employed label smoothing of value \u03f5ls = 0.1 [36]. This hurts perplexity, as the model learns to be more unsure, but improves accuracy and BLEU score. 6 Results. 6.1 Machine Translation.", "In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence\nlength n is smaller than the representation dimensionality d, which is most often the case with sentence representations used by state-of-the-art models in machine translations, such as word-piece [38] and byte-pair [31] representations. To improve computational performance for tasks involving very long sequences, self-attention could be restricted to considering only a neighborhood of size r in the input sequence centered around the respective output position. This would increase the maximum path length to O(n/r). We plan to investigate this approach further in future work. A single convolutional layer with kernel width k < n does not connect all pairs of input and output positions. Doing so requires a stack of O(n/k) convolutional layers in the case of contiguous kernels, or O(logk(n)) in the case of dilated convolutions [18], increasing the length of the longest paths between any two positions in the network. Convolutional layers are generally more expensive than recurrent layers, by a factor of k. Separable convolutions [6], however, decrease the complexity considerably, to O(k \u00b7 n \u00b7 d + n \u00b7 d2). Even with k = n, however, the complexity of a separable convolution is equal to the combination of a self-attention layer and a point-wise feed-forward layer, the approach we take in our model. As side benefit, self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix. Not only do individual attention heads clearly learn to perform different tasks, many appear to exhibit behavior related to the syntactic and semantic structure of the sentences. 5 Training. This section describes the training regime for our models. 5.1 Training Data and Batching. We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.", "We performed only a small number of experiments to select the dropout, both attention and residual (section 5.4), learning rates and beam size on the Section 22 development set, all other parameters remained unchanged from the English-to-German base translation model. During inference, we\nincreased the maximum output length to input length + 300. We used a beam size of 21 and \u03b1 = 0.3 for both WSJ only and the semi-supervised setting. Our results in Table 4 show that despite the lack of task-specific tuning our model performs surprisingly well, yielding better results than all previously reported models with the exception of the Recurrent Neural Network Grammar [8]. In contrast to RNN sequence-to-sequence models [37], the Transformer outperforms the BerkeleyParser [29] even when training only on the WSJ training set of 40K sentences. 7 Conclusion. In this work, we presented the Transformer, the first sequence transduction model based entirely on attention, replacing the recurrent layers most commonly used in encoder-decoder architectures with multi-headed self-attention. For translation tasks, the Transformer can be trained significantly faster than architectures based on recurrent or convolutional layers. On both WMT 2014 English-to-German and WMT 2014 English-to-French translation tasks, we achieve a new state of the art. In the former task our best model outperforms even all previously reported ensembles. We are excited about the future of attention-based models and plan to apply them to other tasks. We plan to extend the Transformer to problems involving input and output modalities other than text and to investigate local, restricted attention mechanisms to efficiently handle large inputs and outputs such as images, audio and video. Making generation less sequential is another research goals of ours. The code we used to train and evaluate our models is available at https://github.com/ tensorflow/tensor2tensor."]}
{"pkey": "spanbert_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "The paper authors present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. SpanBERT consistently outperforms BERT and our better-tuned baselines.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "Finally, SpanBERT improves considerably on top of that, achieving a new state of the art of 79.6% F1 (previous best result is 73.0%). Relation Extraction Table 4 shows the performance on TACRED. SpanBERT exceeds our reimplementation of BERT by 3.3% F1 and achieves close to the current state of the art (Soares et al., 2019) \u2014 Our model performs better than their BERTEM but is 0.7 point behind BERTEM + MTB which used entity-linked text for additional pre-training. Most of this gain (+2.6%) stems from single-sequence training although the contribution of span masking and the span boundary objective is still a considerable 0.7%, resulting largely from higher recall. GLUE Table 5 shows the performance on GLUE. For most tasks, the different models appear to perform similarly. Moving to singlesequence training without the NSP objective substantially improves CoLA, and yields smaller\n(but considerable) improvements on MRPC and MNLI. The main gains from SpanBERT are in the SQuAD-based QNLI dataset (+1.3%) and in RTE (+6.9%), the latter accounting for most of the rise in SpanBERT\u2019s GLUE average. 5.2 Overall Trends. We compared our approach to three BERT baselines on 17 benchmarks, and found that SpanBERT outperforms BERT on almost every task. In 14 tasks, SpanBERT performed better than all baselines. In 2 tasks (MRPC and QQP), it performed on-par in terms of accuracy with singlesequence trained BERT, but still outperformed the other baselines. In one task (SST-2), Google\u2019s BERT baseline performed better than SpanBERT by 0.4% accuracy. When considering the magnitude of the gains, it appears that SpanBERT is especially better at extractive question answering. In SQuAD 1.1, for example, we observe a solid gain of 2.0% F1 even though the baseline is already well above human performance. On MRQA, SpanBERT improves between 2.0% (Natural Questions) and 4.6% (TriviaQA) F1 on top of our BERT baseline.", "Specifically, we run spaCy\u2019s named entity recognizer11 on the corpus and select all the non-numerical named entity mentions as candidates. Noun Phrases Similar as Named Entities, we sample from noun phrases at 50% of the time. The noun phrases are extracted by running spaCy\u2019s constituency parser. Geometric Spans We sample random spans from a geometric distribution, as in our SpanBERT (see Section 3.1). Table 6 shows how different pre-training masking schemes affect performance on the development set of a selection of tasks. All the models are evaluated on the development sets and are based on the default BERT setup of bi-sequence training with NSP; the results are not directly comparable to the main evaluation. With the exception of coreference resolution, masking random spans is preferable to other strategies. Although linguistic masking schemes (named entities and\n11https://spacy.io/\nnoun phrases) are often competitive with random spans, their performance is not consistent; for instance, masking noun phrases achieves parity with random spans on NewsQA, but underperforms on TriviaQA (-1.1% F1). On coreference resolution, we see that masking random subword tokens is preferable to any form of span masking. Nevertheless, we shall see in the following experiment that combining random span masking with the span boundary objective can improve upon this result considerably. 6.2 Auxiliary Objectives. In Section 5, we saw that bi-sequence training with the next sentence prediction (NSP) objective can hurt performance on downstream tasks, when compared to single-sequence training. We test whether this holds true for models pre-trained with span masking, and also evaluate the effect of replacing the NSP objective with the span boundary objective (SBO). Table 7 confirms that single-sequence training typically improves performance. Adding SBO further improves performance, with a substantial gain on coreference resolution (+2.7% F1) over span masking alone."]}
{"pkey": "spanbert_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "However, many NLP tasks involve reasoning about relationships between two or more spans of text. \u2018NFL team\u2019 is critical for answering the question \u2018Which NFL team won Super Bowl 50?\u2019 Such spans provide a more challenging target for self-supervision tasks, for example, predicting \u2018Denver Broncos\u2019 is much harder than predicting only \u2018Denver\u2019 when you know the next word is \u2018Broncos\u2019. In this paper, the paper authors introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on the span selection tasks such as question answering and coreference resolution.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "Unlike the NSP objective, SBO does not appear to have any adverse effects. 7 Related Work. Pre-trained contextualized word representations that can be trained from unlabeled text (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018) have had immense impact on NLP lately, particularly as methods for initializing a large model before fine-tuning it for a specific task (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Beyond differences in model hyperparameters and corpora, these methods mainly differ in their pre-training tasks and loss functions, with a considerable amount of contemporary literature proposing augmentations of BERT\u2019s masked language modeling (MLM) objective. While previous and concurrent work has looked at masking (Sun et al., 2019) or dropping (Song et al., 2019; Chan et al., 2019) multiple words from the input \u2013 particularly as pretraining for language generation tasks \u2013 SpanBERT pretrains span representations (Lee et al., 2016), which are widely used for question answering, coreference resolution and a variety of other tasks. ERNIE\n(Sun et al., 2019) shows improvements on Chinese NLP tasks using phrase and named entity masking. MASS (Song et al., 2019) focuses on language generation tasks, and adopts the encoderdecoder framework to reconstruct a sentence fragment given the remaining part of the sentence. We attempt to more explicitly model spans using the SBO objective, and show that (geometrically distributed) random span masking works as well, and sometimes better than, masking linguisticallycoherent spans. We evaluate on English benchmarks for question answering, relation extraction, and coreference resolution in addition to GLUE. A different ERNIE (Zhang et al., 2019) focuses on integrating structured knowledge bases with contextualized representations with an eye on knowledge-driven tasks like entity typing and relation classification.", "In BERT\u2019s implementation, Y accounts for 15% of the tokens in X; of those, 80% are replaced with [MASK], 10% are replaced with a random token (according to the unigram distribution), and 10% are kept unchanged. The task is to predict the original tokens in Y from the modified input. BERT selects each token in Y independently by randomly selecting a subset. In SpanBERT, we define Y by randomly selecting contiguous spans (Section 3.1). Next Sentence Prediction (NSP) The NSP task takes two sequences (XA, XB) as input, and predicts whetherXB is the direct continuation ofXA. This is implemented in BERT by first reading XA from the corpus, and then (1) either reading XB from the point where XA ended, or (2) randomly sampling XB from a different point in the cor-\npus. The two sequences are separated by a special [SEP]token. Additionally, a special [CLS]token is added to XA, XB to form the input, where the target of [CLS]is whetherXB indeed followsXA in the corpus. In summary, BERT optimizes the MLM and the NSP objectives by masking word pieces uniformly at random in data generated by the bisequence sampling procedure. In the next section, we will present our modifications to the data pipeline, masking, and pre-training objectives. 3 Model. We present SpanBERT, a self-supervised pretraining method designed to better represent and predict spans of text. Our approach is inspired by BERT (Devlin et al., 2019), but deviates from its bi-text classification framework in three ways. First, we use a different random process to mask spans of tokens, rather than individual ones. We also introduce a novel auxiliary objective \u2013 the span boundary objective (SBO) \u2013 which tries to predict the entire masked span using only the representations of the tokens at the span\u2019s boundary. Finally, SpanBERT samples a single contiguous segment of text for each training example (instead of two), and thus does not use BERT\u2019s next sentence prediction objective, which we omit. 3.1 Span Masking."]}
{"pkey": "spanbert_3", "question": "What are the main contributions of the paper?", "answer": "In particular, with the same training data and model size as BERTlarge, our single model obtains 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 respectively. The paper authors also achieve a new state of the art on the OntoNotes coreference resolution task (79.6% F1), strong performance on the TACRED relation extraction benchmark, and even gains on GLUE.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "Finally, SpanBERT improves considerably on top of that, achieving a new state of the art of 79.6% F1 (previous best result is 73.0%). Relation Extraction Table 4 shows the performance on TACRED. SpanBERT exceeds our reimplementation of BERT by 3.3% F1 and achieves close to the current state of the art (Soares et al., 2019) \u2014 Our model performs better than their BERTEM but is 0.7 point behind BERTEM + MTB which used entity-linked text for additional pre-training. Most of this gain (+2.6%) stems from single-sequence training although the contribution of span masking and the span boundary objective is still a considerable 0.7%, resulting largely from higher recall. GLUE Table 5 shows the performance on GLUE. For most tasks, the different models appear to perform similarly. Moving to singlesequence training without the NSP objective substantially improves CoLA, and yields smaller\n(but considerable) improvements on MRPC and MNLI. The main gains from SpanBERT are in the SQuAD-based QNLI dataset (+1.3%) and in RTE (+6.9%), the latter accounting for most of the rise in SpanBERT\u2019s GLUE average. 5.2 Overall Trends. We compared our approach to three BERT baselines on 17 benchmarks, and found that SpanBERT outperforms BERT on almost every task. In 14 tasks, SpanBERT performed better than all baselines. In 2 tasks (MRPC and QQP), it performed on-par in terms of accuracy with singlesequence trained BERT, but still outperformed the other baselines. In one task (SST-2), Google\u2019s BERT baseline performed better than SpanBERT by 0.4% accuracy. When considering the magnitude of the gains, it appears that SpanBERT is especially better at extractive question answering. In SQuAD 1.1, for example, we observe a solid gain of 2.0% F1 even though the baseline is already well above human performance. On MRQA, SpanBERT improves between 2.0% (Natural Questions) and 4.6% (TriviaQA) F1 on top of our BERT baseline.", "(2018) focus on improving language generation speed using a block-wise parallel decoding scheme; they make predictions for multiple time steps in parallel and then back off to the longest prefix validated by a scoring model. Also related are sentence representation methods (Kiros et al., 2015; Logeswaran and Lee, 2018) which focus on predicting surrounding contexts from sentence embeddings. 8 Conclusion. We presented a new method for span-based pretraining which extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Together, our pre-training process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.\nAcknowledgements. We would like to thank Pranav Rajpurkar and Robin Jia for patiently helping us evaluate SpanBERT on SQuAD. We thank the anonymous reviewers, the action editor, and our colleagues at Facebook AI Research and the University of Washington for their insightful feedback that helped improve the paper. A Pre-training Procedure. We describe our pre-training procedure as follows:\n1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre-training:\n(a) Sample a batch of blocks uniformly at random. (b) Mask 15% of word pieces in each block in the batch using the span masking scheme (Section 3.1). (c) For each masked token xi, optimize L(xi) = LMLM(xi) + LSBO(xi) (Section 3.2). B Fine-tuning Hyperparameters. We apply the following fine-tuning hyperparameters to all methods, including the baselines. Extractive Question Answering For all the question answering tasks, we use max_seq_length = 512 and a sliding window of size 128 if the lengths are longer than 512."]}
{"pkey": "spanbert_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "The paper authors present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our approach extends BERT. Together, our pretraining process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "Unlike the NSP objective, SBO does not appear to have any adverse effects. 7 Related Work. Pre-trained contextualized word representations that can be trained from unlabeled text (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018) have had immense impact on NLP lately, particularly as methods for initializing a large model before fine-tuning it for a specific task (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Beyond differences in model hyperparameters and corpora, these methods mainly differ in their pre-training tasks and loss functions, with a considerable amount of contemporary literature proposing augmentations of BERT\u2019s masked language modeling (MLM) objective. While previous and concurrent work has looked at masking (Sun et al., 2019) or dropping (Song et al., 2019; Chan et al., 2019) multiple words from the input \u2013 particularly as pretraining for language generation tasks \u2013 SpanBERT pretrains span representations (Lee et al., 2016), which are widely used for question answering, coreference resolution and a variety of other tasks. ERNIE\n(Sun et al., 2019) shows improvements on Chinese NLP tasks using phrase and named entity masking. MASS (Song et al., 2019) focuses on language generation tasks, and adopts the encoderdecoder framework to reconstruct a sentence fragment given the remaining part of the sentence. We attempt to more explicitly model spans using the SBO objective, and show that (geometrically distributed) random span masking works as well, and sometimes better than, masking linguisticallycoherent spans. We evaluate on English benchmarks for question answering, relation extraction, and coreference resolution in addition to GLUE. A different ERNIE (Zhang et al., 2019) focuses on integrating structured knowledge bases with contextualized representations with an eye on knowledge-driven tasks like entity typing and relation classification.", "In summary, SpanBERT pre-trains span representations by: (1) masking spans of full words using a geometric distribution based masking scheme (Section 3.1), (2) optimizing an auxiliary span-boundary objective (Section 3.2) in addition to MLM using a single-sequence data pipeline (Section 3.3). A procedural description can be found in Appendix A.\n4 Experimental Setup.\n4.1 Tasks. We evaluate on a comprehensive suite of tasks, including seven question answering tasks, corefer-\nence resolution, nine tasks in the GLUE benchmark (Wang et al., 2019), and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training. Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4: NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good testbed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1, p2, . . . , pl) and question Q = (q1, q2, . . . , ql\u2032) into a single sequence X = [CLS]p1p2 . . . pl[SEP]q1q2 . . ."]}
{"pkey": "spanbert_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). The paper authors also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019): NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019).", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "In summary, SpanBERT pre-trains span representations by: (1) masking spans of full words using a geometric distribution based masking scheme (Section 3.1), (2) optimizing an auxiliary span-boundary objective (Section 3.2) in addition to MLM using a single-sequence data pipeline (Section 3.3). A procedural description can be found in Appendix A.\n4 Experimental Setup.\n4.1 Tasks. We evaluate on a comprehensive suite of tasks, including seven question answering tasks, corefer-\nence resolution, nine tasks in the GLUE benchmark (Wang et al., 2019), and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training. Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4: NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good testbed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1, p2, . . . , pl) and question Q = (q1, q2, . . . , ql\u2032) into a single sequence X = [CLS]p1p2 . . . pl[SEP]q1q2 . . .", "Specifically, we run spaCy\u2019s named entity recognizer11 on the corpus and select all the non-numerical named entity mentions as candidates. Noun Phrases Similar as Named Entities, we sample from noun phrases at 50% of the time. The noun phrases are extracted by running spaCy\u2019s constituency parser. Geometric Spans We sample random spans from a geometric distribution, as in our SpanBERT (see Section 3.1). Table 6 shows how different pre-training masking schemes affect performance on the development set of a selection of tasks. All the models are evaluated on the development sets and are based on the default BERT setup of bi-sequence training with NSP; the results are not directly comparable to the main evaluation. With the exception of coreference resolution, masking random spans is preferable to other strategies. Although linguistic masking schemes (named entities and\n11https://spacy.io/\nnoun phrases) are often competitive with random spans, their performance is not consistent; for instance, masking noun phrases achieves parity with random spans on NewsQA, but underperforms on TriviaQA (-1.1% F1). On coreference resolution, we see that masking random subword tokens is preferable to any form of span masking. Nevertheless, we shall see in the following experiment that combining random span masking with the span boundary objective can improve upon this result considerably. 6.2 Auxiliary Objectives. In Section 5, we saw that bi-sequence training with the next sentence prediction (NSP) objective can hurt performance on downstream tasks, when compared to single-sequence training. We test whether this holds true for models pre-trained with span masking, and also evaluate the effect of replacing the NSP objective with the span boundary objective (SBO). Table 7 confirms that single-sequence training typically improves performance. Adding SBO further improves performance, with a substantial gain on coreference resolution (+2.7% F1) over span masking alone."]}
{"pkey": "spanbert_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not specifically mentioned in the paper. But evaluating models on a diverse set of datasets and tasks can help identify and mitigate bias.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "Specifically, we run spaCy\u2019s named entity recognizer11 on the corpus and select all the non-numerical named entity mentions as candidates. Noun Phrases Similar as Named Entities, we sample from noun phrases at 50% of the time. The noun phrases are extracted by running spaCy\u2019s constituency parser. Geometric Spans We sample random spans from a geometric distribution, as in our SpanBERT (see Section 3.1). Table 6 shows how different pre-training masking schemes affect performance on the development set of a selection of tasks. All the models are evaluated on the development sets and are based on the default BERT setup of bi-sequence training with NSP; the results are not directly comparable to the main evaluation. With the exception of coreference resolution, masking random spans is preferable to other strategies. Although linguistic masking schemes (named entities and\n11https://spacy.io/\nnoun phrases) are often competitive with random spans, their performance is not consistent; for instance, masking noun phrases achieves parity with random spans on NewsQA, but underperforms on TriviaQA (-1.1% F1). On coreference resolution, we see that masking random subword tokens is preferable to any form of span masking. Nevertheless, we shall see in the following experiment that combining random span masking with the span boundary objective can improve upon this result considerably. 6.2 Auxiliary Objectives. In Section 5, we saw that bi-sequence training with the next sentence prediction (NSP) objective can hurt performance on downstream tasks, when compared to single-sequence training. We test whether this holds true for models pre-trained with span masking, and also evaluate the effect of replacing the NSP objective with the span boundary objective (SBO). Table 7 confirms that single-sequence training typically improves performance. Adding SBO further improves performance, with a substantial gain on coreference resolution (+2.7% F1) over span masking alone.", "Unlike the NSP objective, SBO does not appear to have any adverse effects. 7 Related Work. Pre-trained contextualized word representations that can be trained from unlabeled text (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018) have had immense impact on NLP lately, particularly as methods for initializing a large model before fine-tuning it for a specific task (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Beyond differences in model hyperparameters and corpora, these methods mainly differ in their pre-training tasks and loss functions, with a considerable amount of contemporary literature proposing augmentations of BERT\u2019s masked language modeling (MLM) objective. While previous and concurrent work has looked at masking (Sun et al., 2019) or dropping (Song et al., 2019; Chan et al., 2019) multiple words from the input \u2013 particularly as pretraining for language generation tasks \u2013 SpanBERT pretrains span representations (Lee et al., 2016), which are widely used for question answering, coreference resolution and a variety of other tasks. ERNIE\n(Sun et al., 2019) shows improvements on Chinese NLP tasks using phrase and named entity masking. MASS (Song et al., 2019) focuses on language generation tasks, and adopts the encoderdecoder framework to reconstruct a sentence fragment given the remaining part of the sentence. We attempt to more explicitly model spans using the SBO objective, and show that (geometrically distributed) random span masking works as well, and sometimes better than, masking linguisticallycoherent spans. We evaluate on English benchmarks for question answering, relation extraction, and coreference resolution in addition to GLUE. A different ERNIE (Zhang et al., 2019) focuses on integrating structured knowledge bases with contextualized representations with an eye on knowledge-driven tasks like entity typing and relation classification."]}
{"pkey": "spanbert_7", "question": "List the limitations of the model discussed in the paper.", "answer": "In two tasks (MRPC and QQP), it performed on-par in terms of accuracy with single-sequence trained BERT, but still outperformed the other baselines. In one task (SST-2), Google\u2019s BERT baseline performed better than SpanBERT by 0.4% accuracy. Finally, the paper authors observe that single-sequence training works considerably better than bi-sequence training with NSP with BERT\u2019s choice of sequence lengths for a wide variety of tasks.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["While recent work Liu et al. (2019a) has applied several taskspecific strategies to increase performance on the individual GLUE tasks, we follow BERT\u2019s singletask setting and only add a linear classifier on top of the [CLS]token for these classification tasks. 4.2 Implementation. We reimplemented BERT\u2019s model and pretraining method in fairseq (Ott et al., 2019). We used the model configuration of BERTlarge as in Devlin et al. (2019) and also pre-trained all our models on the same corpus: BooksCorpus and English Wikipedia using cased Wordpiece tokens. Compared to the original BERT implementation, the main differences in our implementation include: (a) We use different masks at each epoch while BERT samples 10 different masks for each sequence during data processing. (b) We remove all the short-sequence strategies used before (they sampled shorter sequences with a small probability 0.1; they also first pre-trained with smaller se-\n6https://data.quora.com/First-Quora-Dataset-ReleaseQuestion-Pairs\n7Previous work has excluded WNLI on account of construction issues outlined on the GLUE website \u2013 https: //gluebenchmark.com/faq\nquence length of 128 for 90% of the steps). Instead, we always take sequences of up to 512 tokens until it reaches a document boundary. We refer readers to Liu et al. (2019b) for further discussion on these modifications and their effects. As in BERT, the learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. We retain \u03b2 hyperparameters (\u03b21 = 0.9, \u03b22 = 0.999) and a decoupled weight decay (Loshchilov and Hutter, 2019) of 0.1. We also keep a dropout of 0.1 on all layers and attention weights, and a GeLU activation function (Hendrycks and Gimpel, 2016). We deviate from the optimization by running for 2.4M steps and using an epsilon of 1e-8 for AdamW (Kingma and Ba, 2015), which converges to a better set of model parameters.", "Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "UNILM (Dong et al., 2019) uses multiple language modeling objectives \u2013 unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction \u2013 to aid generation tasks like summarization and question generation. XLM (Lample and Conneau, 2019) explores cross-lingual pre-training for multilingual tasks such as translation and crosslingual classification. Kermit (Chan et al., 2019), an insertion based approach, fills in missing tokens (instead of predicting masked ones) during pretraining; they show improvements on machine translation and zero-shot question answering. Concurrent with our work, RoBERTa (Liu et al., 2019b) presents a replication study of BERT pre-training that measures the impact of many key hyperparameters and training data size. Also concurrent, XLNet (Yang et al., 2019) combines an autoregressive loss and the Transformer-XL (Dai et al., 2019) architecture with a more than an eight-fold increase in data to achieve current stateof-the-art results on multiple benchmarks. XLNet also masks spans (of 1-5 tokens) during pretraining, but predicts them autoregressively. Our model focuses on incorporating span-based pretraining, and as a side effect, we present a stronger BERT baseline while controlling for the corpus, architecture, and the number of parameters. Related to our SBO objective, pair2vec (Joshi et al., 2019a) encodes word-pair relations using a negative sampling-based multivariate objective during pre-training. Later, the word-pair representations are injected into the attention-layer of downstream tasks, and thus encode limited down-\nstream context. Unlike pair2vec, our SBO objective yields \u201cpair\u201d (start and end tokens of spans) representations which more fully encode the context during both pre-training and finetuning, and are thus more appropriately viewed as span representations. Stern et al."]}
{"pkey": "spanbert_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). The paper authors also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4: NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018), and Natural Questions (Kwiatkowski et al., 2019).", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Our implementation uses a batch size of 256 sequences with a maximum of 512 tokens.8 For the SBO, we use 200 dimension position embeddings p1,p2, . . . to mark positions relative to the left boundary token. The pre-training was done on 32 Volta V100 GPUs and took 15 days to complete. Fine-tuning is implemented based on HuggingFace\u2019s codebase (Wolf et al., 2019) and more details are given in Appendix B.\n4.3 Baselines. We compare SpanBERT to three baselines:\nGoogle BERT The pre-trained models released by Devlin et al. (2019).9\nOur BERT Our reimplementation of BERT with improved data preprocessing and optimization (Section 4.2). Our BERT-1seq Our reimplementation of BERT trained on single full-length sequences without NSP (Section 3.3). 5 Results. We compare SpanBERT to the baselines per task, and draw conclusions based on the overall trends. 5.1 Per-Task Results. Extractive Question Answering Table 1 shows the performance on both SQuAD 1.1 and 2.0. SpanBERT exceeds our BERT baseline by 2.0% and 2.8% F1 respectively (3.3% and 5.4% over\n8On the average, this is approximately 390 sequences since some documents have fewer than 512 tokens\n9https://github.com/google-research/bert. Google BERT). In SQuAD 1.1, this result accounts for over 27% error reduction, reaching 3.4% F1 above human performance. Table 2 demonstrates that this trend goes beyond SQuAD, and is consistent in every MRQA dataset. On average, we see a 2.9% F1 improvement from our reimplementation of BERT. Although some gains are coming from singlesequence training (+1.1%), most of the improvement stems from span masking and the span boundary objective (+1.8%), with particularly large gains on TriviaQA (+3.2%) and HotpotQA (+2.7%). Coreference Resolution Table 3 shows the performance on the OntoNotes coreference resolution benchmark. Our BERT reimplementation improves the Google BERT model by 1.2% on the average F1 metric and single-sequence training brings another 0.5% gain.", "In summary, SpanBERT pre-trains span representations by: (1) masking spans of full words using a geometric distribution based masking scheme (Section 3.1), (2) optimizing an auxiliary span-boundary objective (Section 3.2) in addition to MLM using a single-sequence data pipeline (Section 3.3). A procedural description can be found in Appendix A.\n4 Experimental Setup.\n4.1 Tasks. We evaluate on a comprehensive suite of tasks, including seven question answering tasks, corefer-\nence resolution, nine tasks in the GLUE benchmark (Wang et al., 2019), and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training. Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4: NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good testbed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1, p2, . . . , pl) and question Q = (q1, q2, . . . , ql\u2032) into a single sequence X = [CLS]p1p2 . . . pl[SEP]q1q2 . . .", "Specifically, our method reaches 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), respectively \u2014 reducing error by as much as 27% compared to our tuned BERT replica. We also observe similar gains on five additional extractive question answering benchmarks (NewsQA, TriviaQA, SearchQA, HotpotQA, and Natural Questions).2\nSpanBERT also arrives at a new state of the art on the challenging CoNLL-2012 (\u201cOntoNotes\u201d) shared task for document-level coreference resolution, where we reach 79.6% F1, exceeding the previous top model by 6.6% absolute. Finally, we demonstrate that SpanBERT also helps on tasks that do not explicitly involve span selection, and show that our approach even improves performance on TACRED (Zhang et al., 2017) and GLUE (Wang et al., 2019). While others show the benefits of adding more data (Yang et al., 2019) and increasing model size (Lample and Conneau, 2019), this work demonstrates the importance of designing good pretraining tasks and objectives, which can also have a remarkable impact. 2 Background: BERT.\nBERT (Devlin et al., 2019) is a self-supervised approach for pre-training a deep transformer encoder (Vaswani et al., 2017), before fine-tuning\n2We use the modified MRQA version of these datasets. See more details in Section 4.1.\nit for a particular downstream task. BERT optimizes two training objectives \u2013 masked language model (MLM) and next sentence prediction (NSP) \u2013 which only require a large collection of unlabeled text. Notation Given a sequence of word or sub-word tokens X = (x1, x2, . . . , xn), BERT trains an encoder that produces a contextualized vector representation for each token: enc(x1, x2, . . . , xn) = x1,x2, . . . ,xn. Masked Language Model (MLM) Also known as a cloze test, MLM is the task of predicting missing tokens in a sequence from their placeholders. Specifically, a subset of tokens Y \u2286 X is sampled and substituted with a different set of tokens."]}
{"pkey": "spanbert_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Subword Tokens: The paper authors sample random Wordpiece tokens, as in the original BERT. The paper does not specify any data preprocessing.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["(2018) focus on improving language generation speed using a block-wise parallel decoding scheme; they make predictions for multiple time steps in parallel and then back off to the longest prefix validated by a scoring model. Also related are sentence representation methods (Kiros et al., 2015; Logeswaran and Lee, 2018) which focus on predicting surrounding contexts from sentence embeddings. 8 Conclusion. We presented a new method for span-based pretraining which extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Together, our pre-training process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.\nAcknowledgements. We would like to thank Pranav Rajpurkar and Robin Jia for patiently helping us evaluate SpanBERT on SQuAD. We thank the anonymous reviewers, the action editor, and our colleagues at Facebook AI Research and the University of Washington for their insightful feedback that helped improve the paper. A Pre-training Procedure. We describe our pre-training procedure as follows:\n1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre-training:\n(a) Sample a batch of blocks uniformly at random. (b) Mask 15% of word pieces in each block in the batch using the span masking scheme (Section 3.1). (c) For each masked token xi, optimize L(xi) = LMLM(xi) + LSBO(xi) (Section 3.2). B Fine-tuning Hyperparameters. We apply the following fine-tuning hyperparameters to all methods, including the baselines. Extractive Question Answering For all the question answering tasks, we use max_seq_length = 512 and a sliding window of size 128 if the lengths are longer than 512.", "Our implementation uses a batch size of 256 sequences with a maximum of 512 tokens.8 For the SBO, we use 200 dimension position embeddings p1,p2, . . . to mark positions relative to the left boundary token. The pre-training was done on 32 Volta V100 GPUs and took 15 days to complete. Fine-tuning is implemented based on HuggingFace\u2019s codebase (Wolf et al., 2019) and more details are given in Appendix B.\n4.3 Baselines. We compare SpanBERT to three baselines:\nGoogle BERT The pre-trained models released by Devlin et al. (2019).9\nOur BERT Our reimplementation of BERT with improved data preprocessing and optimization (Section 4.2). Our BERT-1seq Our reimplementation of BERT trained on single full-length sequences without NSP (Section 3.3). 5 Results. We compare SpanBERT to the baselines per task, and draw conclusions based on the overall trends. 5.1 Per-Task Results. Extractive Question Answering Table 1 shows the performance on both SQuAD 1.1 and 2.0. SpanBERT exceeds our BERT baseline by 2.0% and 2.8% F1 respectively (3.3% and 5.4% over\n8On the average, this is approximately 390 sequences since some documents have fewer than 512 tokens\n9https://github.com/google-research/bert. Google BERT). In SQuAD 1.1, this result accounts for over 27% error reduction, reaching 3.4% F1 above human performance. Table 2 demonstrates that this trend goes beyond SQuAD, and is consistent in every MRQA dataset. On average, we see a 2.9% F1 improvement from our reimplementation of BERT. Although some gains are coming from singlesequence training (+1.1%), most of the improvement stems from span masking and the span boundary objective (+1.8%), with particularly large gains on TriviaQA (+3.2%) and HotpotQA (+2.7%). Coreference Resolution Table 3 shows the performance on the OntoNotes coreference resolution benchmark. Our BERT reimplementation improves the Google BERT model by 1.2% on the average F1 metric and single-sequence training brings another 0.5% gain.", "UNILM (Dong et al., 2019) uses multiple language modeling objectives \u2013 unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction \u2013 to aid generation tasks like summarization and question generation. XLM (Lample and Conneau, 2019) explores cross-lingual pre-training for multilingual tasks such as translation and crosslingual classification. Kermit (Chan et al., 2019), an insertion based approach, fills in missing tokens (instead of predicting masked ones) during pretraining; they show improvements on machine translation and zero-shot question answering. Concurrent with our work, RoBERTa (Liu et al., 2019b) presents a replication study of BERT pre-training that measures the impact of many key hyperparameters and training data size. Also concurrent, XLNet (Yang et al., 2019) combines an autoregressive loss and the Transformer-XL (Dai et al., 2019) architecture with a more than an eight-fold increase in data to achieve current stateof-the-art results on multiple benchmarks. XLNet also masks spans (of 1-5 tokens) during pretraining, but predicts them autoregressively. Our model focuses on incorporating span-based pretraining, and as a side effect, we present a stronger BERT baseline while controlling for the corpus, architecture, and the number of parameters. Related to our SBO objective, pair2vec (Joshi et al., 2019a) encodes word-pair relations using a negative sampling-based multivariate objective during pre-training. Later, the word-pair representations are injected into the attention-layer of downstream tasks, and thus encode limited down-\nstream context. Unlike pair2vec, our SBO objective yields \u201cpair\u201d (start and end tokens of spans) representations which more fully encode the context during both pre-training and finetuning, and are thus more appropriately viewed as span representations. Stern et al."]}
{"pkey": "spanbert_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "Paper does not specifies any data pre-proccesing.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Our implementation uses a batch size of 256 sequences with a maximum of 512 tokens.8 For the SBO, we use 200 dimension position embeddings p1,p2, . . . to mark positions relative to the left boundary token. The pre-training was done on 32 Volta V100 GPUs and took 15 days to complete. Fine-tuning is implemented based on HuggingFace\u2019s codebase (Wolf et al., 2019) and more details are given in Appendix B.\n4.3 Baselines. We compare SpanBERT to three baselines:\nGoogle BERT The pre-trained models released by Devlin et al. (2019).9\nOur BERT Our reimplementation of BERT with improved data preprocessing and optimization (Section 4.2). Our BERT-1seq Our reimplementation of BERT trained on single full-length sequences without NSP (Section 3.3). 5 Results. We compare SpanBERT to the baselines per task, and draw conclusions based on the overall trends. 5.1 Per-Task Results. Extractive Question Answering Table 1 shows the performance on both SQuAD 1.1 and 2.0. SpanBERT exceeds our BERT baseline by 2.0% and 2.8% F1 respectively (3.3% and 5.4% over\n8On the average, this is approximately 390 sequences since some documents have fewer than 512 tokens\n9https://github.com/google-research/bert. Google BERT). In SQuAD 1.1, this result accounts for over 27% error reduction, reaching 3.4% F1 above human performance. Table 2 demonstrates that this trend goes beyond SQuAD, and is consistent in every MRQA dataset. On average, we see a 2.9% F1 improvement from our reimplementation of BERT. Although some gains are coming from singlesequence training (+1.1%), most of the improvement stems from span masking and the span boundary objective (+1.8%), with particularly large gains on TriviaQA (+3.2%) and HotpotQA (+2.7%). Coreference Resolution Table 3 shows the performance on the OntoNotes coreference resolution benchmark. Our BERT reimplementation improves the Google BERT model by 1.2% on the average F1 metric and single-sequence training brings another 0.5% gain.", "In summary, SpanBERT pre-trains span representations by: (1) masking spans of full words using a geometric distribution based masking scheme (Section 3.1), (2) optimizing an auxiliary span-boundary objective (Section 3.2) in addition to MLM using a single-sequence data pipeline (Section 3.3). A procedural description can be found in Appendix A.\n4 Experimental Setup.\n4.1 Tasks. We evaluate on a comprehensive suite of tasks, including seven question answering tasks, corefer-\nence resolution, nine tasks in the GLUE benchmark (Wang et al., 2019), and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training. Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4: NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good testbed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1, p2, . . . , pl) and question Q = (q1, q2, . . . , ql\u2032) into a single sequence X = [CLS]p1p2 . . . pl[SEP]q1q2 . . .", "Finally, SpanBERT improves considerably on top of that, achieving a new state of the art of 79.6% F1 (previous best result is 73.0%). Relation Extraction Table 4 shows the performance on TACRED. SpanBERT exceeds our reimplementation of BERT by 3.3% F1 and achieves close to the current state of the art (Soares et al., 2019) \u2014 Our model performs better than their BERTEM but is 0.7 point behind BERTEM + MTB which used entity-linked text for additional pre-training. Most of this gain (+2.6%) stems from single-sequence training although the contribution of span masking and the span boundary objective is still a considerable 0.7%, resulting largely from higher recall. GLUE Table 5 shows the performance on GLUE. For most tasks, the different models appear to perform similarly. Moving to singlesequence training without the NSP objective substantially improves CoLA, and yields smaller\n(but considerable) improvements on MRPC and MNLI. The main gains from SpanBERT are in the SQuAD-based QNLI dataset (+1.3%) and in RTE (+6.9%), the latter accounting for most of the rise in SpanBERT\u2019s GLUE average. 5.2 Overall Trends. We compared our approach to three BERT baselines on 17 benchmarks, and found that SpanBERT outperforms BERT on almost every task. In 14 tasks, SpanBERT performed better than all baselines. In 2 tasks (MRPC and QQP), it performed on-par in terms of accuracy with singlesequence trained BERT, but still outperformed the other baselines. In one task (SST-2), Google\u2019s BERT baseline performed better than SpanBERT by 0.4% accuracy. When considering the magnitude of the gains, it appears that SpanBERT is especially better at extractive question answering. In SQuAD 1.1, for example, we observe a solid gain of 2.0% F1 even though the baseline is already well above human performance. On MRQA, SpanBERT improves between 2.0% (Natural Questions) and 4.6% (TriviaQA) F1 on top of our BERT baseline."]}
{"pkey": "spanbert_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "SpanBERT (base & cased): 12-layer, 768-hidden, 12-heads , 110M parameters.\nSpanBERT (large & cased): 24-layer, 1024-hidden, 16-heads, 340M parameters.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": [", xe) \u2208 Y , where (s, e) indicates its start and end positions, we represent each token xi in the span using the output encodings of the external boundary tokens xs\u22121 and xe+1, as well as the position embedding of the target token pi\u2212s+1:\nyi = f(xs\u22121,xe+1,pi\u2212s+1)\nwhere position embeddings p1,p2, . . . mark relative positions of the masked tokens with respect to the left boundary token xs\u22121. We implement the representation function f(\u00b7) as a 2- layer feed-forward network with GeLU activations (Hendrycks and Gimpel, 2016) and layer normal-\nization (Ba et al., 2016):\nh0 = [xs\u22121;xe+1;pi\u2212s+1]\nh1 = LayerNorm(GeLU(W1h0)) yi = LayerNorm(GeLU(W2h1)) We then use the vector representation yi to predict the token xi and compute the cross-entropy loss exactly like the MLM objective. SpanBERT sums the loss from both the span boundary and the regular masked language model objectives for each token xi in the masked span (xs, ..., xe), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO:\nL(xi) = LMLM(xi) + LSBO(xi) = \u2212 logP (xi | xi)\u2212 logP (xi | yi)\n3.3 Single-Sequence Training. As described in Section 2, BERT\u2019s examples contain two sequences of text (XA, XB), and an objective that trains the model to predict whether they are connected (NSP). We find that this setting is almost always worse than simply using a single sequence without the NSP objective (see Section 5 for further details). We conjecture that single-sequence training is superior to bi-sequence training with NSP because (a) the model benefits from longer full-length contexts, or (b) conditioning on, often unrelated, context from another document adds noise to the masked language model. Therefore, in our approach, we remove both the NSP objective and the two-segment sampling procedure, and simply sample a single contiguous segment of up to n = 512 tokens, rather than two half-segments that sum up to n tokens together.", "UNILM (Dong et al., 2019) uses multiple language modeling objectives \u2013 unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction \u2013 to aid generation tasks like summarization and question generation. XLM (Lample and Conneau, 2019) explores cross-lingual pre-training for multilingual tasks such as translation and crosslingual classification. Kermit (Chan et al., 2019), an insertion based approach, fills in missing tokens (instead of predicting masked ones) during pretraining; they show improvements on machine translation and zero-shot question answering. Concurrent with our work, RoBERTa (Liu et al., 2019b) presents a replication study of BERT pre-training that measures the impact of many key hyperparameters and training data size. Also concurrent, XLNet (Yang et al., 2019) combines an autoregressive loss and the Transformer-XL (Dai et al., 2019) architecture with a more than an eight-fold increase in data to achieve current stateof-the-art results on multiple benchmarks. XLNet also masks spans (of 1-5 tokens) during pretraining, but predicts them autoregressively. Our model focuses on incorporating span-based pretraining, and as a side effect, we present a stronger BERT baseline while controlling for the corpus, architecture, and the number of parameters. Related to our SBO objective, pair2vec (Joshi et al., 2019a) encodes word-pair relations using a negative sampling-based multivariate objective during pre-training. Later, the word-pair representations are injected into the attention-layer of downstream tasks, and thus encode limited down-\nstream context. Unlike pair2vec, our SBO objective yields \u201cpair\u201d (start and end tokens of spans) representations which more fully encode the context during both pre-training and finetuning, and are thus more appropriately viewed as span representations. Stern et al.", "Finally, we observe that single-sequence training works considerably better than bi-sequence training with next sentence prediction (NSP) with BERT\u2019s choice of sequence lengths for a wide variety of tasks . This is surprising because BERT\u2019s ablations showed gains from the NSP objective (Devlin et al., 2019). However, the ablation studies still involved bi-sequence data processing, i.e. the pre-training stage only controlled for the NSP objective while still sampling two half-length sequences. We hypothesize that bisequence training, as it is implemented in BERT (see Section 2), impedes the model from learning longer-range features, and consequently hurts performance on many downstream tasks. 6 Ablation Studies. We compare our random span masking scheme with linguistically-informed masking schemes, and find that masking random spans is a competitive and often better approach. We then study the impact of the span boundary objective (SBO), and contrast it with BERT\u2019s next sentence prediction\n(NSP) objective.10\n6.1 Masking Schemes. Previous work (Sun et al., 2019) has shown improvements in downstream task performance by masking linguistically-informed spans during pre-training for Chinese data. We compare our random span masking scheme with masking of linguistically-informed spans. Specifically, we train the following five baseline models differing only in the way tokens are masked. Subword Tokens We sample random Wordpiece tokens, as in the original BERT. Whole Words We sample random words, and then mask all of the subword tokens in those words. The total number of masked subtokens is around 15%. Named Entities At 50% of the time, we sample from named entities in the text, and sample random whole words for the other 50%. The total\n10To save time and resources, we use the checkpoints at 1.2M steps for all the ablation experiments. number of masked subtokens is 15%."]}
{"pkey": "spanbert_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors deviate from the optimization by running for 2.4M steps and using an epsilon of 1e-8 for AdamW (Kingma and Ba, 2015), which converges to a better set of model parameters. Our implementation uses a batch size of 256 sequences with a maximum of 512 tokens. The paper authors deviate from the optimization by running for 2.4M steps and using an epsilon of 1e-8 for AdamW (Kingma and Ba, 2015), which converges to a better set of model parameters. Our implementation uses a batch size of 256 sequences with a maximum of 512 tokens. Extractive Question Answering For all the question answering tasks, the paper authors use max seq length = 512 and a sliding window of size 128 if the lengths are longer than 512. The paper authors choose learning rates from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5} and batch sizes from {16, 32} and fine-tune four epochs for all the datasets.\nCoreference Resolution The paper authors divide the documents into multiple chunks of lengths up to max seq length and encode each chunk independently. The paper authors choose max seq length from {128, 256, 384, 512}, BERT learning rates from {1e-5,2e-5}, task-specific learning rates from {1e-4,2e-4, 3e-4}, and fine-tune 20 epochs for all the datasets. The paper authors use batch size = 1 (one document) for all the experiments.\nTACRED/GLUE The paper authors use max seq length =128 and choose learning rates from {5e-6, 1e-5,2e-5, 3e-5, 5e-5} and batch sizes from {16, 32} and fine-tuning 10 epochs for all the datasets. The only exception is CoLA, where the paper authors used four epochs (following Devlin et al., 2019), because 10 epochs lead to severe overfitting.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["We choose learning rates from {5e-6, 1e-5, 2e-5, 3e-5, 5e-5} and batch sizes from {16, 32} and fine-tune 4 epochs for all the datasets. Coreference Resolution We divide the documents into multiple chunks of lengths up to max_seq_length and encode each chunk independently. We choose max_seq_length from {128, 256, 384, 512}, BERT learning rates from {1e-5, 2e-5}, task-specific learning rates from {1e-4, 2e-4, 3e-4} and fine-tune 20 epochs for all the datasets. We use batch size = 1 (one document) for all the experiments. TACRED/GLUE We use max_seq_length = 128 and choose learning rates from {5e-6, 1e5, 2e-5, 3e-5, 5e-5} and batch sizes from {16, 32} and fine-tuning 10 epochs for all the datasets. The only exception is CoLA, where we used 4 epochs (following Devlin et al. (2019)), because 10 epochs lead to severe overfitting.", "While recent work Liu et al. (2019a) has applied several taskspecific strategies to increase performance on the individual GLUE tasks, we follow BERT\u2019s singletask setting and only add a linear classifier on top of the [CLS]token for these classification tasks. 4.2 Implementation. We reimplemented BERT\u2019s model and pretraining method in fairseq (Ott et al., 2019). We used the model configuration of BERTlarge as in Devlin et al. (2019) and also pre-trained all our models on the same corpus: BooksCorpus and English Wikipedia using cased Wordpiece tokens. Compared to the original BERT implementation, the main differences in our implementation include: (a) We use different masks at each epoch while BERT samples 10 different masks for each sequence during data processing. (b) We remove all the short-sequence strategies used before (they sampled shorter sequences with a small probability 0.1; they also first pre-trained with smaller se-\n6https://data.quora.com/First-Quora-Dataset-ReleaseQuestion-Pairs\n7Previous work has excluded WNLI on account of construction issues outlined on the GLUE website \u2013 https: //gluebenchmark.com/faq\nquence length of 128 for 90% of the steps). Instead, we always take sequences of up to 512 tokens until it reaches a document boundary. We refer readers to Liu et al. (2019b) for further discussion on these modifications and their effects. As in BERT, the learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. We retain \u03b2 hyperparameters (\u03b21 = 0.9, \u03b22 = 0.999) and a decoupled weight decay (Loshchilov and Hutter, 2019) of 0.1. We also keep a dropout of 0.1 on all layers and attention weights, and a GeLU activation function (Hendrycks and Gimpel, 2016). We deviate from the optimization by running for 2.4M steps and using an epsilon of 1e-8 for AdamW (Kingma and Ba, 2015), which converges to a better set of model parameters.", "(2018) focus on improving language generation speed using a block-wise parallel decoding scheme; they make predictions for multiple time steps in parallel and then back off to the longest prefix validated by a scoring model. Also related are sentence representation methods (Kiros et al., 2015; Logeswaran and Lee, 2018) which focus on predicting surrounding contexts from sentence embeddings. 8 Conclusion. We presented a new method for span-based pretraining which extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Together, our pre-training process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.\nAcknowledgements. We would like to thank Pranav Rajpurkar and Robin Jia for patiently helping us evaluate SpanBERT on SQuAD. We thank the anonymous reviewers, the action editor, and our colleagues at Facebook AI Research and the University of Washington for their insightful feedback that helped improve the paper. A Pre-training Procedure. We describe our pre-training procedure as follows:\n1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre-training:\n(a) Sample a batch of blocks uniformly at random. (b) Mask 15% of word pieces in each block in the batch using the span masking scheme (Section 3.1). (c) For each masked token xi, optimize L(xi) = LMLM(xi) + LSBO(xi) (Section 3.2). B Fine-tuning Hyperparameters. We apply the following fine-tuning hyperparameters to all methods, including the baselines. Extractive Question Answering For all the question answering tasks, we use max_seq_length = 512 and a sliding window of size 128 if the lengths are longer than 512."]}
{"pkey": "spanbert_13", "question": "Describe the computational resources used to train the model.", "answer": "The pre-training was done on 32 Volta V100 GPUs and took 15 days to complete.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": [", xe) \u2208 Y , where (s, e) indicates its start and end positions, we represent each token xi in the span using the output encodings of the external boundary tokens xs\u22121 and xe+1, as well as the position embedding of the target token pi\u2212s+1:\nyi = f(xs\u22121,xe+1,pi\u2212s+1)\nwhere position embeddings p1,p2, . . . mark relative positions of the masked tokens with respect to the left boundary token xs\u22121. We implement the representation function f(\u00b7) as a 2- layer feed-forward network with GeLU activations (Hendrycks and Gimpel, 2016) and layer normal-\nization (Ba et al., 2016):\nh0 = [xs\u22121;xe+1;pi\u2212s+1]\nh1 = LayerNorm(GeLU(W1h0)) yi = LayerNorm(GeLU(W2h1)) We then use the vector representation yi to predict the token xi and compute the cross-entropy loss exactly like the MLM objective. SpanBERT sums the loss from both the span boundary and the regular masked language model objectives for each token xi in the masked span (xs, ..., xe), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO:\nL(xi) = LMLM(xi) + LSBO(xi) = \u2212 logP (xi | xi)\u2212 logP (xi | yi)\n3.3 Single-Sequence Training. As described in Section 2, BERT\u2019s examples contain two sequences of text (XA, XB), and an objective that trains the model to predict whether they are connected (NSP). We find that this setting is almost always worse than simply using a single sequence without the NSP objective (see Section 5 for further details). We conjecture that single-sequence training is superior to bi-sequence training with NSP because (a) the model benefits from longer full-length contexts, or (b) conditioning on, often unrelated, context from another document adds noise to the masked language model. Therefore, in our approach, we remove both the NSP objective and the two-segment sampling procedure, and simply sample a single contiguous segment of up to n = 512 tokens, rather than two half-segments that sum up to n tokens together.", "(2018) focus on improving language generation speed using a block-wise parallel decoding scheme; they make predictions for multiple time steps in parallel and then back off to the longest prefix validated by a scoring model. Also related are sentence representation methods (Kiros et al., 2015; Logeswaran and Lee, 2018) which focus on predicting surrounding contexts from sentence embeddings. 8 Conclusion. We presented a new method for span-based pretraining which extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Together, our pre-training process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.\nAcknowledgements. We would like to thank Pranav Rajpurkar and Robin Jia for patiently helping us evaluate SpanBERT on SQuAD. We thank the anonymous reviewers, the action editor, and our colleagues at Facebook AI Research and the University of Washington for their insightful feedback that helped improve the paper. A Pre-training Procedure. We describe our pre-training procedure as follows:\n1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre-training:\n(a) Sample a batch of blocks uniformly at random. (b) Mask 15% of word pieces in each block in the batch using the span masking scheme (Section 3.1). (c) For each masked token xi, optimize L(xi) = LMLM(xi) + LSBO(xi) (Section 3.2). B Fine-tuning Hyperparameters. We apply the following fine-tuning hyperparameters to all methods, including the baselines. Extractive Question Answering For all the question answering tasks, we use max_seq_length = 512 and a sliding window of size 128 if the lengths are longer than 512.", "ql\u2032[SEP], pass it to the pre-trained transformer encoder, and train two linear classifiers independently on top of it for predicting the answer span boundary (start and end). For the unanswerable questions in SQuAD 2.0, we simply set the answer span to be the special token [CLS]for both training and testing. Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task (Pradhan et al., 2012) for document-level coreference resolution. We use the independent version of the Joshi et al. (2019b) implementation of the higher-order coref-\n4https://github.com/mrqa/MRQA-Shared-Task-2019. MRQA changed the original datasets to unify them into the same format, e.g. all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept.\nerence model (Lee et al., 2018). The document is divided into non-overlapping segments of a predefined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (\u00b7) over possible antecedent spans Y :\nP (y) = es(x,y)\u2211\ny\u2032\u2208Y e s(x,y\u2032) The span pair scoring function s(x, y) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y:\ns(x, y) = sm(x) + sm(y) + sc(x, y)\nsm(x) = FFNNm(gx)\nsc(x, y) = FFNNc(gx,gy, \u03c6(x, y)) Here gx and gy denote the span representations, which are a concatenation of the two transformer output states of the span endpoints and an attention vector computed over the output representations of the token in the span. FFNNm and FFNNc represent two feedforward neural networks with one hidden layer, and \u03c6(x, y) represents the handengineered features (e.g. speaker and genre information). A more detailed description of the model can be found in Joshi et al. (2019b)."]}
{"pkey": "spanbert_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "ERNIE (Sun et al., 2019) shows improvements on Chinese NLP tasks using phrase and named entity masking.\nMASS (Song et al., 2019) focuses on language generation tasks and adopts the encoder-decoder framework to reconstruct a sentence fragment given the remaining part of the sentence.\nA different ERNIE (Zhang et al., 2019) focuses on integrating structured knowledge bases with contextualized representations with an eye on knowledge-driven tasks like entity typing and relation classification.\nUNILM (Dong et al., 2019) uses multiple language modeling objectives\u2014unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction\u2014to aid generation tasks like summarization and question generation.\nXLM (Lample and Conneau, 2019) explores cross-lingual pre-training for multilingual tasks such as translation and cross-lingual classification.\nKermit (Chan et al., 2019), an insertion-based approach, fills in missing tokens (instead of predicting masked ones) during pretraining; they show improvements on machine translation and zero-shot question answering.\nRoBERTa, 8. XLNet, 9. Transformer-XL, method: pair2vec, etc.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "(2018) focus on improving language generation speed using a block-wise parallel decoding scheme; they make predictions for multiple time steps in parallel and then back off to the longest prefix validated by a scoring model. Also related are sentence representation methods (Kiros et al., 2015; Logeswaran and Lee, 2018) which focus on predicting surrounding contexts from sentence embeddings. 8 Conclusion. We presented a new method for span-based pretraining which extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Together, our pre-training process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.\nAcknowledgements. We would like to thank Pranav Rajpurkar and Robin Jia for patiently helping us evaluate SpanBERT on SQuAD. We thank the anonymous reviewers, the action editor, and our colleagues at Facebook AI Research and the University of Washington for their insightful feedback that helped improve the paper. A Pre-training Procedure. We describe our pre-training procedure as follows:\n1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre-training:\n(a) Sample a batch of blocks uniformly at random. (b) Mask 15% of word pieces in each block in the batch using the span masking scheme (Section 3.1). (c) For each masked token xi, optimize L(xi) = LMLM(xi) + LSBO(xi) (Section 3.2). B Fine-tuning Hyperparameters. We apply the following fine-tuning hyperparameters to all methods, including the baselines. Extractive Question Answering For all the question answering tasks, we use max_seq_length = 512 and a sliding window of size 128 if the lengths are longer than 512.", "ql\u2032[SEP], pass it to the pre-trained transformer encoder, and train two linear classifiers independently on top of it for predicting the answer span boundary (start and end). For the unanswerable questions in SQuAD 2.0, we simply set the answer span to be the special token [CLS]for both training and testing. Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task (Pradhan et al., 2012) for document-level coreference resolution. We use the independent version of the Joshi et al. (2019b) implementation of the higher-order coref-\n4https://github.com/mrqa/MRQA-Shared-Task-2019. MRQA changed the original datasets to unify them into the same format, e.g. all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept.\nerence model (Lee et al., 2018). The document is divided into non-overlapping segments of a predefined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (\u00b7) over possible antecedent spans Y :\nP (y) = es(x,y)\u2211\ny\u2032\u2208Y e s(x,y\u2032) The span pair scoring function s(x, y) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y:\ns(x, y) = sm(x) + sm(y) + sc(x, y)\nsm(x) = FFNNm(gx)\nsc(x, y) = FFNNc(gx,gy, \u03c6(x, y)) Here gx and gy denote the span representations, which are a concatenation of the two transformer output states of the span endpoints and an attention vector computed over the output representations of the token in the span. FFNNm and FFNNc represent two feedforward neural networks with one hidden layer, and \u03c6(x, y) represents the handengineered features (e.g. speaker and genre information). A more detailed description of the model can be found in Joshi et al. (2019b)."]}
{"pkey": "spanbert_15", "question": "What is the pretraining objective of the model? ", "answer": "Pre-training Procedure: The paper authors describe our pre-training procedure as follows:\nDivide the corpus into single contiguous blocks of up to 512 tokens.\nAt each step of pre-training:\na. Sample a batch of blocks uniformly at random.\nb. Mask 15% of word pieces in each block in the batch using the span masking scheme (Section 3.1).\nc. For each masked token xi, optimize L(xi) = LMLM(xi) + LSBO(xi) (Section 3.2).", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "In BERT\u2019s implementation, Y accounts for 15% of the tokens in X; of those, 80% are replaced with [MASK], 10% are replaced with a random token (according to the unigram distribution), and 10% are kept unchanged. The task is to predict the original tokens in Y from the modified input. BERT selects each token in Y independently by randomly selecting a subset. In SpanBERT, we define Y by randomly selecting contiguous spans (Section 3.1). Next Sentence Prediction (NSP) The NSP task takes two sequences (XA, XB) as input, and predicts whetherXB is the direct continuation ofXA. This is implemented in BERT by first reading XA from the corpus, and then (1) either reading XB from the point where XA ended, or (2) randomly sampling XB from a different point in the cor-\npus. The two sequences are separated by a special [SEP]token. Additionally, a special [CLS]token is added to XA, XB to form the input, where the target of [CLS]is whetherXB indeed followsXA in the corpus. In summary, BERT optimizes the MLM and the NSP objectives by masking word pieces uniformly at random in data generated by the bisequence sampling procedure. In the next section, we will present our modifications to the data pipeline, masking, and pre-training objectives. 3 Model. We present SpanBERT, a self-supervised pretraining method designed to better represent and predict spans of text. Our approach is inspired by BERT (Devlin et al., 2019), but deviates from its bi-text classification framework in three ways. First, we use a different random process to mask spans of tokens, rather than individual ones. We also introduce a novel auxiliary objective \u2013 the span boundary objective (SBO) \u2013 which tries to predict the entire masked span using only the representations of the tokens at the span\u2019s boundary. Finally, SpanBERT samples a single contiguous segment of text for each training example (instead of two), and thus does not use BERT\u2019s next sentence prediction objective, which we omit. 3.1 Span Masking.", "Specifically, our method reaches 94.6% and 88.7% F1 on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), respectively \u2014 reducing error by as much as 27% compared to our tuned BERT replica. We also observe similar gains on five additional extractive question answering benchmarks (NewsQA, TriviaQA, SearchQA, HotpotQA, and Natural Questions).2\nSpanBERT also arrives at a new state of the art on the challenging CoNLL-2012 (\u201cOntoNotes\u201d) shared task for document-level coreference resolution, where we reach 79.6% F1, exceeding the previous top model by 6.6% absolute. Finally, we demonstrate that SpanBERT also helps on tasks that do not explicitly involve span selection, and show that our approach even improves performance on TACRED (Zhang et al., 2017) and GLUE (Wang et al., 2019). While others show the benefits of adding more data (Yang et al., 2019) and increasing model size (Lample and Conneau, 2019), this work demonstrates the importance of designing good pretraining tasks and objectives, which can also have a remarkable impact. 2 Background: BERT.\nBERT (Devlin et al., 2019) is a self-supervised approach for pre-training a deep transformer encoder (Vaswani et al., 2017), before fine-tuning\n2We use the modified MRQA version of these datasets. See more details in Section 4.1.\nit for a particular downstream task. BERT optimizes two training objectives \u2013 masked language model (MLM) and next sentence prediction (NSP) \u2013 which only require a large collection of unlabeled text. Notation Given a sequence of word or sub-word tokens X = (x1, x2, . . . , xn), BERT trains an encoder that produces a contextualized vector representation for each token: enc(x1, x2, . . . , xn) = x1,x2, . . . ,xn. Masked Language Model (MLM) Also known as a cloze test, MLM is the task of predicting missing tokens in a sequence from their placeholders. Specifically, a subset of tokens Y \u2286 X is sampled and substituted with a different set of tokens."]}
{"pkey": "spanbert_16", "question": "What is the loss function that is used to train the model?", "answer": "The paper authors then use the vector representation yi to predict the token xi and compute the cross-entropy loss exactly like the MLM objective. SpanBERT sums the loss from both the span boundary and the regular masked language model objectives for each token xi in the masked span (xs,...,xe), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO:\nL(xi) = LMLM(xi) + LSBO(xi) = \u2212 log P(xi | xi) \u2212 log P(xi | yi).\nTo implement SpanBERT, the paper authors build on a well-tuned replica of BERT, which itself substantially outperforms the original BERT. While building on our baseline, the paper authors find that pre-training on single segments, instead of two half-length segments with the next sentence prediction (NSP) objective.\nFigure 1: An illustration of SpanBERT training. The span \"an American football game\" is masked. The SBO uses the output representations of the boundary tokens, x4 and x9 (in blue), to predict each token in the masked span. The equation shows the MLM and SBO loss terms for predicting the token \"football\" (in pink), which as marked by the position embedding p3, is the third token from x4.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": [", xe) \u2208 Y , where (s, e) indicates its start and end positions, we represent each token xi in the span using the output encodings of the external boundary tokens xs\u22121 and xe+1, as well as the position embedding of the target token pi\u2212s+1:\nyi = f(xs\u22121,xe+1,pi\u2212s+1)\nwhere position embeddings p1,p2, . . . mark relative positions of the masked tokens with respect to the left boundary token xs\u22121. We implement the representation function f(\u00b7) as a 2- layer feed-forward network with GeLU activations (Hendrycks and Gimpel, 2016) and layer normal-\nization (Ba et al., 2016):\nh0 = [xs\u22121;xe+1;pi\u2212s+1]\nh1 = LayerNorm(GeLU(W1h0)) yi = LayerNorm(GeLU(W2h1)) We then use the vector representation yi to predict the token xi and compute the cross-entropy loss exactly like the MLM objective. SpanBERT sums the loss from both the span boundary and the regular masked language model objectives for each token xi in the masked span (xs, ..., xe), while reusing the input embedding (Press and Wolf, 2017) for the target tokens in both MLM and SBO:\nL(xi) = LMLM(xi) + LSBO(xi) = \u2212 logP (xi | xi)\u2212 logP (xi | yi)\n3.3 Single-Sequence Training. As described in Section 2, BERT\u2019s examples contain two sequences of text (XA, XB), and an objective that trains the model to predict whether they are connected (NSP). We find that this setting is almost always worse than simply using a single sequence without the NSP objective (see Section 5 for further details). We conjecture that single-sequence training is superior to bi-sequence training with NSP because (a) the model benefits from longer full-length contexts, or (b) conditioning on, often unrelated, context from another document adds noise to the masked language model. Therefore, in our approach, we remove both the NSP objective and the two-segment sampling procedure, and simply sample a single contiguous segment of up to n = 512 tokens, rather than two half-segments that sum up to n tokens together.", "Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "Unlike the NSP objective, SBO does not appear to have any adverse effects. 7 Related Work. Pre-trained contextualized word representations that can be trained from unlabeled text (Dai and Le, 2015; Melamud et al., 2016; Peters et al., 2018) have had immense impact on NLP lately, particularly as methods for initializing a large model before fine-tuning it for a specific task (Howard and Ruder, 2018; Radford et al., 2018; Devlin et al., 2019). Beyond differences in model hyperparameters and corpora, these methods mainly differ in their pre-training tasks and loss functions, with a considerable amount of contemporary literature proposing augmentations of BERT\u2019s masked language modeling (MLM) objective. While previous and concurrent work has looked at masking (Sun et al., 2019) or dropping (Song et al., 2019; Chan et al., 2019) multiple words from the input \u2013 particularly as pretraining for language generation tasks \u2013 SpanBERT pretrains span representations (Lee et al., 2016), which are widely used for question answering, coreference resolution and a variety of other tasks. ERNIE\n(Sun et al., 2019) shows improvements on Chinese NLP tasks using phrase and named entity masking. MASS (Song et al., 2019) focuses on language generation tasks, and adopts the encoderdecoder framework to reconstruct a sentence fragment given the remaining part of the sentence. We attempt to more explicitly model spans using the SBO objective, and show that (geometrically distributed) random span masking works as well, and sometimes better than, masking linguisticallycoherent spans. We evaluate on English benchmarks for question answering, relation extraction, and coreference resolution in addition to GLUE. A different ERNIE (Zhang et al., 2019) focuses on integrating structured knowledge bases with contextualized representations with an eye on knowledge-driven tasks like entity typing and relation classification."]}
{"pkey": "spanbert_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "To implement SpanBERT, the paper authors build on a welltuned replica of BERT, which itself substantially outperforms the original BERT. While building on our baseline, the paper authors find that pre-training on single segments, instead of two half-length segments with the next sentence prediction (NSP) objective,Figure 1: An illustration of SpanBERT training. The span an American football game is masked. The SBO uses the output representations of the boundary tokens, x4 and x9 (in blue), to predict each token in the masked span. The equation shows the MLM and SBO loss terms for predicting the token, football (in pink), which as marked by the position embedding p3, is the third token from x4.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["UNILM (Dong et al., 2019) uses multiple language modeling objectives \u2013 unidirectional (both left-to-right and right-to-left), bidirectional, and sequence-to-sequence prediction \u2013 to aid generation tasks like summarization and question generation. XLM (Lample and Conneau, 2019) explores cross-lingual pre-training for multilingual tasks such as translation and crosslingual classification. Kermit (Chan et al., 2019), an insertion based approach, fills in missing tokens (instead of predicting masked ones) during pretraining; they show improvements on machine translation and zero-shot question answering. Concurrent with our work, RoBERTa (Liu et al., 2019b) presents a replication study of BERT pre-training that measures the impact of many key hyperparameters and training data size. Also concurrent, XLNet (Yang et al., 2019) combines an autoregressive loss and the Transformer-XL (Dai et al., 2019) architecture with a more than an eight-fold increase in data to achieve current stateof-the-art results on multiple benchmarks. XLNet also masks spans (of 1-5 tokens) during pretraining, but predicts them autoregressively. Our model focuses on incorporating span-based pretraining, and as a side effect, we present a stronger BERT baseline while controlling for the corpus, architecture, and the number of parameters. Related to our SBO objective, pair2vec (Joshi et al., 2019a) encodes word-pair relations using a negative sampling-based multivariate objective during pre-training. Later, the word-pair representations are injected into the attention-layer of downstream tasks, and thus encode limited down-\nstream context. Unlike pair2vec, our SBO objective yields \u201cpair\u201d (start and end tokens of spans) representations which more fully encode the context during both pre-training and finetuning, and are thus more appropriately viewed as span representations. Stern et al.", "In summary, SpanBERT pre-trains span representations by: (1) masking spans of full words using a geometric distribution based masking scheme (Section 3.1), (2) optimizing an auxiliary span-boundary objective (Section 3.2) in addition to MLM using a single-sequence data pipeline (Section 3.3). A procedural description can be found in Appendix A.\n4 Experimental Setup.\n4.1 Tasks. We evaluate on a comprehensive suite of tasks, including seven question answering tasks, corefer-\nence resolution, nine tasks in the GLUE benchmark (Wang et al., 2019), and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training. Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4: NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good testbed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1, p2, . . . , pl) and question Q = (q1, q2, . . . , ql\u2032) into a single sequence X = [CLS]p1p2 . . . pl[SEP]q1q2 . . .", "Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT."]}
{"pkey": "spanbert_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "Experimental Setup: Extractive Question Answering,Coreference Resolution, Relation Extraction, The paper authors evaluate on a comprehensive suite of tasks, including seven question answering tasks, coref_x0002_erence resolution, nine tasks in the GLUE benchmark (Wang et al., 2019), and relation extraction. The paper authors expect that the span selection tasks, question answering and coreference resolution, will partic_x0002_ularly benefit from our span-based pre-training.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["In summary, SpanBERT pre-trains span representations by: (1) masking spans of full words using a geometric distribution based masking scheme (Section 3.1), (2) optimizing an auxiliary span-boundary objective (Section 3.2) in addition to MLM using a single-sequence data pipeline (Section 3.3). A procedural description can be found in Appendix A.\n4 Experimental Setup.\n4.1 Tasks. We evaluate on a comprehensive suite of tasks, including seven question answering tasks, corefer-\nence resolution, nine tasks in the GLUE benchmark (Wang et al., 2019), and relation extraction. We expect that the span selection tasks, question answering and coreference resolution, will particularly benefit from our span-based pre-training. Extractive Question Answering Given a short passage of text and a question as input, the task of extractive question answering is to select a contiguous span of text in the passage as the answer. We first evaluate on SQuAD 1.1 and 2.0 (Rajpurkar et al., 2016, 2018), which have served as major question answering benchmarks, particularly for pre-trained models (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019). We also evaluate on five more datasets from the MRQA shared task (Fisch et al., 2019)4: NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017), HotpotQA (Yang et al., 2018) and Natural Questions (Kwiatkowski et al., 2019). Because the MRQA shared task does not have a public test set, we split the development set in half to make new development and test sets. The datasets vary in both domain and collection methodology, making this collection a good testbed for evaluating whether our pre-trained models can generalize well across different data distributions. Following BERT (Devlin et al., 2019), we use the same QA model architecture for all the datasets. We first convert the passage P = (p1, p2, . . . , pl) and question Q = (q1, q2, . . . , ql\u2032) into a single sequence X = [CLS]p1p2 . . . pl[SEP]q1q2 . . .", "Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "ql\u2032[SEP], pass it to the pre-trained transformer encoder, and train two linear classifiers independently on top of it for predicting the answer span boundary (start and end). For the unanswerable questions in SQuAD 2.0, we simply set the answer span to be the special token [CLS]for both training and testing. Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task (Pradhan et al., 2012) for document-level coreference resolution. We use the independent version of the Joshi et al. (2019b) implementation of the higher-order coref-\n4https://github.com/mrqa/MRQA-Shared-Task-2019. MRQA changed the original datasets to unify them into the same format, e.g. all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept.\nerence model (Lee et al., 2018). The document is divided into non-overlapping segments of a predefined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (\u00b7) over possible antecedent spans Y :\nP (y) = es(x,y)\u2211\ny\u2032\u2208Y e s(x,y\u2032) The span pair scoring function s(x, y) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y:\ns(x, y) = sm(x) + sm(y) + sc(x, y)\nsm(x) = FFNNm(gx)\nsc(x, y) = FFNNc(gx,gy, \u03c6(x, y)) Here gx and gy denote the span representations, which are a concatenation of the two transformer output states of the span endpoints and an attention vector computed over the output representations of the token in the span. FFNNm and FFNNc represent two feedforward neural networks with one hidden layer, and \u03c6(x, y) represents the handengineered features (e.g. speaker and genre information). A more detailed description of the model can be found in Joshi et al. (2019b)."]}
{"pkey": "spanbert_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "Ablation Studies: The paper authors compare our random span masking scheme with linguistically-informed masking schemes, and find that masking random spans is a competitive and often better approach. The paper authors then study the impact of the SBO, and contrast it with BERT\u2019s NSP objective.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["Finally, we observe that single-sequence training works considerably better than bi-sequence training with next sentence prediction (NSP) with BERT\u2019s choice of sequence lengths for a wide variety of tasks . This is surprising because BERT\u2019s ablations showed gains from the NSP objective (Devlin et al., 2019). However, the ablation studies still involved bi-sequence data processing, i.e. the pre-training stage only controlled for the NSP objective while still sampling two half-length sequences. We hypothesize that bisequence training, as it is implemented in BERT (see Section 2), impedes the model from learning longer-range features, and consequently hurts performance on many downstream tasks. 6 Ablation Studies. We compare our random span masking scheme with linguistically-informed masking schemes, and find that masking random spans is a competitive and often better approach. We then study the impact of the span boundary objective (SBO), and contrast it with BERT\u2019s next sentence prediction\n(NSP) objective.10\n6.1 Masking Schemes. Previous work (Sun et al., 2019) has shown improvements in downstream task performance by masking linguistically-informed spans during pre-training for Chinese data. We compare our random span masking scheme with masking of linguistically-informed spans. Specifically, we train the following five baseline models differing only in the way tokens are masked. Subword Tokens We sample random Wordpiece tokens, as in the original BERT. Whole Words We sample random words, and then mask all of the subword tokens in those words. The total number of masked subtokens is around 15%. Named Entities At 50% of the time, we sample from named entities in the text, and sample random whole words for the other 50%. The total\n10To save time and resources, we use the checkpoints at 1.2M steps for all the ablation experiments. number of masked subtokens is 15%.", "Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "(2018) focus on improving language generation speed using a block-wise parallel decoding scheme; they make predictions for multiple time steps in parallel and then back off to the longest prefix validated by a scoring model. Also related are sentence representation methods (Kiros et al., 2015; Logeswaran and Lee, 2018) which focus on predicting surrounding contexts from sentence embeddings. 8 Conclusion. We presented a new method for span-based pretraining which extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Together, our pre-training process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.\nAcknowledgements. We would like to thank Pranav Rajpurkar and Robin Jia for patiently helping us evaluate SpanBERT on SQuAD. We thank the anonymous reviewers, the action editor, and our colleagues at Facebook AI Research and the University of Washington for their insightful feedback that helped improve the paper. A Pre-training Procedure. We describe our pre-training procedure as follows:\n1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre-training:\n(a) Sample a batch of blocks uniformly at random. (b) Mask 15% of word pieces in each block in the batch using the span masking scheme (Section 3.1). (c) For each masked token xi, optimize L(xi) = LMLM(xi) + LSBO(xi) (Section 3.2). B Fine-tuning Hyperparameters. We apply the following fine-tuning hyperparameters to all methods, including the baselines. Extractive Question Answering For all the question answering tasks, we use max_seq_length = 512 and a sliding window of size 128 if the lengths are longer than 512."]}
{"pkey": "spanbert_20", "question": "List the future work mentioned in the paper.", "answer": "Together, our pretraining process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.", "title": "SpanBERT: Improving Pre-training by Representing and Predicting Spans", "context": ["ql\u2032[SEP], pass it to the pre-trained transformer encoder, and train two linear classifiers independently on top of it for predicting the answer span boundary (start and end). For the unanswerable questions in SQuAD 2.0, we simply set the answer span to be the special token [CLS]for both training and testing. Coreference Resolution Coreference resolution is the task of clustering mentions in text which refer to the same real-world entities. We evaluate on the CoNLL-2012 shared task (Pradhan et al., 2012) for document-level coreference resolution. We use the independent version of the Joshi et al. (2019b) implementation of the higher-order coref-\n4https://github.com/mrqa/MRQA-Shared-Task-2019. MRQA changed the original datasets to unify them into the same format, e.g. all the contexts are truncated to a maximum of 800 tokens and only answerable questions are kept.\nerence model (Lee et al., 2018). The document is divided into non-overlapping segments of a predefined length.5 Each segment is encoded independently by the pre-trained transformer encoder, which replaces the original LSTM-based encoder. For each mention span x, the model learns a distribution P (\u00b7) over possible antecedent spans Y :\nP (y) = es(x,y)\u2211\ny\u2032\u2208Y e s(x,y\u2032) The span pair scoring function s(x, y) is a feedforward neural network over fixed-length span representations and hand-engineered features over x and y:\ns(x, y) = sm(x) + sm(y) + sc(x, y)\nsm(x) = FFNNm(gx)\nsc(x, y) = FFNNc(gx,gy, \u03c6(x, y)) Here gx and gy denote the span representations, which are a concatenation of the two transformer output states of the span endpoints and an attention vector computed over the output representations of the token in the span. FFNNm and FFNNc represent two feedforward neural networks with one hidden layer, and \u03c6(x, y) represents the handengineered features (e.g. speaker and genre information). A more detailed description of the model can be found in Joshi et al. (2019b).", "Pre-training methods like BERT (Devlin et al., 2019) have shown strong performance gains using self-supervised training that masks individual words or subword units. However, many NLP tasks involve reasoning about relationships between two or more spans of text. For example, in extractive question answering (Rajpurkar et al., 2016), determining that the \u201cDenver Broncos\u201d is a type of \u201cNFL team\u201d is critical for answering the question \u201cWhich NFL team won Super Bowl\n\u2217Equal contribution. 1Our code and pre-trained models are available at\nhttps://github.com/facebookresearch/ SpanBERT. 50?\u201d Such spans provide a more challenging target for self supervision tasks, for example predicting \u201cDenver Broncos\u201d is much harder than predicting only \u201cDenver\u201d when you know the next word is \u201cBroncos\u201d. In this paper, we introduce a span-level pretraining approach that consistently outperforms BERT, with the largest gains on span selection tasks such as question answering and coreference resolution. We present SpanBERT, a pre-training method that is designed to better represent and predict spans of text. Our method differs from BERT in both the masking scheme and the training objectives. First, we mask random contiguous spans, rather than random individual tokens. Second, we introduce a novel span-boundary objective (SBO) so the model learns to predict the entire masked span from the observed tokens at its boundary. Span-based masking forces the model to predict entire spans solely using the context in which they appear. Furthermore, the span-boundary objective encourages the model to store this span-level information at the boundary tokens, which can be easily accessed during the fine-tuning stage. Figure 1 illustrates our approach. To implement SpanBERT, we build on a welltuned replica of BERT, which itself substantially outperforms the original BERT.", "(2018) focus on improving language generation speed using a block-wise parallel decoding scheme; they make predictions for multiple time steps in parallel and then back off to the longest prefix validated by a scoring model. Also related are sentence representation methods (Kiros et al., 2015; Logeswaran and Lee, 2018) which focus on predicting surrounding contexts from sentence embeddings. 8 Conclusion. We presented a new method for span-based pretraining which extends BERT by (1) masking contiguous random spans, rather than random tokens, and (2) training the span boundary representations to predict the entire content of the masked span, without relying on the individual token representations within it. Together, our pre-training process yields models that outperform all BERT baselines on a variety of tasks, and reach substantially better performance on span selection tasks in particular.\nAcknowledgements. We would like to thank Pranav Rajpurkar and Robin Jia for patiently helping us evaluate SpanBERT on SQuAD. We thank the anonymous reviewers, the action editor, and our colleagues at Facebook AI Research and the University of Washington for their insightful feedback that helped improve the paper. A Pre-training Procedure. We describe our pre-training procedure as follows:\n1. Divide the corpus into single contiguous blocks of up to 512 tokens. 2. At each step of pre-training:\n(a) Sample a batch of blocks uniformly at random. (b) Mask 15% of word pieces in each block in the batch using the span masking scheme (Section 3.1). (c) For each masked token xi, optimize L(xi) = LMLM(xi) + LSBO(xi) (Section 3.2). B Fine-tuning Hyperparameters. We apply the following fine-tuning hyperparameters to all methods, including the baselines. Extractive Question Answering For all the question answering tasks, we use max_seq_length = 512 and a sliding window of size 128 if the lengths are longer than 512."]}
{"pkey": "xlnet_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Authors proposed XLNet, a generalized autoregressive pretraining method that\n (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and \n(2) overcomes the limitations of BERT thanks to its autoregressive formulation.\nXLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["z such that only tokens the occur before zt in the permutation can be attended; i.e., positions zi with i < t. Moreover, comparing Figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention. 4The problem of language modeling is essentially density estimation for text data. 5https://openreview.net/forum?id=HJePno0cYm\nJoint View of the Content Stream (Factorization order: 3 \u00e0 2 \u00e0 4 \u00e0 1)\nww w wmem(+) x#x% x' x(\ng# (%)g% (%) g' (%) g( (%)mem(%) h# (%)h% (%) h' (%) h( (%) g# (')g% (') g' (') g( (') h# (')h% (') h' (') h( (')", "Since the same model parameter \u03b8 is shared across all factorization orders during training, in expectation, xt has seen every possible element xi 6= xt in the sequence, hence being able to capture the bidirectional context. Moreover, as this objective fits into the AR framework, it naturally avoids the independence assumption and the pretrain-finetune discrepancy discussed in Section 2.1.\nRemark on Permutation The proposed objective only permutes the factorization order, not the sequence order. In other words, we keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in Transformers to achieve permutation of the factorization order. Note that this choice is necessary, since the model will only encounter text sequences with the natural order during finetuning. To provide an overall picture, we show an example of predicting the token x3 given the same input sequence x but under different factorization orders in the Appendix A.7 with Figure 4.\n2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations. While the permutation language modeling objective has desired properties, naive implementation with standard Transformer parameterization may not work. To see the problem, assume we parameterize the next-token distribution p\u03b8(Xzt | xz<t) using the standard Softmax formulation, i.e., p\u03b8(Xzt = x | xz<t) = exp(e(x)>h\u03b8(xz<t )) \u2211 x\u2032 exp(e(x\u2032)>h\u03b8(xz<t ))\n, where h\u03b8(xz<t) denotes the hidden representation of xz<t produced by the shared Transformer network after proper masking. Now notice that the representation h\u03b8(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the same distribution is predicted regardless of the target position, which is not able to learn useful representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to re-parameterize the next-token distribution to be target position aware:", "In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets. 3.3 Comparison with RoBERTa: Scaling Up. RACE Accuracy Middle High Model NDCG@20 ERR@20. After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1. The results are presented in Tables 2 (reading comprehension & document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and finetuning are in Appendix A.4. \u2022 For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet. \u2022 For classification tasks that already have abundant supervised examples such as MNLI (>390K), Yelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains. 3.4 Ablation Study. We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:\n\u2022"]}
{"pkey": "xlnet_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "1. AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information. This results in a gap between AR language modeling and effective pretraining.\n2. The artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1. 1 Introduction. Unsupervised representation learning has been highly successful in the domain of natural language processing [7, 22, 27, 28, 10]. Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives. AR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model [7, 27, 28]. Specifically, given a text sequence x = (x1, \u00b7 \u00b7 \u00b7 , xT ), AR language modeling factorizes the likelihood into a forward product p(x) = \u220fT t=1 p(xt | x<t) or a backward\none p(x) = \u220f1 t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each conditional distribution. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information. This results in a gap between AR language modeling and effective pretraining. In comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT [10], which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version.", "Since density estimation is not part of the objective, BERT is allowed to utilize \u2217Equal contribution. Order determined by swapping the one in [9]. 1Pretrained models and code are available at https://github.com/zihangdai/xlnet\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n90 6.\n08 23\n7v 2\n[ cs\n.C L\n] 2\nJ an\n2 02\nbidirectional contexts for reconstruction. As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language [9]. Faced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations. \u2022 Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context. \u2022 Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to.", "In other words, the XLNet objective contains more effective training signals, which empirically leads to better performance in Section 3.\nA.5.2 Comparison with Language Modeling. Borrowing examples and notations from Section A.5.1, a standard AR language model like GPT [28] is only able to cover the dependency (x = York,U = {New}) but not (x = New,U = {York}). XLNet, on the other hand, is able to cover both in expectation over all factorization orders. Such a limitation of AR language modeling can be critical in real-world applications. For example, consider a span extraction question answering task with the context \u201cThom Yorke is the singer of Radiohead\u201d and the question \u201cWho is the singer of Radiohead\u201d. The representations of \u201cThom Yorke\u201d are not dependent on \u201cRadiohead\u201d with AR language modeling and thus they will not be chosen as the answer by the standard approach that employs softmax over all token representations. More formally, consider a context-target pair (x,U): \u2022 If U 6\u2286 T<x, where T<x denotes the tokens prior to x in the original sequence, AR language\nmodeling is not able to cover the dependency. \u2022 In comparison, XLNet is able to cover all dependencies in expectation. Approaches like ELMo [27] concatenate forward and backward language models in a shallow manner, which is not sufficient for modeling deep interactions between the two directions. A.5.3 Bridging the Gap Between Language Modeling and Pretraining. With a deep root in density estimation4 [4, 32, 24], language modeling has been a rapidly-developing research area [9, 1, 3]. However, there has been a gap between language modeling and pretraining due to the lack of the capability of bidirectional context modeling, as analyzed in Section A.5.2. It has even been challenged by some machine learning practitioners whether language modeling is a meaningful pursuit if it does not directly improve downstream tasks 5. XLNet generalizes language modeling and bridges such a gap."]}
{"pkey": "xlnet_3", "question": "What are the main contributions of the paper?", "answer": "1. Instead of using a fixed forward or backward factorization order as in conventional AR mod_x0002_els, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.\n2. The paper authors integrate Transformer-XL into XLNet to demonstrate the usefulness of the latest language modeling progress.\n3. Approaches like ELMo [27] concatenate forward and backward language models in a shallow manner, which is not sufficient for modeling deep interactions between the two directions.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["z such that only tokens the occur before zt in the permutation can be attended; i.e., positions zi with i < t. Moreover, comparing Figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention. 4The problem of language modeling is essentially density estimation for text data. 5https://openreview.net/forum?id=HJePno0cYm\nJoint View of the Content Stream (Factorization order: 3 \u00e0 2 \u00e0 4 \u00e0 1)\nww w wmem(+) x#x% x' x(\ng# (%)g% (%) g' (%) g( (%)mem(%) h# (%)h% (%) h' (%) h( (%) g# (')g% (') g' (') g( (') h# (')h% (') h' (') h( (')", "As a result, it further \u201cjustifies\u201d language modeling research. Moreover, it becomes possible to leverage the rapid progress of language modeling research for pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness of the latest language modeling progress. A.6 Qualitative Analysis of Attention Patterns. We compare the attention pattern of BERT and XLNet without finetuning. Firstly, we found 4 typical patterns shared by both, as shown in Fig. 2.\nMore interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The self-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather global information; (b) The relative-stride pattern attends to positions every a few stride apart relative to the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the relative right half. Note that all these three unique patterns involve the relative positions rather than absolute ones, and hence are likely enabled by the \u201crelative attention\u201d mechanism in XLNet. We conjecture these unique patterns contribute to the performance advantage of XLNet. On the other hand, the proposed permutation LM objective mostly contributes to a better data efficiency, whose effects may not be obvious from qualitative visualization. A.7 Visualizing Memory and Permutation In this section, we provide a detailed visualization of the proposed permutation language modeling objective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use attention masks to permute the factorization order, and the difference of the two attention streams. As shown in Figure 5 and 6, given the current position zt, the attention mask is decided by the permutation (or factorization order)", "In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets. 3.3 Comparison with RoBERTa: Scaling Up. RACE Accuracy Middle High Model NDCG@20 ERR@20. After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1. The results are presented in Tables 2 (reading comprehension & document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and finetuning are in Appendix A.4. \u2022 For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet. \u2022 For classification tasks that already have abundant supervised examples such as MNLI (>390K), Yelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains. 3.4 Ablation Study. We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:\n\u2022"]}
{"pkey": "xlnet_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "1. XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.\n2. XLNet consistently outperforms BERT on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1. 1 Introduction. Unsupervised representation learning has been highly successful in the domain of natural language processing [7, 22, 27, 28, 10]. Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives. AR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model [7, 27, 28]. Specifically, given a text sequence x = (x1, \u00b7 \u00b7 \u00b7 , xT ), AR language modeling factorizes the likelihood into a forward product p(x) = \u220fT t=1 p(xt | x<t) or a backward\none p(x) = \u220f1 t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each conditional distribution. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information. This results in a gap between AR language modeling and effective pretraining. In comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT [10], which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version.", "During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context. Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where \u201cSEP\u201d and \u201cCLS\u201d are two special symbols and \u201cA\u201d and \u201cB\u201d are the two segments. Although\nwe follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4). Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s\u2212, where s+ and s\u2212 are learnable model parameters for each attention head. In other words, we only consider whether the two positions are within the same segment, as opposed to considering which specific segments they are from. This is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. When i attends to j, the segment encoding sij is used to compute an attention weight aij = (qi + b)\n>sij , where qi is the query vector as in a standard attention operation and b is a learnable head-specific bias vector. Finally, the value aij is added to the normal attention weight. There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings. 2.6 Discussion. Comparing Eq.", "z such that only tokens the occur before zt in the permutation can be attended; i.e., positions zi with i < t. Moreover, comparing Figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention. 4The problem of language modeling is essentially density estimation for text data. 5https://openreview.net/forum?id=HJePno0cYm\nJoint View of the Content Stream (Factorization order: 3 \u00e0 2 \u00e0 4 \u00e0 1)\nww w wmem(+) x#x% x' x(\ng# (%)g% (%) g' (%) g( (%)mem(%) h# (%)h% (%) h' (%) h( (%) g# (')g% (') g' (') g( (') h# (')h% (') h' (') h( (')"]}
{"pkey": "xlnet_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "Empirically, under comparable experiment setting, XLNet consistently outperforms BERT on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task.\nFollowing BERT, they used the BooksCorpus and English Wikipedia as part of their pretraining data, which have 13GB of plain text combined. In addition, they include Giga5 (16GB text), ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. They use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 110GB of text respectively. After tokenization with SentencePiece, they obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, totaling 32.89B subword pieces.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16]. A.3.5 ClueWeb09-B Dataset. Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents. A.4 Hyperparameters. A.4.1 Pretraining Hyperparameters. The hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2 Hyperparameters for Finetuning. The hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise decay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.", ", T : ht = e(xt) and gt = w Cached layer-m content represetation (memory) from previous segment: h\u0303(m) For the Transformer-XL layer m = 1, \u00b7 \u00b7 \u00b7 ,M , attention with relative positional encoding and position-wise feed-forward are consecutively employed to update the represetntations:\n\u2200t = 1, . . . , T : h\u0302(m)zt = LayerNorm ( h(m\u22121)zt + RelAttn ( h(m\u22121)zt , [ h\u0303(m\u22121),h(m\u22121)z\u2264t ])) h(m)zt = LayerNorm ( h\u0302(m)zt + PosFF ( h\u0302(m)zt\n)) g\u0302(m)zt = LayerNorm ( g(m\u22121)zt + RelAttn ( g(m\u22121)zt , [ h\u0303(m\u22121),h(m\u22121)z<t\n])) g(m)zt = LayerNorm ( g\u0302(m)zt + PosFF ( g\u0302(m)zt\n)) Target-aware prediction distribution:\np\u03b8(Xzt = x | xz<t) = exp\n( e(x)>g (M) zt ) \u2211 x\u2032 exp ( e(x\u2032)>g (M) zt\n) ,\nA.3 Datasets. A.3.1 RACE Dataset. The RACE dataset [18] contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts. This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions. Moreover, the average length of the passages in RACE are longer than 300, which is significantly longer than other popular reading comprehension datasets such as SQuAD [29]. As a result, this dataset serves as a challenging benchmark for long text understanding. We use a sequence length of 512 during finetuning. A.3.2 SQuAD. SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains questions that always have a corresponding answer in the given passages, while SQuAD2.0 [29] introduces unanswerable questions. To finetune an XLNet on SQuAD2.0, we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering [10]. A.3.3 Text classification Datasets. Following previous work on text classification [39, 23], we evaluate XLNet on the following benchmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. A.3.4 GLUE Dataset.", "After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet."]}
{"pkey": "xlnet_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not Specified in the paper. But in their github repo this has been mentioned by the authors.\nFuture Release Plan\nThe paper authors also plan to continuously release more pretrained models under different settings, including:\n1. A pretrained model that is finetuned on Wikipedia. This can be used for tasks with Wikipedia text such as SQuAD and HotpotQA.\n2. Pretrained models with other hyperparameter configurations, targeting specific downstream tasks.\n3. Pretrained models that benefit from new techniques.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context. Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where \u201cSEP\u201d and \u201cCLS\u201d are two special symbols and \u201cA\u201d and \u201cB\u201d are the two segments. Although\nwe follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4). Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s\u2212, where s+ and s\u2212 are learnable model parameters for each attention head. In other words, we only consider whether the two positions are within the same segment, as opposed to considering which specific segments they are from. This is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. When i attends to j, the segment encoding sij is used to compute an attention weight aij = (qi + b)\n>sij , where qi is the query vector as in a standard attention operation and b is a learnable head-specific bias vector. Finally, the value aij is added to the normal attention weight. There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings. 2.6 Discussion. Comparing Eq.", "Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining. \u2022 Inspired by the latest advancements in AR language modeling , XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence. \u2022 Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity. Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Related Work The idea of permutation-based AR modeling has been explored in [32, 12], but there are several key differences. Firstly, previous models aim to improve density estimation by baking an \u201corderless\u201d inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that \u201corderless\u201d does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution.", "z such that only tokens the occur before zt in the permutation can be attended; i.e., positions zi with i < t. Moreover, comparing Figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention. 4The problem of language modeling is essentially density estimation for text data. 5https://openreview.net/forum?id=HJePno0cYm\nJoint View of the Content Stream (Factorization order: 3 \u00e0 2 \u00e0 4 \u00e0 1)\nww w wmem(+) x#x% x' x(\ng# (%)g% (%) g' (%) g( (%)mem(%) h# (%)h% (%) h' (%) h( (%) g# (')g% (') g' (') g( (') h# (')h% (') h' (') h( (')"]}
{"pkey": "xlnet_7", "question": "List the limitations of the model discussed in the paper.", "answer": "1. After the initial publication of their manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, they exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment is based on full data and reuses the hyper-parameters of RoBERTa.\n2. , if they remove the memory caching mechanism, the performance clearly drops, especially for RACE which involves the longest context among the 4 tasks.\n3.  In addition, Table 6 show that both span-based prediction and the bidirectional input pipeline play important roles in XLNet. Finally, they unexpectedly find the the next-sentence prediction objective proposed in the original BERT does not necessarily lead to an improvement in their setting. Hence, they exclude the next-sentence prediction objective from XLNet. \n4. They mentioned in their github repo about the high computational requirement to reproduce the same model. Github link - https://github.com/zihangdai/xlnet", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["XLNet: Generalized Autoregressive Pretraining for Language Understanding. With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1. XLNet: Generalized Autoregressive Pretraining for Language Understanding. With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.", "The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT. \u2022 The importance of using Transformer-XL as the backbone neural architecture. \u2022 The necessity of some implementation details including span-based prediction, the bidirectional\ninput pipeline, and next-sentence prediction. With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implementation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidirectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs. Examining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly contribute the superior performance of XLNet over BERT. Moreover, if we remove the memory caching mechanism (row 5), the performance clearly drops, especially for RACE which involves the longest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly find the the next-sentence prediction objective proposed in the original BERT does not necessarily lead to an improvement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet. Finally, we also perform a qualitative study of the attention patterns, which is included in Appendix A.6 due to page limit. 4 Conclusions. XLNet is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR and AE methods.", "Since density estimation is not part of the objective, BERT is allowed to utilize \u2217Equal contribution. Order determined by swapping the one in [9]. 1Pretrained models and code are available at https://github.com/zihangdai/xlnet\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n90 6.\n08 23\n7v 2\n[ cs\n.C L\n] 2\nJ an\n2 02\nbidirectional contexts for reconstruction. As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language [9]. Faced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations. \u2022 Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context. \u2022 Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to."]}
{"pkey": "xlnet_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "1. They used the BooksCorpus and English Wikipedia as part of their pretraining data, which have 13GB plain text combined. In addition, they include Giga5 (16GB text), ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. They use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], they obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total.\n2. RACE dataset: The RACE dataset [18] contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts. This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions.\n3. SQuAD Dataset: SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains questions that always have a corresponding answer in the given passages, while SQuAD2.0 [29] introduces unanswerable questions.\n4. GLUE Dataset: The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners submitted their predictions on the evaluation server to obtain test set results. In the multi-task setting, they jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and finetune the network on the other datasets.\n5. ClueWeb09-B Dataset: ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet.", "In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets. 3.3 Comparison with RoBERTa: Scaling Up. RACE Accuracy Middle High Model NDCG@20 ERR@20. After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1. The results are presented in Tables 2 (reading comprehension & document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and finetuning are in Appendix A.4. \u2022 For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet. \u2022 For classification tasks that already have abundant supervised examples such as MNLI (>390K), Yelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains. 3.4 Ablation Study. We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:\n\u2022", "The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16]. A.3.5 ClueWeb09-B Dataset. Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents. A.4 Hyperparameters. A.4.1 Pretraining Hyperparameters. The hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2 Hyperparameters for Finetuning. The hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise decay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner."]}
{"pkey": "xlnet_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "They used the BooksCorpus and English Wikipedia as part of their pretraining data, which have 13GB plain text combined. In addition, they include Giga5 (16GB text), ClueWeb 2012-B (extended from [5]), and Common Crawl [6] for pretraining. They use heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], they obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet.", "In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets. 3.3 Comparison with RoBERTa: Scaling Up. RACE Accuracy Middle High Model NDCG@20 ERR@20. After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1. The results are presented in Tables 2 (reading comprehension & document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and finetuning are in Appendix A.4. \u2022 For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet. \u2022 For classification tasks that already have abundant supervised examples such as MNLI (>390K), Yelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains. 3.4 Ablation Study. We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:\n\u2022", "Since density estimation is not part of the objective, BERT is allowed to utilize \u2217Equal contribution. Order determined by swapping the one in [9]. 1Pretrained models and code are available at https://github.com/zihangdai/xlnet\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n90 6.\n08 23\n7v 2\n[ cs\n.C L\n] 2\nJ an\n2 02\nbidirectional contexts for reconstruction. As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language [9]. Faced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations. \u2022 Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context. \u2022 Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to."]}
{"pkey": "xlnet_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "1. They used heuristics to aggressively filter out short or low-quality articles for ClueWeb 2012-B and Common Crawl, which results in 19GB and 110GB text respectively. After tokenization with SentencePiece [17], they obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total.\n2. Their largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, they always use a full sequence length of 512.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16]. A.3.5 ClueWeb09-B Dataset. Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents. A.4 Hyperparameters. A.4.1 Pretraining Hyperparameters. The hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2 Hyperparameters for Finetuning. The hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise decay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.", "After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet.", ", T : ht = e(xt) and gt = w Cached layer-m content represetation (memory) from previous segment: h\u0303(m) For the Transformer-XL layer m = 1, \u00b7 \u00b7 \u00b7 ,M , attention with relative positional encoding and position-wise feed-forward are consecutively employed to update the represetntations:\n\u2200t = 1, . . . , T : h\u0302(m)zt = LayerNorm ( h(m\u22121)zt + RelAttn ( h(m\u22121)zt , [ h\u0303(m\u22121),h(m\u22121)z\u2264t ])) h(m)zt = LayerNorm ( h\u0302(m)zt + PosFF ( h\u0302(m)zt\n)) g\u0302(m)zt = LayerNorm ( g(m\u22121)zt + RelAttn ( g(m\u22121)zt , [ h\u0303(m\u22121),h(m\u22121)z<t\n])) g(m)zt = LayerNorm ( g\u0302(m)zt + PosFF ( g\u0302(m)zt\n)) Target-aware prediction distribution:\np\u03b8(Xzt = x | xz<t) = exp\n( e(x)>g (M) zt ) \u2211 x\u2032 exp ( e(x\u2032)>g (M) zt\n) ,\nA.3 Datasets. A.3.1 RACE Dataset. The RACE dataset [18] contains near 100K questions taken from the English exams for middle and high school Chinese students in the age range between 12 to 18, with the answers generated by human experts. This is one of the most difficult reading comprehension datasets that involve challenging reasoning questions. Moreover, the average length of the passages in RACE are longer than 300, which is significantly longer than other popular reading comprehension datasets such as SQuAD [29]. As a result, this dataset serves as a challenging benchmark for long text understanding. We use a sequence length of 512 during finetuning. A.3.2 SQuAD. SQuAD is a large-scale reading comprehension dataset with two tasks. SQuAD1.1 [30] contains questions that always have a corresponding answer in the given passages, while SQuAD2.0 [29] introduces unanswerable questions. To finetune an XLNet on SQuAD2.0, we jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering [10]. A.3.3 Text classification Datasets. Following previous work on text classification [39, 23], we evaluate XLNet on the following benchmarks: IMDB, Yelp-2, Yelp-5, DBpedia, AG, Amazon-2, and Amazon-5. A.3.4 GLUE Dataset."]}
{"pkey": "xlnet_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "1. XLNet results are obtained with a 24-layer architecture with similar model sizes (aka BERT-Large).\n2. The proposed objective only permutes the factorization order, not the sequence order. In other words, they keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in Transformers to achieve permutation of the factorization order. \n3. Two-Stream Self-Attention for Target-Aware Representations: Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the content representations is exactly the same as the standard self-attention, so during finetuning, the paper authors can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally, the paper authors can use the last-layer query representation.\n4. Incorporating Ideas from Transformer-XL: They integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism. They apply relative positional encodings based on the original sequence.\n5. Modeling Multiple Segments: During the pretraining phase, following BERT, they randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. They only reuse the memory that belongs to the same context. Specifically, the input to their model is the same as BERT: [CLS, A, SEP, B, SEP], where \u201cSEP\u201d and \u201cCLS\u201d are two special symbols and \u201cA\u201d and \u201cB\u201d are the two segments.\n6. Relative Segment Encodings:  Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, they extend the idea of relative encodings from Transformer-XL to also encode the segments.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["We integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism. We apply relative positional encodings based on the original sequence as discussed earlier, which is straightforward. Now we discuss how to integrate the recurrence mechanism into the proposed permutation setting and enable the model to reuse hidden states from previous segments. Without loss of generality, suppose we have two segments taken from a long sequence s; i.e., x\u0303 = s1:T and x = sT+1:2T . Let z\u0303 and z be permutations of [1 \u00b7 \u00b7 \u00b7T ] and [T + 1 \u00b7 \u00b7 \u00b7 2T ] respectively. Then, based on the permutation z\u0303, we process the first segment, and then cache the obtained content representations h\u0303(m) for each layer m. Then, for the next segment x, the attention update with memory can be written as\nh(m)zt \u2190 Attention(Q = h (m\u22121) zt ,KV = [ h\u0303(m\u22121),h(m\u22121)z\u2264t ] ; \u03b8)\nwhere [., .] denotes concatenation along the sequence dimension. Notice that positional encodings only depend on the actual positions in the original sequence. Thus, the above attention update is independent of z\u0303 once the representations h\u0303(m) are obtained. This allows caching and reusing the memory without knowing the factorization order of the previous segment. In expectation, the model learns to utilize the memory over all factorization orders of the last segment. The query stream can be computed in the same way. Finally, Figure 1 (c) presents an overview of the proposed permutation language modeling with two-stream attention (see Appendix A.7 for more detailed illustration). 2.5 Modeling Multiple Segments. Many downstream tasks have multiple input segments, e.g., a question and a context paragraph in question answering. We now discuss how we pretrain XLNet to model multiple segments in the autoregressive framework.", "After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet.", "During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context. Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where \u201cSEP\u201d and \u201cCLS\u201d are two special symbols and \u201cA\u201d and \u201cB\u201d are the two segments. Although\nwe follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4). Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s\u2212, where s+ and s\u2212 are learnable model parameters for each attention head. In other words, we only consider whether the two positions are within the same segment, as opposed to considering which specific segments they are from. This is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. When i attends to j, the segment encoding sij is used to compute an attention weight aij = (qi + b)\n>sij , where qi is the query vector as in a standard attention operation and b is a learnable head-specific bias vector. Finally, the value aij is added to the normal attention weight. There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings. 2.6 Discussion. Comparing Eq."]}
{"pkey": "xlnet_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "1. Hyperparameters for pretraining is given in Table 7 in the paper as: \nNumber of layers 24\nHidden size 1024\nNumber of attention heads 16\nAttention head size 64\nFFN inner hidden size 4096\nHidden Dropout 0.1\nGeLU Dropout 0.0\nAttention dropout 0.1\nPartial prediction K 6\nMax sequence length 512\nBatch size 8192\nLearning rate 4e-4\nNumber of steps 500K\nWarmup steps 40,000\nLearning rate decay linear\nAdam epsilon 1e-6\nWeight decay 0.01\n\n2. Hyperparameters for finetuning is given in Table 8 in the paper for different tasks as :\nRACE -  Dropout 0.1, Attention dropout 0.1 ,Max sequence length 512, Batch size 32, Learning rate 2e-5, Number of steps 12K, Learning rate decay linear, Weight decay 0.01, Adam epsilon 1e-6, Layer-wise lr decay 1.0.\nSQuAD -  Dropout 0.1, Attention dropout 0.1 ,Max sequence length 512, Batch size 48, Learning rate 3e-5, Number of steps 8K, Learning rate decay linear, Weight decay 0.01, Adam epsilon 1e-6, Layer-wise lr decay 0.75.\nMNLI - Dropout 0.1, Attention dropout 0.1 ,Max sequence length 128, Batch size 128, Learning rate 2e-5, Number of steps 10K, Learning rate decay: linear, Weight decay 0.01, Adam epsilon 1e-6, Layer-wise lr decay 1.0.\nYelp-5 - Dropout 0.1, Attention dropout 0.1 ,Max sequence length 512, Batch size 128, Learning rate 1e-5, Number of steps 10K, Learning rate decay: linear, Weight decay 0.01, Adam epsilon 1e-6, Layer-wise lr decay 1.0.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet.", "For example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is \u03b1, then the learning rate of layer m is l\u03b124\u2212m.\nHparam RACE SQuAD MNLI Yelp-5. A.5 Discussion and Analysis. A.5.1 Comparison with BERT. To prove a general point beyond one example, we now turn to more formal expressions. Inspired by previous work [37], given a sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], we define a set of target-context pairs of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For example, given the above sentence, the pairs of interest I could be instantiated as:\nI = {( x = York,U = {New} ) , ( x = York,U = {city} ) , ( x = York,U = {New, city} ) , \u00b7 \u00b7 \u00b7 } . Note that I is merely a virtual notion without unique ground truth, and our analysis will hold regardless of how I is instantiated. Given a set of target tokens T and a set of non-target tokens N = x\\T , BERT and XLNet both maximize log p(T | N ) but with different formulations:\nJBERT = \u2211 x\u2208T log p(x | N ); JXLNet = \u2211 x\u2208T log p(x | N \u222a T<x)\nwhere T<x denote tokens in T that have a factorization order prior to x. Both objectives consist of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair (x,U) \u2208 I such that U \u2286 Vx, then the loss term log p(x | Vx) provides a training signal to the dependency between x and U . For convenience, we say a target-context pair (x,U) \u2208 I is covered by a model (objective) if U \u2286 Vx. Given the definition, let\u2019s consider two cases:\n\u2022 If U \u2286 N , the dependency (x,U) is covered by both BERT and XLNet. \u2022 If U \u2286 N \u222a T<x and U \u2229 T<x 6= \u2205, the dependency can only be covered by XLNet but not BERT. As a result, XLNet is able to cover more dependencies than BERT.", "Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1. 1 Introduction. Unsupervised representation learning has been highly successful in the domain of natural language processing [7, 22, 27, 28, 10]. Typically, these methods first pretrain neural networks on large-scale unlabeled text corpora, and then finetune the models or representations on downstream tasks. Under this shared high-level idea, different unsupervised pretraining objectives have been explored in literature. Among them, autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives. AR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model [7, 27, 28]. Specifically, given a text sequence x = (x1, \u00b7 \u00b7 \u00b7 , xT ), AR language modeling factorizes the likelihood into a forward product p(x) = \u220fT t=1 p(xt | x<t) or a backward\none p(x) = \u220f1 t=T p(xt | x>t). A parametric model (e.g. a neural network) is trained to model each conditional distribution. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. On the contrary, downstream language understanding tasks often require bidirectional context information. This results in a gap between AR language modeling and effective pretraining. In comparison, AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. A notable example is BERT [10], which has been the state-of-the-art pretraining approach. Given the input token sequence, a certain portion of tokens are replaced by a special symbol [MASK], and the model is trained to recover the original tokens from the corrupted version."]}
{"pkey": "xlnet_13", "question": "Describe the computational resources used to train the model.", "answer": "The paper authors train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets. 3.3 Comparison with RoBERTa: Scaling Up. RACE Accuracy Middle High Model NDCG@20 ERR@20. After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1. The results are presented in Tables 2 (reading comprehension & document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and finetuning are in Appendix A.4. \u2022 For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet. \u2022 For classification tasks that already have abundant supervised examples such as MNLI (>390K), Yelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains. 3.4 Ablation Study. We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:\n\u2022", "After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet.", "The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16]. A.3.5 ClueWeb09-B Dataset. Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents. A.4 Hyperparameters. A.4.1 Pretraining Hyperparameters. The hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2 Hyperparameters for Finetuning. The hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise decay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner."]}
{"pkey": "xlnet_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "1. Pretrained models and code are available at https://github.com/zihangdai/xlnet\n2. Also in the github repo this has been mentioned - Most of the SOTA results in their paper were produced on TPUs, which generally have more RAM than common GPUs. As a result, it is currently very difficult (costly) to re-produce most of the XLNet-Large SOTA results in the paper using GPUs with 12GB - 16GB of RAM, because a 16GB GPU is only able to hold a single sequence with length 512 for XLNet-Large. Therefore, a large number (ranging from 32 to 128, equal to batch_size) of GPUs are required to reproduce many results in the paper.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT. \u2022 The importance of using Transformer-XL as the backbone neural architecture. \u2022 The necessity of some implementation details including span-based prediction, the bidirectional\ninput pipeline, and next-sentence prediction. With these purposes in mind, in Table 6, we compare 6 XLNet-Base variants with different implementation details (rows 3 - 8), the original BERT-Base model (row 1), and an additional Transformer-XL baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidirectional input pipeline (row 2). For fair comparison, all models are based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs. Examining rows 1 - 4 of Table 6, we can see both Transformer-XL and the permutation LM clearly contribute the superior performance of XLNet over BERT. Moreover, if we remove the memory caching mechanism (row 5), the performance clearly drops, especially for RACE which involves the longest context among the 4 tasks. In addition, rows 6 - 7 show that both span-based prediction and the bidirectional input pipeline play important roles in XLNet. Finally, we unexpectedly find the the next-sentence prediction objective proposed in the original BERT does not necessarily lead to an improvement in our setting. Hence, we exclude the next-sentence prediction objective from XLNet. Finally, we also perform a qualitative study of the attention patterns, which is included in Appendix A.6 due to page limit. 4 Conclusions. XLNet is a generalized AR pretraining method that uses a permutation language modeling objective to combine the advantages of AR and AE methods.", "p\u03b8(Xzt = x | xz<t) = exp\n( e(x)>g\u03b8(xz<t , zt) )\u2211 x\u2032 exp (e(x \u2032)>g\u03b8(xz<t , zt)) , (4)\nwhere g\u03b8(xz<t , zt) denotes a new type of representations which additionally take the target position zt as input. Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity in target prediction, how to formulate g\u03b8(xz<t , zt) remains a non-trivial problem. Among other possibilities, we propose to \u201cstand\u201d at the target position zt and rely on the position zt to gather information from the context xz<t through attention. For this parameterization to work, there are two requirements that are contradictory in a standard Transformer architecture: (1) to predict the token xzt , g\u03b8(xz<t , zt) should only use the position zt and not the content xzt , otherwise the objective becomes trivial; (2) to predict the other tokens xzj with j > t, g\u03b8(xz<t , zt) should also encode the content xzt to provide full contextual information. To resolve such a contradiction, we propose to use two sets of hidden representations instead of one:\n\u2022 The content representation h\u03b8(xz\u2264t), or abbreviated as hzt , which serves a similar role to the standard hidden states in Transformer. This representation encodes both the context and xzt itself. \u2022 The query representation g\u03b8(xz<t , zt), or abbreviated as gzt , which only has access to the contextual information xz<t and the position zt, but not the content xzt , as discussed above. Computationally, the first layer query stream is initialized with a trainable vector, i.e. g(0)i = w, while the content stream is set to the corresponding word embedding, i.e. h(0)i = e(xi). For each self-attention layer m = 1, . . . ,M , the two streams of representations are schematically2 updated\n2To avoid clutter, we omit the implementation details including multi-head attention, residual connection, layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in Appendix A.2 for reference.", "As a result, it further \u201cjustifies\u201d language modeling research. Moreover, it becomes possible to leverage the rapid progress of language modeling research for pretraining. As an example, we integrate Transformer-XL into XLNet to demonstrate the usefulness of the latest language modeling progress. A.6 Qualitative Analysis of Attention Patterns. We compare the attention pattern of BERT and XLNet without finetuning. Firstly, we found 4 typical patterns shared by both, as shown in Fig. 2.\nMore interestingly, in Fig. 3, we present 3 patterns that only appear in XLNet but not BERT: (a) The self-exclusion pattern attends to all other tokens but itself, probably offering a fast way to gather global information; (b) The relative-stride pattern attends to positions every a few stride apart relative to the query position; (c) The one-side masked pattern is very similar to the lower-left part of Fig. 1-(d), with the upper-right triangle masked out. It seems that the model learns not to attend the relative right half. Note that all these three unique patterns involve the relative positions rather than absolute ones, and hence are likely enabled by the \u201crelative attention\u201d mechanism in XLNet. We conjecture these unique patterns contribute to the performance advantage of XLNet. On the other hand, the proposed permutation LM objective mostly contributes to a better data efficiency, whose effects may not be obvious from qualitative visualization. A.7 Visualizing Memory and Permutation In this section, we provide a detailed visualization of the proposed permutation language modeling objective, including the mechanism of reusing memory (aka the recurrence mechanism), how we use attention masks to permute the factorization order, and the difference of the two attention streams. As shown in Figure 5 and 6, given the current position zt, the attention mask is decided by the permutation (or factorization order)"]}
{"pkey": "xlnet_15", "question": "What is the pretraining objective of the model? ", "answer": "Faced with the pros and cons of existing language pretraining objectives, in this work, they proposed XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations.\n1. Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context.\n2. Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to. Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT.\nIn addition to a novel pretraining objective, XLNet improves architectural designs for pretraining.\na) Inspired by the latest advancements in AR language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.\nb) Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, they propose to reparameterize the Transformer(-XL) network to remove the ambiguity.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["z such that only tokens the occur before zt in the permutation can be attended; i.e., positions zi with i < t. Moreover, comparing Figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention. 4The problem of language modeling is essentially density estimation for text data. 5https://openreview.net/forum?id=HJePno0cYm\nJoint View of the Content Stream (Factorization order: 3 \u00e0 2 \u00e0 4 \u00e0 1)\nww w wmem(+) x#x% x' x(\ng# (%)g% (%) g' (%) g( (%)mem(%) h# (%)h% (%) h' (%) h( (%) g# (')g% (') g' (') g( (') h# (')h% (') h' (') h( (')", "Since density estimation is not part of the objective, BERT is allowed to utilize \u2217Equal contribution. Order determined by swapping the one in [9]. 1Pretrained models and code are available at https://github.com/zihangdai/xlnet\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n90 6.\n08 23\n7v 2\n[ cs\n.C L\n] 2\nJ an\n2 02\nbidirectional contexts for reconstruction. As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language [9]. Faced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations. \u2022 Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context. \u2022 Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to.", "Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining. \u2022 Inspired by the latest advancements in AR language modeling , XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence. \u2022 Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity. Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Related Work The idea of permutation-based AR modeling has been explored in [32, 12], but there are several key differences. Firstly, previous models aim to improve density estimation by baking an \u201corderless\u201d inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that \u201corderless\u201d does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution."]}
{"pkey": "xlnet_16", "question": "What is the loss function that is used to train the model?", "answer": "1. Given a sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], they define a set of target-context pairs of interest, I = {(x, U)}, where U is a set of tokens in x that form a context of x. Intuitively, they want the model to learn the dependency of x on U through a pretraining loss term log p(x | U).\nGiven a set of target tokens T and a set of non-target tokens N = x\\T , BERT and XLNet both maximize log p(T | N ) but with different formulations. where T<x denote tokens in T that have a factorization order prior to x. Both objectives consist\nof multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair (x, U) \u2208 I such that U \u2286 Vx, then the loss term log p(x | Vx) provides a training signal to the dependency between x and U. For convenience, they say a target-context pair (x, U) \u2208 I is covered by a model (objective) if U \u2286 Vx.\n2. For WNLI, they uses the loss described in (Vid Kocijan, Ana-Maria Cretu, Oana-Maria Camburu, Yordan Yordanov, and Thomas Lukasiewicz. A surprisingly robust trick for winograd schema challenge. arXiv preprint arXiv:1905.06290, 2019).\n3. To finetune an XLNet on SQuAD2.0, they jointly apply a logistic regression loss for answerability prediction similar to classification tasks and a standard span extraction loss for question answering.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["with a shared set of parameters as follows (illustrated in Figures 1 (a) and (b)):\ng(m)zt \u2190 Attention(Q = g (m\u22121) zt ,KV = h (m\u22121) z<t ; \u03b8), (query stream: use zt but cannot see xzt) h(m)zt \u2190 Attention(Q = h (m\u22121) zt ,KV = h (m\u22121) z\u2264t ; \u03b8), (content stream: use both zt and xzt). where Q, K, V denote the query, key, and value in an attention operation [33]. The update rule of the content representations is exactly the same as the standard self-attention, so during finetuning, we can simply drop the query stream and use the content stream as a normal Transformer(-XL). Finally, we can use the last-layer query representation g(M)zt to compute Eq. (4). Partial Prediction While the permutation language modeling objective (3) has several benefits, it is a much more challenging optimization problem due to the permutation and causes slow convergence in preliminary experiments. To reduce the optimization difficulty, we choose to only predict the last tokens in a factorization order. Formally, we split z into a non-target subsequence z\u2264c and a target subsequence z>c, where c is the cutting point. The objective is to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence, i.e.,\nmax \u03b8\nEz\u223cZT [ log p\u03b8(xz>c | xz\u2264c) ] = Ez\u223cZT  |z|\u2211 t=c+1 log p\u03b8(xzt | xz<t) . (5) Note that z>c is chosen as the target because it possesses the longest context in the sequence given the current factorization order z. A hyperparameter K is used such that about 1/K tokens are selected for predictions; i.e., |z| /(|z| \u2212 c) \u2248 K. For unselected tokens, their query representations need not be computed, which saves speed and memory. 2.4 Incorporating Ideas from Transformer-XL. Since our objective function fits in the AR framework, we incorporate the state-of-the-art AR language model, Transformer-XL [9], into our pretraining framework, and name our method after it.", "For example, suppose the 24-th layer uses a learning rate l, and the Layer-wise decay rate is \u03b1, then the learning rate of layer m is l\u03b124\u2212m.\nHparam RACE SQuAD MNLI Yelp-5. A.5 Discussion and Analysis. A.5.1 Comparison with BERT. To prove a general point beyond one example, we now turn to more formal expressions. Inspired by previous work [37], given a sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], we define a set of target-context pairs of interest, I = {(x,U)}, where U is a set of tokens in x that form a context of x. Intuitively, we want the model to learn the dependency of x on U through a pretraining loss term log p(x | U). For example, given the above sentence, the pairs of interest I could be instantiated as:\nI = {( x = York,U = {New} ) , ( x = York,U = {city} ) , ( x = York,U = {New, city} ) , \u00b7 \u00b7 \u00b7 } . Note that I is merely a virtual notion without unique ground truth, and our analysis will hold regardless of how I is instantiated. Given a set of target tokens T and a set of non-target tokens N = x\\T , BERT and XLNet both maximize log p(T | N ) but with different formulations:\nJBERT = \u2211 x\u2208T log p(x | N ); JXLNet = \u2211 x\u2208T log p(x | N \u222a T<x)\nwhere T<x denote tokens in T that have a factorization order prior to x. Both objectives consist of multiple loss terms in the form of log p(x | Vx). Intuitively, if there exists a target-context pair (x,U) \u2208 I such that U \u2286 Vx, then the loss term log p(x | Vx) provides a training signal to the dependency between x and U . For convenience, we say a target-context pair (x,U) \u2208 I is covered by a model (objective) if U \u2286 Vx. Given the definition, let\u2019s consider two cases:\n\u2022 If U \u2286 N , the dependency (x,U) is covered by both BERT and XLNet. \u2022 If U \u2286 N \u222a T<x and U \u2229 T<x 6= \u2205, the dependency can only be covered by XLNet but not BERT. As a result, XLNet is able to cover more dependencies than BERT.", "z such that only tokens the occur before zt in the permutation can be attended; i.e., positions zi with i < t. Moreover, comparing Figure 5 and 6, we can see how the query stream and the content stream work differently with a specific permutation through attention masks. The main difference is that the query stream cannot do self-attention and does not have access to the token at the position, while the content stream performs normal self-attention. 4The problem of language modeling is essentially density estimation for text data. 5https://openreview.net/forum?id=HJePno0cYm\nJoint View of the Content Stream (Factorization order: 3 \u00e0 2 \u00e0 4 \u00e0 1)\nww w wmem(+) x#x% x' x(\ng# (%)g% (%) g' (%) g( (%)mem(%) h# (%)h% (%) h' (%) h( (%) g# (')g% (') g' (') g( (') h# (')h% (') h' (') h( (')"]}
{"pkey": "xlnet_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "1. Authors integrate two important techniques in Transformer-XL, namely the relative positional encoding scheme and the segment recurrence mechanism. The paper authors apply relative positional encodings based on the original sequence.\n2. Borrowing ideas from orderless NADE [32], they propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional contexts. \n3. The proposed objective only permutes the factorization order, not the sequence order. In other words, they keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in Transformers to achieve permutation of the factorization order.\n4. Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, they extend the idea of relative encodings from Transformer-XL to also encode the segments. they only consider whether the two positions are within the same segment, as opposed to considering which specific segments they are from. This is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings.\n5.  Two-Stream Self-Attention for Target-Aware Representations: Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity in target prediction, how to formulate g\u03b8(xz<t , zt) remains a non-trivial problem. Among other possibilities, they propose to \u201cstand\u201d at the target position zt and rely on the position zt to gather information from the context xz<t through attention. For this parameterization to work, there are two requirements that are contradictory in a standard Transformer architecture: (1) to predict the token xzt , g\u03b8(xz<t , zt) should only use the position zt and not the content xzt , otherwise the objective becomes trivial; (2) to predict the other tokens xzj with j > t, g\u03b8(xz<t , zt) should also encode the content xzt to provide full contextual information. To resolve such a contradiction, they propose to use two sets of hidden representations instead of one: a) The content representation h\u03b8(xz\u2264t ), or abbreviated as hzt, which serves a similar role to the standard hidden states in Transformer. This representation encodes both the context and xzt itself.\nb) The query representation g\u03b8(xz<t , zt), or abbreviated as gzt , which only has access to the contextual information xz<t and the position zt, but not the content xzt , as discussed above.\nComputationally, the first layer query stream is initialized with a trainable vector, i.e. g(0)i = w, while the content stream is set to the corresponding word embedding, i.e. h(0) i = e(xi). For each self-attention layer m = 1, . . . , M, the two streams of representations are schematically updated with a shared set of parameters. Q, K, V denote the query, key, and value in an attention operation. The update rule of the content representations is exactly the same as the standard self-attention, so during finetuning, the paper authors can simply drop the query stream and use the content stream as a normal Transformer(-XL).", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["Another related idea is to perform autoregressive denoising in the context of text generation [11], which only considers a fixed order though. 2 Proposed Method. 2.1 Background. In this section, we first review and compare the conventional AR language modeling and BERT for language pretraining. Given a text sequence x = [x1, \u00b7 \u00b7 \u00b7 , xT ], AR language modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization:\nmax \u03b8 log p\u03b8(x) = T\u2211 t=1 log p\u03b8(xt | x<t) = T\u2211 t=1 log exp\n( h\u03b8(x1:t\u22121) >e(xt) )\u2211\nx\u2032 exp (h\u03b8(x1:t\u22121) >e(x\u2032))\n, (1)\nwhere h\u03b8(x1:t\u22121) is a context representation produced by neural models, such as RNNs or Transformers, and e(x) denotes the embedding of x. In comparison, BERT is based on denoising auto-encoding. Specifically, for a text sequence x, BERT first constructs a corrupted version x\u0302 by randomly setting a portion (e.g. 15%) of tokens in x to a special symbol [MASK]. Let the masked tokens be x\u0304. The training objective is to reconstruct x\u0304 from x\u0302:\nmax \u03b8 log p\u03b8(x\u0304 | x\u0302) \u2248 T\u2211 t=1 mt log p\u03b8(xt | x\u0302) = T\u2211 t=1 mt log exp\n( H\u03b8(x\u0302) > t e(xt) ) \u2211 x\u2032 exp ( H\u03b8(x\u0302)>t e(x \u2032) ) , (2)\nwhere mt = 1 indicates xt is masked, and H\u03b8 is a Transformer that maps a length-T text sequence x into a sequence of hidden vectors H\u03b8(x) = [H\u03b8(x)1, H\u03b8(x)2, \u00b7 \u00b7 \u00b7 , H\u03b8(x)T ]. The pros and cons of the two pretraining objectives are compared in the following aspects:\n\u2022 Independence Assumption: As emphasized by the \u2248 sign in Eq. (2), BERT factorizes the joint conditional probability p(x\u0304 | x\u0302) based on an independence assumption that all masked tokens x\u0304 are separately reconstructed. In comparison, the AR language modeling objective (1) factorizes p\u03b8(x) using the product rule that holds universally without such an independence assumption. \u2022 Input noise: The input to BERT contains artificial symbols like [MASK] that never occur in downstream tasks, which creates a pretrain-finetune discrepancy.", "p\u03b8(Xzt = x | xz<t) = exp\n( e(x)>g\u03b8(xz<t , zt) )\u2211 x\u2032 exp (e(x \u2032)>g\u03b8(xz<t , zt)) , (4)\nwhere g\u03b8(xz<t , zt) denotes a new type of representations which additionally take the target position zt as input. Two-Stream Self-Attention While the idea of target-aware representations removes the ambiguity in target prediction, how to formulate g\u03b8(xz<t , zt) remains a non-trivial problem. Among other possibilities, we propose to \u201cstand\u201d at the target position zt and rely on the position zt to gather information from the context xz<t through attention. For this parameterization to work, there are two requirements that are contradictory in a standard Transformer architecture: (1) to predict the token xzt , g\u03b8(xz<t , zt) should only use the position zt and not the content xzt , otherwise the objective becomes trivial; (2) to predict the other tokens xzj with j > t, g\u03b8(xz<t , zt) should also encode the content xzt to provide full contextual information. To resolve such a contradiction, we propose to use two sets of hidden representations instead of one:\n\u2022 The content representation h\u03b8(xz\u2264t), or abbreviated as hzt , which serves a similar role to the standard hidden states in Transformer. This representation encodes both the context and xzt itself. \u2022 The query representation g\u03b8(xz<t , zt), or abbreviated as gzt , which only has access to the contextual information xz<t and the position zt, but not the content xzt , as discussed above. Computationally, the first layer query stream is initialized with a trainable vector, i.e. g(0)i = w, while the content stream is set to the corresponding word embedding, i.e. h(0)i = e(xi). For each self-attention layer m = 1, . . . ,M , the two streams of representations are schematically2 updated\n2To avoid clutter, we omit the implementation details including multi-head attention, residual connection, layer normalization and position-wise feed-forward as used in Transformer(-XL). The details are included in Appendix A.2 for reference.", "Since the same model parameter \u03b8 is shared across all factorization orders during training, in expectation, xt has seen every possible element xi 6= xt in the sequence, hence being able to capture the bidirectional context. Moreover, as this objective fits into the AR framework, it naturally avoids the independence assumption and the pretrain-finetune discrepancy discussed in Section 2.1.\nRemark on Permutation The proposed objective only permutes the factorization order, not the sequence order. In other words, we keep the original sequence order, use the positional encodings corresponding to the original sequence, and rely on a proper attention mask in Transformers to achieve permutation of the factorization order. Note that this choice is necessary, since the model will only encounter text sequences with the natural order during finetuning. To provide an overall picture, we show an example of predicting the token x3 given the same input sequence x but under different factorization orders in the Appendix A.7 with Figure 4.\n2.3 Architecture: Two-Stream Self-Attention for Target-Aware Representations. While the permutation language modeling objective has desired properties, naive implementation with standard Transformer parameterization may not work. To see the problem, assume we parameterize the next-token distribution p\u03b8(Xzt | xz<t) using the standard Softmax formulation, i.e., p\u03b8(Xzt = x | xz<t) = exp(e(x)>h\u03b8(xz<t )) \u2211 x\u2032 exp(e(x\u2032)>h\u03b8(xz<t ))\n, where h\u03b8(xz<t) denotes the hidden representation of xz<t produced by the shared Transformer network after proper masking. Now notice that the representation h\u03b8(xz<t) does not depend on which position it will predict, i.e., the value of zt. Consequently, the same distribution is predicted regardless of the target position, which is not able to learn useful representations (see Appendix A.1 for a concrete example). To avoid this problem, we propose to re-parameterize the next-token distribution to be target position aware:"]}
{"pkey": "xlnet_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "1. Their largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, authors always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), they also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where they reuse all pretraining hyper-parameters as in the original BERT. Then, they scale up the training of XLNet-Large by using all the datasets described above. Specifically, they train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was observed that the model still underfits the data at the end of training.\n2. Since the recurrence mechanism is introduced, they use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, they set the partial prediction constant K as 6. They employ an idea of span-based prediction, where they first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens.\n3. Fair Comparison with BERT.\n4. Comparison with RoBERTa: Scaling Up.\nEvaluation metric: \nAccuracy, F1 and Exact Match metric are used as shown in table 3 in the paper.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet.", "XLNet: Generalized Autoregressive Pretraining for Language Understanding. With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment settings, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking.1. XLNet: Generalized Autoregressive Pretraining for Language Understanding. With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining.", "The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16]. A.3.5 ClueWeb09-B Dataset. Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents. A.4 Hyperparameters. A.4.1 Pretraining Hyperparameters. The hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2 Hyperparameters for Finetuning. The hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise decay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner."]}
{"pkey": "xlnet_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "Authors perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects they hope to study:\n1. The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT.\n2. The importance of using Transformer-XL as the backbone neural architecture.\n3. The necessity of some implementation details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction.\nWith these purposes in mind, in Table 6, they compared 6 XLNet-Base variants with different implementation details, the original BERT-Base model, and an additional Transformer-XL baseline trained with the denoising auto-encoding (DAE) objective used in BERT but with the bidirectional input pipeline. For fair comparison, all models are based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and are trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs.\nInspired by the latest advancements in AR language modeling, XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["In Table 1, we compare (1) best performance of three different variants of BERT and (2) XLNet trained with the same data and hyperparameters. As we can see, trained on the same data with an almost identical training recipe, XLNet outperforms BERT by a sizable margin on all the considered datasets. 3.3 Comparison with RoBERTa: Scaling Up. RACE Accuracy Middle High Model NDCG@20 ERR@20. After the initial publication of our manuscript, a few other pretrained models were released such as RoBERTa [21] and ALBERT [19]. Since ALBERT involves increasing the model hidden size from 1024 to 2048/4096 and thus substantially increases the amount of computation in terms of FLOPs, we exclude ALBERT from the following results as it is hard to lead to scientific conclusions. To obtain relatively fair comparison with RoBERTa, the experiment in this section is based on full data and reuses the hyper-parameters of RoBERTa, as described in section 3.1. The results are presented in Tables 2 (reading comprehension & document ranking), 3 (question answering), 4 (text classification) and 5 (natural language understanding), where XLNet generally outperforms BERT and RoBERTa. In addition, we make two more interesting observations:\n3Hyperparameters for pretraining and finetuning are in Appendix A.4. \u2022 For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is usually larger. This superiority at dealing with longer context could come from the Transformer-XL backbone in XLNet. \u2022 For classification tasks that already have abundant supervised examples such as MNLI (>390K), Yelp (>560K) and Amazon (>3M), XLNet still lead to substantial gains. 3.4 Ablation Study. We perform an ablation study to understand the importance of each design choice based on four datasets with diverse characteristics. Specifically, there are three main aspects we hope to study:\n\u2022", "During the pretraining phase, following BERT, we randomly sample two segments (either from the same context or not) and treat the concatenation of two segments as one sequence to perform permutation language modeling. We only reuse the memory that belongs to the same context. Specifically, the input to our model is the same as BERT: [CLS, A, SEP, B, SEP], where \u201cSEP\u201d and \u201cCLS\u201d are two special symbols and \u201cA\u201d and \u201cB\u201d are the two segments. Although\nwe follow the two-segment data format, XLNet-Large does not use the objective of next sentence prediction [10] as it does not show consistent improvement in our ablation study (see Section 3.4). Relative Segment Encodings Architecturally, different from BERT that adds an absolute segment embedding to the word embedding at each position, we extend the idea of relative encodings from Transformer-XL to also encode the segments. Given a pair of positions i and j in the sequence, if i and j are from the same segment, we use a segment encoding sij = s+ or otherwise sij = s\u2212, where s+ and s\u2212 are learnable model parameters for each attention head. In other words, we only consider whether the two positions are within the same segment, as opposed to considering which specific segments they are from. This is consistent with the core idea of relative encodings; i.e., only modeling the relationships between positions. When i attends to j, the segment encoding sij is used to compute an attention weight aij = (qi + b)\n>sij , where qi is the query vector as in a standard attention operation and b is a learnable head-specific bias vector. Finally, the value aij is added to the normal attention weight. There are two benefits of using relative segment encodings. First, the inductive bias of relative encodings improves generalization [9]. Second, it opens the possibility of finetuning on tasks that have more than two input segments, which is not possible using absolute segment encodings. 2.6 Discussion. Comparing Eq.", "After tokenization with SentencePiece [17], we obtain 2.78B, 1.09B, 4.75B, 4.30B, and 19.97B subword pieces for Wikipedia, BooksCorpus, Giga5, ClueWeb, and Common Crawl respectively, which are 32.89B in total. Our largest model XLNet-Large has the same architecture hyperparameters as BERT-Large, which results in a similar model size. During pretraining, we always use a full sequence length of 512. Firstly, to provide a fair comparison with BERT (section 3.2), we also trained XLNet-Large-wikibooks on BooksCorpus and Wikipedia only, where we reuse all pretraining hyper-parameters as in the original BERT. Then, we scale up the training of XLNet-Large by using all the datasets described above. Specifically, we train on 512 TPU v3 chips for 500K steps with an Adam weight decay optimizer, linear learning rate decay, and a batch size of 8192, which takes about 5.5 days. It was\nobserved that the model still underfits the data at the end of training. Finally, we perform ablation study (section 3.4) based on the XLNet-Base-wikibooks. Since the recurrence mechanism is introduced, we use a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. For training XLNet-Large, we set the partial prediction constant K as 6 (see Section 2.3). Our finetuning procedure follows BERT [10] except otherwise specified3. We employ an idea of span-based prediction, where we first sample a length L \u2208 [1, \u00b7 \u00b7 \u00b7 , 5], and then randomly select a consecutive span of L tokens as prediction targets within a context of (KL) tokens. We use a variety of natural language understanding datasets to evaluate the performance of our method. Detailed descriptions of the settings for all the datasets can be found in Appendix A.3.\n3.2 Fair Comparison with BERT. Here, we first compare the performance of BERT and XLNet in a fair setting to decouple the effects of using more data and the improvement from BERT to XLNet."]}
{"pkey": "xlnet_20", "question": "List the future work mentioned in the paper.", "answer": "Future directions are not specified in the paper, but in their github code they have mentioned their future plans as follows: The paper authors plan to continuously release more pretrained models under different settings, including: A pretrained model that is finetuned on Wikipedia. This can be used for tasks with Wikipedia text such as SQuAD and HotpotQA. Pretrained models with other hyperparameter configurations, targeting specific downstream tasks. Pretrained models that benefit from new techniques.", "title": "XLNet: Generalized Autoregressive Pretrainingfor Language Understanding", "context": ["The GLUE dataset [34] is a collection of 9 natural language understanding tasks. The test set labels are removed from the publicly released version, and all the practitioners must submit their predictions on the evaluation server to obtain test set results. In Table 5, we present results of multiple settings, including single-task and multi-task, as well as single models and ensembles. In the multi-task setting, we jointly train an XLNet on the four largest datasets\u2014MNLI, SST-2, QNLI, and QQP\u2014and finetune the network on the other datasets. Only single-task training is employed for the four large datasets. For QNLI, we employed a pairwise relevance ranking scheme as in [20] for our test set submission. However, for fair comparison with BERT, our result on the QNLI dev set is based on a standard classification paradigm. For WNLI, we use the loss described in [16]. A.3.5 ClueWeb09-B Dataset. Following the setting in previous work [8], we use the ClueWeb09-B dataset to evaluate the performance on document ranking. The queries were created by the TREC 2009-2012 Web Tracks based on 50M documents and the task is to rerank the top 100 documents retrieved using a standard retrieval method. Since document ranking, or ad-hoc retrieval, mainly concerns the low-level representations instead of high-level semantics, this dataset serves as a testbed for evaluating the quality of word embeddings. We use a pretrained XLNet to extract word embeddings for the documents and queries without finetuning, and employ a kernel pooling network [36] to rank the documents. A.4 Hyperparameters. A.4.1 Pretraining Hyperparameters. The hyperparameters used for pretraining XLNet are shown in Table 7.\nA.4.2 Hyperparameters for Finetuning. The hyperparameters used for finetuning XLNet on various tasks are shown in Table 8. \u201cLayer-wise decay\u201d means exponentially decaying the learning rates of individual layers in a top-down manner.", "Since density estimation is not part of the objective, BERT is allowed to utilize \u2217Equal contribution. Order determined by swapping the one in [9]. 1Pretrained models and code are available at https://github.com/zihangdai/xlnet\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\nar X\niv :1\n90 6.\n08 23\n7v 2\n[ cs\n.C L\n] 2\nJ an\n2 02\nbidirectional contexts for reconstruction. As an immediate benefit, this closes the aforementioned bidirectional information gap in AR language modeling, leading to improved performance. However, the artificial symbols like [MASK] used by BERT during pretraining are absent from real data at finetuning time, resulting in a pretrain-finetune discrepancy. Moreover, since the predicted tokens are masked in the input, BERT is not able to model the joint probability using the product rule as in AR language modeling. In other words, BERT assumes the predicted tokens are independent of each other given the unmasked tokens, which is oversimplified as high-order, long-range dependency is prevalent in natural language [9]. Faced with the pros and cons of existing language pretraining objectives, in this work, we propose XLNet, a generalized autoregressive method that leverages the best of both AR language modeling and AE while avoiding their limitations. \u2022 Firstly, instead of using a fixed forward or backward factorization order as in conventional AR models, XLNet maximizes the expected log likelihood of a sequence w.r.t. all possible permutations of the factorization order. Thanks to the permutation operation, the context for each position can consist of tokens from both left and right. In expectation, each position learns to utilize contextual information from all positions, i.e., capturing bidirectional context. \u2022 Secondly, as a generalized AR language model, XLNet does not rely on data corruption. Hence, XLNet does not suffer from the pretrain-finetune discrepancy that BERT is subject to.", "Meanwhile, the autoregressive objective also provides a natural way to use the product rule for factorizing the joint probability of the predicted tokens, eliminating the independence assumption made in BERT. In addition to a novel pretraining objective, XLNet improves architectural designs for pretraining. \u2022 Inspired by the latest advancements in AR language modeling , XLNet integrates the segment recurrence mechanism and relative encoding scheme of Transformer-XL [9] into pretraining, which empirically improves the performance especially for tasks involving a longer text sequence. \u2022 Naively applying a Transformer(-XL) architecture to permutation-based language modeling does not work because the factorization order is arbitrary and the target is ambiguous. As a solution, we propose to reparameterize the Transformer(-XL) network to remove the ambiguity. Empirically, under comparable experiment setting, XLNet consistently outperforms BERT [10] on a wide spectrum of problems including GLUE language understanding tasks, reading comprehension tasks like SQuAD and RACE, text classification tasks such as Yelp and IMDB, and the ClueWeb09-B document ranking task. Related Work The idea of permutation-based AR modeling has been explored in [32, 12], but there are several key differences. Firstly, previous models aim to improve density estimation by baking an \u201corderless\u201d inductive bias into the model while XLNet is motivated by enabling AR language models to learn bidirectional contexts. Technically, to construct a valid target-aware prediction distribution, XLNet incorporates the target position into the hidden state via two-stream attention while previous permutation-based AR models relied on implicit position awareness inherent to their MLP architectures. Finally, for both orderless NADE and XLNet, we would like to emphasize that \u201corderless\u201d does not mean that the input sequence can be randomly permuted but that the model allows for different factorization orders of the distribution."]}
