{"pkey": "mbart_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages. The paper authors also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus. The paper authors (1) assume access to a noising function g, defined below, that corrupts text, and (2) train the model to predict the original text X given g(X)", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["For machine translation, the most relevant field is multilingual translation (Firat et al., 2016; Vi\u00e9gas et al., 2016; Aharoni et al., 2019; Arivazhagan et al., 2019) where the ultimate goal is to jointly train one translation model that translates multiple language directions at the same time, and shares representations to improve the translation performance on low-resource languages (Gu et al., 2018). In this paper, we mainly focus on multilingualism in the pre-training stage and fine-tune the learned model in the standard bi-lingual scenario. Compared to multilingual translation, we do not require parallel data across multiple languages but the targeted direction, which potentially improves the scalability to low-resource languages and specific domains. Moreover, multilingual pre-training is unlikely to suffer the interference problems between dissimilar languages, which is typical for regular multilingual translation models. Document Translation As one of the key applications , this work also links to previous efforts for incorporating document-level contexts into neural machine translation (Wang et al., 2017; Jean et al., 2017; Tiedemann and Scherrer, 2017; Miculicich et al., 2018; Tu et al., 2018). Li et al.\n(2019) is the most relevant work which also utilized pre-trained encoder (BERT) for handling longer context. However, none of these works had shown positive results on pure Seq2Seq models at document-level, which involved task-specific techniques, and usually only worked on sentencelevel translation with a constrained range of context. To the extent of our knowledge, our multilingual pre-trained model is the first-of-its-kind work that shows improved results on documentlevel translation with standard Seq2Seq learning. Unsupervised Translation", "We use beam size 5 by default.\nBaselines & Evaluation We train 4 models: a document-level (Doc-) MT model (\u00a74.1) and a corresponded sentence-level (Sent-) MT model (\u00a73.1) as the baseline, both with and without pretraining. We use mBART25 as the common pretrained model for En-De and Zh-En. For En-De, even though our mBART25 Doc-MT model decodes multiple sentences together, the translated sentences can be aligned to the source sentences, which allows us to evaluate BLEU scores both on sentence-level (s-BLEU) and document-level (dBLEU) 2. For Zh-En, however, we cannot produce the same number of translated sentences as the reference due to alignment errors in the test data. We only provide the d-BLEU scores on this direction. We also compare our models with Hierarchical Attention Networks (HAN, Miculicich et al., 2018) on Zh-En, which is the state-of-the-art nonpretraining approach for document-level translation for this pair. They combine two layers of attention \u2013 first within and then across sentences. 4.2 Main Results. We show the main results for both En-De and ZhEn are presented in Table 9. Random v.s. Pre-trained The MT models initialized with pre-trained weights outperform randomly initialized models by large margins, for both sentence-level and document-level training. Our mBART25 models (both Sent-MT and DocMT) also outperform HAN (Miculicich et al.,\n2Standard BLEU scores match n-grams at sentence-level. We also consider document-level where we match n-grams over the whole document resulting in a slightly higher score.\n2018)3, despite the fact that they are not customized for document-level MT in any way. Sent-MT v.s. Doc-MT For cases (En-De, EnZh), the mBART25 Doc-MT models outperform themselves fine-tuned at sentence-level by a margin, which is completely opposite for models without pre-training. For both datasets, randomly initialized Doc-MT fail to work, resulting in much worse results than the sentence-level models.", "The models keep improving by over 3 BLEU for the rest of steps and have not fully converged after 500K steps. mBART25 is consistently\nslightly worse than mBART02. How does the size of bitexts inference the gain from pre-training? Tables 2 and 3 show that pre-training consistently improves for low and medium resource language pairs. To verify this trend, we plot performance for differing sized subsets of the En-De dataset. More precisely, we take the full En-De corpus (28M pairs) and randomly sample 10K, 50K, 100K, 500K, 1M, 5M, 10M datasets. We compare performance without pretraining to the mBART02 results, as shown in Figure 4. The pre-trained model is able to achieve over 20 BLEU with only 10K training examples, while the baseline system scores 0. Unsurprisingly, increasing the size of bi-text corpus improves both models. Our pre-trained model consistently outperforms the baseline models, but the gap reduces with increasing amounts of bi-text, especially after 10M sentence pairs. This result confirms our observation in \u00a73.2 that our pre-training does not help translation in high-resource pairs. Is pre-training complementary to BT? Figure 2 presents that our pre-trained models can be combined with iterative back-translation (BT) on additional data, however, it is still not a fair comparison. Table 6 shows the results when using\nsame monolingual data where we use 79M En and 29M My sentences following Chen et al. (2019). With the same amount of monolingual corpus, mBART pre-training achieves the same performance on En\u2192My as BT, while still 3 BLEU worse on My\u2192En. We suspect BT benefits from bigger monolingual data (En). Moreover, combining mBART02 model with BT, we see further gains even with same monolingual data. Besides, we also provide estimated training costs where BT has a longer pipeline involving training a baseline system (5h), translating monolingual data (300h) and formal training (350h)."]}
{"pkey": "mbart_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages. the paper authors demonstrate the effectiveness of multilingual pre-training in unsupervised machine translation via (1) back-translation ( \u00a75.1) and (3) language transfer (\u00a75.2)", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["The models keep improving by over 3 BLEU for the rest of steps and have not fully converged after 500K steps. mBART25 is consistently\nslightly worse than mBART02. How does the size of bitexts inference the gain from pre-training? Tables 2 and 3 show that pre-training consistently improves for low and medium resource language pairs. To verify this trend, we plot performance for differing sized subsets of the En-De dataset. More precisely, we take the full En-De corpus (28M pairs) and randomly sample 10K, 50K, 100K, 500K, 1M, 5M, 10M datasets. We compare performance without pretraining to the mBART02 results, as shown in Figure 4. The pre-trained model is able to achieve over 20 BLEU with only 10K training examples, while the baseline system scores 0. Unsurprisingly, increasing the size of bi-text corpus improves both models. Our pre-trained model consistently outperforms the baseline models, but the gap reduces with increasing amounts of bi-text, especially after 10M sentence pairs. This result confirms our observation in \u00a73.2 that our pre-training does not help translation in high-resource pairs. Is pre-training complementary to BT? Figure 2 presents that our pre-trained models can be combined with iterative back-translation (BT) on additional data, however, it is still not a fair comparison. Table 6 shows the results when using\nsame monolingual data where we use 79M En and 29M My sentences following Chen et al. (2019). With the same amount of monolingual corpus, mBART pre-training achieves the same performance on En\u2192My as BT, while still 3 BLEU worse on My\u2192En. We suspect BT benefits from bigger monolingual data (En). Moreover, combining mBART02 model with BT, we see further gains even with same monolingual data. Besides, we also provide estimated training costs where BT has a longer pipeline involving training a baseline system (5h), translating monolingual data (300h) and formal training (350h).", "Such large performance gaps indicate that pre-training is critical for document level performance. It is in general difficult to collect high quality documentlevel data in large quantities, suggesting that pretraining may be a strong strategy for future work. We also include a sampled example in appendix B.\n5 Unsupervised Machine Translation. In addition to supervised machine translation, we also evaluate our model on tasks where no bi-text is available for the target language pair. We define three types of unsupervised translation:\n1. No bi-text of any kind is given. A common solution is to learn from back-translation (BT) (Artetxe et al., 2017; Lample et al., 2018c). We show that mBART provides a simple and effective initialize scheme for these methods. 2. No bi-text for the target pair is available, but the target languages both appear in bi-text corpora for other language pairs. Previous work has shown that zero-shot transfer is possible via massively multi-lingual MT (Johnson et al., 2017; Gu et al., 2019) or distillation through pivoting (Chen et al., 2017). We limit our focus to building MT models for single language pairs, and leave multi-lingual pre-training for multi-lingual MT to future work. 3. No bi-text for the target pair is available, but there is bi-text for translating from some other 3d-BLEU is recomputed from the provided system output. language into the target language. This is a new evaluation regime, where we will show that mBART supports effective transfer, even if the source language has no bi-text of any form. In this section, we demonstrate the effectiveness of multilingual pre-training in unsupervised machine translation via (1) back-translation ( \u00a75.1) and (3) language transfer (\u00a75.2). An illustration of both approaches are presented in Figure 5.\n5.1 Unsupervised Machine Translation via Back-Translation.", "Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text."]}
{"pkey": "mbart_3", "question": "What are the main contributions of the paper?", "answer": "mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages. The paper authors also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus. \n\nThe paper authors evaluate performance on two common document-level MT datasets: WMT19 En-De and TED15 Zh-En. The paper authors analyze the results of three pairs: Nl-En, Ar-En and De-Nl using the pre-trained mBART25, mBART06 and mBART02 (EnRo) models. BART-En/Ro To help establish baseline performance levels, the paper authors also train monolingual BART models on the same En and Ro corpus only.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["We use beam size 5 by default.\nBaselines & Evaluation We train 4 models: a document-level (Doc-) MT model (\u00a74.1) and a corresponded sentence-level (Sent-) MT model (\u00a73.1) as the baseline, both with and without pretraining. We use mBART25 as the common pretrained model for En-De and Zh-En. For En-De, even though our mBART25 Doc-MT model decodes multiple sentences together, the translated sentences can be aligned to the source sentences, which allows us to evaluate BLEU scores both on sentence-level (s-BLEU) and document-level (dBLEU) 2. For Zh-En, however, we cannot produce the same number of translated sentences as the reference due to alignment errors in the test data. We only provide the d-BLEU scores on this direction. We also compare our models with Hierarchical Attention Networks (HAN, Miculicich et al., 2018) on Zh-En, which is the state-of-the-art nonpretraining approach for document-level translation for this pair. They combine two layers of attention \u2013 first within and then across sentences. 4.2 Main Results. We show the main results for both En-De and ZhEn are presented in Table 9. Random v.s. Pre-trained The MT models initialized with pre-trained weights outperform randomly initialized models by large margins, for both sentence-level and document-level training. Our mBART25 models (both Sent-MT and DocMT) also outperform HAN (Miculicich et al.,\n2Standard BLEU scores match n-grams at sentence-level. We also consider document-level where we match n-grams over the whole document resulting in a slightly higher score.\n2018)3, despite the fact that they are not customized for document-level MT in any way. Sent-MT v.s. Doc-MT For cases (En-De, EnZh), the mBART25 Doc-MT models outperform themselves fine-tuned at sentence-level by a margin, which is completely opposite for models without pre-training. For both datasets, randomly initialized Doc-MT fail to work, resulting in much worse results than the sentence-level models.", "Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text.", "Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. 1 Introduction."]}
{"pkey": "mbart_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["For machine translation, the most relevant field is multilingual translation (Firat et al., 2016; Vi\u00e9gas et al., 2016; Aharoni et al., 2019; Arivazhagan et al., 2019) where the ultimate goal is to jointly train one translation model that translates multiple language directions at the same time, and shares representations to improve the translation performance on low-resource languages (Gu et al., 2018). In this paper, we mainly focus on multilingualism in the pre-training stage and fine-tune the learned model in the standard bi-lingual scenario. Compared to multilingual translation, we do not require parallel data across multiple languages but the targeted direction, which potentially improves the scalability to low-resource languages and specific domains. Moreover, multilingual pre-training is unlikely to suffer the interference problems between dissimilar languages, which is typical for regular multilingual translation models. Document Translation As one of the key applications , this work also links to previous efforts for incorporating document-level contexts into neural machine translation (Wang et al., 2017; Jean et al., 2017; Tiedemann and Scherrer, 2017; Miculicich et al., 2018; Tu et al., 2018). Li et al.\n(2019) is the most relevant work which also utilized pre-trained encoder (BERT) for handling longer context. However, none of these works had shown positive results on pure Seq2Seq models at document-level, which involved task-specific techniques, and usually only worked on sentencelevel translation with a constrained range of context. To the extent of our knowledge, our multilingual pre-trained model is the first-of-its-kind work that shows improved results on documentlevel translation with standard Seq2Seq learning. Unsupervised Translation", "v.s. Other Pre-training Approaches We also compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only pair with established results. Our mBART model\noutperforms all the other pre-trained models, both with and without BT augmentation. We also show comparisons with the conventional BART model trained on the same En and Ro data only. Both have improvements over baselines, while worse than mBART results, indicating pre-training in a multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a new state-of-the-art for Ro-En translation. 3.3 Analysis. We also present additional analysis, to better quantify when our pre-training helps. How many languages should you pre-train on? We investigate when it is helpful for pre-training to include languages other than the targeted language pair that will be used during fine tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the target language monolingual data is limited (e.g. En-My, the size of My is around 0.5% of En). In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages slightly hurts the final results (<1 BLEU). In these cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to mBART02 on Ro-En suggests that pre-training with similar languages is particularly helpful. How many pre-training steps are needed? We plot Ro-En BLEU score v.s. Pre-training steps in Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same fine-tuning process described in \u00a73.1. Without any pre-training, our model overfits and performs much worse than the baseline. However, after just 25K steps (5% of training), both models outperform the best baseline.", "The models keep improving by over 3 BLEU for the rest of steps and have not fully converged after 500K steps. mBART25 is consistently\nslightly worse than mBART02. How does the size of bitexts inference the gain from pre-training? Tables 2 and 3 show that pre-training consistently improves for low and medium resource language pairs. To verify this trend, we plot performance for differing sized subsets of the En-De dataset. More precisely, we take the full En-De corpus (28M pairs) and randomly sample 10K, 50K, 100K, 500K, 1M, 5M, 10M datasets. We compare performance without pretraining to the mBART02 results, as shown in Figure 4. The pre-trained model is able to achieve over 20 BLEU with only 10K training examples, while the baseline system scores 0. Unsurprisingly, increasing the size of bi-text corpus improves both models. Our pre-trained model consistently outperforms the baseline models, but the gap reduces with increasing amounts of bi-text, especially after 10M sentence pairs. This result confirms our observation in \u00a73.2 that our pre-training does not help translation in high-resource pairs. Is pre-training complementary to BT? Figure 2 presents that our pre-trained models can be combined with iterative back-translation (BT) on additional data, however, it is still not a fair comparison. Table 6 shows the results when using\nsame monolingual data where we use 79M En and 29M My sentences following Chen et al. (2019). With the same amount of monolingual corpus, mBART pre-training achieves the same performance on En\u2192My as BT, while still 3 BLEU worse on My\u2192En. We suspect BT benefits from bigger monolingual data (En). Moreover, combining mBART02 model with BT, we see further gains even with same monolingual data. Besides, we also provide estimated training costs where BT has a longer pipeline involving training a baseline system (5h), translating monolingual data (300h) and formal training (350h)."]}
{"pkey": "mbart_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors evaluate performance on two common document-level MT datasets: WMT19 En-De and TED15 Zh-En. The paper authors also use FLoRes pairs (Guzm\u00e1n et al., 2019, En-Ne and EnSi), En-Hi from IITB (Kunchukuttan et al., 2017), and En-My from WAT19 (Ding et al., 2018, 2019). The paper authors divide the datasets into three categories \u2013 low resource (<1M sentence pairs), medium resource (>1M and <10M), and high resource (>10M). Zh-En dataset is from the IWSLT 2014 and 2015 evaluation campaigns.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["The models keep improving by over 3 BLEU for the rest of steps and have not fully converged after 500K steps. mBART25 is consistently\nslightly worse than mBART02. How does the size of bitexts inference the gain from pre-training? Tables 2 and 3 show that pre-training consistently improves for low and medium resource language pairs. To verify this trend, we plot performance for differing sized subsets of the En-De dataset. More precisely, we take the full En-De corpus (28M pairs) and randomly sample 10K, 50K, 100K, 500K, 1M, 5M, 10M datasets. We compare performance without pretraining to the mBART02 results, as shown in Figure 4. The pre-trained model is able to achieve over 20 BLEU with only 10K training examples, while the baseline system scores 0. Unsurprisingly, increasing the size of bi-text corpus improves both models. Our pre-trained model consistently outperforms the baseline models, but the gap reduces with increasing amounts of bi-text, especially after 10M sentence pairs. This result confirms our observation in \u00a73.2 that our pre-training does not help translation in high-resource pairs. Is pre-training complementary to BT? Figure 2 presents that our pre-trained models can be combined with iterative back-translation (BT) on additional data, however, it is still not a fair comparison. Table 6 shows the results when using\nsame monolingual data where we use 79M En and 29M My sentences following Chen et al. (2019). With the same amount of monolingual corpus, mBART pre-training achieves the same performance on En\u2192My as BT, while still 3 BLEU worse on My\u2192En. We suspect BT benefits from bigger monolingual data (En). Moreover, combining mBART02 model with BT, we see further gains even with same monolingual data. Besides, we also provide estimated training costs where BT has a longer pipeline involving training a baseline system (5h), translating monolingual data (300h) and formal training (350h).", "v.s. Other Pre-training Approaches We also compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only pair with established results. Our mBART model\noutperforms all the other pre-trained models, both with and without BT augmentation. We also show comparisons with the conventional BART model trained on the same En and Ro data only. Both have improvements over baselines, while worse than mBART results, indicating pre-training in a multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a new state-of-the-art for Ro-En translation. 3.3 Analysis. We also present additional analysis, to better quantify when our pre-training helps. How many languages should you pre-train on? We investigate when it is helpful for pre-training to include languages other than the targeted language pair that will be used during fine tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the target language monolingual data is limited (e.g. En-My, the size of My is around 0.5% of En). In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages slightly hurts the final results (<1 BLEU). In these cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to mBART02 on Ro-En suggests that pre-training with similar languages is particularly helpful. How many pre-training steps are needed? We plot Ro-En BLEU score v.s. Pre-training steps in Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same fine-tuning process described in \u00a73.1. Without any pre-training, our model overfits and performs much worse than the baseline. However, after just 25K steps (5% of training), both models outperform the best baseline.", "Furthermore, although the En-X pairs perform similarly, mBART06 outperforms mBART02 by a margin on X-En pairs. Fine-tuning unseen languages on source side is more difficult, deserving more extensive future study. 4 Document-level Machine Translation. We evaluate mBART on document-level machine translation tasks, where the goal is to translate segments of text that contain more than one sentence (up to an entire document). During pre-training, we use document fragments of up to 512 tokens, allowing the models to learn dependencies between sentences. We show that this pre-training significantly improves document-level translation.\n4.1 Experimental Settings. Datasets We evaluate performance on two common document-level MT datasets: WMT19 En-De and TED15 Zh-En (statistics in Table 8). For EnDe, we use the document data from WMT19 to train our model, without any additional sentencelevel data; Zh-En dataset is from the IWSLT 2014 and 2015 evaluation campaigns (Cettolo et al., 2012, 2015). Following Miculicich et al. (2018), we use 2010-2013 TED as the test set. Pre-processing We use the same pre-processing as that in pre-training. For each block, sentences are separated by end of sentence symbols (</S>) and the entire instance is ended with the specific language id (<LID>). The numbers of segmented instances are also shown in Table 8 where on average, every document is split into 2-4 instances. Fine-tuning & Decoding We use the same finetuning scheme as for sentence-level translation (\u00a73.1), without using any task-specific techniques developed by previous work (Miculicich et al.,\n2018; Li et al., 2019), such as constrained contexts or restricted attention. For decoding, we simply pack the source sentences into blocks, and translate each instance block autoregressively. The model does not know how many sentences to generate in advance and decoding stops when <LID> is predicted."]}
{"pkey": "mbart_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "For instance (the 3rd example), only the supervised model translates \u201c\uc790\uc11d\u201d into \u201cmagents\u201d correctly, while the Ja-En and Zh-En guess with irreverent words \u201ccushions\u201d and \u201cjellyfish\u201d, respectively. Also, in the 2nd example, the Ko-En model fails to translate \u201cdeveloped\u201d and copies the source tokens. The paper authors suspect it is because the pre-training stage biases the output distribution.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["The models keep improving by over 3 BLEU for the rest of steps and have not fully converged after 500K steps. mBART25 is consistently\nslightly worse than mBART02. How does the size of bitexts inference the gain from pre-training? Tables 2 and 3 show that pre-training consistently improves for low and medium resource language pairs. To verify this trend, we plot performance for differing sized subsets of the En-De dataset. More precisely, we take the full En-De corpus (28M pairs) and randomly sample 10K, 50K, 100K, 500K, 1M, 5M, 10M datasets. We compare performance without pretraining to the mBART02 results, as shown in Figure 4. The pre-trained model is able to achieve over 20 BLEU with only 10K training examples, while the baseline system scores 0. Unsurprisingly, increasing the size of bi-text corpus improves both models. Our pre-trained model consistently outperforms the baseline models, but the gap reduces with increasing amounts of bi-text, especially after 10M sentence pairs. This result confirms our observation in \u00a73.2 that our pre-training does not help translation in high-resource pairs. Is pre-training complementary to BT? Figure 2 presents that our pre-trained models can be combined with iterative back-translation (BT) on additional data, however, it is still not a fair comparison. Table 6 shows the results when using\nsame monolingual data where we use 79M En and 29M My sentences following Chen et al. (2019). With the same amount of monolingual corpus, mBART pre-training achieves the same performance on En\u2192My as BT, while still 3 BLEU worse on My\u2192En. We suspect BT benefits from bigger monolingual data (En). Moreover, combining mBART02 model with BT, we see further gains even with same monolingual data. Besides, we also provide estimated training costs where BT has a longer pipeline involving training a baseline system (5h), translating monolingual data (300h) and formal training (350h).", "v.s. Other Pre-training Approaches We also compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only pair with established results. Our mBART model\noutperforms all the other pre-trained models, both with and without BT augmentation. We also show comparisons with the conventional BART model trained on the same En and Ro data only. Both have improvements over baselines, while worse than mBART results, indicating pre-training in a multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a new state-of-the-art for Ro-En translation. 3.3 Analysis. We also present additional analysis, to better quantify when our pre-training helps. How many languages should you pre-train on? We investigate when it is helpful for pre-training to include languages other than the targeted language pair that will be used during fine tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the target language monolingual data is limited (e.g. En-My, the size of My is around 0.5% of En). In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages slightly hurts the final results (<1 BLEU). In these cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to mBART02 on Ro-En suggests that pre-training with similar languages is particularly helpful. How many pre-training steps are needed? We plot Ro-En BLEU score v.s. Pre-training steps in Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same fine-tuning process described in \u00a73.1. Without any pre-training, our model overfits and performs much worse than the baseline. However, after just 25K steps (5% of training), both models outperform the best baseline.", "Furthermore, although the En-X pairs perform similarly, mBART06 outperforms mBART02 by a margin on X-En pairs. Fine-tuning unseen languages on source side is more difficult, deserving more extensive future study. 4 Document-level Machine Translation. We evaluate mBART on document-level machine translation tasks, where the goal is to translate segments of text that contain more than one sentence (up to an entire document). During pre-training, we use document fragments of up to 512 tokens, allowing the models to learn dependencies between sentences. We show that this pre-training significantly improves document-level translation.\n4.1 Experimental Settings. Datasets We evaluate performance on two common document-level MT datasets: WMT19 En-De and TED15 Zh-En (statistics in Table 8). For EnDe, we use the document data from WMT19 to train our model, without any additional sentencelevel data; Zh-En dataset is from the IWSLT 2014 and 2015 evaluation campaigns (Cettolo et al., 2012, 2015). Following Miculicich et al. (2018), we use 2010-2013 TED as the test set. Pre-processing We use the same pre-processing as that in pre-training. For each block, sentences are separated by end of sentence symbols (</S>) and the entire instance is ended with the specific language id (<LID>). The numbers of segmented instances are also shown in Table 8 where on average, every document is split into 2-4 instances. Fine-tuning & Decoding We use the same finetuning scheme as for sentence-level translation (\u00a73.1), without using any task-specific techniques developed by previous work (Miculicich et al.,\n2018; Li et al., 2019), such as constrained contexts or restricted attention. For decoding, we simply pack the source sentences into blocks, and translate each instance block autoregressively. The model does not know how many sentences to generate in advance and decoding stops when <LID> is predicted."]}
{"pkey": "mbart_7", "question": "List the limitations of the model discussed in the paper.", "answer": "The paper authors use a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). Pre-training consistently improves over a randomly initialized baseline, with particularly large gains on low resource language pairs. Compared to multilingual translation, the paper authors do not require parallel data across multiple languages but the targeted direction, which potentially improves the scalability to low-resource languages and specific domains.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["v.s. Other Pre-training Approaches We also compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only pair with established results. Our mBART model\noutperforms all the other pre-trained models, both with and without BT augmentation. We also show comparisons with the conventional BART model trained on the same En and Ro data only. Both have improvements over baselines, while worse than mBART results, indicating pre-training in a multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a new state-of-the-art for Ro-En translation. 3.3 Analysis. We also present additional analysis, to better quantify when our pre-training helps. How many languages should you pre-train on? We investigate when it is helpful for pre-training to include languages other than the targeted language pair that will be used during fine tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the target language monolingual data is limited (e.g. En-My, the size of My is around 0.5% of En). In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages slightly hurts the final results (<1 BLEU). In these cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to mBART02 on Ro-En suggests that pre-training with similar languages is particularly helpful. How many pre-training steps are needed? We plot Ro-En BLEU score v.s. Pre-training steps in Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same fine-tuning process described in \u00a73.1. Without any pre-training, our model overfits and performs much worse than the baseline. However, after just 25K steps (5% of training), both models outperform the best baseline.", "Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text.", "The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models. 8 Acknowledgements. We thank Marc\u2019Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun Shen for details about FloRes and WAT datasets. We also thank our colleagues at FAIR and FAIAR for valuable feedback. A Evaluation Details. For all our tasks, we use BLEU scores (Papineni et al., 2002) as the automatic metric to evaluate the translation performance. Normally, we compute the BLEU scores over tokenized text for both system outputs and the references, and we apply language-wise tokenization after over the translation. Note that, since we directly work on raw texts, we automatically get de-tokenized output after recovering sentence-piece subwords. Following the literature, the instructions of language-wise tokenization are as follows:\n\u2022 Gu, Ne, Si, Hi: We use Indic-NLP Library 5 to tokenize the Indic language outputs. \u2022 Ja: We use KyTea 6 to segment Japanese texts. \u2022 Ko: We use Mecab-Ko 7 and its default dictionary to segment the Korean texts\n\u2022 Ar: We apply QCRI Arabic Normalizer 8 over the Arabic texts. \u2022 My: We use the official segmentation tool provided by Ding et al. (2019) for Burmese. \u2022 Ro: Following Sennrich et al. (2016a), we apply Moses tokenization and special normalization for Romanian texts 9. \u2022 Zh: We use the official sacreBleu (Post, 2018)10 Chinese tokenizer (\u2013tok zh). For other languages that are not listed above, we compute BLEU scores with sacreBLEU with DEFAULT tokenization. B Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/ 6http://www.phontron.com/kytea/ 7http://konlpy.org/en/v0.3.0/install/ 8http://alt.qcri.org/tools/arabic-normalizer/ 9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU"]}
{"pkey": "mbart_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors pre-train on a subset of 25 languages \u2013 CC25 \u2013 extracted from the Common Crawl (CC).", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["It is also possible to use other noise types, such as those in Lample et al. (2018c), but we leave the exploration of the optimal noising strategy to future work. Instance format For each instance of a batch, we sample a language id symbol <LID>, and we pack as many consecutive sentences as possible sampled from the corresponding corpus of <LID>, until either it hits the document boundary or reaches the 512 max token length. Sentences in the instance are separated by the end of sentence (</S>) token. Then, we append the selected <LID> token to represent the end of this instance. Pre-training at \u201cmulti-sentence\u201d level enables us to work on both sentence and document translation. Optimization Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU, matching BART (Lewis et al., 2019) configuration. We use the Adam optimizer ( = 1e\u22126, \u03b22 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks. We started the training with dropout 0.1 and reduced it to 0.05 at 250K steps and 0 at 400K steps. All experiments are done with Fairseq (Ott et al., 2019). 2.3 Pre-trained Models. To better measure the effects of different levels of multilinguality during pre-training, we built a range of models as follows:\n\u2022 mBART25 We pre-train a model on all 25 languages, using the setting described in \u00a72.2.\n\u2022 mBART06 To explore the effect of pre-training on related languages, we pretrain a model on a subset of six European languages: Ro, It, Cs, Fr, Es and En. For a fair comparison, we use\u223c 1/4 of the mBART25 batch size, which allows our model to have the same number of updates per language during pre-training. \u2022 mBART02 We pre-train bilingual models, using English and one other language for four language pairs: En-De, En-Ro, En-It. We use a batch size of \u223c 1/12 of that in the mBART25.", "We use the smoothing parameter \u03b1 = 0.7. Pre-processing We tokenize with a sentencepiece model (SPM, Kudo and Richardson, 2018) learned on the full CC data that includes 250, 000 subword tokens. While not all of these languages are used for pre-training, this tokenization supports fine-tuning on additional languages. We do not apply additional preprocessing, such as truecasing or normalizing punctuation/characters. 2.2 Model: mBART. Our models follow the BART (Lewis et al., 2019) sequence-to-sequence pre-training scheme, as reviewed in this section. While BART was only pretrained for English, we systematically study the effects of pre-training on different sets of languages. 1https://commoncrawl.org\nArchitecture We use a standard sequence-tosequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). We include an additional layer-normalization layer on top of both the encoder and decoder, which we found stabilized training at FP16 precision. Learning Our training data coversK languages: D = {D1, ...,DK} where each Di is a collection of monolingual documents in language i. We (1) assume access to a noising function g, defined below, that corrupts text, and (2) train the model to predict the original text X given g(X). More formally, we aim to maximize L\u03b8: L\u03b8 = \u2211 Di\u2208D \u2211 X\u2208Di logP (X|g(X); \u03b8) , (2)\nwhere X is an instance in language i and the distribution P is defined by the Seq2Seq model. Noise function Following Lewis et al. (2019), we use two types of noise in g. We first remove spans of text and replace them with a mask token. We mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). We also permute the order of sentences within each instance. The decoder input is the original text with one position offset. A language id symbol <LID> is used as the initial token to predict the sentence.", "The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models. 8 Acknowledgements. We thank Marc\u2019Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun Shen for details about FloRes and WAT datasets. We also thank our colleagues at FAIR and FAIAR for valuable feedback. A Evaluation Details. For all our tasks, we use BLEU scores (Papineni et al., 2002) as the automatic metric to evaluate the translation performance. Normally, we compute the BLEU scores over tokenized text for both system outputs and the references, and we apply language-wise tokenization after over the translation. Note that, since we directly work on raw texts, we automatically get de-tokenized output after recovering sentence-piece subwords. Following the literature, the instructions of language-wise tokenization are as follows:\n\u2022 Gu, Ne, Si, Hi: We use Indic-NLP Library 5 to tokenize the Indic language outputs. \u2022 Ja: We use KyTea 6 to segment Japanese texts. \u2022 Ko: We use Mecab-Ko 7 and its default dictionary to segment the Korean texts\n\u2022 Ar: We apply QCRI Arabic Normalizer 8 over the Arabic texts. \u2022 My: We use the official segmentation tool provided by Ding et al. (2019) for Burmese. \u2022 Ro: Following Sennrich et al. (2016a), we apply Moses tokenization and special normalization for Romanian texts 9. \u2022 Zh: We use the official sacreBleu (Post, 2018)10 Chinese tokenizer (\u2013tok zh). For other languages that are not listed above, we compute BLEU scores with sacreBLEU with DEFAULT tokenization. B Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/ 6http://www.phontron.com/kytea/ 7http://konlpy.org/en/v0.3.0/install/ 8http://alt.qcri.org/tools/arabic-normalizer/ 9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU"]}
{"pkey": "mbart_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors tokenize with a sentencepiece model (SPM, Kudo and Richardson, 2018) learned on the full CC data that includes 250, 000 subword tokens", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models. 8 Acknowledgements. We thank Marc\u2019Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun Shen for details about FloRes and WAT datasets. We also thank our colleagues at FAIR and FAIAR for valuable feedback. A Evaluation Details. For all our tasks, we use BLEU scores (Papineni et al., 2002) as the automatic metric to evaluate the translation performance. Normally, we compute the BLEU scores over tokenized text for both system outputs and the references, and we apply language-wise tokenization after over the translation. Note that, since we directly work on raw texts, we automatically get de-tokenized output after recovering sentence-piece subwords. Following the literature, the instructions of language-wise tokenization are as follows:\n\u2022 Gu, Ne, Si, Hi: We use Indic-NLP Library 5 to tokenize the Indic language outputs. \u2022 Ja: We use KyTea 6 to segment Japanese texts. \u2022 Ko: We use Mecab-Ko 7 and its default dictionary to segment the Korean texts\n\u2022 Ar: We apply QCRI Arabic Normalizer 8 over the Arabic texts. \u2022 My: We use the official segmentation tool provided by Ding et al. (2019) for Burmese. \u2022 Ro: Following Sennrich et al. (2016a), we apply Moses tokenization and special normalization for Romanian texts 9. \u2022 Zh: We use the official sacreBleu (Post, 2018)10 Chinese tokenizer (\u2013tok zh). For other languages that are not listed above, we compute BLEU scores with sacreBLEU with DEFAULT tokenization. B Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/ 6http://www.phontron.com/kytea/ 7http://konlpy.org/en/v0.3.0/install/ 8http://alt.qcri.org/tools/arabic-normalizer/ 9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU", "\u2022 BART-En/Ro To help establish baseline performance levels, we also train monolingual BART models on the same En and Ro corpus only. \u2022 Random As additional baselines, we will also include a comparison with a model randomly initialized without pre-training for each translation task. Since the sizes of different downstream datasets vary, we always grid-search the hyper-parameters (architecture, dropout, etc.) to find the best non-pretrained configuration. All models use the same vocabulary (\u00a72.1). Not all tokens will frequently occur in all pre-training corpora, but later experiments show that this large vocabulary can improve generalization in multilingual settings even for unseen languages. 3 Sentence-level Machine Translation. This section shows that mBART pre-training provides consistent performance gains in low to medium resource sentence-level MT settings, including bi-text only and with back translation, and outperforms other existing pre-training schemes (\u00a73.2). We also present a detailed analysis to understand better which factors contribute the most to these gains (\u00a73.3), and show that pre-training can even improve performance for languages not present in the pre-training data at all (\u00a73.4). 3.1 Experimental Settings. Datasets We gather 24 pairs of publicly available parallel corpora that cover all the languages in CC25 (Table 1). Most pairs are from previous WMT (Gu, Kk, Tr, Ro, Et, Lt, Fi, Lv, Cs, Es, Zh, De, Ru, Fr \u2194 En) and IWSLT (Vi, Ja, Ko, Nl, Ar, It \u2194 En) competitions. We also use FLoRes pairs (Guzm\u00e1n et al., 2019, En-Ne and EnSi), En-Hi from IITB (Kunchukuttan et al., 2017),\nand En-My from WAT19 (Ding et al., 2018, 2019). We divide the datasets into three categories \u2013 low resource (<1M sentence pairs), medium resource (>1M and <10M), and high resource (>10M). Fine-tuning & Decoding We fine-tune our multilingual pre-trained models on a single pair of bitext data, feeding the source language into the encoder and decoding the target language.", "It is also possible to use other noise types, such as those in Lample et al. (2018c), but we leave the exploration of the optimal noising strategy to future work. Instance format For each instance of a batch, we sample a language id symbol <LID>, and we pack as many consecutive sentences as possible sampled from the corresponding corpus of <LID>, until either it hits the document boundary or reaches the 512 max token length. Sentences in the instance are separated by the end of sentence (</S>) token. Then, we append the selected <LID> token to represent the end of this instance. Pre-training at \u201cmulti-sentence\u201d level enables us to work on both sentence and document translation. Optimization Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU, matching BART (Lewis et al., 2019) configuration. We use the Adam optimizer ( = 1e\u22126, \u03b22 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks. We started the training with dropout 0.1 and reduced it to 0.05 at 250K steps and 0 at 400K steps. All experiments are done with Fairseq (Ott et al., 2019). 2.3 Pre-trained Models. To better measure the effects of different levels of multilinguality during pre-training, we built a range of models as follows:\n\u2022 mBART25 We pre-train a model on all 25 languages, using the setting described in \u00a72.2.\n\u2022 mBART06 To explore the effect of pre-training on related languages, we pretrain a model on a subset of six European languages: Ro, It, Cs, Fr, Es and En. For a fair comparison, we use\u223c 1/4 of the mBART25 batch size, which allows our model to have the same number of updates per language during pre-training. \u2022 mBART02 We pre-train bilingual models, using English and one other language for four language pairs: En-De, En-Ro, En-It. We use a batch size of \u223c 1/12 of that in the mBART25."]}
{"pkey": "mbart_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "The paper authors tokenize with a sentencepiece model (SPM, Kudo and Richardson, 2018) learned on the full CC data that includes 250, 000 subword tokens. While not all of these languages are used for pre-training, this tokenization supports fine-tuning on additional languages. The paper authors do not apply additional preprocessing, such as truecasing or normalizing punctuation/characters.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models. 8 Acknowledgements. We thank Marc\u2019Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun Shen for details about FloRes and WAT datasets. We also thank our colleagues at FAIR and FAIAR for valuable feedback. A Evaluation Details. For all our tasks, we use BLEU scores (Papineni et al., 2002) as the automatic metric to evaluate the translation performance. Normally, we compute the BLEU scores over tokenized text for both system outputs and the references, and we apply language-wise tokenization after over the translation. Note that, since we directly work on raw texts, we automatically get de-tokenized output after recovering sentence-piece subwords. Following the literature, the instructions of language-wise tokenization are as follows:\n\u2022 Gu, Ne, Si, Hi: We use Indic-NLP Library 5 to tokenize the Indic language outputs. \u2022 Ja: We use KyTea 6 to segment Japanese texts. \u2022 Ko: We use Mecab-Ko 7 and its default dictionary to segment the Korean texts\n\u2022 Ar: We apply QCRI Arabic Normalizer 8 over the Arabic texts. \u2022 My: We use the official segmentation tool provided by Ding et al. (2019) for Burmese. \u2022 Ro: Following Sennrich et al. (2016a), we apply Moses tokenization and special normalization for Romanian texts 9. \u2022 Zh: We use the official sacreBleu (Post, 2018)10 Chinese tokenizer (\u2013tok zh). For other languages that are not listed above, we compute BLEU scores with sacreBLEU with DEFAULT tokenization. B Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/ 6http://www.phontron.com/kytea/ 7http://konlpy.org/en/v0.3.0/install/ 8http://alt.qcri.org/tools/arabic-normalizer/ 9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU", "We use the smoothing parameter \u03b1 = 0.7. Pre-processing We tokenize with a sentencepiece model (SPM, Kudo and Richardson, 2018) learned on the full CC data that includes 250, 000 subword tokens. While not all of these languages are used for pre-training, this tokenization supports fine-tuning on additional languages. We do not apply additional preprocessing, such as truecasing or normalizing punctuation/characters. 2.2 Model: mBART. Our models follow the BART (Lewis et al., 2019) sequence-to-sequence pre-training scheme, as reviewed in this section. While BART was only pretrained for English, we systematically study the effects of pre-training on different sets of languages. 1https://commoncrawl.org\nArchitecture We use a standard sequence-tosequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). We include an additional layer-normalization layer on top of both the encoder and decoder, which we found stabilized training at FP16 precision. Learning Our training data coversK languages: D = {D1, ...,DK} where each Di is a collection of monolingual documents in language i. We (1) assume access to a noising function g, defined below, that corrupts text, and (2) train the model to predict the original text X given g(X). More formally, we aim to maximize L\u03b8: L\u03b8 = \u2211 Di\u2208D \u2211 X\u2208Di logP (X|g(X); \u03b8) , (2)\nwhere X is an instance in language i and the distribution P is defined by the Seq2Seq model. Noise function Following Lewis et al. (2019), we use two types of noise in g. We first remove spans of text and replace them with a mask token. We mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). We also permute the order of sentences within each instance. The decoder input is the original text with one position offset. A language id symbol <LID> is used as the initial token to predict the sentence.", "Furthermore, although the En-X pairs perform similarly, mBART06 outperforms mBART02 by a margin on X-En pairs. Fine-tuning unseen languages on source side is more difficult, deserving more extensive future study. 4 Document-level Machine Translation. We evaluate mBART on document-level machine translation tasks, where the goal is to translate segments of text that contain more than one sentence (up to an entire document). During pre-training, we use document fragments of up to 512 tokens, allowing the models to learn dependencies between sentences. We show that this pre-training significantly improves document-level translation.\n4.1 Experimental Settings. Datasets We evaluate performance on two common document-level MT datasets: WMT19 En-De and TED15 Zh-En (statistics in Table 8). For EnDe, we use the document data from WMT19 to train our model, without any additional sentencelevel data; Zh-En dataset is from the IWSLT 2014 and 2015 evaluation campaigns (Cettolo et al., 2012, 2015). Following Miculicich et al. (2018), we use 2010-2013 TED as the test set. Pre-processing We use the same pre-processing as that in pre-training. For each block, sentences are separated by end of sentence symbols (</S>) and the entire instance is ended with the specific language id (<LID>). The numbers of segmented instances are also shown in Table 8 where on average, every document is split into 2-4 instances. Fine-tuning & Decoding We use the same finetuning scheme as for sentence-level translation (\u00a73.1), without using any task-specific techniques developed by previous work (Miculicich et al.,\n2018; Li et al., 2019), such as constrained contexts or restricted attention. For decoding, we simply pack the source sentences into blocks, and translate each instance block autoregressively. The model does not know how many sentences to generate in advance and decoding stops when <LID> is predicted."]}
{"pkey": "mbart_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "The paper authors use a standard sequence-to-sequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). Pre-training consistently improves over a randomly initialized baseline, with particularly large gains on low resource language pairs. Compared to multilingual translation, the paper authors do not require parallel data across multiple languages but the targeted direction, which potentially improves the scalability to low-resource languages and specific domains.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["We use the smoothing parameter \u03b1 = 0.7. Pre-processing We tokenize with a sentencepiece model (SPM, Kudo and Richardson, 2018) learned on the full CC data that includes 250, 000 subword tokens. While not all of these languages are used for pre-training, this tokenization supports fine-tuning on additional languages. We do not apply additional preprocessing, such as truecasing or normalizing punctuation/characters. 2.2 Model: mBART. Our models follow the BART (Lewis et al., 2019) sequence-to-sequence pre-training scheme, as reviewed in this section. While BART was only pretrained for English, we systematically study the effects of pre-training on different sets of languages. 1https://commoncrawl.org\nArchitecture We use a standard sequence-tosequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). We include an additional layer-normalization layer on top of both the encoder and decoder, which we found stabilized training at FP16 precision. Learning Our training data coversK languages: D = {D1, ...,DK} where each Di is a collection of monolingual documents in language i. We (1) assume access to a noising function g, defined below, that corrupts text, and (2) train the model to predict the original text X given g(X). More formally, we aim to maximize L\u03b8: L\u03b8 = \u2211 Di\u2208D \u2211 X\u2208Di logP (X|g(X); \u03b8) , (2)\nwhere X is an instance in language i and the distribution P is defined by the Seq2Seq model. Noise function Following Lewis et al. (2019), we use two types of noise in g. We first remove spans of text and replace them with a mask token. We mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). We also permute the order of sentences within each instance. The decoder input is the original text with one position offset. A language id symbol <LID> is used as the initial token to predict the sentence.", "It is also possible to use other noise types, such as those in Lample et al. (2018c), but we leave the exploration of the optimal noising strategy to future work. Instance format For each instance of a batch, we sample a language id symbol <LID>, and we pack as many consecutive sentences as possible sampled from the corresponding corpus of <LID>, until either it hits the document boundary or reaches the 512 max token length. Sentences in the instance are separated by the end of sentence (</S>) token. Then, we append the selected <LID> token to represent the end of this instance. Pre-training at \u201cmulti-sentence\u201d level enables us to work on both sentence and document translation. Optimization Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU, matching BART (Lewis et al., 2019) configuration. We use the Adam optimizer ( = 1e\u22126, \u03b22 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks. We started the training with dropout 0.1 and reduced it to 0.05 at 250K steps and 0 at 400K steps. All experiments are done with Fairseq (Ott et al., 2019). 2.3 Pre-trained Models. To better measure the effects of different levels of multilinguality during pre-training, we built a range of models as follows:\n\u2022 mBART25 We pre-train a model on all 25 languages, using the setting described in \u00a72.2.\n\u2022 mBART06 To explore the effect of pre-training on related languages, we pretrain a model on a subset of six European languages: Ro, It, Cs, Fr, Es and En. For a fair comparison, we use\u223c 1/4 of the mBART25 batch size, which allows our model to have the same number of updates per language during pre-training. \u2022 mBART02 We pre-train bilingual models, using English and one other language for four language pairs: En-De, En-Ro, En-It. We use a batch size of \u223c 1/12 of that in the mBART25.", "\u2022 BART-En/Ro To help establish baseline performance levels, we also train monolingual BART models on the same En and Ro corpus only. \u2022 Random As additional baselines, we will also include a comparison with a model randomly initialized without pre-training for each translation task. Since the sizes of different downstream datasets vary, we always grid-search the hyper-parameters (architecture, dropout, etc.) to find the best non-pretrained configuration. All models use the same vocabulary (\u00a72.1). Not all tokens will frequently occur in all pre-training corpora, but later experiments show that this large vocabulary can improve generalization in multilingual settings even for unseen languages. 3 Sentence-level Machine Translation. This section shows that mBART pre-training provides consistent performance gains in low to medium resource sentence-level MT settings, including bi-text only and with back translation, and outperforms other existing pre-training schemes (\u00a73.2). We also present a detailed analysis to understand better which factors contribute the most to these gains (\u00a73.3), and show that pre-training can even improve performance for languages not present in the pre-training data at all (\u00a73.4). 3.1 Experimental Settings. Datasets We gather 24 pairs of publicly available parallel corpora that cover all the languages in CC25 (Table 1). Most pairs are from previous WMT (Gu, Kk, Tr, Ro, Et, Lt, Fi, Lv, Cs, Es, Zh, De, Ru, Fr \u2194 En) and IWSLT (Vi, Ja, Ko, Nl, Ar, It \u2194 En) competitions. We also use FLoRes pairs (Guzm\u00e1n et al., 2019, En-Ne and EnSi), En-Hi from IITB (Kunchukuttan et al., 2017),\nand En-My from WAT19 (Ding et al., 2018, 2019). We divide the datasets into three categories \u2013 low resource (<1M sentence pairs), medium resource (>1M and <10M), and high resource (>10M). Fine-tuning & Decoding We fine-tune our multilingual pre-trained models on a single pair of bitext data, feeding the source language into the encoder and decoding the target language."]}
{"pkey": "mbart_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU. The paper authors use the Adam optimizer (_x000f_ = 1e\u22126, \u03b22 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks. The paper authors started the training with dropout 0.1 and reduced it to 0.05 at 250K steps and 0 at 400K steps. During pre-training, the paper authors use document fragments of up to 512 tokens, allowing the models to learn dependencies between sentences. For all directions, the paper authors train with 0.3 dropout, 0.2 label smoothing, 2500 warm-up steps, 3e\u22125 maximum learning rate. The paper authors use a maximum of 40K training updates for all low and medium resource pairs and 100K for high resource pairs.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["It is also possible to use other noise types, such as those in Lample et al. (2018c), but we leave the exploration of the optimal noising strategy to future work. Instance format For each instance of a batch, we sample a language id symbol <LID>, and we pack as many consecutive sentences as possible sampled from the corresponding corpus of <LID>, until either it hits the document boundary or reaches the 512 max token length. Sentences in the instance are separated by the end of sentence (</S>) token. Then, we append the selected <LID> token to represent the end of this instance. Pre-training at \u201cmulti-sentence\u201d level enables us to work on both sentence and document translation. Optimization Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU, matching BART (Lewis et al., 2019) configuration. We use the Adam optimizer ( = 1e\u22126, \u03b22 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks. We started the training with dropout 0.1 and reduced it to 0.05 at 250K steps and 0 at 400K steps. All experiments are done with Fairseq (Ott et al., 2019). 2.3 Pre-trained Models. To better measure the effects of different levels of multilinguality during pre-training, we built a range of models as follows:\n\u2022 mBART25 We pre-train a model on all 25 languages, using the setting described in \u00a72.2.\n\u2022 mBART06 To explore the effect of pre-training on related languages, we pretrain a model on a subset of six European languages: Ro, It, Cs, Fr, Es and En. For a fair comparison, we use\u223c 1/4 of the mBART25 batch size, which allows our model to have the same number of updates per language during pre-training. \u2022 mBART02 We pre-train bilingual models, using English and one other language for four language pairs: En-De, En-Ro, En-It. We use a batch size of \u223c 1/12 of that in the mBART25.", "v.s. Other Pre-training Approaches We also compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only pair with established results. Our mBART model\noutperforms all the other pre-trained models, both with and without BT augmentation. We also show comparisons with the conventional BART model trained on the same En and Ro data only. Both have improvements over baselines, while worse than mBART results, indicating pre-training in a multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a new state-of-the-art for Ro-En translation. 3.3 Analysis. We also present additional analysis, to better quantify when our pre-training helps. How many languages should you pre-train on? We investigate when it is helpful for pre-training to include languages other than the targeted language pair that will be used during fine tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the target language monolingual data is limited (e.g. En-My, the size of My is around 0.5% of En). In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages slightly hurts the final results (<1 BLEU). In these cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to mBART02 on Ro-En suggests that pre-training with similar languages is particularly helpful. How many pre-training steps are needed? We plot Ro-En BLEU score v.s. Pre-training steps in Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same fine-tuning process described in \u00a73.1. Without any pre-training, our model overfits and performs much worse than the baseline. However, after just 25K steps (5% of training), both models outperform the best baseline.", "Datasets We evaluate our pre-trained models on both similar (En-De, En-Ro) and dissimilar pairs (En-Ne, En-Si), which are determined by measuring the subword units that are shared between the source and target languages. We use the same test sets as the supervised benchmarks \u00a73.1, and directly use the pre-training data (CC25) for backtranslation to avoid introducing new information. Learning Following the same procedure described in Lample et al. (2018c); Lample and Conneau (2019), we first initialize the translation model with the pre-trained weights, and then learn to predict the monolingual sentences conditioned on source sentences generated by on-thefly back-translation (BT). Lample and Conneau (2019) only pre-train an encoder, so perform additional de-noising training to learn a seq2seq model \u2013 a step which is unnecessary for mBART\u2019s pretrained seq2seq model. However, we do constrain mBART to only generating tokens in target language 4 for the first 1000 steps of on-the-fly BT, to avoid it simply copying the source text. Results Table 10 shows the unsupervised translation results compared with non-pretrained mod-\n4We mask out the output probability of predicting tokens which appear less than 1% in the target monolingual corpus. els, as well as models with existing pre-training methods. Our models achieve large gains over non-pretrained models for all directions, and outperform XLM significantly for dissimilar pairs (En-Ne, En-Si) where the existing approaches completely fail. For similar pairs, our model also performs well against XLM and MASS, with the best numbers for En-X pairs. 5.2 Unsupervised Machine Translation via Language Transfer. The second case of unsupervised machine translation assumes the target language appears in a bitext corpus with some other source language."]}
{"pkey": "mbart_13", "question": "Describe the computational resources used to train the model.", "answer": "Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU, matching BART (Lewis et al., 2019) configuration. The paper authors use the Adam optimizer (_x000f_ = 1e\u22126, \u03b22 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models. 8 Acknowledgements. We thank Marc\u2019Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun Shen for details about FloRes and WAT datasets. We also thank our colleagues at FAIR and FAIAR for valuable feedback. A Evaluation Details. For all our tasks, we use BLEU scores (Papineni et al., 2002) as the automatic metric to evaluate the translation performance. Normally, we compute the BLEU scores over tokenized text for both system outputs and the references, and we apply language-wise tokenization after over the translation. Note that, since we directly work on raw texts, we automatically get de-tokenized output after recovering sentence-piece subwords. Following the literature, the instructions of language-wise tokenization are as follows:\n\u2022 Gu, Ne, Si, Hi: We use Indic-NLP Library 5 to tokenize the Indic language outputs. \u2022 Ja: We use KyTea 6 to segment Japanese texts. \u2022 Ko: We use Mecab-Ko 7 and its default dictionary to segment the Korean texts\n\u2022 Ar: We apply QCRI Arabic Normalizer 8 over the Arabic texts. \u2022 My: We use the official segmentation tool provided by Ding et al. (2019) for Burmese. \u2022 Ro: Following Sennrich et al. (2016a), we apply Moses tokenization and special normalization for Romanian texts 9. \u2022 Zh: We use the official sacreBleu (Post, 2018)10 Chinese tokenizer (\u2013tok zh). For other languages that are not listed above, we compute BLEU scores with sacreBLEU with DEFAULT tokenization. B Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/ 6http://www.phontron.com/kytea/ 7http://konlpy.org/en/v0.3.0/install/ 8http://alt.qcri.org/tools/arabic-normalizer/ 9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU", "It is also possible to use other noise types, such as those in Lample et al. (2018c), but we leave the exploration of the optimal noising strategy to future work. Instance format For each instance of a batch, we sample a language id symbol <LID>, and we pack as many consecutive sentences as possible sampled from the corresponding corpus of <LID>, until either it hits the document boundary or reaches the 512 max token length. Sentences in the instance are separated by the end of sentence (</S>) token. Then, we append the selected <LID> token to represent the end of this instance. Pre-training at \u201cmulti-sentence\u201d level enables us to work on both sentence and document translation. Optimization Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU, matching BART (Lewis et al., 2019) configuration. We use the Adam optimizer ( = 1e\u22126, \u03b22 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks. We started the training with dropout 0.1 and reduced it to 0.05 at 250K steps and 0 at 400K steps. All experiments are done with Fairseq (Ott et al., 2019). 2.3 Pre-trained Models. To better measure the effects of different levels of multilinguality during pre-training, we built a range of models as follows:\n\u2022 mBART25 We pre-train a model on all 25 languages, using the setting described in \u00a72.2.\n\u2022 mBART06 To explore the effect of pre-training on related languages, we pretrain a model on a subset of six European languages: Ro, It, Cs, Fr, Es and En. For a fair comparison, we use\u223c 1/4 of the mBART25 batch size, which allows our model to have the same number of updates per language during pre-training. \u2022 mBART02 We pre-train bilingual models, using English and one other language for four language pairs: En-De, En-Ro, En-It. We use a batch size of \u223c 1/12 of that in the mBART25.", "As shown in Figure 1, we load the pre-trained weights and train the MT model on bi-texts with teacher forcing. For all directions, we train with 0.3 dropout, 0.2 label smoothing, 2500 warm-up steps, 3e\u22125 maximum learning rate. We use a maximum of 40K training updates for all low and medium resource pairs and 100K for high resource pairs. The final models are selected based on validation likelihood. For decoding, we use beam-search with beam size 5 for all directions. The final results are reported in BLEU (Papineni et al., 2002) with language-specific settings, see appendix A.\n3.2 Main Results. As shown in Table 2, initializing with the pretrained mBART25 weights shows gains on all the low and medium resource pairs when compared with randomly initialized baselines. We observe gains of 12+ BLEU on low resource pairs such as En-Vi, En-Tr, and noisily aligned pairs like En-Hi. Fine-tuning fails in extremely low-resource setting such as En-Gu, which only have roughly 10k ex-\namples for tuning. In these settings, unsupervised translation is more appropriate, see \u00a75.2. For high resource cases (Table 3), we do not observe consistent gains, and pre-training slightly hurts performance when >25M parallel sentence are available. When a significant amount of bi-text data is given, we suspect that supervised training washes out the pre-trained weights completely. + Back Translation Back-translation (BT, Sennrich et al., 2016b) is a standard approach to augment bi-text with target side monolingual data. We combine our pre-training with BT and test it on low resource language pairs \u2013 En-Si and En-Ne \u2013 using the FLoRes dataset (Guzm\u00e1n et al., 2019). For a fair comparison, we use the same monolingual data as (Guzm\u00e1n et al., 2019) to generate BT data. Figure 2 shows that initializing the model with our mBART25 pre-trained parameters improves BLEU scores at each iteration of back translation, resulting in new state-of-the-art results in all four translation directions."]}
{"pkey": "mbart_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "Our full model (including 25 languages) is trained on 256 Nvidia V100 GPUs (32GB) for 500K steps. The total batch size is around 128K tokens per GPU, matching BART (Lewis et al., 2019) configuration. The paper authors use the Adam optimizer (_x000f_ = 1e\u22126, \u03b22 = 0.98) and linear learning rate decay scheduling. The total training time was approximately 2.5 weeks.The paper authors started the training with dropout 0.1 and reduced it to 0.05 at 250K steps and 0 at 400K steps. The paper authors mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). The paper authors also permute the order of sentences within each instance. The decoder input is the original text with one position offset. A language id symbol <LID> is used as the initial token to predict the sentence.The paper authors use a standard sequence-tosequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). The paper authors evaluate performance on two common document-level MT datasets: WMT19 En-De and TED15 Zh-En. The paper authors also use FLoRes pairs (Guzm\u00e1n et al., 2019, En-Ne and EnSi), En-Hi from IITB (Kunchukuttan et al., 2017), and En-My from WAT19 (Ding et al., 2018, 2019). The paper authors divide the datasets into three categories \u2013 low resource (<1M sentence pairs), medium resource (>1M and <10M), and high resource (>10M). Zh-En dataset is from the IWSLT 2014 and 2015 evaluation campaigns.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["Despite its wide adoption for other NLP tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Raffel et al., 2019), selfsupervised pretraining is not yet common practice in machine translation (MT). Existing MT approaches only pre-train parts of the model, including the encoder (Lample and Conneau, 2019) and the decoder (Edunov et al., 2019), or use pretraining objectives that only reconstruct parts of text (Song et al., 2019), or only focus on English\n* Equal contribution. corpora (Lewis et al., 2019; Raffel et al., 2019). In this paper, we show that significant performance gains are possible by pre-training a complete autoregressive model with an objective that noises and reconstructs full texts across many languages. In this work, we present mBART \u2013 a multilingual sequence-to-sequence (Seq2Seq) denoising auto-encoder. mBART is trained by applying the BART (Lewis et al., 2019) to large-scale monolingual corpora across many languages. The input texts are noised by masking phrases and permuting sentences, and a single Transformer (Vaswani et al., 2017) model is learned to recover the texts. Different from other pre-training approaches for MT (Lample and Conneau, 2019; Song et al., 2019), mBART pre-trains a complete autoregressive Seq2Seq model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes. Extensive experiments demonstrate that this simple approach works remarkably well. We first focus on existing MT benchmarks. For supervised sentence-level MT, mBART initialization leads to significant gains (up to 12 BLEU points) across low/medium-resource pairs (<10M bi-text pairs), without sacrificing performance in high-resource settings.", "The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models. 8 Acknowledgements. We thank Marc\u2019Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun Shen for details about FloRes and WAT datasets. We also thank our colleagues at FAIR and FAIAR for valuable feedback. A Evaluation Details. For all our tasks, we use BLEU scores (Papineni et al., 2002) as the automatic metric to evaluate the translation performance. Normally, we compute the BLEU scores over tokenized text for both system outputs and the references, and we apply language-wise tokenization after over the translation. Note that, since we directly work on raw texts, we automatically get de-tokenized output after recovering sentence-piece subwords. Following the literature, the instructions of language-wise tokenization are as follows:\n\u2022 Gu, Ne, Si, Hi: We use Indic-NLP Library 5 to tokenize the Indic language outputs. \u2022 Ja: We use KyTea 6 to segment Japanese texts. \u2022 Ko: We use Mecab-Ko 7 and its default dictionary to segment the Korean texts\n\u2022 Ar: We apply QCRI Arabic Normalizer 8 over the Arabic texts. \u2022 My: We use the official segmentation tool provided by Ding et al. (2019) for Burmese. \u2022 Ro: Following Sennrich et al. (2016a), we apply Moses tokenization and special normalization for Romanian texts 9. \u2022 Zh: We use the official sacreBleu (Post, 2018)10 Chinese tokenizer (\u2013tok zh). For other languages that are not listed above, we compute BLEU scores with sacreBLEU with DEFAULT tokenization. B Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/ 6http://www.phontron.com/kytea/ 7http://konlpy.org/en/v0.3.0/install/ 8http://alt.qcri.org/tools/arabic-normalizer/ 9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU", "Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text."]}
{"pkey": "mbart_15", "question": "What is the pretraining objective of the model? ", "answer": "mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, The paper authors (1) assume access to a noising function g, defined below, that corrupts text, and (2) train the model to predict the original text X given g(X). ollowing Lewis et al. (2019), the paper authors use two types of noise in g. The paper authors first remove spans of text and replace them with a mask token. The paper authors mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). The paper authors also permute the order of sentences within each instance. The decoder input is the original text with one position offset. A language id symbol <LID> is used as the initial token to predict the sentence.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["The models keep improving by over 3 BLEU for the rest of steps and have not fully converged after 500K steps. mBART25 is consistently\nslightly worse than mBART02. How does the size of bitexts inference the gain from pre-training? Tables 2 and 3 show that pre-training consistently improves for low and medium resource language pairs. To verify this trend, we plot performance for differing sized subsets of the En-De dataset. More precisely, we take the full En-De corpus (28M pairs) and randomly sample 10K, 50K, 100K, 500K, 1M, 5M, 10M datasets. We compare performance without pretraining to the mBART02 results, as shown in Figure 4. The pre-trained model is able to achieve over 20 BLEU with only 10K training examples, while the baseline system scores 0. Unsurprisingly, increasing the size of bi-text corpus improves both models. Our pre-trained model consistently outperforms the baseline models, but the gap reduces with increasing amounts of bi-text, especially after 10M sentence pairs. This result confirms our observation in \u00a73.2 that our pre-training does not help translation in high-resource pairs. Is pre-training complementary to BT? Figure 2 presents that our pre-trained models can be combined with iterative back-translation (BT) on additional data, however, it is still not a fair comparison. Table 6 shows the results when using\nsame monolingual data where we use 79M En and 29M My sentences following Chen et al. (2019). With the same amount of monolingual corpus, mBART pre-training achieves the same performance on En\u2192My as BT, while still 3 BLEU worse on My\u2192En. We suspect BT benefits from bigger monolingual data (En). Moreover, combining mBART02 model with BT, we see further gains even with same monolingual data. Besides, we also provide estimated training costs where BT has a longer pipeline involving training a baseline system (5h), translating monolingual data (300h) and formal training (350h).", "v.s. Other Pre-training Approaches We also compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only pair with established results. Our mBART model\noutperforms all the other pre-trained models, both with and without BT augmentation. We also show comparisons with the conventional BART model trained on the same En and Ro data only. Both have improvements over baselines, while worse than mBART results, indicating pre-training in a multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a new state-of-the-art for Ro-En translation. 3.3 Analysis. We also present additional analysis, to better quantify when our pre-training helps. How many languages should you pre-train on? We investigate when it is helpful for pre-training to include languages other than the targeted language pair that will be used during fine tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the target language monolingual data is limited (e.g. En-My, the size of My is around 0.5% of En). In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages slightly hurts the final results (<1 BLEU). In these cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to mBART02 on Ro-En suggests that pre-training with similar languages is particularly helpful. How many pre-training steps are needed? We plot Ro-En BLEU score v.s. Pre-training steps in Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same fine-tuning process described in \u00a73.1. Without any pre-training, our model overfits and performs much worse than the baseline. However, after just 25K steps (5% of training), both models outperform the best baseline.", "Despite its wide adoption for other NLP tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Raffel et al., 2019), selfsupervised pretraining is not yet common practice in machine translation (MT). Existing MT approaches only pre-train parts of the model, including the encoder (Lample and Conneau, 2019) and the decoder (Edunov et al., 2019), or use pretraining objectives that only reconstruct parts of text (Song et al., 2019), or only focus on English\n* Equal contribution. corpora (Lewis et al., 2019; Raffel et al., 2019). In this paper, we show that significant performance gains are possible by pre-training a complete autoregressive model with an objective that noises and reconstructs full texts across many languages. In this work, we present mBART \u2013 a multilingual sequence-to-sequence (Seq2Seq) denoising auto-encoder. mBART is trained by applying the BART (Lewis et al., 2019) to large-scale monolingual corpora across many languages. The input texts are noised by masking phrases and permuting sentences, and a single Transformer (Vaswani et al., 2017) model is learned to recover the texts. Different from other pre-training approaches for MT (Lample and Conneau, 2019; Song et al., 2019), mBART pre-trains a complete autoregressive Seq2Seq model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes. Extensive experiments demonstrate that this simple approach works remarkably well. We first focus on existing MT benchmarks. For supervised sentence-level MT, mBART initialization leads to significant gains (up to 12 BLEU points) across low/medium-resource pairs (<10M bi-text pairs), without sacrificing performance in high-resource settings."]}
{"pkey": "mbart_16", "question": "What is the loss function that is used to train the model?", "answer": "MLE loss in unsupervised setup (Figure 5). In pre-training, the paper authors aim to maximize L\u03b8: L\u03b8 = \u2211 Di\u2208D \u2211 X \u2208Di log P (X|g(X); \u03b8). For supervised sentence-level MT, mBART initialization leads to significant gains (up to 12 BLEU points) across low/medium-resource pairs.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["We use the smoothing parameter \u03b1 = 0.7. Pre-processing We tokenize with a sentencepiece model (SPM, Kudo and Richardson, 2018) learned on the full CC data that includes 250, 000 subword tokens. While not all of these languages are used for pre-training, this tokenization supports fine-tuning on additional languages. We do not apply additional preprocessing, such as truecasing or normalizing punctuation/characters. 2.2 Model: mBART. Our models follow the BART (Lewis et al., 2019) sequence-to-sequence pre-training scheme, as reviewed in this section. While BART was only pretrained for English, we systematically study the effects of pre-training on different sets of languages. 1https://commoncrawl.org\nArchitecture We use a standard sequence-tosequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). We include an additional layer-normalization layer on top of both the encoder and decoder, which we found stabilized training at FP16 precision. Learning Our training data coversK languages: D = {D1, ...,DK} where each Di is a collection of monolingual documents in language i. We (1) assume access to a noising function g, defined below, that corrupts text, and (2) train the model to predict the original text X given g(X). More formally, we aim to maximize L\u03b8: L\u03b8 = \u2211 Di\u2208D \u2211 X\u2208Di logP (X|g(X); \u03b8) , (2)\nwhere X is an instance in language i and the distribution P is defined by the Seq2Seq model. Noise function Following Lewis et al. (2019), we use two types of noise in g. We first remove spans of text and replace them with a mask token. We mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). We also permute the order of sentences within each instance. The decoder input is the original text with one position offset. A language id symbol <LID> is used as the initial token to predict the sentence.", "The models keep improving by over 3 BLEU for the rest of steps and have not fully converged after 500K steps. mBART25 is consistently\nslightly worse than mBART02. How does the size of bitexts inference the gain from pre-training? Tables 2 and 3 show that pre-training consistently improves for low and medium resource language pairs. To verify this trend, we plot performance for differing sized subsets of the En-De dataset. More precisely, we take the full En-De corpus (28M pairs) and randomly sample 10K, 50K, 100K, 500K, 1M, 5M, 10M datasets. We compare performance without pretraining to the mBART02 results, as shown in Figure 4. The pre-trained model is able to achieve over 20 BLEU with only 10K training examples, while the baseline system scores 0. Unsurprisingly, increasing the size of bi-text corpus improves both models. Our pre-trained model consistently outperforms the baseline models, but the gap reduces with increasing amounts of bi-text, especially after 10M sentence pairs. This result confirms our observation in \u00a73.2 that our pre-training does not help translation in high-resource pairs. Is pre-training complementary to BT? Figure 2 presents that our pre-trained models can be combined with iterative back-translation (BT) on additional data, however, it is still not a fair comparison. Table 6 shows the results when using\nsame monolingual data where we use 79M En and 29M My sentences following Chen et al. (2019). With the same amount of monolingual corpus, mBART pre-training achieves the same performance on En\u2192My as BT, while still 3 BLEU worse on My\u2192En. We suspect BT benefits from bigger monolingual data (En). Moreover, combining mBART02 model with BT, we see further gains even with same monolingual data. Besides, we also provide estimated training costs where BT has a longer pipeline involving training a baseline system (5h), translating monolingual data (300h) and formal training (350h).", "v.s. Other Pre-training Approaches We also compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only pair with established results. Our mBART model\noutperforms all the other pre-trained models, both with and without BT augmentation. We also show comparisons with the conventional BART model trained on the same En and Ro data only. Both have improvements over baselines, while worse than mBART results, indicating pre-training in a multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a new state-of-the-art for Ro-En translation. 3.3 Analysis. We also present additional analysis, to better quantify when our pre-training helps. How many languages should you pre-train on? We investigate when it is helpful for pre-training to include languages other than the targeted language pair that will be used during fine tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the target language monolingual data is limited (e.g. En-My, the size of My is around 0.5% of En). In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages slightly hurts the final results (<1 BLEU). In these cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to mBART02 on Ro-En suggests that pre-training with similar languages is particularly helpful. How many pre-training steps are needed? We plot Ro-En BLEU score v.s. Pre-training steps in Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same fine-tuning process described in \u00a73.1. Without any pre-training, our model overfits and performs much worse than the baseline. However, after just 25K steps (5% of training), both models outperform the best baseline."]}
{"pkey": "mbart_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The paper authors use a standard sequence-tosequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). The paper authors include an additional layer-normalization layer on top of both the encoder and decoder, which the paper authors found stabilized training at FP16 precision", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["We use the smoothing parameter \u03b1 = 0.7. Pre-processing We tokenize with a sentencepiece model (SPM, Kudo and Richardson, 2018) learned on the full CC data that includes 250, 000 subword tokens. While not all of these languages are used for pre-training, this tokenization supports fine-tuning on additional languages. We do not apply additional preprocessing, such as truecasing or normalizing punctuation/characters. 2.2 Model: mBART. Our models follow the BART (Lewis et al., 2019) sequence-to-sequence pre-training scheme, as reviewed in this section. While BART was only pretrained for English, we systematically study the effects of pre-training on different sets of languages. 1https://commoncrawl.org\nArchitecture We use a standard sequence-tosequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). We include an additional layer-normalization layer on top of both the encoder and decoder, which we found stabilized training at FP16 precision. Learning Our training data coversK languages: D = {D1, ...,DK} where each Di is a collection of monolingual documents in language i. We (1) assume access to a noising function g, defined below, that corrupts text, and (2) train the model to predict the original text X given g(X). More formally, we aim to maximize L\u03b8: L\u03b8 = \u2211 Di\u2208D \u2211 X\u2208Di logP (X|g(X); \u03b8) , (2)\nwhere X is an instance in language i and the distribution P is defined by the Seq2Seq model. Noise function Following Lewis et al. (2019), we use two types of noise in g. We first remove spans of text and replace them with a mask token. We mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). We also permute the order of sentences within each instance. The decoder input is the original text with one position offset. A language id symbol <LID> is used as the initial token to predict the sentence.", "As shown in Figure 1, we load the pre-trained weights and train the MT model on bi-texts with teacher forcing. For all directions, we train with 0.3 dropout, 0.2 label smoothing, 2500 warm-up steps, 3e\u22125 maximum learning rate. We use a maximum of 40K training updates for all low and medium resource pairs and 100K for high resource pairs. The final models are selected based on validation likelihood. For decoding, we use beam-search with beam size 5 for all directions. The final results are reported in BLEU (Papineni et al., 2002) with language-specific settings, see appendix A.\n3.2 Main Results. As shown in Table 2, initializing with the pretrained mBART25 weights shows gains on all the low and medium resource pairs when compared with randomly initialized baselines. We observe gains of 12+ BLEU on low resource pairs such as En-Vi, En-Tr, and noisily aligned pairs like En-Hi. Fine-tuning fails in extremely low-resource setting such as En-Gu, which only have roughly 10k ex-\namples for tuning. In these settings, unsupervised translation is more appropriate, see \u00a75.2. For high resource cases (Table 3), we do not observe consistent gains, and pre-training slightly hurts performance when >25M parallel sentence are available. When a significant amount of bi-text data is given, we suspect that supervised training washes out the pre-trained weights completely. + Back Translation Back-translation (BT, Sennrich et al., 2016b) is a standard approach to augment bi-text with target side monolingual data. We combine our pre-training with BT and test it on low resource language pairs \u2013 En-Si and En-Ne \u2013 using the FLoRes dataset (Guzm\u00e1n et al., 2019). For a fair comparison, we use the same monolingual data as (Guzm\u00e1n et al., 2019) to generate BT data. Figure 2 shows that initializing the model with our mBART25 pre-trained parameters improves BLEU scores at each iteration of back translation, resulting in new state-of-the-art results in all four translation directions.", "Datasets We only consider X\u2192En translation, and choose the bitexts of 12 language pairs from \u00a73.1, covering Indic languages (Ne, Hi, Si, Gu), European languages (Ro, It, Cs, Nl), East Asian languages (Zh, Ja, Ko) and Arabic languages (Ar). Results As illustrated in Figure 5 (b), we take the pre-trained mBART25 model and finetune on each language pair, and then directly apply them to the rest of pairs, as seen in Table 11. We also present the direct fine-tuning performance (\u00a73) on the diagonal, for reference. We can always obtain reasonable transferring scores at all pairs over different fine-tuned models except from Gu-En where the supervised model completely fails (0.3 BLEU). In some cases, we can achieve similar (Cs-En) or even much better (Ne-En, Gu-En) results compared to the supervised results. As a comparison, we also apply the same procedure on randomly initialized models without pretraining, which always ends up with \u2248 0 BLEU. This indicates that multilingual pre-training is essential and produces universal representations across languages, so that once the model learns to translate one language to En, it learns to trans-\nlate all languages with similar representations. We also present three examples of language transferring between Zh, Ja and Ko in appendix B.\nWhen is language transfer useful? Table 11 also shows mixed results at each pair. First, for most pairs, language transfer works better when fine-tuning is also conducted in the same language family, especially between Indic languages (Hi, Ne, Gu). However, significant vocabulary sharing is not required for effective transfer. For instance, Zh-En and It-En achieve the best transfer learning results on Ko-En and Ar-En, respectively. How-\never, the vocabulary overlapping (even character overlapping) between Zh and Ko, It and Ar is low. w/ Back-Translation We also present the comparison on 4 pairs of unsupervised MT with backtranslation (BT) v.s. language transfer in Table 12. The results are also mixed."]}
{"pkey": "mbart_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "Unsupervised Machine Translation: the paper authors also evaluate our model on tasks where no bi-text is available for the target language pair. the paper authors demonstrate the effectiveness of multilingual pre-training in unsupervised machine translation via (1) back-translation ( \u00a75.1) and (3) language transfer (\u00a75.2). The paper authors can always obtain reasonable transferring scores at all pairs over different fine-tuned models except from Gu-En where the supervised model completely fails (0.3 BLEU). Previous work has shown that zero-shot transfer is possible via massively multi-lingual MT (Johnson et al., 2017; Gu et al., 2019) or distillation through pivoting (Chen et al., 2017). The paper authors limit our focus to building MT models for single language pairs, and leave multi-lingual pre-training for multi-lingual MT to future work.\n\nSupervised\nmBART pre-training provides consistent performance gains in low to medium resource sentence-level MT settings, including bi-text only and with back translation, and outperforms other existing pre-training schemes (\u00a73.2). The paper authors evaluate mBART on document-level machine translation tasks, where the goal is to translate segments of text that contain more than one sentence (up to an entire document).The models keep improving by over 3 BLEU for the rest of steps and have not fully converged after 500K steps.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["Despite its wide adoption for other NLP tasks (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019; Lewis et al., 2019; Raffel et al., 2019), selfsupervised pretraining is not yet common practice in machine translation (MT). Existing MT approaches only pre-train parts of the model, including the encoder (Lample and Conneau, 2019) and the decoder (Edunov et al., 2019), or use pretraining objectives that only reconstruct parts of text (Song et al., 2019), or only focus on English\n* Equal contribution. corpora (Lewis et al., 2019; Raffel et al., 2019). In this paper, we show that significant performance gains are possible by pre-training a complete autoregressive model with an objective that noises and reconstructs full texts across many languages. In this work, we present mBART \u2013 a multilingual sequence-to-sequence (Seq2Seq) denoising auto-encoder. mBART is trained by applying the BART (Lewis et al., 2019) to large-scale monolingual corpora across many languages. The input texts are noised by masking phrases and permuting sentences, and a single Transformer (Vaswani et al., 2017) model is learned to recover the texts. Different from other pre-training approaches for MT (Lample and Conneau, 2019; Song et al., 2019), mBART pre-trains a complete autoregressive Seq2Seq model. mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes. Extensive experiments demonstrate that this simple approach works remarkably well. We first focus on existing MT benchmarks. For supervised sentence-level MT, mBART initialization leads to significant gains (up to 12 BLEU points) across low/medium-resource pairs (<10M bi-text pairs), without sacrificing performance in high-resource settings.", "The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models. 8 Acknowledgements. We thank Marc\u2019Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun Shen for details about FloRes and WAT datasets. We also thank our colleagues at FAIR and FAIAR for valuable feedback. A Evaluation Details. For all our tasks, we use BLEU scores (Papineni et al., 2002) as the automatic metric to evaluate the translation performance. Normally, we compute the BLEU scores over tokenized text for both system outputs and the references, and we apply language-wise tokenization after over the translation. Note that, since we directly work on raw texts, we automatically get de-tokenized output after recovering sentence-piece subwords. Following the literature, the instructions of language-wise tokenization are as follows:\n\u2022 Gu, Ne, Si, Hi: We use Indic-NLP Library 5 to tokenize the Indic language outputs. \u2022 Ja: We use KyTea 6 to segment Japanese texts. \u2022 Ko: We use Mecab-Ko 7 and its default dictionary to segment the Korean texts\n\u2022 Ar: We apply QCRI Arabic Normalizer 8 over the Arabic texts. \u2022 My: We use the official segmentation tool provided by Ding et al. (2019) for Burmese. \u2022 Ro: Following Sennrich et al. (2016a), we apply Moses tokenization and special normalization for Romanian texts 9. \u2022 Zh: We use the official sacreBleu (Post, 2018)10 Chinese tokenizer (\u2013tok zh). For other languages that are not listed above, we compute BLEU scores with sacreBLEU with DEFAULT tokenization. B Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/ 6http://www.phontron.com/kytea/ 7http://konlpy.org/en/v0.3.0/install/ 8http://alt.qcri.org/tools/arabic-normalizer/ 9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU", "Datasets We only consider X\u2192En translation, and choose the bitexts of 12 language pairs from \u00a73.1, covering Indic languages (Ne, Hi, Si, Gu), European languages (Ro, It, Cs, Nl), East Asian languages (Zh, Ja, Ko) and Arabic languages (Ar). Results As illustrated in Figure 5 (b), we take the pre-trained mBART25 model and finetune on each language pair, and then directly apply them to the rest of pairs, as seen in Table 11. We also present the direct fine-tuning performance (\u00a73) on the diagonal, for reference. We can always obtain reasonable transferring scores at all pairs over different fine-tuned models except from Gu-En where the supervised model completely fails (0.3 BLEU). In some cases, we can achieve similar (Cs-En) or even much better (Ne-En, Gu-En) results compared to the supervised results. As a comparison, we also apply the same procedure on randomly initialized models without pretraining, which always ends up with \u2248 0 BLEU. This indicates that multilingual pre-training is essential and produces universal representations across languages, so that once the model learns to translate one language to En, it learns to trans-\nlate all languages with similar representations. We also present three examples of language transferring between Zh, Ja and Ko in appendix B.\nWhen is language transfer useful? Table 11 also shows mixed results at each pair. First, for most pairs, language transfer works better when fine-tuning is also conducted in the same language family, especially between Indic languages (Hi, Ne, Gu). However, significant vocabulary sharing is not required for effective transfer. For instance, Zh-En and It-En achieve the best transfer learning results on Ko-En and Ar-En, respectively. How-\never, the vocabulary overlapping (even character overlapping) between Zh and Ko, It and Ar is low. w/ Back-Translation We also present the comparison on 4 pairs of unsupervised MT with backtranslation (BT) v.s. language transfer in Table 12. The results are also mixed."]}
{"pkey": "mbart_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "mBART is trained once for all languages, providing a set of parameters that can be fine-tuned for any of the language pairs in both supervised and unsupervised settings, without any task-specific or language-specific modifications or initialization schemes.\n\nThe paper authors first remove spans of text and replace them with a mask token. The paper authors mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). The paper authors also permute the order of sentences within each instance. The decoder input is the original text with one position offset\n\nwe will also include a comparison with a model randomly initialized without pre-training for each translation task", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["We use the smoothing parameter \u03b1 = 0.7. Pre-processing We tokenize with a sentencepiece model (SPM, Kudo and Richardson, 2018) learned on the full CC data that includes 250, 000 subword tokens. While not all of these languages are used for pre-training, this tokenization supports fine-tuning on additional languages. We do not apply additional preprocessing, such as truecasing or normalizing punctuation/characters. 2.2 Model: mBART. Our models follow the BART (Lewis et al., 2019) sequence-to-sequence pre-training scheme, as reviewed in this section. While BART was only pretrained for English, we systematically study the effects of pre-training on different sets of languages. 1https://commoncrawl.org\nArchitecture We use a standard sequence-tosequence Transformer architecture (Vaswani et al., 2017), with 12 layers of encoder and 12 layers of decoder with model dimension of 1024 on 16 heads (\u223c 680M parameters). We include an additional layer-normalization layer on top of both the encoder and decoder, which we found stabilized training at FP16 precision. Learning Our training data coversK languages: D = {D1, ...,DK} where each Di is a collection of monolingual documents in language i. We (1) assume access to a noising function g, defined below, that corrupts text, and (2) train the model to predict the original text X given g(X). More formally, we aim to maximize L\u03b8: L\u03b8 = \u2211 Di\u2208D \u2211 X\u2208Di logP (X|g(X); \u03b8) , (2)\nwhere X is an instance in language i and the distribution P is defined by the Seq2Seq model. Noise function Following Lewis et al. (2019), we use two types of noise in g. We first remove spans of text and replace them with a mask token. We mask 35% of the words in each instance by random sampling a span length according to a Poisson distribution (\u03bb = 3.5). We also permute the order of sentences within each instance. The decoder input is the original text with one position offset. A language id symbol <LID> is used as the initial token to predict the sentence.", "Datasets We only consider X\u2192En translation, and choose the bitexts of 12 language pairs from \u00a73.1, covering Indic languages (Ne, Hi, Si, Gu), European languages (Ro, It, Cs, Nl), East Asian languages (Zh, Ja, Ko) and Arabic languages (Ar). Results As illustrated in Figure 5 (b), we take the pre-trained mBART25 model and finetune on each language pair, and then directly apply them to the rest of pairs, as seen in Table 11. We also present the direct fine-tuning performance (\u00a73) on the diagonal, for reference. We can always obtain reasonable transferring scores at all pairs over different fine-tuned models except from Gu-En where the supervised model completely fails (0.3 BLEU). In some cases, we can achieve similar (Cs-En) or even much better (Ne-En, Gu-En) results compared to the supervised results. As a comparison, we also apply the same procedure on randomly initialized models without pretraining, which always ends up with \u2248 0 BLEU. This indicates that multilingual pre-training is essential and produces universal representations across languages, so that once the model learns to translate one language to En, it learns to trans-\nlate all languages with similar representations. We also present three examples of language transferring between Zh, Ja and Ko in appendix B.\nWhen is language transfer useful? Table 11 also shows mixed results at each pair. First, for most pairs, language transfer works better when fine-tuning is also conducted in the same language family, especially between Indic languages (Hi, Ne, Gu). However, significant vocabulary sharing is not required for effective transfer. For instance, Zh-En and It-En achieve the best transfer learning results on Ko-En and Ar-En, respectively. How-\never, the vocabulary overlapping (even character overlapping) between Zh and Ko, It and Ar is low. w/ Back-Translation We also present the comparison on 4 pairs of unsupervised MT with backtranslation (BT) v.s. language transfer in Table 12. The results are also mixed.", "v.s. Other Pre-training Approaches We also compare our pre-trained models with recent selfsupervised pre-training methods, as shown in Table 4. We consider En-Ro translation, the only pair with established results. Our mBART model\noutperforms all the other pre-trained models, both with and without BT augmentation. We also show comparisons with the conventional BART model trained on the same En and Ro data only. Both have improvements over baselines, while worse than mBART results, indicating pre-training in a multilingual setting is essential. Moreover, combining BT leads to additional gains, resulting in a new state-of-the-art for Ro-En translation. 3.3 Analysis. We also present additional analysis, to better quantify when our pre-training helps. How many languages should you pre-train on? We investigate when it is helpful for pre-training to include languages other than the targeted language pair that will be used during fine tuning. Table 5 shows performance on four X-En pairs. Pretraining on more languages helps most when the target language monolingual data is limited (e.g. En-My, the size of My is around 0.5% of En). In contrast, when monolingual data is plentiful (De, Ro), pre-training on multiple languages slightly hurts the final results (<1 BLEU). In these cases, additional languages may reduce the capacity available for each test language. Additionally, the fact that mBART06 performs similar to mBART02 on Ro-En suggests that pre-training with similar languages is particularly helpful. How many pre-training steps are needed? We plot Ro-En BLEU score v.s. Pre-training steps in Figure 3, where we take the saved checkpoints (every 25K steps) and apply the same fine-tuning process described in \u00a73.1. Without any pre-training, our model overfits and performs much worse than the baseline. However, after just 25K steps (5% of training), both models outperform the best baseline."]}
{"pkey": "mbart_20", "question": "List the future work mentioned in the paper.", "answer": "In future work, the paper authors will scale-up the current pre\n\ntraining to more languages, e.g., an mBART100 model. The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models.", "title": "mBART: Multilingual Denoising Pre-training for Neural Machine Translation", "context": ["The size of our model makes it expensive to deploy in production \u2013 future work will explore pre-training more efficient models. 8 Acknowledgements. We thank Marc\u2019Aurelio Ranzato, Guillaume Lample, Alexis Conneau, and Michael Auli for sharing their expertise on low-resource and unsupervised machine translation, Peng-Jen Chen, Jiajun Shen for details about FloRes and WAT datasets. We also thank our colleagues at FAIR and FAIAR for valuable feedback. A Evaluation Details. For all our tasks, we use BLEU scores (Papineni et al., 2002) as the automatic metric to evaluate the translation performance. Normally, we compute the BLEU scores over tokenized text for both system outputs and the references, and we apply language-wise tokenization after over the translation. Note that, since we directly work on raw texts, we automatically get de-tokenized output after recovering sentence-piece subwords. Following the literature, the instructions of language-wise tokenization are as follows:\n\u2022 Gu, Ne, Si, Hi: We use Indic-NLP Library 5 to tokenize the Indic language outputs. \u2022 Ja: We use KyTea 6 to segment Japanese texts. \u2022 Ko: We use Mecab-Ko 7 and its default dictionary to segment the Korean texts\n\u2022 Ar: We apply QCRI Arabic Normalizer 8 over the Arabic texts. \u2022 My: We use the official segmentation tool provided by Ding et al. (2019) for Burmese. \u2022 Ro: Following Sennrich et al. (2016a), we apply Moses tokenization and special normalization for Romanian texts 9. \u2022 Zh: We use the official sacreBleu (Post, 2018)10 Chinese tokenizer (\u2013tok zh). For other languages that are not listed above, we compute BLEU scores with sacreBLEU with DEFAULT tokenization. B Translation Examples\n5https://anoopkunchukuttan.github.io/indic_nlp_library/ 6http://www.phontron.com/kytea/ 7http://konlpy.org/en/v0.3.0/install/ 8http://alt.qcri.org/tools/arabic-normalizer/ 9https://github.com/rsennrich/wmt16-script\n10https://github.com/mjpost/sacreBLEU", "Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training. Multilingual Denoising Pre-training for Neural Machine Translation. This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART \u2013 a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text.", "Such large performance gaps indicate that pre-training is critical for document level performance. It is in general difficult to collect high quality documentlevel data in large quantities, suggesting that pretraining may be a strong strategy for future work. We also include a sampled example in appendix B.\n5 Unsupervised Machine Translation. In addition to supervised machine translation, we also evaluate our model on tasks where no bi-text is available for the target language pair. We define three types of unsupervised translation:\n1. No bi-text of any kind is given. A common solution is to learn from back-translation (BT) (Artetxe et al., 2017; Lample et al., 2018c). We show that mBART provides a simple and effective initialize scheme for these methods. 2. No bi-text for the target pair is available, but the target languages both appear in bi-text corpora for other language pairs. Previous work has shown that zero-shot transfer is possible via massively multi-lingual MT (Johnson et al., 2017; Gu et al., 2019) or distillation through pivoting (Chen et al., 2017). We limit our focus to building MT models for single language pairs, and leave multi-lingual pre-training for multi-lingual MT to future work. 3. No bi-text for the target pair is available, but there is bi-text for translating from some other 3d-BLEU is recomputed from the provided system output. language into the target language. This is a new evaluation regime, where we will show that mBART supports effective transfer, even if the source language has no bi-text of any form. In this section, we demonstrate the effectiveness of multilingual pre-training in unsupervised machine translation via (1) back-translation ( \u00a75.1) and (3) language transfer (\u00a75.2). An illustration of both approaches are presented in Figure 5.\n5.1 Unsupervised Machine Translation via Back-Translation."]}
{"pkey": "distilbert_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "In this work, the paper authors propose a method to pre-train a smaller general purpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, the paper authors leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", ". As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. . As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al."]}
{"pkey": "distilbert_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "The last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. \n\nThe trend toward bigger models raises several concerns. First is the environmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al.", ". As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. . As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster."]}
{"pkey": "distilbert_3", "question": "What are the main contributions of the paper?", "answer": "In this paper, the paper authors show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models. The paper authors also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, the paper authors show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al.", "[2015] we used a softmax-temperature: pi =\nexp(zi/T )\u2211 j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors. 3 DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers. Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers 3E.g."]}
{"pkey": "distilbert_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.\n\nThe paper authors show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": [". As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. . As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.", "[2015] we used a softmax-temperature: pi =\nexp(zi/T )\u2211 j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors. 3 DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers. Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers 3E.g.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study."]}
{"pkey": "distilbert_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. The paper authors further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]).", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["BERT-base\u2019s predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world. . . ). Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100. 4 Experiments. General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task benchmark.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study.", "To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models."]}
{"pkey": "distilbert_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not mentioned in the paper.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al.", ". As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. . As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster."]}
{"pkey": "distilbert_7", "question": "List the limitations of the model discussed in the paper.", "answer": "You can use the raw model for either masked language modeling or next sentence prediction, but it's mostly intended to be fine-tuned on a downstream task.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", "In this section, we investigate the influence of various components of the triple loss and the student initialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4 presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance. 5 Related work. Task-specific distillation Most of the prior works focus on building task-specific distillation setups. Tang et al. [2019] transfer fine-tune classification model BERT to an LSTM-based classifier. Chatterjee [2019] distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. Turc et al. [2019] use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher\u2019s knowledge to pre-train with additional distillation signal. Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation. However, as shown in the ablation study, leveraging the teacher\u2019s knowledge with initialization and additional losses leads to substantial gains. Other compression techniques have been studied to compress large models. Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without significantly degrading the performance Michel et al. [2019].", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al."]}
{"pkey": "distilbert_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015].", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["BERT-base\u2019s predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world. . . ). Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100. 4 Experiments. General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task benchmark.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study.", ". As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. . As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster."]}
{"pkey": "distilbert_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Based on wordpiece", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study.", "BERT-base\u2019s predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world. . . ). Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100. 4 Experiments. General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task benchmark.", "[2015] we used a softmax-temperature: pi =\nexp(zi/T )\u2211 j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors. 3 DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers. Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers 3E.g."]}
{"pkey": "distilbert_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "The texts are lowercased and tokenized using WordPiece and a vocabulary size of 30,000. The inputs of the model are then of the form:\n\n[CLS] Sentence A [SEP] Sentence B [SEP]\n\nWith probability 0.5, sentence A and sentence B correspond to two consecutive sentences in the original corpus and in the other cases, it's another random sentence in the corpus. Note that what is considered a sentence here is a consecutive span of text usually longer than a single sentence. The only constrain is that the result with the two \"sentences\" has a combined length of less than 512 tokens.\nThe details of the masking procedure for each sentence are the following:\n\n15% of the tokens are masked.\nIn 80% of the cases, the masked tokens are replaced by [MASK].\nIn 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace.\nIn the 10% remaining cases, the masked tokens are left as is.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["BERT-base\u2019s predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world. . . ). Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100. 4 Experiments. General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task benchmark.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study.", "In this section, we investigate the influence of various components of the triple loss and the student initialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4 presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance. 5 Related work. Task-specific distillation Most of the prior works focus on building task-specific distillation setups. Tang et al. [2019] transfer fine-tune classification model BERT to an LSTM-based classifier. Chatterjee [2019] distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. Turc et al. [2019] use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher\u2019s knowledge to pre-train with additional distillation signal. Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation. However, as shown in the ablation study, leveraging the teacher\u2019s knowledge with initialization and additional losses leads to substantial gains. Other compression techniques have been studied to compress large models. Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without significantly degrading the performance Michel et al. [2019]."]}
{"pkey": "distilbert_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus the paper authors focus on reducing the number of layers.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["[2015] we used a softmax-temperature: pi =\nexp(zi/T )\u2211 j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors. 3 DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers. Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers 3E.g.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study.", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al."]}
{"pkey": "distilbert_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. \n\nTo further investigate the speed-up/size trade-off of DistilBERT, the paper authors compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["[2015] we used a softmax-temperature: pi =\nexp(zi/T )\u2211 j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors. 3 DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers. Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers 3E.g.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study.", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al."]}
{"pkey": "distilbert_13", "question": "Describe the computational resources used to train the model.", "answer": "DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours.\n\nCPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\n\nDistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["[2015] we used a softmax-temperature: pi =\nexp(zi/T )\u2211 j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors. 3 DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers. Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers 3E.g.", ". As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. . As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster.", "To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models."]}
{"pkey": "distilbert_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al.", "To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", "BERT-base\u2019s predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world. . . ). Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100. 4 Experiments. General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task benchmark."]}
{"pkey": "distilbert_15", "question": "What is the pretraining objective of the model? ", "answer": "The paper authors applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective.\n\nIn this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["In this section, we investigate the influence of various components of the triple loss and the student initialization on the performances of the distilled model. We report the macro-score on GLUE. Table 4 presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance. 5 Related work. Task-specific distillation Most of the prior works focus on building task-specific distillation setups. Tang et al. [2019] transfer fine-tune classification model BERT to an LSTM-based classifier. Chatterjee [2019] distill BERT model fine-tuned on SQuAD in a smaller Transformer model previously initialized from BERT. In the present work, we found it beneficial to use a general-purpose pre-training distillation rather than a task-specific distillation. Turc et al. [2019] use the original pretraining objective to train smaller student, then fine-tuned via distillation. As shown in the ablation study, we found it beneficial to leverage the teacher\u2019s knowledge to pre-train with additional distillation signal. Multi-distillation Yang et al. [2019] combine the knowledge of an ensemble of teachers using multi-task learning to regularize the distillation. The authors apply Multi-Task Knowledge Distillation to learn a compact question answering model from a set of large question answering models. An application of multi-distillation is multi-linguality: Tsai et al. [2019] adopts a similar approach to us by pre-training a multilingual model from scratch solely through distillation. However, as shown in the ablation study, leveraging the teacher\u2019s knowledge with initialization and additional losses leads to substantial gains. Other compression techniques have been studied to compress large models. Recent developments in weights pruning reveal that it is possible to remove some heads in the self-attention at test time without significantly degrading the performance Michel et al. [2019].", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study."]}
{"pkey": "distilbert_16", "question": "What is the loss function that is used to train the model?", "answer": "we introduce a triple loss combining language modeling, distillation and cosine-distance losses.\n\nThe final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. The paper authors found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al.", "[2015] we used a softmax-temperature: pi =\nexp(zi/T )\u2211 j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors. 3 DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers. Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers 3E.g.", ". As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. . As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster."]}
{"pkey": "distilbert_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus the paper authors focus on reducing the number of layers.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["[2015] we used a softmax-temperature: pi =\nexp(zi/T )\u2211 j exp(zj/T )\nwhere T controls the smoothness of the output distribution and zi is the model score for the class i. The same temperature T is applied to the student and the teacher at training time, while at inference, T is set to 1 to recover a standard softmax. The final training objective is a linear combination of the distillation loss Lce with the supervised training loss, in our case the masked language modeling loss Lmlm [Devlin et al., 2018]. We found it beneficial to add a cosine embedding loss (Lcos) which will tend to align the directions of the student and teacher hidden states vectors. 3 DistilBERT: a distilled version of BERT. Student architecture In the present work, the student - DistilBERT - has the same general architecture as BERT. The token-type embeddings and the pooler are removed while the number of layers is reduced by a factor of 2. Most of the operations used in the Transformer architecture (linear layer and layer normalisation) are highly optimized in modern linear algebra frameworks and our investigations showed that variations on the last dimension of the tensor (hidden size dimension) have a smaller impact on computation efficiency (for a fixed parameters budget) than variations on other factors like the number of layers. Thus we focus on reducing the number of layers. Student initialization In addition to the previously described optimization and architectural choices, an important element in our training procedure is to find the right initialization for the sub-network to converge. Taking advantage of the common dimensionality between teacher and student networks, we initialize the student from the teacher by taking one layer out of two.\n2https://github.com/huggingface/transformers 3E.g.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study.", ". As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. . As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-theedge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller generalpurpose language representation model, called DistilBERT, which can then be finetuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster."]}
{"pkey": "distilbert_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark\n[Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. The paper authors report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). The paper authors compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4\n\nThe paper authors further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark\n\nThe paper authors also studied whether the paper authors could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a teacher for an additional term in the loss (knowledge distillation). The paper authors were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM.\n\nTo further investigate the speed-up/size trade-off of DistilBERT, the paper authors compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1.\n\nThe paper authors studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["BERT-base\u2019s predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world. . . ). Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100. 4 Experiments. General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task benchmark.", "To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al."]}
{"pkey": "distilbert_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "In this section, the paper authors investigate the influence of various components of the triple loss and the student initialization on the performances of the distilled model. The paper authors report the macro-score on GLUE. Table 4 presents the deltas with the full triple loss: removing the Masked Language Modeling loss has little impact while the two distillation losses account for a large portion of the performance.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", "Downstream tasks We further study the performances of DistilBERT on several downstream tasks under efficient inference constraints: a classification task (IMDb sentiment classification - Maas et al. [2011]) and a question answering task (SQuAD v1.1 - Rajpurkar et al. [2016]). As shown in Table 2, DistilBERT is only 0.6% point behind BERT in test accuracy on the IMDb benchmark while being 40% smaller. On SQuAD, DistilBERT is within 3.9 points of the full BERT. We also studied whether we could add another step of distillation during the adaptation phase by fine-tuning DistilBERT on SQuAD using a BERT model previously fine-tuned on SQuAD as a\n4We use jiant [Wang et al., 2019] to compute the baseline.\nteacher for an additional term in the loss (knowledge distillation). In this setting, there are thus two successive steps of distillation, one during the pre-training phase and one during the adaptation phase. In this case, we were able to reach interesting performances given the size of the model: 79.8 F1 and 70.4 EM, i.e. within 3 points of the full model. Size and inference speed\nTo further investigate the speed-up/size trade-off of DistilBERT, we compare (in Table 3) the number of parameters of each model along with the inference time needed to do a full pass on the STSB development set on CPU (Intel Xeon E5-2690 v3 Haswell @2.9GHz) using a batch size of 1. DistilBERT has 40% fewer parameters than BERT and is 60% faster than BERT. On device computation We studied whether DistilBERT could be used for on-the-edge applications by building a mobile application for question answering. We compare the average inference time on a recent smartphone (iPhone 7 Plus) against our previously trained question answering model based on BERT-base. Excluding the tokenization step, DistilBERT is 71% faster than BERT, and the whole model weighs 207 MB (which could be further reduced with quantization). Our code is available5. 4.2 Ablation study.", "We also show that our compressed models are small enough to run on the edge, e.g. on mobile devices. Using a triple loss, we show that a 40% smaller Transformer (Vaswani et al. [2017]) pre-trained through distillation via the supervision of a bigger Transformer language model can achieve similar performance on a variety of downstream tasks, while being 60% faster at inference time. Further ablation studies indicate that all the components of the triple loss are important for best performances. We have made the trained weights available along with the training code in the Transformers2 library from HuggingFace [Wolf et al., 2019]. 2 Knowledge distillation. Knowledge distillation [Bucila et al., 2006, Hinton et al., 2015] is a compression technique in which a compact model - the student - is trained to reproduce the behaviour of a larger model - the teacher - or an ensemble of models. In supervised learning, a classification model is generally trained to predict an instance class by maximizing the estimated probability of gold labels. A standard training objective thus involves minimizing the cross-entropy between the model\u2019s predicted distribution and the one-hot empirical distribution of training labels. A model performing well on the training set will predict an output distribution with high probability on the correct class and with near-zero probabilities on other classes. But some of these \"near-zero\" probabilities are larger than others and reflect, in part, the generalization capabilities of the model and how well it will perform on the test set3. Training loss The student is trained with a distillation loss over the soft target probabilities of the teacher: Lce = \u2211 i ti \u2217 log(si) where ti (resp. si) is a probability estimated by the teacher (resp. the student). This objective results in a rich training signal by leveraging the full teacher distribution. Following Hinton et al."]}
{"pkey": "distilbert_20", "question": "List the future work mentioned in the paper.", "answer": "Not mentioned in the paper.", "title": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter", "context": ["To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study. 1 Introduction\nThe last two years have seen the rise of Transfer Learning approaches in Natural Language Processing (NLP) with large-scale pre-trained language models becoming a basic tool in many NLP tasks [Devlin et al., 2018, Radford et al., 2019, Liu et al., 2019]. While these models lead to significant improvement, they often have several hundred million parameters and current research1 on pre-trained models indicates that training even larger models still leads to better performances on downstream tasks. The trend toward bigger models raises several concerns. First is the\nenvironmental cost of exponentially scaling these models\u2019 computational requirements as mentioned in Schwartz et al. [2019], Strubell et al. [2019]. Second, while operating these models on-device in real-time has the potential to enable novel and interesting language processing applications, the growing computational and memory requirements of these models may hamper wide adoption.\n1See for instance the recently released MegatronLM (https://nv-adlr.github.io/MegatronLM)\nEMC^2: 5th Edition Co-located with NeurIPS\u201919\nar X\niv :1\n91 0. 01 10\n8v 4\n[ cs\n.C L\n] 1\nM ar\n2 02\nIn this paper, we show that it is possible to reach similar performances on many downstream-tasks using much smaller language models pre-trained with knowledge distillation, resulting in models that are lighter and faster at inference time, while also requiring a smaller computational training budget. Our general-purpose pre-trained models can be fine-tuned with good performances on several downstream tasks, keeping the flexibility of larger models.", "Some layers can be reduced to one head. A separate line of study leverages quantization to derive smaller models (Gupta et al. [2015]). Pruning and quantization are orthogonal to the present work. 5https://github.com/huggingface/swift-coreml-transformers\n6 Conclusion and future work. We introduced DistilBERT, a general-purpose pre-trained version of BERT, 40% smaller, 60% faster, that retains 97% of the language understanding capabilities. We showed that a general-purpose language model can be successfully trained with distillation and analyzed the various components with an ablation study. We further demonstrated that DistilBERT is a compelling option for edge applications.", "BERT-base\u2019s predictions for a masked token in \"I think this is the beginning of a beautiful [MASK]\" comprise two high probability tokens (day and life) and a long tail of valid predictions (future, story, world. . . ). Distillation We applied best practices for training BERT model recently proposed in Liu et al. [2019]. As such, DistilBERT is distilled on very large batches leveraging gradient accumulation (up to 4K examples per batch) using dynamic masking and without the next sentence prediction objective. Data and compute power We train DistilBERT on the same corpus as the original BERT model: a concatenation of English Wikipedia and Toronto Book Corpus [Zhu et al., 2015]. DistilBERT was trained on 8 16GB V100 GPUs for approximately 90 hours. For the sake of comparison, the RoBERTa model [Liu et al., 2019] required 1 day of training on 1024 32GB V100. 4 Experiments. General Language Understanding We assess the language understanding and generalization capabilities of DistilBERT on the General Language Understanding Evaluation (GLUE) benchmark [Wang et al., 2018], a collection of 9 datasets for evaluating natural language understanding systems. We report scores on the development sets for each task by fine-tuning DistilBERT without the use of ensembling or multi-tasking scheme for fine-tuning (which are mostly orthogonal to the present work). We compare the results to the baseline provided by the authors of GLUE: an ELMo (Peters et al. [2018]) encoder followed by two BiLSTMs.4\nThe results on each of the 9 tasks are showed on Table 1 along with the macro-score (average of individual scores). Among the 9 tasks, DistilBERT is always on par or improving over the ELMo baseline (up to 19 points of accuracy on STS-B). DistilBERT also compares surprisingly well to BERT, retaining 97% of the performance with 40% fewer parameters. 4.1 Downstream task benchmark."]}
{"pkey": "bart_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "The paper authors present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["A difference is that UniLM predictions are conditionally independent, whereas BART\u2019s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context. MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder. XL-Net (Yang et al., 2019) extends BERT by pre-\ndicting masked tokens auto-regressively in a permuted order. This objective allows predictions to condition on both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation. Several papers have explored using pre-trained representations to improve machine translation. The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be used to improve machine translation decoders. 8 Conclusions. We introduced BART, a pre-training approach that learns to map corrupted documents to the original. BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.", "We use the same pre-training data as Liu et al. (2019), consisting of 160Gb of news, books, stories, and web text. 5.2 Discriminative Tasks. Table 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011). The most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective. Overall, BART performs similarly, with only small differences between the models on most tasks. suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classification performance.\n5.3 Generation Tasks. We also experiment with several text generation tasks. BART is fine-tuned as a standard sequence-to-sequence model from the input to the output text. During finetuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1. During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017). Summarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties. Summaries in the CNN/DailyMail tend to resemble source sentences. Extractive models do well here, and even the baseline of the first-three source sentences is highly competitive. Nevertheless, BART outperforms all existing work. In contrast, XSum is highly abstractive, and extractive models perform poorly. BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a significant advance in performance on this problem. Qualitatively, sample quality is high (see \u00a76)."]}
{"pkey": "bart_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "The paper does not explicitly mention any gaps in previous literature that it tries to address. However, it can be seen as building on previous pretraining methods, such as BERT and GPT, and attempting to improve upon them by using a denoising autoencoder and novel noising approaches.   BART: \"Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which masked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019). However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.\"", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["A difference is that UniLM predictions are conditionally independent, whereas BART\u2019s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context. MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder. XL-Net (Yang et al., 2019) extends BERT by pre-\ndicting masked tokens auto-regressively in a permuted order. This objective allows predictions to condition on both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation. Several papers have explored using pre-trained representations to improve machine translation. The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be used to improve machine translation decoders. 8 Conclusions. We introduced BART, a pre-training approach that learns to map corrupted documents to the original. BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.", "For sequence classification tasks, the same input is fed into the encoder and decoder, and the final hidden state of the final decoder token is fed into new multi-class linear classifier. This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a). 3.2 Token Classification Tasks. For token classification tasks, such as answer endpoint classification for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word. This representation is used to classify the token. 3.3 Sequence Generation Tasks. Because BART has an autoregressive decoder, it can be directly fine tuned for sequence generation tasks such as abstractive question answering and summarization. In both of these tasks, information is copied from the\ninput but manipulated, which is closely related to the denoising pre-training objective. Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively. 3.4 Machine Translation. We also explore using BART to improve machine translation decoders for translating into English. Previous work Edunov et al. (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited. We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b). More precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder. The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English. The new encoder can use a separate vocabulary from the original BART model."]}
{"pkey": "bart_3", "question": "What are the main contributions of the paper?", "answer": "- They never used WMT'14 dataset\n- The paper never mentioned about wikitext-103 dataset\n- The paper does not discuss the LAMBADA language modeling task.\nBART: \"we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks\", 'A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. The paper authors evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token.'", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["A difference is that UniLM predictions are conditionally independent, whereas BART\u2019s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context. MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder. XL-Net (Yang et al., 2019) extends BERT by pre-\ndicting masked tokens auto-regressively in a permuted order. This objective allows predictions to condition on both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation. Several papers have explored using pre-trained representations to improve machine translation. The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be used to improve machine translation decoders. 8 Conclusions. We introduced BART, a pre-training approach that learns to map corrupted documents to the original. BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.", "2.2 Pre-training BART. BART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document. Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption. In the extreme case, where all information about the source is lost, BART is equivalent to a language model. We experiment with several previously proposed and novel transformations, but we believe there is a significant potential for development of other new alternatives. The transformations we used are summarized below, and examples are shown in Figure 2.\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements. Token Deletion Random tokens are deleted from the input. In contrast to token masking, the model must decide which positions are missing inputs. Text Infilling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3). Each span is replaced with a single [MASK] token. 0-length spans correspond to the insertion of [MASK] tokens. Text infilling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length. Text infilling teaches the model to predict how many tokens are missing from a span. Sentence Permutation A document is divided into sentences based on full stops, and these sentences are shuffled in a random order. Document Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document. 3 Fine-tuning BART. The representations produced by BART can be used in several ways for downstream applications.\n3.1 Sequence Classification Tasks."]}
{"pkey": "bart_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "- The BART model is a general-purpose pretraining method that can be applied to a wide range of text-based tasks. This means that it is possible to extend the model to other domains by pretraining it on a dataset that is specific to that domain. However, the effectiveness of the model in a new domain would depend on the quality and size of the pretraining dataset\n- No such information about how it handles out of domain data\nBART: \"BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes\". \"The representations produced by BART can be used in several ways for downstream applications.\"", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["A difference is that UniLM predictions are conditionally independent, whereas BART\u2019s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context. MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder. XL-Net (Yang et al., 2019) extends BERT by pre-\ndicting masked tokens auto-regressively in a permuted order. This objective allows predictions to condition on both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation. Several papers have explored using pre-trained representations to improve machine translation. The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be used to improve machine translation decoders. 8 Conclusions. We introduced BART, a pre-training approach that learns to map corrupted documents to the original. BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "2.2 Pre-training BART. BART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document. Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption. In the extreme case, where all information about the source is lost, BART is equivalent to a language model. We experiment with several previously proposed and novel transformations, but we believe there is a significant potential for development of other new alternatives. The transformations we used are summarized below, and examples are shown in Figure 2.\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements. Token Deletion Random tokens are deleted from the input. In contrast to token masking, the model must decide which positions are missing inputs. Text Infilling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3). Each span is replaced with a single [MASK] token. 0-length spans correspond to the insertion of [MASK] tokens. Text infilling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length. Text infilling teaches the model to predict how many tokens are missing from a span. Sentence Permutation A document is divided into sentences based on full stops, and these sentences are shuffled in a random order. Document Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document. 3 Fine-tuning BART. The representations produced by BART can be used in several ways for downstream applications.\n3.1 Sequence Classification Tasks.", "The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence. Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART. A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input. BART achieves the most consistently strong performance. With the exception of ELI5, BART models using text-infilling perform well on all tasks. 5 Large-scale Pre-training Experiments. Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora. To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model. 5.1 Experimental Setup. We pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019). Based on the results in Section \u00a74, we use a combination of text infilling and sentence permutation. We mask 30% of tokens in each document, and permute all sentences. Although sentence permutation only shows significant additive gains\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task. To help the model better fit the data, we disabled dropout for the final 10% of training steps."]}
{"pkey": "bart_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "Tasks: Sequence Classification Tasks, Token Classification Task, Sequence Generation Tasks,  Machine Translation Datasets: SQuAD 1.1 MNLI ELI5 XSum ConvAI2 CNN/DM", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["MNLI (Williams et al., 2017), a bitext classification task to predict whether one sentence entails another. The fine-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder. In contrast to BERT, the representation of the EOS token is used to classify the sentences relations. ELI5 (Fan et al., 2019), a long-form abstractive question answering dataset. Models generate answers conditioned on the concatenation of a question and supporting documents. XSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries. ConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona. CNN/DM (Hermann et al., 2015), a news summarization dataset. Summaries here are typically closely related to source sentences. 4.3 Results. Results are shown in Table 1. Several trends are clear:\nPerformance of pre-training methods varies significantly across tasks The effectiveness of pre-training methods is highly dependent on the task. For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results. Token masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation. The successful methods either use token deletion or masking, or self-attention masks. Deletion appears to outperform masking on generation tasks. Left-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training. Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions. However, BART achieves similar performance with only half the number of bidirectional layers.", "The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence. Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART. A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input. BART achieves the most consistently strong performance. With the exception of ELI5, BART models using text-infilling perform well on all tasks. 5 Large-scale Pre-training Experiments. Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora. To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model. 5.1 Experimental Setup. We pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019). Based on the results in Section \u00a74, we use a combination of text infilling and sentence permutation. We mask 30% of tokens in each document, and permute all sentences. Although sentence permutation only shows significant additive gains\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task. To help the model better fit the data, we disabled dropout for the final 10% of training steps.", "Dialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-specified persona. BART outperforms previous work on two automated metrics. Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. We find BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly specified by the question. 5.4 Translation. We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. (2016). We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. Experiment results are presented in Table 6. We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the performance of both steps of our model in the fixed BART and tuned BART rows. For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. We use a beam width of 5 and a length penalty of \u03b1 = 1. Preliminary results suggested that our approach was less effective without back-translation data, and prone to overfitting\u2014future work should explore additional regularization techniques. 6 Qualitative Analysis. BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively. Table 7 shows example summaries generated by BART. Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data."]}
{"pkey": "bart_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not Specified in paper - the paper does not provide any specific analysis or evaluation of the model's bias or prejudice.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["A difference is that UniLM predictions are conditionally independent, whereas BART\u2019s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context. MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder. XL-Net (Yang et al., 2019) extends BERT by pre-\ndicting masked tokens auto-regressively in a permuted order. This objective allows predictions to condition on both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation. Several papers have explored using pre-trained representations to improve machine translation. The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be used to improve machine translation decoders. 8 Conclusions. We introduced BART, a pre-training approach that learns to map corrupted documents to the original. BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "2.2 Pre-training BART. BART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document. Unlike existing denoising autoencoders, which are tailored to specific noising schemes, BART allows us to apply any type of document corruption. In the extreme case, where all information about the source is lost, BART is equivalent to a language model. We experiment with several previously proposed and novel transformations, but we believe there is a significant potential for development of other new alternatives. The transformations we used are summarized below, and examples are shown in Figure 2.\nToken Masking Following BERT (Devlin et al., 2019), random tokens are sampled and replaced with [MASK] elements. Token Deletion Random tokens are deleted from the input. In contrast to token masking, the model must decide which positions are missing inputs. Text Infilling A number of text spans are sampled, with span lengths drawn from a Poisson distribution (\u03bb = 3). Each span is replaced with a single [MASK] token. 0-length spans correspond to the insertion of [MASK] tokens. Text infilling is inspired by SpanBERT (Joshi et al., 2019), but SpanBERT samples span lengths from a different (clamped geometric) distribution, and replaces each span with a sequence of [MASK] tokens of exactly the same length. Text infilling teaches the model to predict how many tokens are missing from a span. Sentence Permutation A document is divided into sentences based on full stops, and these sentences are shuffled in a random order. Document Rotation A token is chosen uniformly at random, and the document is rotated so that it begins with that token. This task trains the model to identify the start of the document. 3 Fine-tuning BART. The representations produced by BART can be used in several ways for downstream applications.\n3.1 Sequence Classification Tasks.", "Following Narayan et al. (2018), we remove the first sentence of the article prior to summarizing it, so there is no easy extractive summary of the document. Unsurprisingly, model output is fluent and grammatical English. However, model output is also highly abstractive, with few phrases copied from the input. The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). In the first example, inferring that fish are protecting reefs from global warming requires non-trivial inference from the text. However, the claim that the work was published in Science is not supported by the source. These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation. 7 Related Work. Early methods for pretraining were based on language models. GPT (Radford et al., 2018) only models leftward context, which is problematic for some tasks. ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train interactions between these features. Radford et al. (2019) demonstrated that very large language models can act as unsupervised multitask models. BERT (Devlin et al., 2019) introduced masked language modelling, which allows pre-training to learn interactions between left and right context words. Recent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). Predictions are not made auto-regressively, reducing the effectiveness of BERT for generation tasks. UniLM (Dong et al., 2019) fine-tunes BERT with an ensemble of masks, some of which allow only leftward context. Like BART, this allows UniLM to be used for both generative and discriminative tasks."]}
{"pkey": "bart_7", "question": "List the limitations of the model discussed in the paper.", "answer": "You can use the raw model for text infilling. However, the model is mostly meant to be fine-tuned on a supervised dataset.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["A difference is that UniLM predictions are conditionally independent, whereas BART\u2019s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context. MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder. XL-Net (Yang et al., 2019) extends BERT by pre-\ndicting masked tokens auto-regressively in a permuted order. This objective allows predictions to condition on both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation. Several papers have explored using pre-trained representations to improve machine translation. The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be used to improve machine translation decoders. 8 Conclusions. We introduced BART, a pre-training approach that learns to map corrupted documents to the original. BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.", "For sequence classification tasks, the same input is fed into the encoder and decoder, and the final hidden state of the final decoder token is fed into new multi-class linear classifier. This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a). 3.2 Token Classification Tasks. For token classification tasks, such as answer endpoint classification for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word. This representation is used to classify the token. 3.3 Sequence Generation Tasks. Because BART has an autoregressive decoder, it can be directly fine tuned for sequence generation tasks such as abstractive question answering and summarization. In both of these tasks, information is copied from the\ninput but manipulated, which is closely related to the denoising pre-training objective. Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively. 3.4 Machine Translation. We also explore using BART to improve machine translation decoders for translating into English. Previous work Edunov et al. (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited. We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b). More precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder. The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English. The new encoder can use a separate vocabulary from the original BART model."]}
{"pkey": "bart_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "Not specifically specified in paper. The paper authors use the same pre-training data as Liu et al. (2019), consisting of 160Gb of news, books, stories, and web text.\" \"For reference, the paper authors compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["Dialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-specified persona. BART outperforms previous work on two automated metrics. Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. We find BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly specified by the question. 5.4 Translation. We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. (2016). We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. Experiment results are presented in Table 6. We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the performance of both steps of our model in the fixed BART and tuned BART rows. For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. We use a beam width of 5 and a length penalty of \u03b1 = 1. Preliminary results suggested that our approach was less effective without back-translation data, and prone to overfitting\u2014future work should explore additional regularization techniques. 6 Qualitative Analysis. BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively. Table 7 shows example summaries generated by BART. Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.", "The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence. Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART. A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input. BART achieves the most consistently strong performance. With the exception of ELI5, BART models using text-infilling perform well on all tasks. 5 Large-scale Pre-training Experiments. Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora. To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model. 5.1 Experimental Setup. We pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019). Based on the results in Section \u00a74, we use a combination of text infilling and sentence permutation. We mask 30% of tokens in each document, and permute all sentences. Although sentence permutation only shows significant additive gains\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task. To help the model better fit the data, we disabled dropout for the final 10% of training steps.", "We use the same pre-training data as Liu et al. (2019), consisting of 160Gb of news, books, stories, and web text. 5.2 Discriminative Tasks. Table 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011). The most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective. Overall, BART performs similarly, with only small differences between the models on most tasks. suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classification performance.\n5.3 Generation Tasks. We also experiment with several text generation tasks. BART is fine-tuned as a standard sequence-to-sequence model from the input to the output text. During finetuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1. During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017). Summarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties. Summaries in the CNN/DailyMail tend to resemble source sentences. Extractive models do well here, and even the baseline of the first-three source sentences is highly competitive. Nevertheless, BART outperforms all existing work. In contrast, XSum is highly abstractive, and extractive models perform poorly. BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a significant advance in performance on this problem. Qualitatively, sample quality is high (see \u00a76)."]}
{"pkey": "bart_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019). BART uses the standard sequence-to-sequence Transformer architecture from Vaswani et al., 2017, except, following GPT, that the paper authors modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02).", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["For sequence classification tasks, the same input is fed into the encoder and decoder, and the final hidden state of the final decoder token is fed into new multi-class linear classifier. This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a). 3.2 Token Classification Tasks. For token classification tasks, such as answer endpoint classification for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word. This representation is used to classify the token. 3.3 Sequence Generation Tasks. Because BART has an autoregressive decoder, it can be directly fine tuned for sequence generation tasks such as abstractive question answering and summarization. In both of these tasks, information is copied from the\ninput but manipulated, which is closely related to the denoising pre-training objective. Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively. 3.4 Machine Translation. We also explore using BART to improve machine translation decoders for translating into English. Previous work Edunov et al. (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited. We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b). More precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder. The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English. The new encoder can use a separate vocabulary from the original BART model.", "The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence. Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART. A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input. BART achieves the most consistently strong performance. With the exception of ELI5, BART models using text-infilling perform well on all tasks. 5 Large-scale Pre-training Experiments. Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora. To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model. 5.1 Experimental Setup. We pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019). Based on the results in Section \u00a74, we use a combination of text infilling and sentence permutation. We mask 30% of tokens in each document, and permute all sentences. Although sentence permutation only shows significant additive gains\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task. To help the model better fit the data, we disabled dropout for the final 10% of training steps.", "We train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model. In the first step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder first layer. In the second step, we train all model parameters for a small number of iterations. 4 Comparing Pre-training Objectives. BART supports a much wider range of noising schemes during pre-training than previous work. We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n4.1 Comparison Objectives. While many pre-training objectives have been proposed, fair comparisons between these have been difficult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and fine-tuning procedures. We\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks. We aim, as much as possible, to control for differences unrelated to the pre-training objective. However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective). For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data. We compare the following approaches:\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model. This model is equivalent to the BART decoder, without cross-attention. Permuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively."]}
{"pkey": "bart_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. Token Masking, Token Deletion, Text Infilling, Sentence Permutation, Document Rotation.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["MNLI (Williams et al., 2017), a bitext classification task to predict whether one sentence entails another. The fine-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder. In contrast to BERT, the representation of the EOS token is used to classify the sentences relations. ELI5 (Fan et al., 2019), a long-form abstractive question answering dataset. Models generate answers conditioned on the concatenation of a question and supporting documents. XSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries. ConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona. CNN/DM (Hermann et al., 2015), a news summarization dataset. Summaries here are typically closely related to source sentences. 4.3 Results. Results are shown in Table 1. Several trends are clear:\nPerformance of pre-training methods varies significantly across tasks The effectiveness of pre-training methods is highly dependent on the task. For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results. Token masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation. The successful methods either use token deletion or masking, or self-attention masks. Deletion appears to outperform masking on generation tasks. Left-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training. Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions. However, BART achieves similar performance with only half the number of bidirectional layers.", "Dialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-specified persona. BART outperforms previous work on two automated metrics. Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. We find BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly specified by the question. 5.4 Translation. We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. (2016). We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. Experiment results are presented in Table 6. We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the performance of both steps of our model in the fixed BART and tuned BART rows. For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. We use a beam width of 5 and a length penalty of \u03b1 = 1. Preliminary results suggested that our approach was less effective without back-translation data, and prone to overfitting\u2014future work should explore additional regularization techniques. 6 Qualitative Analysis. BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively. Table 7 shows example summaries generated by BART. Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.", "The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence. Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART. A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input. BART achieves the most consistently strong performance. With the exception of ELI5, BART models using text-infilling perform well on all tasks. 5 Large-scale Pre-training Experiments. Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora. To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model. 5.1 Experimental Setup. We pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019). Based on the results in Section \u00a74, we use a combination of text infilling and sentence permutation. We mask 30% of tokens in each document, and permute all sentences. Although sentence permutation only shows significant additive gains\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task. To help the model better fit the data, we disabled dropout for the final 10% of training steps."]}
{"pkey": "bart_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "BART uses the standard sequence-to-sequence Transformer architecture from Vaswani et al., 2017, except, following GPT, that the paper authors modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N(0, 0.02). For our base model, the paper authors use 6 layers in the encoder and decoder, and for our large model the paper authors use 12 layers in each. The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before word prediction, which BART does not. In total, BART contains roughly 10% more parameters than the equivalently sized BERT model. The paper authors pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. The paper authors use a batch size of 8000, and train the model for 500000 steps.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["MNLI (Williams et al., 2017), a bitext classification task to predict whether one sentence entails another. The fine-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder. In contrast to BERT, the representation of the EOS token is used to classify the sentences relations. ELI5 (Fan et al., 2019), a long-form abstractive question answering dataset. Models generate answers conditioned on the concatenation of a question and supporting documents. XSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries. ConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona. CNN/DM (Hermann et al., 2015), a news summarization dataset. Summaries here are typically closely related to source sentences. 4.3 Results. Results are shown in Table 1. Several trends are clear:\nPerformance of pre-training methods varies significantly across tasks The effectiveness of pre-training methods is highly dependent on the task. For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results. Token masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation. The successful methods either use token deletion or masking, or self-attention masks. Deletion appears to outperform masking on generation tasks. Left-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training. Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions. However, BART achieves similar performance with only half the number of bidirectional layers.", "We train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model. In the first step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder first layer. In the second step, we train all model parameters for a small number of iterations. 4 Comparing Pre-training Objectives. BART supports a much wider range of noising schemes during pre-training than previous work. We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n4.1 Comparison Objectives. While many pre-training objectives have been proposed, fair comparisons between these have been difficult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and fine-tuning procedures. We\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks. We aim, as much as possible, to control for differences unrelated to the pre-training objective. However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective). For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data. We compare the following approaches:\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model. This model is equivalent to the BART decoder, without cross-attention. Permuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.", "These layers are trained to essentially translate the foreign language to noised\nar X\niv :1\n91 0.\n13 46\n1v 1\n[ cs\n.C L\n] 2\n9 O\nct 2\n01 9\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model. This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark. To better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019). We find that BART exhibits the most consistently strong performance across the full range of tasks we consider. 2 Model.\nBART is a denoising autoencoder that maps a corrupted document to the original document it was derived from. It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder. For pre-training, we optimize the negative log likelihood of the original document. 2.1 Architecture. BART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02). For our base model, we use 6 layers in the encoder and de-\ncoder, and for our large model we use 12 layers in each. The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not. In total, BART contains roughly 10% more parameters than the equivalently sized BERT model."]}
{"pkey": "bart_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "Training setup information like learning rate or optimizer used are not mentioned, but following details are available: In total, BART contains roughly 10% more parameters than the equivalently sized BERT model. The paper authors pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. The paper authors use a batch size of 8000, and train the model for 500000 steps. For pre-training, the paper authors optimize the negative log likelihood of the original document.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["We train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model. In the first step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder first layer. In the second step, we train all model parameters for a small number of iterations. 4 Comparing Pre-training Objectives. BART supports a much wider range of noising schemes during pre-training than previous work. We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n4.1 Comparison Objectives. While many pre-training objectives have been proposed, fair comparisons between these have been difficult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and fine-tuning procedures. We\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks. We aim, as much as possible, to control for differences unrelated to the pre-training objective. However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective). For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data. We compare the following approaches:\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model. This model is equivalent to the BART decoder, without cross-attention. Permuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively.", "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance. 1 Introduction. Self-supervised methods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019). The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out. Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which\nmasked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019). However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.", "The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence. Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART. A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input. BART achieves the most consistently strong performance. With the exception of ELI5, BART models using text-infilling perform well on all tasks. 5 Large-scale Pre-training Experiments. Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora. To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model. 5.1 Experimental Setup. We pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019). Based on the results in Section \u00a74, we use a combination of text infilling and sentence permutation. We mask 30% of tokens in each document, and permute all sentences. Although sentence permutation only shows significant additive gains\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task. To help the model better fit the data, we disabled dropout for the final 10% of training steps."]}
{"pkey": "bart_13", "question": "Describe the computational resources used to train the model.", "answer": "\"Not specified in paper\" But with below argument the paper authors can approximate it. \"It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answer_x0002_ing, and summarization tasks.\"", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["For consistency with other models, we do not implement the relative positional embeddings or attention across segments from XLNet. Masked Language Model Following BERT (Devlin et al., 2019), we replace 15% of tokens with [MASK] symbols, and train the model to independently predict the original tokens. Multitask Masked Language Model As in UniLM (Dong et al., 2019), we train a Masked Language Model with additional self-attention masks. Self attention masks are chosen randomly in with the follow proportions: 1/6 left-to-right, 1/6 right-to-left, 1/3 unmasked, and 1/3 with the first 50% of tokens unmasked and a left-to-right mask for the remainder. Masked Seq-to-Seq Inspired by MASS (Song et al., 2019), we mask a span containing 50% of tokens, and train a sequence to sequence model to predict the masked tokens. For the Permuted LM, Masked LM and Multitask Masked LM, we use two-stream attention (Yang et al., 2019) to efficiently compute likelihoods of the output part of the sequence (using a diagonal self-attention mask on the output to predict words left-to-right). We experiment with (1) treating the task as a standard sequence-to-sequence problem, where the source input to the encoder and the target is the decoder output, or (2) adding the source as prefix to the target in the decoder, with a loss only on the target part of the sequence. We find the former works better for BART models, and the latter for other models. To most directly compare our models on their ability to model their fine-tuning objective (the log likelihood of the human text), we report perplexity in Table 1.\n4.2 Tasks. SQuAD (Rajpurkar et al., 2016)a an extractive question answering task on Wikipedia paragraphs. Answers are text spans extracted from a given document context. Similar to BERT (Devlin et al., 2019), we use concatenated question and context as input to the encoder of BART, and additionally pass them to the decoder. The model includes classifiers to predict the start and end indices of each token.", "Dialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-specified persona. BART outperforms previous work on two automated metrics. Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. We find BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly specified by the question. 5.4 Translation. We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. (2016). We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. Experiment results are presented in Table 6. We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the performance of both steps of our model in the fixed BART and tuned BART rows. For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. We use a beam width of 5 and a length penalty of \u03b1 = 1. Preliminary results suggested that our approach was less effective without back-translation data, and prone to overfitting\u2014future work should explore additional regularization techniques. 6 Qualitative Analysis. BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively. Table 7 shows example summaries generated by BART. Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.", "We train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model. In the first step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder first layer. In the second step, we train all model parameters for a small number of iterations. 4 Comparing Pre-training Objectives. BART supports a much wider range of noising schemes during pre-training than previous work. We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n4.1 Comparison Objectives. While many pre-training objectives have been proposed, fair comparisons between these have been difficult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and fine-tuning procedures. We\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks. We aim, as much as possible, to control for differences unrelated to the pre-training objective. However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective). For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data. We compare the following approaches:\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model. This model is equivalent to the BART decoder, without cross-attention. Permuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively."]}
{"pkey": "bart_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "No such specific details are provided, but the paper authors can refer to the following details: Architectural Details: BART uses the standard sequence-to-sequence Transformer architecture from Vaswani et al., 2017, except, following GPT, that the paper authors modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N(0, 0.02). For our base model, the paper authors use 6 layers in the encoder and decoder, and for our large model the paper authors use 12 layers in each. The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before word prediction, which BART does not. Training Details: Following RoBERTa (Liu et al., 2019), the paper authors use a batch size of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2. During generation, the paper authors set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["A difference is that UniLM predictions are conditionally independent, whereas BART\u2019s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context. MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder. XL-Net (Yang et al., 2019) extends BERT by pre-\ndicting masked tokens auto-regressively in a permuted order. This objective allows predictions to condition on both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation. Several papers have explored using pre-trained representations to improve machine translation. The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be used to improve machine translation decoders. 8 Conclusions. We introduced BART, a pre-training approach that learns to map corrupted documents to the original. BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.", "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance. 1 Introduction. Self-supervised methods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019). The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out. Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which\nmasked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019). However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability."]}
{"pkey": "bart_15", "question": "What is the pretraining objective of the model? ", "answer": "BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. The paper authors re-implement strong pre-training approaches recently proposed for discriminative and generation tasks. The paper authors compare the following approaches: Language Model, Permuted Language Model, Masked Language Model, Multitask Masked Language Model, Masked Seq-to-Seq. BART is particularly effective when fine-tuned for text generation but also works well for comprehension tasks. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers.", "For sequence classification tasks, the same input is fed into the encoder and decoder, and the final hidden state of the final decoder token is fed into new multi-class linear classifier. This approach is related to the CLS token in BERT; however we add the additional token to the end so that representation for the token in the decoder can attend to decoder states from the complete input (Figure 3a). 3.2 Token Classification Tasks. For token classification tasks, such as answer endpoint classification for SQuAD, we feed the complete document into the encoder and decoder, and use the top hidden state of the decoder as a representation for each word. This representation is used to classify the token. 3.3 Sequence Generation Tasks. Because BART has an autoregressive decoder, it can be directly fine tuned for sequence generation tasks such as abstractive question answering and summarization. In both of these tasks, information is copied from the\ninput but manipulated, which is closely related to the denoising pre-training objective. Here, the encoder input is the input sequence, and the decoder generates outputs autoregressively. 3.4 Machine Translation. We also explore using BART to improve machine translation decoders for translating into English. Previous work Edunov et al. (2019) has shown that models can be improved by incorporating pre-trained encoders, but gains from using pre-trained language models in decoders have been limited. We show that it is possible to use the entire BART model (both encoder and decoder) as a single pretrained decoder for machine translation, by adding a new set of encoder parameters that are learned from bitext (see Figure 3b). More precisely, we replace BART\u2019s encoder embedding layer with a new randomly initialized encoder. The model is trained end-to-end, which trains the new encoder to map foreign words into an input that BART can de-noise to English. The new encoder can use a separate vocabulary from the original BART model.", "The pre-training objective is not the only important factor Our Permuted Language Model performs less well than XLNet (Yang et al., 2019). Some of this difference is likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence. Pure language models perform best on ELI5 The ELI5 dataset is an outlier, with much higher perplexities than other tasks, and is the only generation task where other models outperform BART. A pure language model performs best, suggesting that BART is less effective when the output is only loosely constrained by the input. BART achieves the most consistently strong performance. With the exception of ELI5, BART models using text-infilling perform well on all tasks. 5 Large-scale Pre-training Experiments. Recent work has shown that downstream performance can dramatically improve when pre-training is scaled to large batch sizes (Yang et al., 2019; Liu et al., 2019) and corpora. To test how well BART performs in this regime, and to create a useful model for downstream tasks, we trained BART using the same scale as the RoBERTa model. 5.1 Experimental Setup. We pre-train a large model with 12 layers in each of the encoder and decoder, and a hidden size of 1024. Following RoBERTa (Liu et al., 2019), we use a batch size of 8000, and train the model for 500000 steps. Documents are tokenized with the same byte-pair encoding as GPT-2 (Radford et al., 2019). Based on the results in Section \u00a74, we use a combination of text infilling and sentence permutation. We mask 30% of tokens in each document, and permute all sentences. Although sentence permutation only shows significant additive gains\non the CNN/DM summarization dataset, we hypothesised that larger pre-trained models may be better able to learn from this task. To help the model better fit the data, we disabled dropout for the final 10% of training steps."]}
{"pkey": "bart_16", "question": "What is the loss function that is used to train the model?", "answer": "For pre-training, the paper authors optimize the negative log likelihood of the original document. BART is trained by corrupting documents and then optimizing a reconstruction loss\u2014the cross-entropy between the decoder\u2019s output and the original document. During finetuning, the paper authors use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1. To most directly compare our models on their ability to model their fine-tuning objective (the log likelihood of the human text), the paper authors report perplexity in Table 1.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. We present BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.", "These layers are trained to essentially translate the foreign language to noised\nar X\niv :1\n91 0.\n13 46\n1v 1\n[ cs\n.C L\n] 2\n9 O\nct 2\n01 9\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model. This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark. To better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019). We find that BART exhibits the most consistently strong performance across the full range of tasks we consider. 2 Model.\nBART is a denoising autoencoder that maps a corrupted document to the original document it was derived from. It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder. For pre-training, we optimize the negative log likelihood of the original document. 2.1 Architecture. BART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02). For our base model, we use 6 layers in the encoder and de-\ncoder, and for our large model we use 12 layers in each. The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not. In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.", "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers."]}
{"pkey": "bart_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "BART uses the standard sequence-to-sequence Transformer architecture from Vaswani et al., 2017, which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), except, following GPT, that the paper authors modify ReLU activation functions to GeLUs. For our base model, the paper authors use 6 layers in the encoder and decoder, and for our large model, the paper authors use 12 layers in each. The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder. (2) BERT uses an additional feed-forward network before word prediction, which BART does not. BART keeps the original Transformer architecture, but it modifies the pretraining objective with text infilling corruption, where some text spans are replaced with a single mask token.{https://huggingface.co/docs/transformers/model_summary#:~:text=The%20pretraining%20objective%20is%20to,richer%20representation%20of%20the%20inputs.}", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["Following Narayan et al. (2018), we remove the first sentence of the article prior to summarizing it, so there is no easy extractive summary of the document. Unsurprisingly, model output is fluent and grammatical English. However, model output is also highly abstractive, with few phrases copied from the input. The output is also generally factually accurate, and integrates supporting evidence from across the input document with background knowledge (for example, correctly completing names, or inferring that PG&E operates in California). In the first example, inferring that fish are protecting reefs from global warming requires non-trivial inference from the text. However, the claim that the work was published in Science is not supported by the source. These samples demonstrate that the BART pretraining has learned a strong combination of natural language understanding and generation. 7 Related Work. Early methods for pretraining were based on language models. GPT (Radford et al., 2018) only models leftward context, which is problematic for some tasks. ELMo (Peters et al., 2018) concatenates left-only and right-only representations, but does not pre-train interactions between these features. Radford et al. (2019) demonstrated that very large language models can act as unsupervised multitask models. BERT (Devlin et al., 2019) introduced masked language modelling, which allows pre-training to learn interactions between left and right context words. Recent work has shown that very strong performance can be achieved by training for longer (Liu et al., 2019), by tying parameters across layers (Lan et al., 2019), and by masking spans instead of words (Joshi et al., 2019). Predictions are not made auto-regressively, reducing the effectiveness of BERT for generation tasks. UniLM (Dong et al., 2019) fine-tunes BERT with an ensemble of masks, some of which allow only leftward context. Like BART, this allows UniLM to be used for both generative and discriminative tasks.", "MNLI (Williams et al., 2017), a bitext classification task to predict whether one sentence entails another. The fine-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder. In contrast to BERT, the representation of the EOS token is used to classify the sentences relations. ELI5 (Fan et al., 2019), a long-form abstractive question answering dataset. Models generate answers conditioned on the concatenation of a question and supporting documents. XSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries. ConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona. CNN/DM (Hermann et al., 2015), a news summarization dataset. Summaries here are typically closely related to source sentences. 4.3 Results. Results are shown in Table 1. Several trends are clear:\nPerformance of pre-training methods varies significantly across tasks The effectiveness of pre-training methods is highly dependent on the task. For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results. Token masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation. The successful methods either use token deletion or masking, or self-attention masks. Deletion appears to outperform masking on generation tasks. Left-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training. Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions. However, BART achieves similar performance with only half the number of bidirectional layers.", "We train the source encoder in two steps, in both cases backpropagating the cross-entropy loss from the output of the BART model. In the first step, we freeze most of BART parameters and only update the randomly initialized source encoder, the BART positional embeddings, and the self-attention input projection matrix of BART\u2019s encoder first layer. In the second step, we train all model parameters for a small number of iterations. 4 Comparing Pre-training Objectives. BART supports a much wider range of noising schemes during pre-training than previous work. We compare a range of options using base-size models (6 encoder and 6 decoder layers, with a hidden size of 768), evaluated on a representative subset of the tasks we will consider for the full large scale experiments in \u00a75.\n4.1 Comparison Objectives. While many pre-training objectives have been proposed, fair comparisons between these have been difficult to perform, at least in part due to differences in training data, training resources, architectural differences between models, and fine-tuning procedures. We\nre-implement strong pre-training approaches recently proposed for discriminative and generation tasks. We aim, as much as possible, to control for differences unrelated to the pre-training objective. However, we do make minor changes to the learning rate and usage of layer normalisation in order to improve performance (tuning these separately for each objective). For reference, we compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data. We compare the following approaches:\nLanguage Model Similarly to GPT (Radford et al., 2018), we train a left-to-right Transformer language model. This model is equivalent to the BART decoder, without cross-attention. Permuted Language Model Based on XLNet (Yang et al., 2019), we sample 1/6 of the tokens, and generate them in a random order autoregressively."]}
{"pkey": "bart_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "We compare our implementations with published numbers from BERT, which was also trained for 1M steps on a combination of books and Wikipedia data. The paper authors compare the following approaches: Language Model, Permuted Language Model, Masked Language Model, Multitask Masked Language Model, Masked Seq-to-Seq. There are no specific details provided about the Input or Expected output but here are the descriptions of the tasks. The model was evaluated on the following tasks: SQuAD: an extractive question answering task on Wikipedia paragraphs. The paper authors use concatenated question and context as input to the encoder, and additionally pass them to the decoder. The model includes classifiers to predict the start and end indices of each token. MNLI: a bitext classification task to predict whether one sentence entails another. Appended an EOS token, and passes them to both the BART encoder and decoder. The representation of the <EOS> token is used to classify the sentences relations. ELI5: a long-form abstractive question answering dataset. XSum: a news summarization dataset with highly abstractive summaries. ConvAI2: a dialogue response generation task, conditioned on context and a persona. CNN/DM: a news summarization dataset. Summaries here are typically closely related to source sentences. Results: Performance of pre-training methods varies significantly across tasks. A simple language model achieves the best ELI5 performance, but the worst SQuAD results. Token masking is crucial. Deletion appears to outperform masking on generation tasks. Left-to-right pre-training improves generation. The Masked Language Model and the Permuted Language Model perform less well. Bidirectional encoders are crucial for SQuAD likely due to not including other architectural improvements, such as relative-position embeddings or segment-level recurrence. Pure language models perform best on ELI5. BART achieves the most consistently strong performance with the exception of ELI5. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["Dialogue We evaluate dialogue response generation on CONVAI2 (Dinan et al., 2019), in which agents must generate responses conditioned on both the previous context and a textually-specified persona. BART outperforms previous work on two automated metrics. Abstractive QA We use the recently proposed ELI5 dataset to test the model\u2019s ability to generate long freeform answers. We find BART outperforms the best previous work by 1.2 ROUGE-L, but the dataset remains a challenging, because answers are only weakly specified by the question. 5.4 Translation. We also evaluated performance on WMT16 RomanianEnglish, augmented with back-translation data from Sennrich et al. (2016). We use a 6-layer transformer source encoder to map Romanian into a representation that BART is able to de-noise into English, following the approach introduced in \u00a73.4. Experiment results are presented in Table 6. We compare our results against a baseline Transformer architecture (Vaswani et al., 2017) with Transformerlarge settings (the baseline row). We show the performance of both steps of our model in the fixed BART and tuned BART rows. For each row we experiment on the original WMT16 Romanian-English augmented with back-translation data. We use a beam width of 5 and a length penalty of \u03b1 = 1. Preliminary results suggested that our approach was less effective without back-translation data, and prone to overfitting\u2014future work should explore additional regularization techniques. 6 Qualitative Analysis. BART shows large improvements on summarization metrics, of up to 6 points over the prior state-of-the-art. To understand BART\u2019s performance beyond automated metrics, we analyse its generations qualitatively. Table 7 shows example summaries generated by BART. Examples are taken from WikiNews articles published after the creation of the pre-training corpus, to eliminate the possibility of the events described being present in the model\u2019s training data.", "We use the same pre-training data as Liu et al. (2019), consisting of 160Gb of news, books, stories, and web text. 5.2 Discriminative Tasks. Table 2 compares the performance of BART with several recent approaches on the well-studied SQuAD and GLUE tasks (Warstadt et al., 2018; Socher et al., 2013; Dolan & Brockett, 2005; Agirre et al., 2007; Williams et al., 2018; Dagan et al., 2006; Levesque et al., 2011). The most directly comparable baseline is RoBERTa, which was pre-trained with the same resources, but a different objective. Overall, BART performs similarly, with only small differences between the models on most tasks. suggesting that BART\u2019s improvements on generation tasks do not come at the expense of classification performance.\n5.3 Generation Tasks. We also experiment with several text generation tasks. BART is fine-tuned as a standard sequence-to-sequence model from the input to the output text. During finetuning we use a label smoothed cross entropy loss (Pereyra et al., 2017), with the smoothing parameter set to 0.1. During generation, we set beam size as 5, remove duplicated trigrams in beam search, and tuned the model with min-len, max-len, length penalty on the validation set (Fan et al., 2017). Summarization To provide a comparison with the state-of-the-art in summarization, we present results on two summarization datasets, CNN/DailyMail and XSum, which have distinct properties. Summaries in the CNN/DailyMail tend to resemble source sentences. Extractive models do well here, and even the baseline of the first-three source sentences is highly competitive. Nevertheless, BART outperforms all existing work. In contrast, XSum is highly abstractive, and extractive models perform poorly. BART outperforms the best previous work, which leverages BERT, by roughly 6.0 points on all ROUGE metrics\u2014representing a significant advance in performance on this problem. Qualitatively, sample quality is high (see \u00a76).", "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance. 1 Introduction. Self-supervised methods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019). The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out. Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which\nmasked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019). However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability."]}
{"pkey": "bart_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "The paper authors also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance. We also report an ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019). The paper authors find that BART exhibits the most consistently strong performance across the full range of tasks the paper authors consider.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["These layers are trained to essentially translate the foreign language to noised\nar X\niv :1\n91 0.\n13 46\n1v 1\n[ cs\n.C L\n] 2\n9 O\nct 2\n01 9\nEnglish, by propagation through BART, thereby using BART as a pre-trained target-side language model. This approach improves performance over a strong back-translation MT baseline by 1.1 BLEU on the WMT Romanian-English benchmark. To better understand these effects, we also report an ablation analysis that replicates other recently proposed training objectives. This study allows us to carefully control for a number of factors, including data and optimization parameters, which have been shown to be as important for overall performance as the selection of training objectives (Liu et al., 2019). We find that BART exhibits the most consistently strong performance across the full range of tasks we consider. 2 Model.\nBART is a denoising autoencoder that maps a corrupted document to the original document it was derived from. It is implemented as a sequence-to-sequence model with a bidirectional encoder over corrupted text and a left-to-right autoregressive decoder. For pre-training, we optimize the negative log likelihood of the original document. 2.1 Architecture. BART uses the standard sequence-to-sequence Transformer architecture from (Vaswani et al., 2017), except, following GPT, that we modify ReLU activation functions to GeLUs (Hendrycks & Gimpel, 2016) and initialise parameters from N (0, 0.02). For our base model, we use 6 layers in the encoder and de-\ncoder, and for our large model we use 12 layers in each. The architecture is closely related to that used in BERT, with the following differences: (1) each layer of the decoder additionally performs cross-attention over the final hidden layer of the encoder (as in the transformer sequence-to-sequence model); and (2) BERT uses an additional feed-forward network before wordprediction, which BART does not. In total, BART contains roughly 10% more parameters than the equivalently sized BERT model.", "It uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where spans of text are replaced with a single mask token. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa with comparable training resources on GLUE and SQuAD, achieves new stateof-the-art results on a range of abstractive dialogue, question answering, and summarization tasks, with gains of up to 6 ROUGE. BART also provides a 1.1 BLEU increase over a back-translation system for machine translation, with only target language pretraining. We also report ablation experiments that replicate other pretraining schemes within the BART framework, to better measure which factors most influence end-task performance. 1 Introduction. Self-supervised methods have achieved remarkable success in a wide range of NLP tasks (Mikolov et al., 2013; Peters et al., 2018; Devlin et al., 2019; Joshi et al., 2019; Yang et al., 2019; Liu et al., 2019). The most successful approaches have been variants of masked language models, which are denoising autoencoders that are trained to reconstruct text where a random subset of the words has been masked out. Recent work has shown gains by improving the distribution of masked tokens (Joshi et al., 2019), the order in which\nmasked tokens are predicted (Yang et al., 2019), and the available context for replacing masked tokens (Dong et al., 2019). However, these methods typically focus on particular types of end tasks (e.g. span prediction, generation, etc.), limiting their applicability.", "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers."]}
{"pkey": "bart_20", "question": "List the future work mentioned in the paper.", "answer": "Preliminary results suggested that our approach was less effective without back-translation data, and prone to overfitting\u2014future work should explore additional regularization techniques. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension", "context": ["A difference is that UniLM predictions are conditionally independent, whereas BART\u2019s are autoregressive. BART reduces the mismatch between pre-training and generation tasks, because the decoder is always trained on uncorrupted context. MASS (Song et al., 2019) is perhaps the most similar model to BART. An input sequence where a contiguous span of tokens is masked is mapped to a sequence consisting of the missing tokens. MASS is less effective for discriminative tasks, because disjoint sets of tokens are fed into the encoder and decoder. XL-Net (Yang et al., 2019) extends BERT by pre-\ndicting masked tokens auto-regressively in a permuted order. This objective allows predictions to condition on both left and right context. In contrast, the BART decoder works left-to-right during pre-training, matching the setting during generation. Several papers have explored using pre-trained representations to improve machine translation. The largest improvements have come from pre-training on both source and target languages (Song et al., 2019; Lample & Conneau, 2019), but this requires pretraining on all languages of interest. Other work has shown that encoders can be improved using pre-trained representations (Edunov et al., 2019), but gains in decoders are more limited. We show how BART can be used to improve machine translation decoders. 8 Conclusions. We introduced BART, a pre-training approach that learns to map corrupted documents to the original. BART achieves similar performance to RoBERTa on discriminative tasks, while achieving new state-of-theart results on a number of text generation tasks. Future work should explore new methods for corrupting documents for pre-training, perhaps tailoring them to specific end tasks.", "MNLI (Williams et al., 2017), a bitext classification task to predict whether one sentence entails another. The fine-tuned model concatenates the two sentences with appended an EOS token, and passes them to both the BART encoder and decoder. In contrast to BERT, the representation of the EOS token is used to classify the sentences relations. ELI5 (Fan et al., 2019), a long-form abstractive question answering dataset. Models generate answers conditioned on the concatenation of a question and supporting documents. XSum (Narayan et al., 2018), a news summarization dataset with highly abstractive summaries. ConvAI2 (Dinan et al., 2019), a dialogue response generation task, conditioned on context and a persona. CNN/DM (Hermann et al., 2015), a news summarization dataset. Summaries here are typically closely related to source sentences. 4.3 Results. Results are shown in Table 1. Several trends are clear:\nPerformance of pre-training methods varies significantly across tasks The effectiveness of pre-training methods is highly dependent on the task. For example, a simple language model achieves the best ELI5 performance, but the worst SQUAD results. Token masking is crucial Pre-training objectives based on rotating documents or permuting sentences perform poorly in isolation. The successful methods either use token deletion or masking, or self-attention masks. Deletion appears to outperform masking on generation tasks. Left-to-right pre-training improves generation The Masked Language Model and the Permuted Language Model perform less well than others on generation, and are the only models we consider that do not include left-to-right auto-regressive language modelling during pre-training. Bidirectional encoders are crucial for SQuAD As noted in previous work (Devlin et al., 2019), just left-to-right decoder performs poorly on SQuAD, because future context is crucial in classification decisions. However, BART achieves similar performance with only half the number of bidirectional layers.", "In this paper, we present BART, which pre-trains a model combining Bidirectional and Auto-Regressive Transformers. BART is a denoising autoencoder built with a sequence-to-sequence model that is applicable to a very wide range of end tasks. Pretraining has two stages (1) text is corrupted with an arbitrary noising function, and (2) a sequence-to-sequence model is learned to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which, despite its simplicity, can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right decoder), and many other more recent pretraining schemes (see Figure 1). A key advantage of this setup is the noising flexibility; arbitrary transformations can be applied to the original text, including changing its length. We evaluate a number of noising approaches, finding the best performance by both randomly shuffling the order of the original sentences and using a novel in-filling scheme, where arbitrary length spans of text (including zero length) are replaced with a single mask token. This approach generalizes the original word masking and next sentence prediction objectives in BERT by forcing the model to reason more about overall sentence length and make longer range transformations to the input. BART is particularly effective when fine tuned for text generation but also works well for comprehension tasks. It matches the performance of RoBERTa (Liu et al., 2019) with comparable training resources on GLUE (Wang et al., 2018) and SQuAD (Rajpurkar et al., 2016), and achieves new state-of-the-art results on a range of abstractive dialogue, question answering, and summarization tasks. For example, it improves performance by 6 ROUGE over previous work on XSum (Narayan et al., 2018). BART also opens up new ways of thinking about fine tuning. We present a new scheme for machine translation where a BART model is stacked above a few additional transformer layers."]}
{"pkey": "roberta_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "The paper authors find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background.", "On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1). We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems build upon either BERT (Devlin et al., 2019) or XLNet (Yang et al., 2019), both of which rely on additional external training data. In contrast, our submission does not use any additional data. Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation. 5.3 RACE Results. In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct. We modify RoBERTa for this task by concate-\nnating each candidate answer with the corresponding question and passage. We then encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer. We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.\nResults on the RACE test sets are presented in Table 7. RoBERTa achieves state-of-the-art results on both middle-school and high-school settings. 6 Related Work. Pretraining methods have been designed with different training objectives, including language modeling (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018), machine translation (McCann et al., 2017), and masked language modeling (Devlin et al., 2019; Lample and Conneau, 2019). Many recent papers have used a basic recipe of finetuning models for each end task (Howard and Ruder, 2018; Radford et al., 2018), and pretraining with some variant of a masked language model objective.", "As discussed in Section 2, BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training. We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets. 7Studying architectural changes, including larger architectures, is an important area for future work. Masking SQuAD 2.0 MNLI-m SST-2\nResults Table 1 compares the published BERTBASE results from Devlin et al. (2019) to our reimplementation with either static or dynamic masking. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking. Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments. 4.2 Model Input Format and Next Sentence Prediction. In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p = 0.5) or from distinct documents. In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss. The NSP loss was hypothesized to be an important factor in training the original BERT model. Devlin et al. (2019) observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1."]}
{"pkey": "roberta_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. The paper authors also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks. GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1. SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in\n5The authors and their affiliated institutions are not in any way affiliated with the creation of the OpenWebText dataset.", "Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. A more detailed comparison of these encodings is left to future work. 5 RoBERTa. In the previous section we propose modifications to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined impact. We call this configuration RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4). Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data. For example, the recently proposed XLNet architecture (Yang et al., 2019) is pretrained using nearly 10 times more data than the original BERT (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to BERT. To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in\nDevlin et al. (2019). We pretrain our model using 1024 V100 GPUs for approximately one day. Results We present our results in Table 4. When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERTLARGE results, reaffirming the importance of the design choices we explored in Section 4. Next, we combine this data with the three additional datasets described in Section 3.2.", "Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background."]}
{"pkey": "roberta_3", "question": "What are the main contributions of the paper?", "answer": "The contributions of this paper are: (1) The paper authors present a set of important BERT design choices and training strategies and introduce alternatives that lead to better downstream task performance; (2) The paper authors use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background.", "L\n] 2\n6 Ju\nl 2 01\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.1\n1 Introduction. Self-training methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLNet (Yang et al., 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances. \u2217Equal contribution. 1Our models and code are available at:\nhttps://github.com/pytorch/fairseq\nWe present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods.", "Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks. GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1. SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in\n5The authors and their affiliated institutions are not in any way affiliated with the creation of the OpenWebText dataset."]}
{"pkey": "roberta_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as the paper authors will show, hyperparameter choices have significant impact on the final results. The paper authors present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. The paper authors find that performance can be substantially improved by training the model longer, with bigger batches over more data; removing the next sentence prediction objective; training on longer sequences; and dynamically changing the masking pattern applied to the training data. Our improved pretraining procedure, which the paper authors call RoBERTa, achieves state-of-the-art results on GLUE, RACE and SQuAD, without multi-task finetuning for GLUE or additional data for SQuAD.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["Unlike Devlin et al. (2019), we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates. We train only with full-length sequences. We train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 \u00d7 32GB Nvidia V100 GPUs interconnected by Infiniband (Micikevicius et al., 2018). 3.2 Data.\nBERT-style pretraining crucially relies on large quantities of text. Baevski et al. (2019) demonstrate that increasing data size can result in improved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT (Radford et al., 2019; Yang et al., 2019; Zellers et al., 2019). Unfortunately, not all of the additional datasets can be publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison. We consider five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. We use the following text corpora:\n\u2022 BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA. This is the original data used to\ntrain BERT. (16GB). \u2022 CC-NEWS, which we collected from the English portion of the CommonCrawl News\ndataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering).4\n\u2022 OPENWEBTEXT (Gokaslan and Cohen, 2019), an open-source recreation of the WebText cor-\n4We use news-please (Hamborg et al., 2017) to collect and extract CC-NEWS. CC-NEWS is similar to the REALNEWS dataset described in Zellers et al. (2019). pus described in Radford et al. (2019). The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).5\n\u2022 STORIES, a dataset introduced in Trinh and Le (2018) containing a subset of CommonCrawl\ndata filtered to match the story-like style of Winograd schemas. (31GB). 3.3 Evaluation.", "We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training. In the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Specifically\n9Our experiments conflate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work. we consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.\n5.1 GLUE Results. For GLUE we consider two finetuning settings. In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep for each task, with batch sizes \u2208 {16, 32} and learning rates \u2208 {1e\u22125, 2e\u22125, 3e\u22125}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task\u2019s evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling. In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard.", "Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. A more detailed comparison of these encodings is left to future work. 5 RoBERTa. In the previous section we propose modifications to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined impact. We call this configuration RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4). Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data. For example, the recently proposed XLNet architecture (Yang et al., 2019) is pretrained using nearly 10 times more data than the original BERT (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to BERT. To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in\nDevlin et al. (2019). We pretrain our model using 1024 V100 GPUs for approximately one day. Results We present our results in Table 4. When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERTLARGE results, reaffirming the importance of the design choices we explored in Section 4. Next, we combine this data with the three additional datasets described in Section 3.2."]}
{"pkey": "roberta_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE, and STS-B. The paper authors also match state-of-the-art results on SQuAD and RACE.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks. GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1. SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in\n5The authors and their affiliated institutions are not in any way affiliated with the creation of the OpenWebText dataset.", "Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background.", "6The datasets are: CoLA (Warstadt et al., 2018), Stanford Sentiment Treebank (SST) (Socher et al., 2013), Microsoft Research Paragraph Corpus (MRPC) (Dolan and Brockett, 2005), Semantic Textual Similarity Benchmark (STS) (Agirre et al., 2007), Quora Question Pairs (QQP) (Iyer et al., 2016), MultiGenre NLI (MNLI) (Williams et al., 2018), Question NLI (QNLI) (Rajpurkar et al., 2016), Recognizing Textual Entailment (RTE) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and Winograd NLI (WNLI) (Levesque et al., 2011). V2.0 some questions are not answered in the provided context, making the task more challenging. For SQuAD V1.1 we adopt the same span prediction method as BERT (Devlin et al., 2019). For SQuAD V2.0, we add an additional binary classifier to predict whether the question is answerable, which we train jointly by summing the classification and span loss terms. During evaluation, we only predict span indices on pairs that are classified as answerable. RACE The ReAding Comprehension from Examinations (RACE) (Lai et al., 2017) task is a large-scale reading comprehension dataset with more than 28,000 passages and nearly 100,000 questions. The dataset is collected from English examinations in China, which are designed for middle and high school students. In RACE, each passage is associated with multiple questions. For every question, the task is to select one correct answer from four options. RACE has significantly longer context than other popular reading comprehension datasets and the proportion of questions that requires reasoning is very large. 4 Training Procedure Analysis. This section explores and quantifies which choices are important for successfully pretraining BERT models. We keep the model architecture fixed.7 Specifically, we begin by training BERT models with the same configuration as BERTBASE (L = 12, H = 768, A = 12, 110M params). 4.1 Static vs. Dynamic Masking."]}
{"pkey": "roberta_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not mentioned in the paper.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1). We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems build upon either BERT (Devlin et al., 2019) or XLNet (Yang et al., 2019), both of which rely on additional external training data. In contrast, our submission does not use any additional data. Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation. 5.3 RACE Results. In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct. We modify RoBERTa for this task by concate-\nnating each candidate answer with the corresponding question and passage. We then encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer. We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.\nResults on the RACE test sets are presented in Table 7. RoBERTa achieves state-of-the-art results on both middle-school and high-school settings. 6 Related Work. Pretraining methods have been designed with different training objectives, including language modeling (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018), machine translation (McCann et al., 2017), and masked language modeling (Devlin et al., 2019; Lample and Conneau, 2019). Many recent papers have used a basic recipe of finetuning models for each end task (Howard and Ruder, 2018; Radford et al., 2018), and pretraining with some variant of a masked language model objective.", "Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks. GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1. SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in\n5The authors and their affiliated institutions are not in any way affiliated with the creation of the OpenWebText dataset.", "Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background."]}
{"pkey": "roberta_7", "question": "List the limitations of the model discussed in the paper.", "answer": "The paper authors train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 \u00d7 32GB Nvidia V100 GPUs interconnected by Infiniband. The paper authors pretrain our model using 1024 V100 GPUs for approximately one day. The paper authors expect future work may further improve these results by incorporating more sophisticated multi-task finetuning procedures. BPE achieving slightly worse end-task performance on some tasks. Nevertheless, the paper authors believe the advantages of a universal encoding scheme outweighs the minor degradation in performance.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training. In the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Specifically\n9Our experiments conflate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work. we consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.\n5.1 GLUE Results. For GLUE we consider two finetuning settings. In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep for each task, with batch sizes \u2208 {16, 32} and learning rates \u2208 {1e\u22125, 2e\u22125, 3e\u22125}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task\u2019s evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling. In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard.", "As discussed in Section 2, BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training. We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets. 7Studying architectural changes, including larger architectures, is an important area for future work. Masking SQuAD 2.0 MNLI-m SST-2\nResults Table 1 compares the published BERTBASE results from Devlin et al. (2019) to our reimplementation with either static or dynamic masking. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking. Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments. 4.2 Model Input Format and Next Sentence Prediction. In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p = 0.5) or from distinct documents. In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss. The NSP loss was hypothesized to be an important factor in training the original BERT model. Devlin et al. (2019) observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1.", "L\n] 2\n6 Ju\nl 2 01\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.1\n1 Introduction. Self-training methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLNet (Yang et al., 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances. \u2217Equal contribution. 1Our models and code are available at:\nhttps://github.com/pytorch/fairseq\nWe present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods."]}
{"pkey": "roberta_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors consider five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. The paper authors use the following text corpora: BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA. This is the original data used to train BERT. (16GB). CC-NEWS, which the paper authors collected from the English portion of the CommonCrawl News dataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering). OPENWEBTEXT (Gokaslan and Cohen, 2019), an open-source recreation of the WebText corpus described in Radford et al. (2019). The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB). STORIES, a dataset introduced in Trinh and Le (2018) containing a subset of CommonCrawl data filtered to match the story-like style of Winograd schemas. (31GB). The paper authors instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background.", "L\n] 2\n6 Ju\nl 2 01\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.1\n1 Introduction. Self-training methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLNet (Yang et al., 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances. \u2217Equal contribution. 1Our models and code are available at:\nhttps://github.com/pytorch/fairseq\nWe present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods.", "RoBERTa: A Robustly Optimized BERT Pretraining Approach. Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. RoBERTa: A Robustly Optimized BERT Pretraining Approach. Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. ar X\niv :1\n90 7.\n11 69\n2v 1\n[ cs\n.C"]}
{"pkey": "roberta_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural language corpora. (Section 4.4)", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["We leave further exploration of the limits of large batch training to future work. 4.4 Text Encoding. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units, which are extracted by performing statistical analysis of the training corpus. BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work. Radford et al. (2019) introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any \u201cunknown\u201d tokens. 8Large batch training can improve training efficiency even without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches are accumulated locally before each optimization step. This functionality is supported natively in FAIRSEQ (Ott et al., 2019). The original BERT implementation (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following Radford et al. (2019), we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERTBASE and BERTLARGE, respectively. Early experiments revealed only slight differences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks.", "In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section. 2.1 Setup. BERT takes as input a concatenation of two segments (sequences of tokens), x1, . . . , xN and y1, . . . , yM . Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: [CLS ], x1, . . . , xN , [SEP ], y1, . . . , yM , [EOS ]. M and N are constrained such that M +N < T , where T is a parameter that controls the maximum sequence length during training. The model is first pretrained on a large unlabeled text corpus and subsequently finetuned using end-task labeled data. 2.2 Architecture. BERT uses the now ubiquitous transformer architecture (Vaswani et al., 2017), which we will not review in detail. We use a transformer architecture with L layers. Each block uses A self-attention heads and hidden dimension H . 2.3 Training Objectives. During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction. Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK ]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK ], 10% are left unchanged,\nand 10% are replaced by a randomly selected vocabulary token. In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1). Next Sentence Prediction (NSP) NSP is a binary classification loss for predicting whether two segments follow each other in the original text.", "However, some recent work has questioned the necessity of the NSP loss (Lample and Conneau, 2019; Yang et al., 2019; Joshi et al., 2019). To better understand this discrepancy, we com-\npare several alternative training formats:\n\u2022 SEGMENT-PAIR+NSP: This follows the original input format used in BERT (Devlin et al., 2019),\nwith the NSP loss. Each input has a pair of segments, which can each contain multiple natural sentences, but the total combined length must be less than 512 tokens. \u2022 SENTENCE-PAIR+NSP: Each input contains a pair of natural sentences, either sampled from\na contiguous portion of one document or from separate documents. Since these inputs are significantly shorter than 512 tokens, we increase the batch size so that the total number of tokens remains similar to SEGMENT-PAIR+NSP. We retain the NSP loss. \u2022 FULL-SENTENCES: Each input is packed with full sentences sampled contiguously from one\nor more documents, such that the total length is at most 512 tokens. Inputs may cross document boundaries. When we reach the end of one document, we begin sampling sentences from the next document and add an extra separator token between documents. We remove the NSP loss. \u2022 DOC-SENTENCES: Inputs are constructed similarly to FULL-SENTENCES, except that they\nmay not cross document boundaries. Inputs sampled near the end of a document may be shorter than 512 tokens, so we dynamically increase the batch size in these cases to achieve a similar number of total tokens as FULLSENTENCES. We remove the NSP loss. Results Table 2 shows results for the four different settings. We first compare the original SEGMENT-PAIR input format from Devlin et al. (2019) to the SENTENCE-PAIR format; both formats retain the NSP loss, but the latter uses single sentences. We find that using individual sentences hurts performance on downstream tasks, which we hypothesize is because the model is not able to learn long-range dependencies."]}
{"pkey": "roberta_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "We use dynamic masking in the remainder of the experiments. (Section 4.1) BPE achieving slightly worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweigh the minor degradation in performance and use this encoding in the remainder of our experiments. (Section 4.4) The paper authors pretrain with sequences of at most T = 512 tokens. (Section 3.1) The texts are tokenized using a byte version of Byte-Pair Encoding (BPE) and a vocabulary size of 50,000. The inputs of the model take pieces of 512 contiguous tokens that may span over documents. The beginning of a new document is marked with <s> and the end of one by </s>. The details of the masking procedure for each sentence are the following: 15% of the tokens are masked. In 80% of the cases, the masked tokens are replaced by <mask>. In 10% of the cases, the masked tokens are replaced by a random token (different) from the one they replace. In the remaining 10% of cases, the masked tokens are left as is.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["We leave further exploration of the limits of large batch training to future work. 4.4 Text Encoding. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units, which are extracted by performing statistical analysis of the training corpus. BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work. Radford et al. (2019) introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any \u201cunknown\u201d tokens. 8Large batch training can improve training efficiency even without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches are accumulated locally before each optimization step. This functionality is supported natively in FAIRSEQ (Ott et al., 2019). The original BERT implementation (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following Radford et al. (2019), we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERTBASE and BERTLARGE, respectively. Early experiments revealed only slight differences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks.", "As discussed in Section 2, BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training. We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets. 7Studying architectural changes, including larger architectures, is an important area for future work. Masking SQuAD 2.0 MNLI-m SST-2\nResults Table 1 compares the published BERTBASE results from Devlin et al. (2019) to our reimplementation with either static or dynamic masking. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking. Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments. 4.2 Model Input Format and Next Sentence Prediction. In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p = 0.5) or from distinct documents. In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss. The NSP loss was hypothesized to be an important factor in training the original BERT model. Devlin et al. (2019) observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1.", "Unlike Devlin et al. (2019), we do not randomly inject short sequences, and we do not train with a reduced sequence length for the first 90% of updates. We train only with full-length sequences. We train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 \u00d7 32GB Nvidia V100 GPUs interconnected by Infiniband (Micikevicius et al., 2018). 3.2 Data.\nBERT-style pretraining crucially relies on large quantities of text. Baevski et al. (2019) demonstrate that increasing data size can result in improved end-task performance. Several efforts have trained on datasets larger and more diverse than the original BERT (Radford et al., 2019; Yang et al., 2019; Zellers et al., 2019). Unfortunately, not all of the additional datasets can be publicly released. For our study, we focus on gathering as much data as possible for experimentation, allowing us to match the overall quality and quantity of data as appropriate for each comparison. We consider five English-language corpora of varying sizes and domains, totaling over 160GB of uncompressed text. We use the following text corpora:\n\u2022 BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA. This is the original data used to\ntrain BERT. (16GB). \u2022 CC-NEWS, which we collected from the English portion of the CommonCrawl News\ndataset (Nagel, 2016). The data contains 63 million English news articles crawled between September 2016 and February 2019. (76GB after filtering).4\n\u2022 OPENWEBTEXT (Gokaslan and Cohen, 2019), an open-source recreation of the WebText cor-\n4We use news-please (Hamborg et al., 2017) to collect and extract CC-NEWS. CC-NEWS is similar to the REALNEWS dataset described in Zellers et al. (2019). pus described in Radford et al. (2019). The text is web content extracted from URLs shared on Reddit with at least three upvotes. (38GB).5\n\u2022 STORIES, a dataset introduced in Trinh and Le (2018) containing a subset of CommonCrawl\ndata filtered to match the story-like style of Winograd schemas. (31GB). 3.3 Evaluation."]}
{"pkey": "roberta_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "Section 4 explores and quantifies which choices are important for successfully pretraining BERT models. The paper authors keep the model architecture fixed. Specifically, the paper authors begin by training BERT models with the same configuration as BERTBASE (L = 12, H = 768, A = 12, 110M params) (Section 4). RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4). The paper authors begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). (Section 5)", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section. 2.1 Setup. BERT takes as input a concatenation of two segments (sequences of tokens), x1, . . . , xN and y1, . . . , yM . Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: [CLS ], x1, . . . , xN , [SEP ], y1, . . . , yM , [EOS ]. M and N are constrained such that M +N < T , where T is a parameter that controls the maximum sequence length during training. The model is first pretrained on a large unlabeled text corpus and subsequently finetuned using end-task labeled data. 2.2 Architecture. BERT uses the now ubiquitous transformer architecture (Vaswani et al., 2017), which we will not review in detail. We use a transformer architecture with L layers. Each block uses A self-attention heads and hidden dimension H . 2.3 Training Objectives. During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction. Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK ]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK ], 10% are left unchanged,\nand 10% are replaced by a randomly selected vocabulary token. In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1). Next Sentence Prediction (NSP) NSP is a binary classification loss for predicting whether two segments follow each other in the original text.", "Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degredation in performance and use this encoding in the remainder of our experiments. A more detailed comparison of these encodings is left to future work. 5 RoBERTa. In the previous section we propose modifications to the BERT pretraining procedure that improve end-task performance. We now aggregate these improvements and evaluate their combined impact. We call this configuration RoBERTa for Robustly optimized BERT approach. Specifically, RoBERTa is trained with dynamic masking (Section 4.1), FULL-SENTENCES without NSP loss (Section 4.2), large mini-batches (Section 4.3) and a larger byte-level BPE (Section 4.4). Additionally, we investigate two other important factors that have been under-emphasized in previous work: (1) the data used for pretraining, and (2) the number of training passes through the data. For example, the recently proposed XLNet architecture (Yang et al., 2019) is pretrained using nearly 10 times more data than the original BERT (Devlin et al., 2019). It is also trained with a batch size eight times larger for half as many optimization steps, thus seeing four times as many sequences in pretraining compared to BERT. To help disentangle the importance of these factors from other modeling choices (e.g., the pretraining objective), we begin by training RoBERTa following the BERTLARGE architecture (L = 24, H = 1024, A = 16, 355M parameters). We pretrain for 100K steps over a comparable BOOKCORPUS plus WIKIPEDIA dataset as was used in\nDevlin et al. (2019). We pretrain our model using 1024 V100 GPUs for approximately one day. Results We present our results in Table 4. When controlling for training data, we observe that RoBERTa provides a large improvement over the originally reported BERTLARGE results, reaffirming the importance of the design choices we explored in Section 4. Next, we combine this data with the three additional datasets described in Section 3.2.", "Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference (Bowman et al., 2015), which require reasoning about the relationships between pairs of sentences. 2.4 Optimization. BERT is optimized with Adam (Kingma and Ba, 2015) using the following parameters: \u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1e-6 and L2 weight decay of 0.01. The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function (Hendrycks and Gimpel, 2016). Models are pretrained for S = 1,000,000 updates, with minibatches containing B = 256 sequences of maximum length T = 512 tokens. 2.5 Data. BERT is trained on a combination of BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA, which totals 16GB of uncompressed text.3\n3 Experimental Setup. In this section, we describe the experimental setup for our replication study of BERT. 3.1 Implementation. We reimplement BERT in FAIRSEQ (Ott et al., 2019). We primarily follow the original BERT\n3Yang et al. (2019) use the same dataset but report having only 13GB of text after data cleaning. This is most likely due to subtle differences in cleaning of the Wikipedia data. optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting \u03b22 = 0.98 to improve stability when training with large batch sizes. We pretrain with sequences of at most T = 512 tokens."]}
{"pkey": "roberta_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors primarily follow the original BERT optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. The paper authors additionally found training to be very sensitive to the Adam epsilon term, and in some cases the paper authors obtained better performance or improved stability after tuning it. Similarly, the paper authors found setting \u03b22 = 0.98 to improve stability when training with large batch sizes. The paper authors pretrain with sequences of at most T = 512 tokens. (Section 3.1) We pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. (Section 5) Table 9\nRoberta(base)        Roberta(large)\nWarmup Steps        30k\nPeak Learning Rate        4e-4\nBatch Size        8k\nWeight Decay        0.01\nMax Steps        500k\nLearning Rate Decay        Linear\nAdam \u03b5        1e-6\nAdam \u03b21        0.9\nAdam \u03b22        0.98", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference (Bowman et al., 2015), which require reasoning about the relationships between pairs of sentences. 2.4 Optimization. BERT is optimized with Adam (Kingma and Ba, 2015) using the following parameters: \u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1e-6 and L2 weight decay of 0.01. The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function (Hendrycks and Gimpel, 2016). Models are pretrained for S = 1,000,000 updates, with minibatches containing B = 256 sequences of maximum length T = 512 tokens. 2.5 Data. BERT is trained on a combination of BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA, which totals 16GB of uncompressed text.3\n3 Experimental Setup. In this section, we describe the experimental setup for our replication study of BERT. 3.1 Implementation. We reimplement BERT in FAIRSEQ (Ott et al., 2019). We primarily follow the original BERT\n3Yang et al. (2019) use the same dataset but report having only 13GB of text after data cleaning. This is most likely due to subtle differences in cleaning of the Wikipedia data. optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting \u03b22 = 0.98 to improve stability when training with large batch sizes. We pretrain with sequences of at most T = 512 tokens.", "We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training. In the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Specifically\n9Our experiments conflate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work. we consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.\n5.1 GLUE Results. For GLUE we consider two finetuning settings. In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep for each task, with batch sizes \u2208 {16, 32} and learning rates \u2208 {1e\u22125, 2e\u22125, 3e\u22125}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task\u2019s evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling. In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard.", "We next compare training without the NSP loss and training with blocks of text from a single document (DOC-SENTENCES). We find that this setting outperforms the originally published BERTBASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to Devlin et al. (2019). It is possible that the original BERT implementation may only have removed the loss term while still retaining the SEGMENT-PAIR input format. Finally we find that restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES). However, because the DOC-SENTENCES format results in variable batch sizes, we use FULLSENTENCES in the remainder of our experiments for easier comparison with related work. 4.3 Training with large batches. Past work in Neural Machine Translation has shown that training with very large mini-batches can both improve optimization speed and end-task performance when the learning rate is increased appropriately (Ott et al., 2018). Recent work has shown that BERT is also amenable to large batch training (You et al., 2019). Devlin et al. (2019) originally trained BERTBASE for 1M steps with a batch size of 256 sequences. This is equivalent in computational cost, via gradient accumulation, to training for 125K steps with a batch size of 2K sequences, or for 31K steps with a batch size of 8K.\nIn Table 3 we compare perplexity and end-\ntask performance of BERTBASE as we increase the batch size, controlling for the number of passes through the training data. We observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training,8 and in later experiments we train with batches of 8K sequences. Notably You et al. (2019) train BERT with even larger batche sizes, up to 32K sequences."]}
{"pkey": "roberta_13", "question": "Describe the computational resources used to train the model.", "answer": "The paper authors train with mixed precision floating point arithmetic on DGX-1 machines, each with 8 \u00d7 32GB Nvidia V100 GPUs interconnected by Infiniband. The paper authors pretrain our model using 1024 V100 GPUs for approximately one day. (Section 5)", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["L\n] 2\n6 Ju\nl 2 01\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.1\n1 Introduction. Self-training methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLNet (Yang et al., 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances. \u2217Equal contribution. 1Our models and code are available at:\nhttps://github.com/pytorch/fairseq\nWe present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods.", "RoBERTa: A Robustly Optimized BERT Pretraining Approach. Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. RoBERTa: A Robustly Optimized BERT Pretraining Approach. Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. ar X\niv :1\n90 7.\n11 69\n2v 1\n[ cs\n.C", "We next compare training without the NSP loss and training with blocks of text from a single document (DOC-SENTENCES). We find that this setting outperforms the originally published BERTBASE results and that removing the NSP loss matches or slightly improves downstream task performance, in contrast to Devlin et al. (2019). It is possible that the original BERT implementation may only have removed the loss term while still retaining the SEGMENT-PAIR input format. Finally we find that restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES). However, because the DOC-SENTENCES format results in variable batch sizes, we use FULLSENTENCES in the remainder of our experiments for easier comparison with related work. 4.3 Training with large batches. Past work in Neural Machine Translation has shown that training with very large mini-batches can both improve optimization speed and end-task performance when the learning rate is increased appropriately (Ott et al., 2018). Recent work has shown that BERT is also amenable to large batch training (You et al., 2019). Devlin et al. (2019) originally trained BERTBASE for 1M steps with a batch size of 256 sequences. This is equivalent in computational cost, via gradient accumulation, to training for 125K steps with a batch size of 2K sequences, or for 31K steps with a batch size of 8K.\nIn Table 3 we compare perplexity and end-\ntask performance of BERTBASE as we increase the batch size, controlling for the number of passes through the training data. We observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training,8 and in later experiments we train with batches of 8K sequences. Notably You et al. (2019) train BERT with even larger batche sizes, up to 32K sequences."]}
{"pkey": "roberta_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "The paper authors additionally use a novel dataset, CC-NEWS, and release our models and code for pretraining and finetuning at: https://github.com/pytorch/fairseq.", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks. GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1. SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in\n5The authors and their affiliated institutions are not in any way affiliated with the creation of the OpenWebText dataset.", "On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1). We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems build upon either BERT (Devlin et al., 2019) or XLNet (Yang et al., 2019), both of which rely on additional external training data. In contrast, our submission does not use any additional data. Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation. 5.3 RACE Results. In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct. We modify RoBERTa for this task by concate-\nnating each candidate answer with the corresponding question and passage. We then encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer. We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.\nResults on the RACE test sets are presented in Table 7. RoBERTa achieves state-of-the-art results on both middle-school and high-school settings. 6 Related Work. Pretraining methods have been designed with different training objectives, including language modeling (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018), machine translation (McCann et al., 2017), and masked language modeling (Devlin et al., 2019; Lample and Conneau, 2019). Many recent papers have used a basic recipe of finetuning models for each end task (Howard and Ruder, 2018; Radford et al., 2018), and pretraining with some variant of a masked language model objective.", "Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background."]}
{"pkey": "roberta_15", "question": "What is the pretraining objective of the model? ", "answer": "we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Introduction). RoBERTa uses the same masked language modeling pretraining objective and architecture as BERTLARGE, yet consistently outperforms both BERTLARGE and XLNetLARGE (Section 5.1).", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["RoBERTa: A Robustly Optimized BERT Pretraining Approach. Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. RoBERTa: A Robustly Optimized BERT Pretraining Approach. Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code. ar X\niv :1\n90 7.\n11 69\n2v 1\n[ cs\n.C", "In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section. 2.1 Setup. BERT takes as input a concatenation of two segments (sequences of tokens), x1, . . . , xN and y1, . . . , yM . Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: [CLS ], x1, . . . , xN , [SEP ], y1, . . . , yM , [EOS ]. M and N are constrained such that M +N < T , where T is a parameter that controls the maximum sequence length during training. The model is first pretrained on a large unlabeled text corpus and subsequently finetuned using end-task labeled data. 2.2 Architecture. BERT uses the now ubiquitous transformer architecture (Vaswani et al., 2017), which we will not review in detail. We use a transformer architecture with L layers. Each block uses A self-attention heads and hidden dimension H . 2.3 Training Objectives. During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction. Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK ]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK ], 10% are left unchanged,\nand 10% are replaced by a randomly selected vocabulary token. In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1). Next Sentence Prediction (NSP) NSP is a binary classification loss for predicting whether two segments follow each other in the original text.", "L\n] 2\n6 Ju\nl 2 01\nLanguage model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.1\n1 Introduction. Self-training methods such as ELMo (Peters et al., 2018), GPT (Radford et al., 2018), BERT (Devlin et al., 2019), XLM (Lample and Conneau, 2019), and XLNet (Yang et al., 2019) have brought significant performance gains, but it can be challenging to determine which aspects of the methods contribute the most. Training is computationally expensive, limiting the amount of tuning that can be done, and is often done with private training data of varying sizes, limiting our ability to measure the effects of the modeling advances. \u2217Equal contribution. 1Our models and code are available at:\nhttps://github.com/pytorch/fairseq\nWe present a replication study of BERT pretraining (Devlin et al., 2019), which includes a careful evaluation of the effects of hyperparmeter tuning and training set size. We find that BERT was significantly undertrained and propose an improved recipe for training BERT models, which we call RoBERTa, that can match or exceed the performance of all of the post-BERT methods."]}
{"pkey": "roberta_16", "question": "What is the loss function that is used to train the model?", "answer": "RoBERTa uses the same masked language modeling pretraining objective and architecture as BERTLARGE, yet consistently outperforms both BERTLARGE and XLNetLARGE (Section 5.1). The MLM objective is a cross-entropy loss on predicting the masked tokens (Section 2.3).", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["We leave further exploration of the limits of large batch training to future work. 4.4 Text Encoding. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units, which are extracted by performing statistical analysis of the training corpus. BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work. Radford et al. (2019) introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any \u201cunknown\u201d tokens. 8Large batch training can improve training efficiency even without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches are accumulated locally before each optimization step. This functionality is supported natively in FAIRSEQ (Ott et al., 2019). The original BERT implementation (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following Radford et al. (2019), we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERTBASE and BERTLARGE, respectively. Early experiments revealed only slight differences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks.", "As discussed in Section 2, BERT relies on randomly masking and predicting tokens. The original BERT implementation performed masking once during data preprocessing, resulting in a single static mask. To avoid using the same mask for each training instance in every epoch, training data was duplicated 10 times so that each sequence is masked in 10 different ways over the 40 epochs of training. Thus, each training sequence was seen with the same mask four times during training. We compare this strategy with dynamic masking where we generate the masking pattern every time we feed a sequence to the model. This becomes crucial when pretraining for more steps or with larger datasets. 7Studying architectural changes, including larger architectures, is an important area for future work. Masking SQuAD 2.0 MNLI-m SST-2\nResults Table 1 compares the published BERTBASE results from Devlin et al. (2019) to our reimplementation with either static or dynamic masking. We find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking. Given these results and the additional efficiency benefits of dynamic masking, we use dynamic masking in the remainder of the experiments. 4.2 Model Input Format and Next Sentence Prediction. In the original BERT pretraining procedure, the model observes two concatenated document segments, which are either sampled contiguously from the same document (with p = 0.5) or from distinct documents. In addition to the masked language modeling objective, the model is trained to predict whether the observed document segments come from the same or distinct documents via an auxiliary Next Sentence Prediction (NSP) loss. The NSP loss was hypothesized to be an important factor in training the original BERT model. Devlin et al. (2019) observe that removing NSP hurts performance, with significant performance degradation on QNLI, MNLI, and SQuAD 1.1.", "Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference (Bowman et al., 2015), which require reasoning about the relationships between pairs of sentences. 2.4 Optimization. BERT is optimized with Adam (Kingma and Ba, 2015) using the following parameters: \u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1e-6 and L2 weight decay of 0.01. The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function (Hendrycks and Gimpel, 2016). Models are pretrained for S = 1,000,000 updates, with minibatches containing B = 256 sequences of maximum length T = 512 tokens. 2.5 Data. BERT is trained on a combination of BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA, which totals 16GB of uncompressed text.3\n3 Experimental Setup. In this section, we describe the experimental setup for our replication study of BERT. 3.1 Implementation. We reimplement BERT in FAIRSEQ (Ott et al., 2019). We primarily follow the original BERT\n3Yang et al. (2019) use the same dataset but report having only 13GB of text after data cleaning. This is most likely due to subtle differences in cleaning of the Wikipedia data. optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting \u03b22 = 0.98 to improve stability when training with large batch sizes. We pretrain with sequences of at most T = 512 tokens."]}
{"pkey": "roberta_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "This section explores and quantifies which choices are important for successfully pretraining BERT models. The paper authors keep the model architecture fixed. Specifically, the paper authors begin by training BERT models with the same configuration as BERTBASE (L =12, H = 768, A = 12, 110M params). (Section 4) RoBERTa matches the architecture and training objective of BERTLARGE. (Section 5)  Crucially, RoBERTa uses the same masked language modeling pretraining objective and architecture as BERTLARGE, yet consistently outperforms both BERTLARGE and XLNetLARGE. (Section 5.1)  Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. The paper authors also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. (Introduction)", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section. 2.1 Setup. BERT takes as input a concatenation of two segments (sequences of tokens), x1, . . . , xN and y1, . . . , yM . Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: [CLS ], x1, . . . , xN , [SEP ], y1, . . . , yM , [EOS ]. M and N are constrained such that M +N < T , where T is a parameter that controls the maximum sequence length during training. The model is first pretrained on a large unlabeled text corpus and subsequently finetuned using end-task labeled data. 2.2 Architecture. BERT uses the now ubiquitous transformer architecture (Vaswani et al., 2017), which we will not review in detail. We use a transformer architecture with L layers. Each block uses A self-attention heads and hidden dimension H . 2.3 Training Objectives. During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction. Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK ]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK ], 10% are left unchanged,\nand 10% are replaced by a randomly selected vocabulary token. In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1). Next Sentence Prediction (NSP) NSP is a binary classification loss for predicting whether two segments follow each other in the original text.", "We leave further exploration of the limits of large batch training to future work. 4.4 Text Encoding. Byte-Pair Encoding (BPE) (Sennrich et al., 2016) is a hybrid between character- and word-level representations that allows handling the large vocabularies common in natural language corpora. Instead of full words, BPE relies on subwords units, which are extracted by performing statistical analysis of the training corpus. BPE vocabulary sizes typically range from 10K-100K subword units. However, unicode characters can account for a sizeable portion of this vocabulary when modeling large and diverse corpora, such as the ones considered in this work. Radford et al. (2019) introduce a clever implementation of BPE that uses bytes instead of unicode characters as the base subword units. Using bytes makes it possible to learn a subword vocabulary of a modest size (50K units) that can still encode any input text without introducing any \u201cunknown\u201d tokens. 8Large batch training can improve training efficiency even without large scale parallel hardware through gradient accumulation, whereby gradients from multiple mini-batches are accumulated locally before each optimization step. This functionality is supported natively in FAIRSEQ (Ott et al., 2019). The original BERT implementation (Devlin et al., 2019) uses a character-level BPE vocabulary of size 30K, which is learned after preprocessing the input with heuristic tokenization rules. Following Radford et al. (2019), we instead consider training BERT with a larger byte-level BPE vocabulary containing 50K subword units, without any additional preprocessing or tokenization of the input. This adds approximately 15M and 20M additional parameters for BERTBASE and BERTLARGE, respectively. Early experiments revealed only slight differences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks.", "We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training. In the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Specifically\n9Our experiments conflate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work. we consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.\n5.1 GLUE Results. For GLUE we consider two finetuning settings. In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep for each task, with batch sizes \u2208 {16, 32} and learning rates \u2208 {1e\u22125, 2e\u22125, 3e\u22125}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task\u2019s evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling. In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard."]}
{"pkey": "roberta_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors find that our reimplementation with static masking performs similar to the original BERT model, and dynamic masking is comparable or slightly better than static masking. Given these results and the additional efficiency benefits of dynamic masking, the paper authors use dynamic masking in the remainder of the experiments. (Section 4.1) Table 1 The paper authors find that using individual sentences hurts performance on downstream tasks, which the paper authors hypothesize is because the model is not able to learn long-range dependencies. (Section 4.2) Table 2 Removing the NSP loss matches or slightly improves downstream task performance. (Section 4.2) Table 2  The paper authors find that restricting sequences to come from a single document (DOC-SENTENCES) performs slightly better than packing sequences from multiple documents (FULL-SENTENCES). However, because the DOC-SENTENCES format results in variable batch sizes, the paper authors use FULLSENTENCES in the remainder of our experiments for easier comparison with related work. (Section 4.2) Table 2. The paper authors compare perplexity and end task performance of BERTBASE as the paper authors increase the batch size, controlling for the number of passes through the training data. The paper authors observe that training with large batches improves perplexity for the masked language modeling objective, as well as end-task accuracy. Large batches are also easier to parallelize via distributed data parallel training, and in later experiments the paper authors train with batches of 8K sequences. (Section 4.3) Table 3 Early experiments revealed only slight differences between these encodings, with the Radford et al. (2019) BPE achieving slightly worse end-task performance on some tasks. Nevertheless, we believe the advantages of a universal encoding scheme outweighs the minor degradation in performance and use this encoding in the remainder of our experiments. (Section 4.4) The paper authors pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. The paper authors again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. The paper authors note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training. (Section 5) Table 4 In the rest of the paper, the paper authors evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD, and RACE. Specifically, the paper authors consider RoBERTa trained for 500K steps over all five of the datasets (Section 5) Table 4 For GLUE the paper authors consider two finetuning settings. In the first setting (single-task, dev) the paper authors finetune RoBERTa separately for each of the GLUE tasks. The paper authors finetune for 10 epochs and perform early stopping based on each task\u2019s evaluation metric on the dev set. Our submission depends only on single-task finetuning. (Section 5.1) Table 5 The paper authors only finetune RoBERTa using the provided SQuAD training data. RoBERTa matches the state-of-the-art set by XLNet. On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1). (Section 5.2) Table 6 In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct. RoBERTa achieves state-of-the-art results on both middle-school and high-school settings. (Section 5.3) Table 7", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training. In the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Specifically\n9Our experiments conflate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work. we consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.\n5.1 GLUE Results. For GLUE we consider two finetuning settings. In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep for each task, with batch sizes \u2208 {16, 32} and learning rates \u2208 {1e\u22125, 2e\u22125, 3e\u22125}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task\u2019s evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling. In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard.", "In this section, we give a brief overview of the BERT (Devlin et al., 2019) pretraining approach and some of the training choices that we will examine experimentally in the following section. 2.1 Setup. BERT takes as input a concatenation of two segments (sequences of tokens), x1, . . . , xN and y1, . . . , yM . Segments usually consist of more than one natural sentence. The two segments are presented as a single input sequence to BERT with special tokens delimiting them: [CLS ], x1, . . . , xN , [SEP ], y1, . . . , yM , [EOS ]. M and N are constrained such that M +N < T , where T is a parameter that controls the maximum sequence length during training. The model is first pretrained on a large unlabeled text corpus and subsequently finetuned using end-task labeled data. 2.2 Architecture. BERT uses the now ubiquitous transformer architecture (Vaswani et al., 2017), which we will not review in detail. We use a transformer architecture with L layers. Each block uses A self-attention heads and hidden dimension H . 2.3 Training Objectives. During pretraining, BERT uses two objectives: masked language modeling and next sentence prediction. Masked Language Model (MLM) A random sample of the tokens in the input sequence is selected and replaced with the special token [MASK ]. The MLM objective is a cross-entropy loss on predicting the masked tokens. BERT uniformly selects 15% of the input tokens for possible replacement. Of the selected tokens, 80% are replaced with [MASK ], 10% are left unchanged,\nand 10% are replaced by a randomly selected vocabulary token. In the original implementation, random masking and replacement is performed once in the beginning and saved for the duration of training, although in practice, data is duplicated so the mask is not always the same for every training sentence (see Section 4.1). Next Sentence Prediction (NSP) NSP is a binary classification loss for predicting whether two segments follow each other in the original text.", "Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background."]}
{"pkey": "roberta_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "The paper authors find that this setting outperforms the originally published BERTBASE results and that removing the NSP loss matches or slightly improves downstream task performance. (Section 4.2)", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["On the SQuAD v2.0 development set, RoBERTa sets a new state-of-the-art, improving over XLNet by 0.4 points (EM) and 0.6 points (F1). We also submit RoBERTa to the public SQuAD 2.0 leaderboard and evaluate its performance relative to other systems. Most of the top systems build upon either BERT (Devlin et al., 2019) or XLNet (Yang et al., 2019), both of which rely on additional external training data. In contrast, our submission does not use any additional data. Our single RoBERTa model outperforms all but one of the single model submissions, and is the top scoring system among those that do not rely on data augmentation. 5.3 RACE Results. In RACE, systems are provided with a passage of text, an associated question, and four candidate answers. Systems are required to classify which of the four candidate answers is correct. We modify RoBERTa for this task by concate-\nnating each candidate answer with the corresponding question and passage. We then encode each of these four sequences and pass the resulting [CLS] representations through a fully-connected layer, which is used to predict the correct answer. We truncate question-answer pairs that are longer than 128 tokens and, if needed, the passage so that the total length is at most 512 tokens.\nResults on the RACE test sets are presented in Table 7. RoBERTa achieves state-of-the-art results on both middle-school and high-school settings. 6 Related Work. Pretraining methods have been designed with different training objectives, including language modeling (Dai and Le, 2015; Peters et al., 2018; Howard and Ruder, 2018), machine translation (McCann et al., 2017), and masked language modeling (Devlin et al., 2019; Lample and Conneau, 2019). Many recent papers have used a basic recipe of finetuning models for each end task (Howard and Ruder, 2018; Radford et al., 2018), and pretraining with some variant of a masked language model objective.", "Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks. GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1. SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in\n5The authors and their affiliated institutions are not in any way affiliated with the creation of the OpenWebText dataset.", "Positive examples are created by taking consecutive sentences from the text corpus. Negative examples are created by pairing segments from different documents. Positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as Natural Language Inference (Bowman et al., 2015), which require reasoning about the relationships between pairs of sentences. 2.4 Optimization. BERT is optimized with Adam (Kingma and Ba, 2015) using the following parameters: \u03b21 = 0.9, \u03b22 = 0.999, \u01eb = 1e-6 and L2 weight decay of 0.01. The learning rate is warmed up over the first 10,000 steps to a peak value of 1e-4, and then linearly decayed. BERT trains with a dropout of 0.1 on all layers and attention weights, and a GELU activation function (Hendrycks and Gimpel, 2016). Models are pretrained for S = 1,000,000 updates, with minibatches containing B = 256 sequences of maximum length T = 512 tokens. 2.5 Data. BERT is trained on a combination of BOOKCORPUS (Zhu et al., 2015) plus English WIKIPEDIA, which totals 16GB of uncompressed text.3\n3 Experimental Setup. In this section, we describe the experimental setup for our replication study of BERT. 3.1 Implementation. We reimplement BERT in FAIRSEQ (Ott et al., 2019). We primarily follow the original BERT\n3Yang et al. (2019) use the same dataset but report having only 13GB of text after data cleaning. This is most likely due to subtle differences in cleaning of the Wikipedia data. optimization hyperparameters, given in Section 2, except for the peak learning rate and number of warmup steps, which are tuned separately for each setting. We additionally found training to be very sensitive to the Adam epsilon term, and in some cases we obtained better performance or improved stability after tuning it. Similarly, we found setting \u03b22 = 0.98 to improve stability when training with large batch sizes. We pretrain with sequences of at most T = 512 tokens."]}
{"pkey": "roberta_20", "question": "List the future work mentioned in the paper.", "answer": "The paper authors re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling. It is possible that these other methods could also improve with more tuning. The paper authors leave this exploration to future work. (Introduction) Studying architectural changes, including larger architectures, is an important area for future work. (Section 4.1) The paper authors leave further exploration of the limits of large batch training to future work. (Section 4.3) A more detailed comparison of these encodings is left to future work. (Section 4.3) Our experiments conflate increases in data size and diversity. The paper authors leave a more careful analysis of these two dimensions to future work. (Section 5) The paper authors expect future work may further improve these results by incorporating more sophisticated multi-task finetuning procedures. (Section 5.1)", "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach", "context": ["We train RoBERTa over the combined data with the same number of training steps as before (100K). In total, we pretrain over 160GB of text. We observe further improvements in performance across all downstream tasks, validating the importance of data size and diversity in pretraining.9\nFinally, we pretrain RoBERTa for significantly longer, increasing the number of pretraining steps from 100K to 300K, and then further to 500K. We again observe significant gains in downstream task performance, and the 300K and 500K step models outperform XLNetLARGE across most tasks. We note that even our longest-trained model does not appear to overfit our data and would likely benefit from additional training. In the rest of the paper, we evaluate our best RoBERTa model on the three different benchmarks: GLUE, SQuaD and RACE. Specifically\n9Our experiments conflate increases in data size and diversity. We leave a more careful analysis of these two dimensions to future work. we consider RoBERTa trained for 500K steps over all five of the datasets introduced in Section 3.2.\n5.1 GLUE Results. For GLUE we consider two finetuning settings. In the first setting (single-task, dev) we finetune RoBERTa separately for each of the GLUE tasks, using only the training data for the corresponding task. We consider a limited hyperparameter sweep for each task, with batch sizes \u2208 {16, 32} and learning rates \u2208 {1e\u22125, 2e\u22125, 3e\u22125}, with a linear warmup for the first 6% of steps followed by a linear decay to 0. We finetune for 10 epochs and perform early stopping based on each task\u2019s evaluation metric on the dev set. The rest of the hyperparameters remain the same as during pretraining. In this setting, we report the median development set results for each task over five random initializations, without model ensembling. In the second setting (ensembles, test), we compare RoBERTa to other approaches on the test set via the GLUE leaderboard.", "Our modifications are simple, they include: (1) training the model longer, with bigger batches, over more data; (2) removing the next sentence prediction objective; (3) training on longer sequences; and (4) dynamically changing the masking pattern applied to the training data. We also collect a large new dataset (CC-NEWS) of comparable size to other privately used datasets, to better control for training set size effects. When controlling for training data, our improved training procedure improves upon the published BERT results on both GLUE and SQuAD. When trained for longer over additional data, our model achieves a score of 88.5 on the public GLUE leaderboard, matching the 88.4 reported by Yang et al. (2019). Our model establishes a new state-of-the-art on 4/9 of the GLUE tasks: MNLI, QNLI, RTE and STS- B. We also match state-of-the-art results on SQuAD and RACE. Overall, we re-establish that BERT\u2019s masked language model training objective is competitive with other recently proposed training objectives such as perturbed autoregressive language modeling (Yang et al., 2019).2 In summary, the contributions of this paper are: (1) We present a set of important BERT design choices and training strategies and introduce\n2It is possible that these other methods could also improve with more tuning. We leave this exploration to future work.\nalternatives that lead to better downstream task performance; (2) We use a novel dataset, CCNEWS, and confirm that using more data for pretraining further improves performance on downstream tasks; (3) Our training improvements show that masked language model pretraining, under the right design choices, is competitive with all other recently published methods. We release our model, pretraining and fine-tuning code implemented in PyTorch (Paszke et al., 2017). 2 Background.", "Following previous work, we evaluate our pretrained models on downstream tasks using the following three benchmarks. GLUE The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019b) is a collection of 9 datasets for evaluating natural language understanding systems.6 Tasks are framed as either single-sentence classification or sentence-pair classification tasks. The GLUE organizers provide training and development data splits as well as a submission server and leaderboard that allows participants to evaluate and compare their systems on private held-out test data. For the replication study in Section 4, we report results on the development sets after finetuning the pretrained models on the corresponding singletask training data (i.e., without multi-task training or ensembling). Our finetuning procedure follows the original BERT paper (Devlin et al., 2019). In Section 5 we additionally report test set results obtained from the public leaderboard. These results depend on a several task-specific modifications, which we describe in Section 5.1. SQuAD The Stanford Question Answering Dataset (SQuAD) provides a paragraph of context and a question. The task is to answer the question by extracting the relevant span from the context. We evaluate on two versions of SQuAD: V1.1 and V2.0 (Rajpurkar et al., 2016, 2018). In V1.1 the context always contains an answer, whereas in\n5The authors and their affiliated institutions are not in any way affiliated with the creation of the OpenWebText dataset."]}
{"pkey": "gpt2_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "In this paper, the paper authors connect these two lines of work and continue the trend of more general methods of transfer. We demonstrate language models can perform down-stream tasks in a zero-shot setting \u2013 without any parameter or architecture modification. The paper authors demonstrate this approach shows potential by highlighting the ability of language models to perform a wide range of tasks in a zero-shot setting. We achieve promising, competitive, and state of the art results depending on the task.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["Those methods often adopt the similar idea of planning-then-generation as above (Shen et al., 2019; Zhao et al., 2020; Bosselut et al., 2018; See et al., 2019; Hua and Wang, 2020; Rashkin et al., 2020). Another line of work instead focuses on extending the transformer architecture (Vaswani et al., 2017) to model longer text sequences (e.g., Dai et al., 2019; Wang et al., 2020; Choromanski et al., 2021, etc). For example, Liu et al. (2018) used a hybrid retrieval-generation architecture for producing long summaries; Dai et al. (2019) showed long text samples qualitatively. Our work systematically examines the pretrained LMs in generating long domain-specific text, and proposes a new approach that empowers pretrained LMs for producing samples of significantly higherquality. 3 Progressive Generation of Text. One of the main challenges in generating long coherent passages is modeling long-range dependencies across the entire sequences (e.g., 1000 tokens). We propose a progressive generation approach that is conceptually simple yet effective. Intuitively, progressive generation divides the complex problem of generating the full passage into a series of much easier steps of generating coarser-grained intermediate sequences. Contrary to generating everything from left to right from scratch, our progressive generation allows the model to first plan globally and then shift attention to increasingly finer details, which results in more coherent text. Figure 2 illustrates the generation process. 3.1 Generation Process. Let y := [y1, y2, . . . , yT ] be the output text, where each yi is a token of language (a word or a subword). The output sequences are generated either conditionally on any other information x (e.g., generations of a story given a prompt), or unconditionally (in which case we assume x \u2261 \u2205 while keeping the same notation). Instead of generating the full passage y", "We observe that generation of some words (e.g., stop words) does not require many contexts, while other words are decisive and have long-term impact on the whole content of the passage. Motivated by this observation, our approach first produces a sequence of most informative words, then progressively refines the sequence by adding finergrained details in multiple stages, until completing a full passage. The generation at each stage is conditioning on the output of the preceding stage which provides anchors and steers the current generation (Figure 2). The intermediate words produced at each stage are defined based on a simple TF-IDF informativeness metric. The approach enjoys several core advantages: (1) Although the progressive approach implements a conceptually non-monotonic generation process, generation at each stage can still be performed in a left-to-right manner and thus is directly compatible with the powerful pretrained monotonic LMs. The LMs at different stages are easily fine-tuned to accommodate a target domain using only small, independently constructed data. Intuitively, each LM\nis addressing a sub-task of mapping a sequence to a finer-resolution one, which is much simpler than the overall task of mapping from conditions to full passages of text. In this work, we use BART (Lewis et al., 2020) for generation at each stage, though one can also plug in other off-the-shelf LMs.", "In this work, we study the problem of generating coherent, much longer passages of text (e.g., 1000 tokens). GPT-3 (Brown et al., 2020) was reported to produce long essays, yet the results seem to need extensive human curations (e.g., MarketMuse; Gardian), and the system is not publicly available to adapt to arbitrary desired domains. In this work, we examine fine-tuning of largescale LMs for domain-specific generation of extra-\nar X\niv :2\n00 6.\n15 72\n0v 2\n[ cs\n.C L\n] 1\n4 A\npr 2\n02 1\nlong text. We find that samples produced by GPT-2 fine-tuned on small domain-specific corpora exhibit various imperfections, including excessive repetitiveness and incoherence between sentences far apart. Figure 1 measures the coherence of text generated by the fine-tuned GPT-2 w.r.t the BERT next sentence prediction (Devlin et al., 2019) score. As the figure shows, GPT-2 models (regardless of the model size) exhibit a significant gap in the score compared with human text, hence falling short in generating coherent text. We hypothesize that the problem is mainly caused by the sequential generation order of the LMs, which makes global content planning of the passage difficult, especially when the generated text is long and contains thousands of words. One could potentially adopt the recent planning-thengeneration or non-monotonic methods (Sec 2), yet those methods either require specialized neural architectures that need costly retraining for each domain (Gu et al., 2019; Stern et al., 2019; Chan et al., 2019; Fan et al., 2019), or rely on dedicated intermediate content plans (e.g., summaries, SRL labels) (Fan et al., 2019; Yao et al., 2019) with limited flexibility and producing sub-optimal results as shown in our experiments. To overcome the limitations, we introduce a new method for Progressive Generation of Text (ProGen)."]}
{"pkey": "gpt2_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Current ML systems need hundreds to thousands of examples to induce functions which generalize well. This suggests that multitask training many need just as many effective training pairs to realize its promise with current approaches. It will be very difficult to continue to scale the creation of datasets and the design of objectives to the degree that may be re_x0002_quired to brute force our way there with current techniques. This motivates exploring additional setups for performing multitask learning.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["In the multi-stage generation, the intermediate sequences are not natural language. Yet we found that fine-tuning pretrained LMs (such as BART and GPT-2) to generate the intermediate sequences is indeed very efficient in terms of data and computation. We tried training other models such as small sequence-to-sequence models and n-gram models from scratch, which we found is much harder, requiring more data, or yielding inferior performance. This again highlights the importance of using pretrained LMs, as enabled by our simple method design. Stage-level exposure bias and data noising. In the above training process, the outputs of each LM are conditioning on the ground-truth input sequences extracted from the real corpus. In contrast, at generation time, the LM takes as inputs the imperfect sequences produced at the previous stage, which can result in new mistakes in the outputs since the LM has never be exposed to noisy inputs during training. Thus, the discrepancy between training and generation can lead to mistakes in generation accumulating through the stages. The phenomenon resembles the exposure bias issue (Ranzato et al., 2016) of sequential generation models at token level, where the model is trained to predict the next token given the previous ground-truth tokens, while at generation time tokens generated by the model itself are instead used to make the next prediction. To alleviate the issue and increase the robustness of each intermediate LM, we draw on the rich literature of addressing token-level exposure bias (Xie et al., 2017; Tan et al., 2019). Specifically, during training, we inject noise into the ground-truth inputs at each stage by randomly picking an n-gram (n \u2208 {1, 2, 3, 4}) and replacing it with another randomly sampled n-gram. The data noising encourages the LMs to learn to recover from the mistakes in inputs, leading to a more robust system during generation. 4 Experiments. 4.1 Setup.\nDomains.", "The Pearson correlation coefficient of human scores is 0.52, showing moderate inter-rater agreement. Table 1 shows the results. All systems receive close fluency scores. Our approach obtained significantly higher coherence scores at both passage and sentence levels. In particular, over 86% sentences\nin our model generations are considered as coherent with the context, improving over other models by at least 10 absolute percent. 4.4 Ablation Study and Analysis. Sample efficiency. We study how the progressive generation could improve the sample efficiency of large LMs fine-tuned to target domains. The intuition is that by focusing on the subsets of informative words, the early stages can more efficiently capture the domain-specific characteristics and then steer the subsequent refinement stages. Figure 5 shows the results where we report the FBD score averaged over FBD-S/M/D. We can see our approach can make more efficient use of the training data in learning to generate high quality samples. For example, with only 1K training examples, our method achieves comparable results with large LMs trained on 30K examples. Generation with gold plans. To investigate the importance of dividing the generation process into stages and what the stages learn separately, we add another set of text into our comparison. It is a 2- stages model whose first stage is the ground truth (gold plan) while the second stage kept the same (a BART model), shown as GoldPlan in Table 3. Note that with gold plan, our model greatly decreases the gap with human text in terms of lexical (TID) and semantic (FBD-D) quality metrics. The results highlight the importance of plans in text\ngeneration. The intermediate plans act as an information bottleneck, and high-quality plans could lead to high-quality text generation. Effect of data noising. We study the ablation of data noising, to check whether the noising operation really helps reduce stage-wise exposure bias (Sec 3.2) as we expected.", "We observe that generation of some words (e.g., stop words) does not require many contexts, while other words are decisive and have long-term impact on the whole content of the passage. Motivated by this observation, our approach first produces a sequence of most informative words, then progressively refines the sequence by adding finergrained details in multiple stages, until completing a full passage. The generation at each stage is conditioning on the output of the preceding stage which provides anchors and steers the current generation (Figure 2). The intermediate words produced at each stage are defined based on a simple TF-IDF informativeness metric. The approach enjoys several core advantages: (1) Although the progressive approach implements a conceptually non-monotonic generation process, generation at each stage can still be performed in a left-to-right manner and thus is directly compatible with the powerful pretrained monotonic LMs. The LMs at different stages are easily fine-tuned to accommodate a target domain using only small, independently constructed data. Intuitively, each LM\nis addressing a sub-task of mapping a sequence to a finer-resolution one, which is much simpler than the overall task of mapping from conditions to full passages of text. In this work, we use BART (Lewis et al., 2020) for generation at each stage, though one can also plug in other off-the-shelf LMs."]}
{"pkey": "gpt2_3", "question": "What are the main contributions of the paper?", "answer": "When a large language model is trained on a sufficiently large and diverse dataset it is able to perform well across many domains and datasets. GPT-2 zero-shots to state of the art performance on 7 out of 8 tested language model_x0002_ing datasets. The diversity of tasks the model is able to perform in a zero-shot setting suggests that high-capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["The Pearson correlation coefficient of human scores is 0.52, showing moderate inter-rater agreement. Table 1 shows the results. All systems receive close fluency scores. Our approach obtained significantly higher coherence scores at both passage and sentence levels. In particular, over 86% sentences\nin our model generations are considered as coherent with the context, improving over other models by at least 10 absolute percent. 4.4 Ablation Study and Analysis. Sample efficiency. We study how the progressive generation could improve the sample efficiency of large LMs fine-tuned to target domains. The intuition is that by focusing on the subsets of informative words, the early stages can more efficiently capture the domain-specific characteristics and then steer the subsequent refinement stages. Figure 5 shows the results where we report the FBD score averaged over FBD-S/M/D. We can see our approach can make more efficient use of the training data in learning to generate high quality samples. For example, with only 1K training examples, our method achieves comparable results with large LMs trained on 30K examples. Generation with gold plans. To investigate the importance of dividing the generation process into stages and what the stages learn separately, we add another set of text into our comparison. It is a 2- stages model whose first stage is the ground truth (gold plan) while the second stage kept the same (a BART model), shown as GoldPlan in Table 3. Note that with gold plan, our model greatly decreases the gap with human text in terms of lexical (TID) and semantic (FBD-D) quality metrics. The results highlight the importance of plans in text\ngeneration. The intermediate plans act as an information bottleneck, and high-quality plans could lead to high-quality text generation. Effect of data noising. We study the ablation of data noising, to check whether the noising operation really helps reduce stage-wise exposure bias (Sec 3.2) as we expected.", "Those methods often adopt the similar idea of planning-then-generation as above (Shen et al., 2019; Zhao et al., 2020; Bosselut et al., 2018; See et al., 2019; Hua and Wang, 2020; Rashkin et al., 2020). Another line of work instead focuses on extending the transformer architecture (Vaswani et al., 2017) to model longer text sequences (e.g., Dai et al., 2019; Wang et al., 2020; Choromanski et al., 2021, etc). For example, Liu et al. (2018) used a hybrid retrieval-generation architecture for producing long summaries; Dai et al. (2019) showed long text samples qualitatively. Our work systematically examines the pretrained LMs in generating long domain-specific text, and proposes a new approach that empowers pretrained LMs for producing samples of significantly higherquality. 3 Progressive Generation of Text. One of the main challenges in generating long coherent passages is modeling long-range dependencies across the entire sequences (e.g., 1000 tokens). We propose a progressive generation approach that is conceptually simple yet effective. Intuitively, progressive generation divides the complex problem of generating the full passage into a series of much easier steps of generating coarser-grained intermediate sequences. Contrary to generating everything from left to right from scratch, our progressive generation allows the model to first plan globally and then shift attention to increasingly finer details, which results in more coherent text. Figure 2 illustrates the generation process. 3.1 Generation Process. Let y := [y1, y2, . . . , yT ] be the output text, where each yi is a token of language (a word or a subword). The output sequences are generated either conditionally on any other information x (e.g., generations of a story given a prompt), or unconditionally (in which case we assume x \u2261 \u2205 while keeping the same notation). Instead of generating the full passage y", "We observe that generation of some words (e.g., stop words) does not require many contexts, while other words are decisive and have long-term impact on the whole content of the passage. Motivated by this observation, our approach first produces a sequence of most informative words, then progressively refines the sequence by adding finergrained details in multiple stages, until completing a full passage. The generation at each stage is conditioning on the output of the preceding stage which provides anchors and steers the current generation (Figure 2). The intermediate words produced at each stage are defined based on a simple TF-IDF informativeness metric. The approach enjoys several core advantages: (1) Although the progressive approach implements a conceptually non-monotonic generation process, generation at each stage can still be performed in a left-to-right manner and thus is directly compatible with the powerful pretrained monotonic LMs. The LMs at different stages are easily fine-tuned to accommodate a target domain using only small, independently constructed data. Intuitively, each LM\nis addressing a sub-task of mapping a sequence to a finer-resolution one, which is much simpler than the overall task of mapping from conditions to full passages of text. In this work, we use BART (Lewis et al., 2020) for generation at each stage, though one can also plug in other off-the-shelf LMs."]}
{"pkey": "gpt2_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["Progressive Generation of Long Text with Pretrained Language Models. Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent.1 Progressive Generation of Long Text with Pretrained Language Models. Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains.", "We use it as an additional lexical-based quality measure. Fr\u00e9chet BERT Distance (FBD) is a semanticbased metric (Montahaei et al., 2019) that measures the Fr\u00e9chet Distance in the BERT feature space between the generated and real text. By using the BERT features from shallow (S), medium (M), and deep (D) layers, we can compute FBD-S/M/D, respectively. Backward BLEU (B-BLEU) is a diversity metric (Shi et al., 2018) measuring how well the generated text covers n-grams occurred in the test set. Harmonic BLEU (HA-BLEU) (Shi et al., 2018) is an aggregated quality and diversity metric that incorporates both the standard BLEU (i.e., precision) and the Backward BLEU (i.e., recall). 4.2.2 Results. Figures 3 and 4 show the results of the various systems on the news and story domains, respectively, measured with different metrics against test set. We give more complete results in the appendix. We can see that our progressive generation approach consistently outperforms the standard, single-stage LMs (GPT2-Small, GPT2-Large and BART) by a large margin on almost all metrics in both domains. Further, by increasing the number of progression stages, our method steadily achieves even stronger performance. This highlights the benefits of the flexible progressive generation strategy. The various models using pretrained LMs with previous planning-then-generation strategies show\nmixed results across the different metrics. For example, Summary achieves strong performance in terms of the semantic-based quality metric FBD-D (partially because the summaries are closer to the real text in the BERT feature space), but significantly falls behind other models in terms of diversity (B-BLEU4) and other quality metrics like MSJ and HA-BLEU. Similarly, the SRL-based methods give only mediocre results in terms of the semanticbased FBD-D. In contrast, our approach maintains a relatively consistent performance level.", "Those methods often adopt the similar idea of planning-then-generation as above (Shen et al., 2019; Zhao et al., 2020; Bosselut et al., 2018; See et al., 2019; Hua and Wang, 2020; Rashkin et al., 2020). Another line of work instead focuses on extending the transformer architecture (Vaswani et al., 2017) to model longer text sequences (e.g., Dai et al., 2019; Wang et al., 2020; Choromanski et al., 2021, etc). For example, Liu et al. (2018) used a hybrid retrieval-generation architecture for producing long summaries; Dai et al. (2019) showed long text samples qualitatively. Our work systematically examines the pretrained LMs in generating long domain-specific text, and proposes a new approach that empowers pretrained LMs for producing samples of significantly higherquality. 3 Progressive Generation of Text. One of the main challenges in generating long coherent passages is modeling long-range dependencies across the entire sequences (e.g., 1000 tokens). We propose a progressive generation approach that is conceptually simple yet effective. Intuitively, progressive generation divides the complex problem of generating the full passage into a series of much easier steps of generating coarser-grained intermediate sequences. Contrary to generating everything from left to right from scratch, our progressive generation allows the model to first plan globally and then shift attention to increasingly finer details, which results in more coherent text. Figure 2 illustrates the generation process. 3.1 Generation Process. Let y := [y1, y2, . . . , yT ] be the output text, where each yi is a token of language (a word or a subword). The output sequences are generated either conditionally on any other information x (e.g., generations of a story given a prompt), or unconditionally (in which case we assume x \u2261 \u2205 while keeping the same notation). Instead of generating the full passage y"]}
{"pkey": "gpt2_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses the paper authors use a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic-based cleaning contains slightly over 8 million documents for a total of 40 GB of text. GPT-2 zero-shots to state-of-the-art performance on 7 out of 8 tested language modeling datasets.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["We evaluate on two text generation domains including: (1) CNN News (Hermann et al., 2015) for unconditional generation. (2) WritingPrompts (Fan et al., 2018) for conditional story generation. The task is to generate a story given a prompt. The two datasets are chosen since they both contain long documents, with CNN\u2019s average and maximum length being 512 and 926, and WritingPrompts\u2019s being 437 and 942, respectively. To demonstrate the data efficiency of our approaches adapting to target domains, we sample 1,000 documents in each dataset for training. Model configs. We use BARTs for all stages of generation. Due to computation limitations, we experiment models with 2, 3, 4-stages generations. In\nour 2-stage model, our first stage covers about 25% of all content; in the 3-stage model, the first and second stages cover 15% and 25% of all content, respectively; and in the 4-stage model, our first three stages cover 15%, 20%, 25% of all content. For model training, we follow the same protocol as (See et al., 2019) to fine-tune all pretrained models until convergence. To combat exposure bias, we add noise to the training data as described in Sec 3.2, with the probability of replacing 1,2,3,4- grams 0.1/0.05/0.025/0.0125. In the generation phase, we use top-p decoding (Holtzman et al., 2020) with p = 0.95 to generate 1024 tokens at maximum. Experiments were conducted with RTX6000 GPUs. It took around 4 hours for model fine-tuning and generation with a single GPU. Comparison methods. We compare with a wide range of baselines, categorized into two groups: (1) The large pretrained LMs including BART (Lewis et al., 2020) and GPT-2 in both small and large sizes (Radford et al., 2019). The LMs generate text in a standard left-to-right manner; (2) Progressive generation with various strategies adopted in the prior planning-then-generation work. Same as our proposed method, each stage adapts a pretrained BART for generation.", "The Pearson correlation coefficient of human scores is 0.52, showing moderate inter-rater agreement. Table 1 shows the results. All systems receive close fluency scores. Our approach obtained significantly higher coherence scores at both passage and sentence levels. In particular, over 86% sentences\nin our model generations are considered as coherent with the context, improving over other models by at least 10 absolute percent. 4.4 Ablation Study and Analysis. Sample efficiency. We study how the progressive generation could improve the sample efficiency of large LMs fine-tuned to target domains. The intuition is that by focusing on the subsets of informative words, the early stages can more efficiently capture the domain-specific characteristics and then steer the subsequent refinement stages. Figure 5 shows the results where we report the FBD score averaged over FBD-S/M/D. We can see our approach can make more efficient use of the training data in learning to generate high quality samples. For example, with only 1K training examples, our method achieves comparable results with large LMs trained on 30K examples. Generation with gold plans. To investigate the importance of dividing the generation process into stages and what the stages learn separately, we add another set of text into our comparison. It is a 2- stages model whose first stage is the ground truth (gold plan) while the second stage kept the same (a BART model), shown as GoldPlan in Table 3. Note that with gold plan, our model greatly decreases the gap with human text in terms of lexical (TID) and semantic (FBD-D) quality metrics. The results highlight the importance of plans in text\ngeneration. The intermediate plans act as an information bottleneck, and high-quality plans could lead to high-quality text generation. Effect of data noising. We study the ablation of data noising, to check whether the noising operation really helps reduce stage-wise exposure bias (Sec 3.2) as we expected.", "Specifically, Summary first generates a short summary text as the content plan and conditioning on the summary produces the full passage of text (Fan et al., 2019). For training, summaries are obtained using the state-of-the-art pretrained CNN news summarization model based on BART; Keyword first generates a series of keywords, based on which the full text is generated in the next stage. Following (Yao et al., 2019), the keywords are extracted with the RAKE algorithm (Rose et al., 2010) for training; SRL follows the recent work (Fan et al., 2019) by first generating a sequence of predicates and arguments and then producing the full text conditionally. The same semantic role labeling tool as in the prior work is used here to create training data. SRL+NER and SRL+Coref further augment the SRL method by an additional stage of generating entity anonymized text conditioning on the predicates sequence prior to the final stage (Fan et al., 2019). SRL+NER uses an NER model to mask all entities, while SRL+Coref applies coreference resolution to mask all clusters of mentions. We use the same NER and coreference tools as in (Fan et al., 2019). Finally, as a reference, we also present the results of Human-written text (i.e., the text in the dev set). 4.2 Automatic Evaluation. 4.2.1 Evaluation Metrics. To evaluate the generation quality for the domainspecific open-ended generation as studied here, we primarily measure the \u201ccloseness\u201d between two sets of text, one generated by the model and the other the real text from the target domain. We evaluate with a broad array of automatic metrics, including lexical-based quality metrics and semanticbased quality metrics. We also evaluate the generation diversity. MS-Jaccard (MSJ) is a lexical-based metric (Montahaei et al., 2019), where MSJ-n measures the similarity of n-grams frequencies between two sets of text with Jaccard index. TF-IDF Distance (TID) is defined as the distance between the average TF-IDF features of two text sets."]}
{"pkey": "gpt2_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Our experiments, while much noisier across tasks, suggest similar trends hold for sub-tasks of an objective and continue into the 1B+ parameter regime.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["In the multi-stage generation, the intermediate sequences are not natural language. Yet we found that fine-tuning pretrained LMs (such as BART and GPT-2) to generate the intermediate sequences is indeed very efficient in terms of data and computation. We tried training other models such as small sequence-to-sequence models and n-gram models from scratch, which we found is much harder, requiring more data, or yielding inferior performance. This again highlights the importance of using pretrained LMs, as enabled by our simple method design. Stage-level exposure bias and data noising. In the above training process, the outputs of each LM are conditioning on the ground-truth input sequences extracted from the real corpus. In contrast, at generation time, the LM takes as inputs the imperfect sequences produced at the previous stage, which can result in new mistakes in the outputs since the LM has never be exposed to noisy inputs during training. Thus, the discrepancy between training and generation can lead to mistakes in generation accumulating through the stages. The phenomenon resembles the exposure bias issue (Ranzato et al., 2016) of sequential generation models at token level, where the model is trained to predict the next token given the previous ground-truth tokens, while at generation time tokens generated by the model itself are instead used to make the next prediction. To alleviate the issue and increase the robustness of each intermediate LM, we draw on the rich literature of addressing token-level exposure bias (Xie et al., 2017; Tan et al., 2019). Specifically, during training, we inject noise into the ground-truth inputs at each stage by randomly picking an n-gram (n \u2208 {1, 2, 3, 4}) and replacing it with another randomly sampled n-gram. The data noising encourages the LMs to learn to recover from the mistakes in inputs, leading to a more robust system during generation. 4 Experiments. 4.1 Setup.\nDomains.", "The Pearson correlation coefficient of human scores is 0.52, showing moderate inter-rater agreement. Table 1 shows the results. All systems receive close fluency scores. Our approach obtained significantly higher coherence scores at both passage and sentence levels. In particular, over 86% sentences\nin our model generations are considered as coherent with the context, improving over other models by at least 10 absolute percent. 4.4 Ablation Study and Analysis. Sample efficiency. We study how the progressive generation could improve the sample efficiency of large LMs fine-tuned to target domains. The intuition is that by focusing on the subsets of informative words, the early stages can more efficiently capture the domain-specific characteristics and then steer the subsequent refinement stages. Figure 5 shows the results where we report the FBD score averaged over FBD-S/M/D. We can see our approach can make more efficient use of the training data in learning to generate high quality samples. For example, with only 1K training examples, our method achieves comparable results with large LMs trained on 30K examples. Generation with gold plans. To investigate the importance of dividing the generation process into stages and what the stages learn separately, we add another set of text into our comparison. It is a 2- stages model whose first stage is the ground truth (gold plan) while the second stage kept the same (a BART model), shown as GoldPlan in Table 3. Note that with gold plan, our model greatly decreases the gap with human text in terms of lexical (TID) and semantic (FBD-D) quality metrics. The results highlight the importance of plans in text\ngeneration. The intermediate plans act as an information bottleneck, and high-quality plans could lead to high-quality text generation. Effect of data noising. We study the ablation of data noising, to check whether the noising operation really helps reduce stage-wise exposure bias (Sec 3.2) as we expected.", "We observe that generation of some words (e.g., stop words) does not require many contexts, while other words are decisive and have long-term impact on the whole content of the passage. Motivated by this observation, our approach first produces a sequence of most informative words, then progressively refines the sequence by adding finergrained details in multiple stages, until completing a full passage. The generation at each stage is conditioning on the output of the preceding stage which provides anchors and steers the current generation (Figure 2). The intermediate words produced at each stage are defined based on a simple TF-IDF informativeness metric. The approach enjoys several core advantages: (1) Although the progressive approach implements a conceptually non-monotonic generation process, generation at each stage can still be performed in a left-to-right manner and thus is directly compatible with the powerful pretrained monotonic LMs. The LMs at different stages are easily fine-tuned to accommodate a target domain using only small, independently constructed data. Intuitively, each LM\nis addressing a sub-task of mapping a sequence to a finer-resolution one, which is much simpler than the overall task of mapping from conditions to full passages of text. In this work, we use BART (Lewis et al., 2020) for generation at each stage, though one can also plug in other off-the-shelf LMs."]}
{"pkey": "gpt2_7", "question": "List the limitations of the model discussed in the paper.", "answer": "Because large-scale language models like GPT-2 do not distinguish fact from fiction, the paper authors don\u2019t support use cases that require the generated text to be true. Additionally, language models like GPT-2 reflect the biases inherent to the systems they were trained on, so the paper authors do not recommend that they be deployed into systems that interact with humans > unless the deployers first carry out a study of biases relevant to the intended use-case. The paper authors found no statistically significant difference in gender, race, and religious bias probes between 774M and 1.5B, implying all versions of GPT-2 should be approached with similar levels of caution around use cases that are sensitive to biases around human attributes.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["directly, we propose to add multiple intermediate stages: x \u2192 c1 \u2192 c2 \u00b7 \u00b7 \u00b7 \u2192 cK \u2192 y, where for each stage k \u2208 {1, . . . ,K}, ck is an intermediate sequence containing information of the passage at certain granularity. For instance, at the first stage, c1 can be seen as a highest-level content plan consisting of the most informative tokens such as key entities. Then, based on the plan, we gradually refine them into subsequent ck, each of which contains finer-grained information than that of the preceding stage. At the final stage, we refine cK into the full passage by adding the least informative words (e.g., stop words). The generation process corresponds to a decomposition of the conditional probability as:\nP (y, {ck}|x) = P (c1|x) \u03a0Kk=2P (ck|ck\u22121,x)P (y|cK ,x) . (1)\nAs the above intuition, ck at early stages as the high-level content plans should contain informative or important words, to serve as skeletons for subsequent enrichment. We next concretely define the order of generation, namely, which words should each stage generates. Specifically, we propose a simple method\nthat constructs a vocabulary Vk for each stage k, based on the importance of words in the target domain. Each particular stage k only produces tokens belonging to its vocabulary Vk. By the progressive nature of the generation process, we have V1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 VK \u2282 V . That is, V1 contains the smallest core set of words in the domain, and the vocabularies gradually expand at later stages until arriving the full vocabulary V . Note that vocabularies in later stages are supersets of those in earlier stages. This allows the later stages to remedy and polish potential mistakes made in earlier stages when necessary. We discuss the construction of the vocabularies in the below. Stage-wise vocabularies based on word importance. Given a text corpusD of the target domain with the full vocabulary V , we define the importance scores of words in V based on the TF-IDF metric.", "In this work, we study the problem of generating coherent, much longer passages of text (e.g., 1000 tokens). GPT-3 (Brown et al., 2020) was reported to produce long essays, yet the results seem to need extensive human curations (e.g., MarketMuse; Gardian), and the system is not publicly available to adapt to arbitrary desired domains. In this work, we examine fine-tuning of largescale LMs for domain-specific generation of extra-\nar X\niv :2\n00 6.\n15 72\n0v 2\n[ cs\n.C L\n] 1\n4 A\npr 2\n02 1\nlong text. We find that samples produced by GPT-2 fine-tuned on small domain-specific corpora exhibit various imperfections, including excessive repetitiveness and incoherence between sentences far apart. Figure 1 measures the coherence of text generated by the fine-tuned GPT-2 w.r.t the BERT next sentence prediction (Devlin et al., 2019) score. As the figure shows, GPT-2 models (regardless of the model size) exhibit a significant gap in the score compared with human text, hence falling short in generating coherent text. We hypothesize that the problem is mainly caused by the sequential generation order of the LMs, which makes global content planning of the passage difficult, especially when the generated text is long and contains thousands of words. One could potentially adopt the recent planning-thengeneration or non-monotonic methods (Sec 2), yet those methods either require specialized neural architectures that need costly retraining for each domain (Gu et al., 2019; Stern et al., 2019; Chan et al., 2019; Fan et al., 2019), or rely on dedicated intermediate content plans (e.g., summaries, SRL labels) (Fan et al., 2019; Yao et al., 2019) with limited flexibility and producing sub-optimal results as shown in our experiments. To overcome the limitations, we introduce a new method for Progressive Generation of Text (ProGen).", "We evaluate on two text generation domains including: (1) CNN News (Hermann et al., 2015) for unconditional generation. (2) WritingPrompts (Fan et al., 2018) for conditional story generation. The task is to generate a story given a prompt. The two datasets are chosen since they both contain long documents, with CNN\u2019s average and maximum length being 512 and 926, and WritingPrompts\u2019s being 437 and 942, respectively. To demonstrate the data efficiency of our approaches adapting to target domains, we sample 1,000 documents in each dataset for training. Model configs. We use BARTs for all stages of generation. Due to computation limitations, we experiment models with 2, 3, 4-stages generations. In\nour 2-stage model, our first stage covers about 25% of all content; in the 3-stage model, the first and second stages cover 15% and 25% of all content, respectively; and in the 4-stage model, our first three stages cover 15%, 20%, 25% of all content. For model training, we follow the same protocol as (See et al., 2019) to fine-tune all pretrained models until convergence. To combat exposure bias, we add noise to the training data as described in Sec 3.2, with the probability of replacing 1,2,3,4- grams 0.1/0.05/0.025/0.0125. In the generation phase, we use top-p decoding (Holtzman et al., 2020) with p = 0.95 to generate 1024 tokens at maximum. Experiments were conducted with RTX6000 GPUs. It took around 4 hours for model fine-tuning and generation with a single GPU. Comparison methods. We compare with a wide range of baselines, categorized into two groups: (1) The large pretrained LMs including BART (Lewis et al., 2020) and GPT-2 in both small and large sizes (Radford et al., 2019). The LMs generate text in a standard left-to-right manner; (2) Progressive generation with various strategies adopted in the prior planning-then-generation work. Same as our proposed method, each stage adapts a pretrained BART for generation."]}
{"pkey": "gpt2_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The resulting dataset, WebText, contains the text subset of these 45 million links. To extract the text from HTML responses the paper authors use a combination of the Dragnet (Peters & Lecocq, 2013) and Newspaper1 content extractors. All results presented in this paper use a preliminary version of WebText which does not include links created after Dec 2017 and which after de-duplication and some heuristic based cleaning contains slightly over 8 million documents for a total of 40 GB of text.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["We evaluate on two text generation domains including: (1) CNN News (Hermann et al., 2015) for unconditional generation. (2) WritingPrompts (Fan et al., 2018) for conditional story generation. The task is to generate a story given a prompt. The two datasets are chosen since they both contain long documents, with CNN\u2019s average and maximum length being 512 and 926, and WritingPrompts\u2019s being 437 and 942, respectively. To demonstrate the data efficiency of our approaches adapting to target domains, we sample 1,000 documents in each dataset for training. Model configs. We use BARTs for all stages of generation. Due to computation limitations, we experiment models with 2, 3, 4-stages generations. In\nour 2-stage model, our first stage covers about 25% of all content; in the 3-stage model, the first and second stages cover 15% and 25% of all content, respectively; and in the 4-stage model, our first three stages cover 15%, 20%, 25% of all content. For model training, we follow the same protocol as (See et al., 2019) to fine-tune all pretrained models until convergence. To combat exposure bias, we add noise to the training data as described in Sec 3.2, with the probability of replacing 1,2,3,4- grams 0.1/0.05/0.025/0.0125. In the generation phase, we use top-p decoding (Holtzman et al., 2020) with p = 0.95 to generate 1024 tokens at maximum. Experiments were conducted with RTX6000 GPUs. It took around 4 hours for model fine-tuning and generation with a single GPU. Comparison methods. We compare with a wide range of baselines, categorized into two groups: (1) The large pretrained LMs including BART (Lewis et al., 2020) and GPT-2 in both small and large sizes (Radford et al., 2019). The LMs generate text in a standard left-to-right manner; (2) Progressive generation with various strategies adopted in the prior planning-then-generation work. Same as our proposed method, each stage adapts a pretrained BART for generation.", "In this work, we study the problem of generating coherent, much longer passages of text (e.g., 1000 tokens). GPT-3 (Brown et al., 2020) was reported to produce long essays, yet the results seem to need extensive human curations (e.g., MarketMuse; Gardian), and the system is not publicly available to adapt to arbitrary desired domains. In this work, we examine fine-tuning of largescale LMs for domain-specific generation of extra-\nar X\niv :2\n00 6.\n15 72\n0v 2\n[ cs\n.C L\n] 1\n4 A\npr 2\n02 1\nlong text. We find that samples produced by GPT-2 fine-tuned on small domain-specific corpora exhibit various imperfections, including excessive repetitiveness and incoherence between sentences far apart. Figure 1 measures the coherence of text generated by the fine-tuned GPT-2 w.r.t the BERT next sentence prediction (Devlin et al., 2019) score. As the figure shows, GPT-2 models (regardless of the model size) exhibit a significant gap in the score compared with human text, hence falling short in generating coherent text. We hypothesize that the problem is mainly caused by the sequential generation order of the LMs, which makes global content planning of the passage difficult, especially when the generated text is long and contains thousands of words. One could potentially adopt the recent planning-thengeneration or non-monotonic methods (Sec 2), yet those methods either require specialized neural architectures that need costly retraining for each domain (Gu et al., 2019; Stern et al., 2019; Chan et al., 2019; Fan et al., 2019), or rely on dedicated intermediate content plans (e.g., summaries, SRL labels) (Fan et al., 2019; Yao et al., 2019) with limited flexibility and producing sub-optimal results as shown in our experiments. To overcome the limitations, we introduce a new method for Progressive Generation of Text (ProGen).", "In particular, our 4-stage model, ProGen-4, is steadily among the best across all metrics, further validating\nthe advantage of the proposed simple yet flexible multi-stage generation. These results also indicate the necessity of using a large diverse set of automatic metrics for a comprehensive evaluation, and motivate human studies for further assessment. 1K 10K 20K 30K Training Size\n60\n65\n70\nFr ec\nhe t B\ner t D\nist an\nce (F\nBD ) BART\nGPT2-Small GPT2-Large ProGen (10K) ProGen (1K)\nFigure 5: Sample efficiency on the story domain with the FBD metric (the lower, the better). FBD-D \u2193 MSJ-4 \u2191 HA-BL4 \u2191\nProGen-2 39.94 16.50 30.45 -Noise 47.18 16.25 31.39\nProGen-3 38.30 16.68 30.64 -Noise 39.64 16.65 30.72\nProGen-4 36.49 16.96 31.32 -Noise 39.78 16.85 30.86\nTable 2: Effect of noise on CNN. FBD-D \u2193 TID \u2193\nProGen-2 39.94 6.2 GoldPlan 30.16 3.5\nHuman 25.63 2.6\nTable 3: GoldPlan Results on CNN. 4.3 Human Evaluation. In our human study, we asked three university students who are proficient English speakers to evaluate the coherence and fluency of the generated text. To better assess the coherence of the long passages of text, we evaluate at both the passage level and the finer-grained sentence level. More concretely, for passage-level coherence, human raters assign a coherence score to each full-length text sample, on a 5-point Likert scale. For a more detailed assessment, we further evaluate sentencelevel coherence, where human raters label each sentence in the text passage with 0 or 1, indicating whether the particular sentence is coherent with the proceeding context in the passage. We then calculate the average percentage of coherent sentences in the generated text by each model. Human raters also evaluate the language quality for a fluency score on a 5-point Likert scale. We compare our method with the systems that show highest generation quality in automatic evaluation, including BART, GPT2-Small, and Summary. We evaluated 50 examples for each comparison model on the CNN domain."]}
{"pkey": "gpt2_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The texts are tokenized using a byte-level version of Byte Pair Encoding (BPE) (for unicode characters) and a vocabulary size of 50,257. The inputs are sequences of 1024 consecutive tokens.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["We then rank all the words and assign the top Vk words to the intermediate vocabulary Vk. Here Vk is a hyper-parameter controlling the size of Vk. More concretely, for each word w \u2208 V , we first compute its standard TF-IDF score (Salton and McGill, 1986) in each document d \u2208 D, which essentially measures how important w is to d. The importance of the word w in the domain is then defined as the average TF-IDF score across all documents containing w:\nimportance(w,D) = \u2211\nd\u2208D TF_IDF(w,d) DF(w,D) , (2)\nwhere TF_IDF(w,d) is the TF-IDF score of word w in document d; and DF(w,D) is the document\nAlgorithm 1 Training for Progressive Text Generation Inputs: Domain corpus D Vocabulary sizes for K stages K pretrained LMs (e.g. GPT-2 or BART)\n1 : Construct stage-wise vocabularies {Vk} based on word importance Eq.(2) 2: Extract intermediate sequences {c\u2217k} using {Vk}; add data noises (Sec 3.2) 3: Fine-tune all LMs independently (Sec 3.2) Output: Fine-tuned LMs for generation at all stages in a progressive manner\nfrequency, i.e., the number of documents in the corpus that contain the word w.\nPretrained language models as building blocks. Compared to many of the previous planning-thengeneration and non-monotonic generation methods, one of the key advantages of our progressive generation design is the direct compatibility with the powerful pretrained LMs that perform left-to-right generation. Specifically, although our approach implements a non-monotonic generation process that produces importance words first, we can generate intermediate sequences ck at each stage still in a left-to-right manner. Thus, we can plug pretrained LM, such as GPT-2 or BART, into each stage to carry out the generation. As described more in section 3.2, for each stage k, we can conveniently construct stage-specific training data from the domain corpus D using the stage-wise vocabulary Vk, and fine-tune the stage-k LM in order to generate intermediate sequences at the stage that are pertaining to the target domain.", "directly, we propose to add multiple intermediate stages: x \u2192 c1 \u2192 c2 \u00b7 \u00b7 \u00b7 \u2192 cK \u2192 y, where for each stage k \u2208 {1, . . . ,K}, ck is an intermediate sequence containing information of the passage at certain granularity. For instance, at the first stage, c1 can be seen as a highest-level content plan consisting of the most informative tokens such as key entities. Then, based on the plan, we gradually refine them into subsequent ck, each of which contains finer-grained information than that of the preceding stage. At the final stage, we refine cK into the full passage by adding the least informative words (e.g., stop words). The generation process corresponds to a decomposition of the conditional probability as:\nP (y, {ck}|x) = P (c1|x) \u03a0Kk=2P (ck|ck\u22121,x)P (y|cK ,x) . (1)\nAs the above intuition, ck at early stages as the high-level content plans should contain informative or important words, to serve as skeletons for subsequent enrichment. We next concretely define the order of generation, namely, which words should each stage generates. Specifically, we propose a simple method\nthat constructs a vocabulary Vk for each stage k, based on the importance of words in the target domain. Each particular stage k only produces tokens belonging to its vocabulary Vk. By the progressive nature of the generation process, we have V1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 VK \u2282 V . That is, V1 contains the smallest core set of words in the domain, and the vocabularies gradually expand at later stages until arriving the full vocabulary V . Note that vocabularies in later stages are supersets of those in earlier stages. This allows the later stages to remedy and polish potential mistakes made in earlier stages when necessary. We discuss the construction of the vocabularies in the below. Stage-wise vocabularies based on word importance. Given a text corpusD of the target domain with the full vocabulary V , we define the importance scores of words in V based on the TF-IDF metric.", "One can add masks on the pretrained LM\u2019s to-\nken distributions to ensure the stage-k LM only produces tokens belonging to Vk. In practice, we found it is not necessary, as the pretrained LM can usually quickly learns the pattern through finetuning and generate appropriate tokens during inference. In our experiments we use BART for all stages, since BART is an encoder-decoder model which can conveniently take as inputs the resulting sequence from the preceding stage and generate new. (For the first stage in an unconditional generation task, we simply set x = \u2205.) We note that GPT2, and other relevant pretraiened LMs, can indeed also be used as a conditional generator (Radford et al., 2019; Liu et al., 2018) and thus be plugged into any of stages. 3.2 Training. Our approach permits straightforward training/finetuning of the (pretrained) LMs at different stages given the domain corpus D. In particular, we can easily construct independent training data for each stage, and train all LMs in parallel. Note that no additional resources such as pretrained summarization or semantic role labeling models are requested as in previous work, making our approach directly applicable to a potentially broader set of domains and languages. We plan to explore the use of our method in multi-lingual setting in the future. More concretely, for each stage k, we use the stage vocabularies Vk\u22121 and Vk to filter all relevant tokens in the documents as training data. That is, given a document, we extract the subsequence c\u2217k\u22121 of all tokens from the document that are belonging to Vk\u22121, and similarly extract sub-sequence c\u2217k belonging to Vk. The c\u2217k\u22121 and c\u2217k are then used as the input and the ground-truth output, respectively, for training the LM at stage k with maximum likelihood learning. Therefore, given the stage-wise vocabularies {Vk}, we can automatically extract training data from the domain corpus D for different stages, and train the LMs separately."]}
{"pkey": "gpt2_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "Byte Pair Encoding (BPE) (Sennrich et al., 2015) is a practical middle ground between character and word level language modeling which effectively interpolates between word level inputs for frequent symbol sequences and char_x0002_acter level inputs for infrequent symbol sequences", "title": "Language Models are Unsupervised Multitask Learners", "context": ["We evaluate on two text generation domains including: (1) CNN News (Hermann et al., 2015) for unconditional generation. (2) WritingPrompts (Fan et al., 2018) for conditional story generation. The task is to generate a story given a prompt. The two datasets are chosen since they both contain long documents, with CNN\u2019s average and maximum length being 512 and 926, and WritingPrompts\u2019s being 437 and 942, respectively. To demonstrate the data efficiency of our approaches adapting to target domains, we sample 1,000 documents in each dataset for training. Model configs. We use BARTs for all stages of generation. Due to computation limitations, we experiment models with 2, 3, 4-stages generations. In\nour 2-stage model, our first stage covers about 25% of all content; in the 3-stage model, the first and second stages cover 15% and 25% of all content, respectively; and in the 4-stage model, our first three stages cover 15%, 20%, 25% of all content. For model training, we follow the same protocol as (See et al., 2019) to fine-tune all pretrained models until convergence. To combat exposure bias, we add noise to the training data as described in Sec 3.2, with the probability of replacing 1,2,3,4- grams 0.1/0.05/0.025/0.0125. In the generation phase, we use top-p decoding (Holtzman et al., 2020) with p = 0.95 to generate 1024 tokens at maximum. Experiments were conducted with RTX6000 GPUs. It took around 4 hours for model fine-tuning and generation with a single GPU. Comparison methods. We compare with a wide range of baselines, categorized into two groups: (1) The large pretrained LMs including BART (Lewis et al., 2020) and GPT-2 in both small and large sizes (Radford et al., 2019). The LMs generate text in a standard left-to-right manner; (2) Progressive generation with various strategies adopted in the prior planning-then-generation work. Same as our proposed method, each stage adapts a pretrained BART for generation.", "Specifically, Summary first generates a short summary text as the content plan and conditioning on the summary produces the full passage of text (Fan et al., 2019). For training, summaries are obtained using the state-of-the-art pretrained CNN news summarization model based on BART; Keyword first generates a series of keywords, based on which the full text is generated in the next stage. Following (Yao et al., 2019), the keywords are extracted with the RAKE algorithm (Rose et al., 2010) for training; SRL follows the recent work (Fan et al., 2019) by first generating a sequence of predicates and arguments and then producing the full text conditionally. The same semantic role labeling tool as in the prior work is used here to create training data. SRL+NER and SRL+Coref further augment the SRL method by an additional stage of generating entity anonymized text conditioning on the predicates sequence prior to the final stage (Fan et al., 2019). SRL+NER uses an NER model to mask all entities, while SRL+Coref applies coreference resolution to mask all clusters of mentions. We use the same NER and coreference tools as in (Fan et al., 2019). Finally, as a reference, we also present the results of Human-written text (i.e., the text in the dev set). 4.2 Automatic Evaluation. 4.2.1 Evaluation Metrics. To evaluate the generation quality for the domainspecific open-ended generation as studied here, we primarily measure the \u201ccloseness\u201d between two sets of text, one generated by the model and the other the real text from the target domain. We evaluate with a broad array of automatic metrics, including lexical-based quality metrics and semanticbased quality metrics. We also evaluate the generation diversity. MS-Jaccard (MSJ) is a lexical-based metric (Montahaei et al., 2019), where MSJ-n measures the similarity of n-grams frequencies between two sets of text with Jaccard index. TF-IDF Distance (TID) is defined as the distance between the average TF-IDF features of two text sets.", "We observe that generation of some words (e.g., stop words) does not require many contexts, while other words are decisive and have long-term impact on the whole content of the passage. Motivated by this observation, our approach first produces a sequence of most informative words, then progressively refines the sequence by adding finergrained details in multiple stages, until completing a full passage. The generation at each stage is conditioning on the output of the preceding stage which provides anchors and steers the current generation (Figure 2). The intermediate words produced at each stage are defined based on a simple TF-IDF informativeness metric. The approach enjoys several core advantages: (1) Although the progressive approach implements a conceptually non-monotonic generation process, generation at each stage can still be performed in a left-to-right manner and thus is directly compatible with the powerful pretrained monotonic LMs. The LMs at different stages are easily fine-tuned to accommodate a target domain using only small, independently constructed data. Intuitively, each LM\nis addressing a sub-task of mapping a sequence to a finer-resolution one, which is much simpler than the overall task of mapping from conditions to full passages of text. In this work, we use BART (Lewis et al., 2020) for generation at each stage, though one can also plug in other off-the-shelf LMs."]}
{"pkey": "gpt2_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "The paper authors use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final selfattention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used. The paper authors scale the weights of residual layers at initialization by a factor of 1/ \u221a N where N is the number of residual layers", "title": "Language Models are Unsupervised Multitask Learners", "context": ["Those methods often adopt the similar idea of planning-then-generation as above (Shen et al., 2019; Zhao et al., 2020; Bosselut et al., 2018; See et al., 2019; Hua and Wang, 2020; Rashkin et al., 2020). Another line of work instead focuses on extending the transformer architecture (Vaswani et al., 2017) to model longer text sequences (e.g., Dai et al., 2019; Wang et al., 2020; Choromanski et al., 2021, etc). For example, Liu et al. (2018) used a hybrid retrieval-generation architecture for producing long summaries; Dai et al. (2019) showed long text samples qualitatively. Our work systematically examines the pretrained LMs in generating long domain-specific text, and proposes a new approach that empowers pretrained LMs for producing samples of significantly higherquality. 3 Progressive Generation of Text. One of the main challenges in generating long coherent passages is modeling long-range dependencies across the entire sequences (e.g., 1000 tokens). We propose a progressive generation approach that is conceptually simple yet effective. Intuitively, progressive generation divides the complex problem of generating the full passage into a series of much easier steps of generating coarser-grained intermediate sequences. Contrary to generating everything from left to right from scratch, our progressive generation allows the model to first plan globally and then shift attention to increasingly finer details, which results in more coherent text. Figure 2 illustrates the generation process. 3.1 Generation Process. Let y := [y1, y2, . . . , yT ] be the output text, where each yi is a token of language (a word or a subword). The output sequences are generated either conditionally on any other information x (e.g., generations of a story given a prompt), or unconditionally (in which case we assume x \u2261 \u2205 while keeping the same notation). Instead of generating the full passage y", "We evaluate on two text generation domains including: (1) CNN News (Hermann et al., 2015) for unconditional generation. (2) WritingPrompts (Fan et al., 2018) for conditional story generation. The task is to generate a story given a prompt. The two datasets are chosen since they both contain long documents, with CNN\u2019s average and maximum length being 512 and 926, and WritingPrompts\u2019s being 437 and 942, respectively. To demonstrate the data efficiency of our approaches adapting to target domains, we sample 1,000 documents in each dataset for training. Model configs. We use BARTs for all stages of generation. Due to computation limitations, we experiment models with 2, 3, 4-stages generations. In\nour 2-stage model, our first stage covers about 25% of all content; in the 3-stage model, the first and second stages cover 15% and 25% of all content, respectively; and in the 4-stage model, our first three stages cover 15%, 20%, 25% of all content. For model training, we follow the same protocol as (See et al., 2019) to fine-tune all pretrained models until convergence. To combat exposure bias, we add noise to the training data as described in Sec 3.2, with the probability of replacing 1,2,3,4- grams 0.1/0.05/0.025/0.0125. In the generation phase, we use top-p decoding (Holtzman et al., 2020) with p = 0.95 to generate 1024 tokens at maximum. Experiments were conducted with RTX6000 GPUs. It took around 4 hours for model fine-tuning and generation with a single GPU. Comparison methods. We compare with a wide range of baselines, categorized into two groups: (1) The large pretrained LMs including BART (Lewis et al., 2020) and GPT-2 in both small and large sizes (Radford et al., 2019). The LMs generate text in a standard left-to-right manner; (2) Progressive generation with various strategies adopted in the prior planning-then-generation work. Same as our proposed method, each stage adapts a pretrained BART for generation.", "We then rank all the words and assign the top Vk words to the intermediate vocabulary Vk. Here Vk is a hyper-parameter controlling the size of Vk. More concretely, for each word w \u2208 V , we first compute its standard TF-IDF score (Salton and McGill, 1986) in each document d \u2208 D, which essentially measures how important w is to d. The importance of the word w in the domain is then defined as the average TF-IDF score across all documents containing w:\nimportance(w,D) = \u2211\nd\u2208D TF_IDF(w,d) DF(w,D) , (2)\nwhere TF_IDF(w,d) is the TF-IDF score of word w in document d; and DF(w,D) is the document\nAlgorithm 1 Training for Progressive Text Generation Inputs: Domain corpus D Vocabulary sizes for K stages K pretrained LMs (e.g. GPT-2 or BART)\n1 : Construct stage-wise vocabularies {Vk} based on word importance Eq.(2) 2: Extract intermediate sequences {c\u2217k} using {Vk}; add data noises (Sec 3.2) 3: Fine-tune all LMs independently (Sec 3.2) Output: Fine-tuned LMs for generation at all stages in a progressive manner\nfrequency, i.e., the number of documents in the corpus that contain the word w.\nPretrained language models as building blocks. Compared to many of the previous planning-thengeneration and non-monotonic generation methods, one of the key advantages of our progressive generation design is the direct compatibility with the powerful pretrained LMs that perform left-to-right generation. Specifically, although our approach implements a non-monotonic generation process that produces importance words first, we can generate intermediate sequences ck at each stage still in a left-to-right manner. Thus, we can plug pretrained LM, such as GPT-2 or BART, into each stage to carry out the generation. As described more in section 3.2, for each stage k, we can conveniently construct stage-specific training data from the domain corpus D using the stage-wise vocabulary Vk, and fine-tune the stage-k LM in order to generate intermediate sequences at the stage that are pertaining to the target domain."]}
{"pkey": "gpt2_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The largest model from BERT (Devlin et al., 2018). Our largest model, which the paper authors call GPT-2, has over an order of magnitude more parameters than GPT. The learning rate of each model was manually tuned for the best perplexity on a 5% held-out sample of WebText. All models still underfit WebText and held-out perplexity has as of yet improved given more training time.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["Those methods often adopt the similar idea of planning-then-generation as above (Shen et al., 2019; Zhao et al., 2020; Bosselut et al., 2018; See et al., 2019; Hua and Wang, 2020; Rashkin et al., 2020). Another line of work instead focuses on extending the transformer architecture (Vaswani et al., 2017) to model longer text sequences (e.g., Dai et al., 2019; Wang et al., 2020; Choromanski et al., 2021, etc). For example, Liu et al. (2018) used a hybrid retrieval-generation architecture for producing long summaries; Dai et al. (2019) showed long text samples qualitatively. Our work systematically examines the pretrained LMs in generating long domain-specific text, and proposes a new approach that empowers pretrained LMs for producing samples of significantly higherquality. 3 Progressive Generation of Text. One of the main challenges in generating long coherent passages is modeling long-range dependencies across the entire sequences (e.g., 1000 tokens). We propose a progressive generation approach that is conceptually simple yet effective. Intuitively, progressive generation divides the complex problem of generating the full passage into a series of much easier steps of generating coarser-grained intermediate sequences. Contrary to generating everything from left to right from scratch, our progressive generation allows the model to first plan globally and then shift attention to increasingly finer details, which results in more coherent text. Figure 2 illustrates the generation process. 3.1 Generation Process. Let y := [y1, y2, . . . , yT ] be the output text, where each yi is a token of language (a word or a subword). The output sequences are generated either conditionally on any other information x (e.g., generations of a story given a prompt), or unconditionally (in which case we assume x \u2261 \u2205 while keeping the same notation). Instead of generating the full passage y", "In the multi-stage generation, the intermediate sequences are not natural language. Yet we found that fine-tuning pretrained LMs (such as BART and GPT-2) to generate the intermediate sequences is indeed very efficient in terms of data and computation. We tried training other models such as small sequence-to-sequence models and n-gram models from scratch, which we found is much harder, requiring more data, or yielding inferior performance. This again highlights the importance of using pretrained LMs, as enabled by our simple method design. Stage-level exposure bias and data noising. In the above training process, the outputs of each LM are conditioning on the ground-truth input sequences extracted from the real corpus. In contrast, at generation time, the LM takes as inputs the imperfect sequences produced at the previous stage, which can result in new mistakes in the outputs since the LM has never be exposed to noisy inputs during training. Thus, the discrepancy between training and generation can lead to mistakes in generation accumulating through the stages. The phenomenon resembles the exposure bias issue (Ranzato et al., 2016) of sequential generation models at token level, where the model is trained to predict the next token given the previous ground-truth tokens, while at generation time tokens generated by the model itself are instead used to make the next prediction. To alleviate the issue and increase the robustness of each intermediate LM, we draw on the rich literature of addressing token-level exposure bias (Xie et al., 2017; Tan et al., 2019). Specifically, during training, we inject noise into the ground-truth inputs at each stage by randomly picking an n-gram (n \u2208 {1, 2, 3, 4}) and replacing it with another randomly sampled n-gram. The data noising encourages the LMs to learn to recover from the mistakes in inputs, leading to a more robust system during generation. 4 Experiments. 4.1 Setup.\nDomains.", "We then rank all the words and assign the top Vk words to the intermediate vocabulary Vk. Here Vk is a hyper-parameter controlling the size of Vk. More concretely, for each word w \u2208 V , we first compute its standard TF-IDF score (Salton and McGill, 1986) in each document d \u2208 D, which essentially measures how important w is to d. The importance of the word w in the domain is then defined as the average TF-IDF score across all documents containing w:\nimportance(w,D) = \u2211\nd\u2208D TF_IDF(w,d) DF(w,D) , (2)\nwhere TF_IDF(w,d) is the TF-IDF score of word w in document d; and DF(w,D) is the document\nAlgorithm 1 Training for Progressive Text Generation Inputs: Domain corpus D Vocabulary sizes for K stages K pretrained LMs (e.g. GPT-2 or BART)\n1 : Construct stage-wise vocabularies {Vk} based on word importance Eq.(2) 2: Extract intermediate sequences {c\u2217k} using {Vk}; add data noises (Sec 3.2) 3: Fine-tune all LMs independently (Sec 3.2) Output: Fine-tuned LMs for generation at all stages in a progressive manner\nfrequency, i.e., the number of documents in the corpus that contain the word w.\nPretrained language models as building blocks. Compared to many of the previous planning-thengeneration and non-monotonic generation methods, one of the key advantages of our progressive generation design is the direct compatibility with the powerful pretrained LMs that perform left-to-right generation. Specifically, although our approach implements a non-monotonic generation process that produces importance words first, we can generate intermediate sequences ck at each stage still in a left-to-right manner. Thus, we can plug pretrained LM, such as GPT-2 or BART, into each stage to carry out the generation. As described more in section 3.2, for each stage k, we can conveniently construct stage-specific training data from the domain corpus D using the stage-wise vocabulary Vk, and fine-tune the stage-k LM in order to generate intermediate sequences at the stage that are pertaining to the target domain."]}
{"pkey": "gpt2_13", "question": "Describe the computational resources used to train the model.", "answer": "The larger model was trained on 256 cloud TPU v3 cores.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["We evaluate on two text generation domains including: (1) CNN News (Hermann et al., 2015) for unconditional generation. (2) WritingPrompts (Fan et al., 2018) for conditional story generation. The task is to generate a story given a prompt. The two datasets are chosen since they both contain long documents, with CNN\u2019s average and maximum length being 512 and 926, and WritingPrompts\u2019s being 437 and 942, respectively. To demonstrate the data efficiency of our approaches adapting to target domains, we sample 1,000 documents in each dataset for training. Model configs. We use BARTs for all stages of generation. Due to computation limitations, we experiment models with 2, 3, 4-stages generations. In\nour 2-stage model, our first stage covers about 25% of all content; in the 3-stage model, the first and second stages cover 15% and 25% of all content, respectively; and in the 4-stage model, our first three stages cover 15%, 20%, 25% of all content. For model training, we follow the same protocol as (See et al., 2019) to fine-tune all pretrained models until convergence. To combat exposure bias, we add noise to the training data as described in Sec 3.2, with the probability of replacing 1,2,3,4- grams 0.1/0.05/0.025/0.0125. In the generation phase, we use top-p decoding (Holtzman et al., 2020) with p = 0.95 to generate 1024 tokens at maximum. Experiments were conducted with RTX6000 GPUs. It took around 4 hours for model fine-tuning and generation with a single GPU. Comparison methods. We compare with a wide range of baselines, categorized into two groups: (1) The large pretrained LMs including BART (Lewis et al., 2020) and GPT-2 in both small and large sizes (Radford et al., 2019). The LMs generate text in a standard left-to-right manner; (2) Progressive generation with various strategies adopted in the prior planning-then-generation work. Same as our proposed method, each stage adapts a pretrained BART for generation.", "We then rank all the words and assign the top Vk words to the intermediate vocabulary Vk. Here Vk is a hyper-parameter controlling the size of Vk. More concretely, for each word w \u2208 V , we first compute its standard TF-IDF score (Salton and McGill, 1986) in each document d \u2208 D, which essentially measures how important w is to d. The importance of the word w in the domain is then defined as the average TF-IDF score across all documents containing w:\nimportance(w,D) = \u2211\nd\u2208D TF_IDF(w,d) DF(w,D) , (2)\nwhere TF_IDF(w,d) is the TF-IDF score of word w in document d; and DF(w,D) is the document\nAlgorithm 1 Training for Progressive Text Generation Inputs: Domain corpus D Vocabulary sizes for K stages K pretrained LMs (e.g. GPT-2 or BART)\n1 : Construct stage-wise vocabularies {Vk} based on word importance Eq.(2) 2: Extract intermediate sequences {c\u2217k} using {Vk}; add data noises (Sec 3.2) 3: Fine-tune all LMs independently (Sec 3.2) Output: Fine-tuned LMs for generation at all stages in a progressive manner\nfrequency, i.e., the number of documents in the corpus that contain the word w.\nPretrained language models as building blocks. Compared to many of the previous planning-thengeneration and non-monotonic generation methods, one of the key advantages of our progressive generation design is the direct compatibility with the powerful pretrained LMs that perform left-to-right generation. Specifically, although our approach implements a non-monotonic generation process that produces importance words first, we can generate intermediate sequences ck at each stage still in a left-to-right manner. Thus, we can plug pretrained LM, such as GPT-2 or BART, into each stage to carry out the generation. As described more in section 3.2, for each stage k, we can conveniently construct stage-specific training data from the domain corpus D using the stage-wise vocabulary Vk, and fine-tune the stage-k LM in order to generate intermediate sequences at the stage that are pertaining to the target domain.", "One can add masks on the pretrained LM\u2019s to-\nken distributions to ensure the stage-k LM only produces tokens belonging to Vk. In practice, we found it is not necessary, as the pretrained LM can usually quickly learns the pattern through finetuning and generate appropriate tokens during inference. In our experiments we use BART for all stages, since BART is an encoder-decoder model which can conveniently take as inputs the resulting sequence from the preceding stage and generate new. (For the first stage in an unconditional generation task, we simply set x = \u2205.) We note that GPT2, and other relevant pretraiened LMs, can indeed also be used as a conditional generator (Radford et al., 2019; Liu et al., 2018) and thus be plugged into any of stages. 3.2 Training. Our approach permits straightforward training/finetuning of the (pretrained) LMs at different stages given the domain corpus D. In particular, we can easily construct independent training data for each stage, and train all LMs in parallel. Note that no additional resources such as pretrained summarization or semantic role labeling models are requested as in previous work, making our approach directly applicable to a potentially broader set of domains and languages. We plan to explore the use of our method in multi-lingual setting in the future. More concretely, for each stage k, we use the stage vocabularies Vk\u22121 and Vk to filter all relevant tokens in the documents as training data. That is, given a document, we extract the subsequence c\u2217k\u22121 of all tokens from the document that are belonging to Vk\u22121, and similarly extract sub-sequence c\u2217k belonging to Vk. The c\u2217k\u22121 and c\u2217k are then used as the input and the ground-truth output, respectively, for training the LM at stage k with maximum likelihood learning. Therefore, given the stage-wise vocabularies {Vk}, we can automatically extract training data from the domain corpus D for different stages, and train the LMs separately."]}
{"pkey": "gpt2_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "Preliminary code for downloading and using the small model is available at https://github.com/openai/gpt-2", "title": "Language Models are Unsupervised Multitask Learners", "context": ["We observe that generation of some words (e.g., stop words) does not require many contexts, while other words are decisive and have long-term impact on the whole content of the passage. Motivated by this observation, our approach first produces a sequence of most informative words, then progressively refines the sequence by adding finergrained details in multiple stages, until completing a full passage. The generation at each stage is conditioning on the output of the preceding stage which provides anchors and steers the current generation (Figure 2). The intermediate words produced at each stage are defined based on a simple TF-IDF informativeness metric. The approach enjoys several core advantages: (1) Although the progressive approach implements a conceptually non-monotonic generation process, generation at each stage can still be performed in a left-to-right manner and thus is directly compatible with the powerful pretrained monotonic LMs. The LMs at different stages are easily fine-tuned to accommodate a target domain using only small, independently constructed data. Intuitively, each LM\nis addressing a sub-task of mapping a sequence to a finer-resolution one, which is much simpler than the overall task of mapping from conditions to full passages of text. In this work, we use BART (Lewis et al., 2020) for generation at each stage, though one can also plug in other off-the-shelf LMs.", "One can add masks on the pretrained LM\u2019s to-\nken distributions to ensure the stage-k LM only produces tokens belonging to Vk. In practice, we found it is not necessary, as the pretrained LM can usually quickly learns the pattern through finetuning and generate appropriate tokens during inference. In our experiments we use BART for all stages, since BART is an encoder-decoder model which can conveniently take as inputs the resulting sequence from the preceding stage and generate new. (For the first stage in an unconditional generation task, we simply set x = \u2205.) We note that GPT2, and other relevant pretraiened LMs, can indeed also be used as a conditional generator (Radford et al., 2019; Liu et al., 2018) and thus be plugged into any of stages. 3.2 Training. Our approach permits straightforward training/finetuning of the (pretrained) LMs at different stages given the domain corpus D. In particular, we can easily construct independent training data for each stage, and train all LMs in parallel. Note that no additional resources such as pretrained summarization or semantic role labeling models are requested as in previous work, making our approach directly applicable to a potentially broader set of domains and languages. We plan to explore the use of our method in multi-lingual setting in the future. More concretely, for each stage k, we use the stage vocabularies Vk\u22121 and Vk to filter all relevant tokens in the documents as training data. That is, given a document, we extract the subsequence c\u2217k\u22121 of all tokens from the document that are belonging to Vk\u22121, and similarly extract sub-sequence c\u2217k belonging to Vk. The c\u2217k\u22121 and c\u2217k are then used as the input and the ground-truth output, respectively, for training the LM at stage k with maximum likelihood learning. Therefore, given the stage-wise vocabularies {Vk}, we can automatically extract training data from the domain corpus D for different stages, and train the LMs separately.", "directly, we propose to add multiple intermediate stages: x \u2192 c1 \u2192 c2 \u00b7 \u00b7 \u00b7 \u2192 cK \u2192 y, where for each stage k \u2208 {1, . . . ,K}, ck is an intermediate sequence containing information of the passage at certain granularity. For instance, at the first stage, c1 can be seen as a highest-level content plan consisting of the most informative tokens such as key entities. Then, based on the plan, we gradually refine them into subsequent ck, each of which contains finer-grained information than that of the preceding stage. At the final stage, we refine cK into the full passage by adding the least informative words (e.g., stop words). The generation process corresponds to a decomposition of the conditional probability as:\nP (y, {ck}|x) = P (c1|x) \u03a0Kk=2P (ck|ck\u22121,x)P (y|cK ,x) . (1)\nAs the above intuition, ck at early stages as the high-level content plans should contain informative or important words, to serve as skeletons for subsequent enrichment. We next concretely define the order of generation, namely, which words should each stage generates. Specifically, we propose a simple method\nthat constructs a vocabulary Vk for each stage k, based on the importance of words in the target domain. Each particular stage k only produces tokens belonging to its vocabulary Vk. By the progressive nature of the generation process, we have V1 \u2282 \u00b7 \u00b7 \u00b7 \u2282 VK \u2282 V . That is, V1 contains the smallest core set of words in the domain, and the vocabularies gradually expand at later stages until arriving the full vocabulary V . Note that vocabularies in later stages are supersets of those in earlier stages. This allows the later stages to remedy and polish potential mistakes made in earlier stages when necessary. We discuss the construction of the vocabularies in the below. Stage-wise vocabularies based on word importance. Given a text corpusD of the target domain with the full vocabulary V , we define the importance scores of words in V based on the TF-IDF metric."]}
{"pkey": "gpt2_15", "question": "What is the pretraining objective of the model? ", "answer": "While zero-shot performance establishes a baseline of the potential performance of GPT-2 on many tasks, it is not clear where the ceiling is with finetuning.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["The Pearson correlation coefficient of human scores is 0.52, showing moderate inter-rater agreement. Table 1 shows the results. All systems receive close fluency scores. Our approach obtained significantly higher coherence scores at both passage and sentence levels. In particular, over 86% sentences\nin our model generations are considered as coherent with the context, improving over other models by at least 10 absolute percent. 4.4 Ablation Study and Analysis. Sample efficiency. We study how the progressive generation could improve the sample efficiency of large LMs fine-tuned to target domains. The intuition is that by focusing on the subsets of informative words, the early stages can more efficiently capture the domain-specific characteristics and then steer the subsequent refinement stages. Figure 5 shows the results where we report the FBD score averaged over FBD-S/M/D. We can see our approach can make more efficient use of the training data in learning to generate high quality samples. For example, with only 1K training examples, our method achieves comparable results with large LMs trained on 30K examples. Generation with gold plans. To investigate the importance of dividing the generation process into stages and what the stages learn separately, we add another set of text into our comparison. It is a 2- stages model whose first stage is the ground truth (gold plan) while the second stage kept the same (a BART model), shown as GoldPlan in Table 3. Note that with gold plan, our model greatly decreases the gap with human text in terms of lexical (TID) and semantic (FBD-D) quality metrics. The results highlight the importance of plans in text\ngeneration. The intermediate plans act as an information bottleneck, and high-quality plans could lead to high-quality text generation. Effect of data noising. We study the ablation of data noising, to check whether the noising operation really helps reduce stage-wise exposure bias (Sec 3.2) as we expected.", "Progressive Generation of Long Text with Pretrained Language Models. Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent.1 Progressive Generation of Long Text with Pretrained Language Models. Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains.", "In the multi-stage generation, the intermediate sequences are not natural language. Yet we found that fine-tuning pretrained LMs (such as BART and GPT-2) to generate the intermediate sequences is indeed very efficient in terms of data and computation. We tried training other models such as small sequence-to-sequence models and n-gram models from scratch, which we found is much harder, requiring more data, or yielding inferior performance. This again highlights the importance of using pretrained LMs, as enabled by our simple method design. Stage-level exposure bias and data noising. In the above training process, the outputs of each LM are conditioning on the ground-truth input sequences extracted from the real corpus. In contrast, at generation time, the LM takes as inputs the imperfect sequences produced at the previous stage, which can result in new mistakes in the outputs since the LM has never be exposed to noisy inputs during training. Thus, the discrepancy between training and generation can lead to mistakes in generation accumulating through the stages. The phenomenon resembles the exposure bias issue (Ranzato et al., 2016) of sequential generation models at token level, where the model is trained to predict the next token given the previous ground-truth tokens, while at generation time tokens generated by the model itself are instead used to make the next prediction. To alleviate the issue and increase the robustness of each intermediate LM, we draw on the rich literature of addressing token-level exposure bias (Xie et al., 2017; Tan et al., 2019). Specifically, during training, we inject noise into the ground-truth inputs at each stage by randomly picking an n-gram (n \u2208 {1, 2, 3, 4}) and replacing it with another randomly sampled n-gram. The data noising encourages the LMs to learn to recover from the mistakes in inputs, leading to a more robust system during generation. 4 Experiments. 4.1 Setup.\nDomains."]}
{"pkey": "gpt2_16", "question": "What is the loss function that is used to train the model?", "answer": "Not mentioned in the paper.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["The Pearson correlation coefficient of human scores is 0.52, showing moderate inter-rater agreement. Table 1 shows the results. All systems receive close fluency scores. Our approach obtained significantly higher coherence scores at both passage and sentence levels. In particular, over 86% sentences\nin our model generations are considered as coherent with the context, improving over other models by at least 10 absolute percent. 4.4 Ablation Study and Analysis. Sample efficiency. We study how the progressive generation could improve the sample efficiency of large LMs fine-tuned to target domains. The intuition is that by focusing on the subsets of informative words, the early stages can more efficiently capture the domain-specific characteristics and then steer the subsequent refinement stages. Figure 5 shows the results where we report the FBD score averaged over FBD-S/M/D. We can see our approach can make more efficient use of the training data in learning to generate high quality samples. For example, with only 1K training examples, our method achieves comparable results with large LMs trained on 30K examples. Generation with gold plans. To investigate the importance of dividing the generation process into stages and what the stages learn separately, we add another set of text into our comparison. It is a 2- stages model whose first stage is the ground truth (gold plan) while the second stage kept the same (a BART model), shown as GoldPlan in Table 3. Note that with gold plan, our model greatly decreases the gap with human text in terms of lexical (TID) and semantic (FBD-D) quality metrics. The results highlight the importance of plans in text\ngeneration. The intermediate plans act as an information bottleneck, and high-quality plans could lead to high-quality text generation. Effect of data noising. We study the ablation of data noising, to check whether the noising operation really helps reduce stage-wise exposure bias (Sec 3.2) as we expected.", "One can add masks on the pretrained LM\u2019s to-\nken distributions to ensure the stage-k LM only produces tokens belonging to Vk. In practice, we found it is not necessary, as the pretrained LM can usually quickly learns the pattern through finetuning and generate appropriate tokens during inference. In our experiments we use BART for all stages, since BART is an encoder-decoder model which can conveniently take as inputs the resulting sequence from the preceding stage and generate new. (For the first stage in an unconditional generation task, we simply set x = \u2205.) We note that GPT2, and other relevant pretraiened LMs, can indeed also be used as a conditional generator (Radford et al., 2019; Liu et al., 2018) and thus be plugged into any of stages. 3.2 Training. Our approach permits straightforward training/finetuning of the (pretrained) LMs at different stages given the domain corpus D. In particular, we can easily construct independent training data for each stage, and train all LMs in parallel. Note that no additional resources such as pretrained summarization or semantic role labeling models are requested as in previous work, making our approach directly applicable to a potentially broader set of domains and languages. We plan to explore the use of our method in multi-lingual setting in the future. More concretely, for each stage k, we use the stage vocabularies Vk\u22121 and Vk to filter all relevant tokens in the documents as training data. That is, given a document, we extract the subsequence c\u2217k\u22121 of all tokens from the document that are belonging to Vk\u22121, and similarly extract sub-sequence c\u2217k belonging to Vk. The c\u2217k\u22121 and c\u2217k are then used as the input and the ground-truth output, respectively, for training the LM at stage k with maximum likelihood learning. Therefore, given the stage-wise vocabularies {Vk}, we can automatically extract training data from the domain corpus D for different stages, and train the LMs separately.", "In the multi-stage generation, the intermediate sequences are not natural language. Yet we found that fine-tuning pretrained LMs (such as BART and GPT-2) to generate the intermediate sequences is indeed very efficient in terms of data and computation. We tried training other models such as small sequence-to-sequence models and n-gram models from scratch, which we found is much harder, requiring more data, or yielding inferior performance. This again highlights the importance of using pretrained LMs, as enabled by our simple method design. Stage-level exposure bias and data noising. In the above training process, the outputs of each LM are conditioning on the ground-truth input sequences extracted from the real corpus. In contrast, at generation time, the LM takes as inputs the imperfect sequences produced at the previous stage, which can result in new mistakes in the outputs since the LM has never be exposed to noisy inputs during training. Thus, the discrepancy between training and generation can lead to mistakes in generation accumulating through the stages. The phenomenon resembles the exposure bias issue (Ranzato et al., 2016) of sequential generation models at token level, where the model is trained to predict the next token given the previous ground-truth tokens, while at generation time tokens generated by the model itself are instead used to make the next prediction. To alleviate the issue and increase the robustness of each intermediate LM, we draw on the rich literature of addressing token-level exposure bias (Xie et al., 2017; Tan et al., 2019). Specifically, during training, we inject noise into the ground-truth inputs at each stage by randomly picking an n-gram (n \u2208 {1, 2, 3, 4}) and replacing it with another randomly sampled n-gram. The data noising encourages the LMs to learn to recover from the mistakes in inputs, leading to a more robust system during generation. 4 Experiments. 4.1 Setup.\nDomains."]}
{"pkey": "gpt2_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The paper authors use a Transformer (Vaswani et al., 2017) based architecture for our LMs. The model largely follows the details of the OpenAI GPT model (Radford et al., 2018) with a few modifications. Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016), and an additional layer of normalization was added after the final self-attention block. A modified initialization which accounts for the accumulation on the residual path with model depth is used. The paper authors scale the weights of residual layers at initialization by a factor of 1/ \u221aN where N is the number of residual layers", "title": "Language Models are Unsupervised Multitask Learners", "context": ["We use it as an additional lexical-based quality measure. Fr\u00e9chet BERT Distance (FBD) is a semanticbased metric (Montahaei et al., 2019) that measures the Fr\u00e9chet Distance in the BERT feature space between the generated and real text. By using the BERT features from shallow (S), medium (M), and deep (D) layers, we can compute FBD-S/M/D, respectively. Backward BLEU (B-BLEU) is a diversity metric (Shi et al., 2018) measuring how well the generated text covers n-grams occurred in the test set. Harmonic BLEU (HA-BLEU) (Shi et al., 2018) is an aggregated quality and diversity metric that incorporates both the standard BLEU (i.e., precision) and the Backward BLEU (i.e., recall). 4.2.2 Results. Figures 3 and 4 show the results of the various systems on the news and story domains, respectively, measured with different metrics against test set. We give more complete results in the appendix. We can see that our progressive generation approach consistently outperforms the standard, single-stage LMs (GPT2-Small, GPT2-Large and BART) by a large margin on almost all metrics in both domains. Further, by increasing the number of progression stages, our method steadily achieves even stronger performance. This highlights the benefits of the flexible progressive generation strategy. The various models using pretrained LMs with previous planning-then-generation strategies show\nmixed results across the different metrics. For example, Summary achieves strong performance in terms of the semantic-based quality metric FBD-D (partially because the summaries are closer to the real text in the BERT feature space), but significantly falls behind other models in terms of diversity (B-BLEU4) and other quality metrics like MSJ and HA-BLEU. Similarly, the SRL-based methods give only mediocre results in terms of the semanticbased FBD-D. In contrast, our approach maintains a relatively consistent performance level.", "Those methods often adopt the similar idea of planning-then-generation as above (Shen et al., 2019; Zhao et al., 2020; Bosselut et al., 2018; See et al., 2019; Hua and Wang, 2020; Rashkin et al., 2020). Another line of work instead focuses on extending the transformer architecture (Vaswani et al., 2017) to model longer text sequences (e.g., Dai et al., 2019; Wang et al., 2020; Choromanski et al., 2021, etc). For example, Liu et al. (2018) used a hybrid retrieval-generation architecture for producing long summaries; Dai et al. (2019) showed long text samples qualitatively. Our work systematically examines the pretrained LMs in generating long domain-specific text, and proposes a new approach that empowers pretrained LMs for producing samples of significantly higherquality. 3 Progressive Generation of Text. One of the main challenges in generating long coherent passages is modeling long-range dependencies across the entire sequences (e.g., 1000 tokens). We propose a progressive generation approach that is conceptually simple yet effective. Intuitively, progressive generation divides the complex problem of generating the full passage into a series of much easier steps of generating coarser-grained intermediate sequences. Contrary to generating everything from left to right from scratch, our progressive generation allows the model to first plan globally and then shift attention to increasingly finer details, which results in more coherent text. Figure 2 illustrates the generation process. 3.1 Generation Process. Let y := [y1, y2, . . . , yT ] be the output text, where each yi is a token of language (a word or a subword). The output sequences are generated either conditionally on any other information x (e.g., generations of a story given a prompt), or unconditionally (in which case we assume x \u2261 \u2205 while keeping the same notation). Instead of generating the full passage y", "On the other hand, it is possible to adopt some of the content planning strategies (e.g., summaries or SRL sequences as the plans (Fan et al., 2019)), and repurpose pretrained LMs for generation in each stage. However, these strategies\nwith dedicated intermediate plans and a pre-fixed number (typically 2) of stages can have limited flexibility, leading to sub-optimal results as shown in our empirical study. Besides, creating training data for planning requires additional resources (e.g., pretrained summarization models or SRL models) which are not always available (e.g., in certain domains or for low-resource languages). In contrast, we propose a simple way for designing the intermediate stages based on word informativeness, which can flexibly increase the number of stages for improved results, and easily create training data for all stages without additional models. Non-monotonic generation and refinement. Another relevant line of research is non-monotonic generation (Welleck et al., 2019; Gu et al., 2019; Stern et al., 2019; Chan et al., 2019; Zhang et al., 2020), infilling (Zhu et al., 2019; Shen et al., 2020; Qin et al., 2020), or refinement (Lee et al., 2018; Novak et al., 2016; Mansimov et al., 2019; Kasai et al., 2020) that differs from the restricted left-toright generation in conventional LMs. Again, those approaches largely depend on specialized architectures and inference, making them difficult to be integrated with the powerful pretrained LMs. The prior studies have focused on generating short text. Our proposed coarse-to-fine progressive generation conceptually presents a non-monotonic process built upon the pretrained monotonic LMs, which permits fast adaptation to any target domain and generation of much longer text. Long text generation. Previous work has made attempts to generate text of up to two or three hundred tokens."]}
{"pkey": "gpt2_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "GPT-2, is a 1.5B parameter Transformer that achieves  state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText.", "title": "Language Models are Unsupervised Multitask Learners", "context": ["We observe that generation of some words (e.g., stop words) does not require many contexts, while other words are decisive and have long-term impact on the whole content of the passage. Motivated by this observation, our approach first produces a sequence of most informative words, then progressively refines the sequence by adding finergrained details in multiple stages, until completing a full passage. The generation at each stage is conditioning on the output of the preceding stage which provides anchors and steers the current generation (Figure 2). The intermediate words produced at each stage are defined based on a simple TF-IDF informativeness metric. The approach enjoys several core advantages: (1) Although the progressive approach implements a conceptually non-monotonic generation process, generation at each stage can still be performed in a left-to-right manner and thus is directly compatible with the powerful pretrained monotonic LMs. The LMs at different stages are easily fine-tuned to accommodate a target domain using only small, independently constructed data. Intuitively, each LM\nis addressing a sub-task of mapping a sequence to a finer-resolution one, which is much simpler than the overall task of mapping from conditions to full passages of text. In this work, we use BART (Lewis et al., 2020) for generation at each stage, though one can also plug in other off-the-shelf LMs.", "In the multi-stage generation, the intermediate sequences are not natural language. Yet we found that fine-tuning pretrained LMs (such as BART and GPT-2) to generate the intermediate sequences is indeed very efficient in terms of data and computation. We tried training other models such as small sequence-to-sequence models and n-gram models from scratch, which we found is much harder, requiring more data, or yielding inferior performance. This again highlights the importance of using pretrained LMs, as enabled by our simple method design. Stage-level exposure bias and data noising. In the above training process, the outputs of each LM are conditioning on the ground-truth input sequences extracted from the real corpus. In contrast, at generation time, the LM takes as inputs the imperfect sequences produced at the previous stage, which can result in new mistakes in the outputs since the LM has never be exposed to noisy inputs during training. Thus, the discrepancy between training and generation can lead to mistakes in generation accumulating through the stages. The phenomenon resembles the exposure bias issue (Ranzato et al., 2016) of sequential generation models at token level, where the model is trained to predict the next token given the previous ground-truth tokens, while at generation time tokens generated by the model itself are instead used to make the next prediction. To alleviate the issue and increase the robustness of each intermediate LM, we draw on the rich literature of addressing token-level exposure bias (Xie et al., 2017; Tan et al., 2019). Specifically, during training, we inject noise into the ground-truth inputs at each stage by randomly picking an n-gram (n \u2208 {1, 2, 3, 4}) and replacing it with another randomly sampled n-gram. The data noising encourages the LMs to learn to recover from the mistakes in inputs, leading to a more robust system during generation. 4 Experiments. 4.1 Setup.\nDomains.", "One can add masks on the pretrained LM\u2019s to-\nken distributions to ensure the stage-k LM only produces tokens belonging to Vk. In practice, we found it is not necessary, as the pretrained LM can usually quickly learns the pattern through finetuning and generate appropriate tokens during inference. In our experiments we use BART for all stages, since BART is an encoder-decoder model which can conveniently take as inputs the resulting sequence from the preceding stage and generate new. (For the first stage in an unconditional generation task, we simply set x = \u2205.) We note that GPT2, and other relevant pretraiened LMs, can indeed also be used as a conditional generator (Radford et al., 2019; Liu et al., 2018) and thus be plugged into any of stages. 3.2 Training. Our approach permits straightforward training/finetuning of the (pretrained) LMs at different stages given the domain corpus D. In particular, we can easily construct independent training data for each stage, and train all LMs in parallel. Note that no additional resources such as pretrained summarization or semantic role labeling models are requested as in previous work, making our approach directly applicable to a potentially broader set of domains and languages. We plan to explore the use of our method in multi-lingual setting in the future. More concretely, for each stage k, we use the stage vocabularies Vk\u22121 and Vk to filter all relevant tokens in the documents as training data. That is, given a document, we extract the subsequence c\u2217k\u22121 of all tokens from the document that are belonging to Vk\u22121, and similarly extract sub-sequence c\u2217k belonging to Vk. The c\u2217k\u22121 and c\u2217k are then used as the input and the ground-truth output, respectively, for training the LM at stage k with maximum likelihood learning. Therefore, given the stage-wise vocabularies {Vk}, we can automatically extract training data from the domain corpus D for different stages, and train the LMs separately."]}
{"pkey": "gpt2_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "Learning to perform a single task can be expressed in a probabilistic framework as estimating a conditional distri_x0002_bution p(output|input). Since a general system should be able to perform many different tasks, even for the same input, it should condition not only on the input but also on the task to be performed. That is, it should model p(output|input, task). This has been variously formalizedin multitask and meta-learning settings. Task conditioning is often implemented at an architectural level, such as the task specific encoders and decoders in (Kaiser et al., 2017) or at an algorithmic level such as the inner and outer loop optimization framework of MAML (Finn et al., 2017). But as exemplified in McCann et al. (2018), language provides a flexible way to specify tasks, inputs, and outputs all as a sequence of symbols. For example, a translation training example can be written as the sequence (translate to french, english text, french text). Like_x0002_wise, a reading comprehension training example can be written as (answer the question, document, question, answer).", "title": "Language Models are Unsupervised Multitask Learners", "context": ["The Pearson correlation coefficient of human scores is 0.52, showing moderate inter-rater agreement. Table 1 shows the results. All systems receive close fluency scores. Our approach obtained significantly higher coherence scores at both passage and sentence levels. In particular, over 86% sentences\nin our model generations are considered as coherent with the context, improving over other models by at least 10 absolute percent. 4.4 Ablation Study and Analysis. Sample efficiency. We study how the progressive generation could improve the sample efficiency of large LMs fine-tuned to target domains. The intuition is that by focusing on the subsets of informative words, the early stages can more efficiently capture the domain-specific characteristics and then steer the subsequent refinement stages. Figure 5 shows the results where we report the FBD score averaged over FBD-S/M/D. We can see our approach can make more efficient use of the training data in learning to generate high quality samples. For example, with only 1K training examples, our method achieves comparable results with large LMs trained on 30K examples. Generation with gold plans. To investigate the importance of dividing the generation process into stages and what the stages learn separately, we add another set of text into our comparison. It is a 2- stages model whose first stage is the ground truth (gold plan) while the second stage kept the same (a BART model), shown as GoldPlan in Table 3. Note that with gold plan, our model greatly decreases the gap with human text in terms of lexical (TID) and semantic (FBD-D) quality metrics. The results highlight the importance of plans in text\ngeneration. The intermediate plans act as an information bottleneck, and high-quality plans could lead to high-quality text generation. Effect of data noising. We study the ablation of data noising, to check whether the noising operation really helps reduce stage-wise exposure bias (Sec 3.2) as we expected.", "As seen from Figure 1, ProGen can generate more much coherent text compared with GPT-2 and nearly match human text in terms of the BERTNSP score; (2) In contrast to the typical 2-stage planning-then-generation in prior work, the simple progressive strategy offers added flexibility for an arbitrary number of intermediate stages, yielding improved results; (3) The training data for each stage is extracted from domain corpus using the simple TF-IDF metric, without need of additional resources (e.g., pretrained summarization models) as in prior work, making the method broadly applicable to various domains and languages. We conduct extensive empirical studies on the CNN News (Hermann et al., 2015) and WritingPrompts (Fan et al., 2018) corpora, evaluating various systems by a wide-range of automatic metrics as well as human judgement. Results show that ProGen achieves strongly improved performance by decomposing the generation into more progressive stages. Our method produces diverse text passages of higher quality and coherence than a broad set of models, including fine-tuned GPT-2, BART, and other various planning-then-generation strategies. 2 Related Work. Content planning in generation. The idea of separate content planning and surface realization has been studied in early text generation systems (Reiter and Dale, 1997). Recent neural approaches have also adopted similar planning-thengeneration strategies for data-to-text (Moryossef et al., 2019; Puduppully et al., 2019), storytelling (Fan et al., 2019; Yao et al., 2019; Xu et al., 2020), machine translation (Ford et al., 2018), and others (Hua and Wang, 2019; Yao et al., 2017). These models often involve customized architectures incompatible with the existing large LMs. Scaling those models for long text generation thus can require expensive training, which restricts systematic studies.", "Progressive Generation of Long Text with Pretrained Language Models. Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains. To overcome the limitations, we propose a simple but effective method of generating text in a progressive manner, inspired by generating images from low to high resolution. Our method first produces domain-specific content keywords and then progressively refines them into complete passages in multiple stages. The simple design allows our approach to take advantage of pretrained LMs at each stage and effectively adapt to any target domain given only a small set of examples. We conduct a comprehensive empirical study with a broad set of evaluation metrics, and show that our approach significantly improves upon the fine-tuned large LMs and various planning-then-generation methods in terms of quality and sample efficiency. Human evaluation also validates that our model generations are more coherent.1 Progressive Generation of Long Text with Pretrained Language Models. Large-scale language models (LMs) pretrained on massive corpora of text, such as GPT-2, are powerful open-domain text generators. However, as our systematic examination reveals, it is still challenging for such models to generate coherent long passages of text (e.g., 1000 tokens), especially when the models are fine-tuned to the target domain on a small corpus. Previous planning-then-generation methods also fall short of producing such long text in various domains."]}
{"pkey": "gpt2_20", "question": "List the future work mentioned in the paper.", "answer": "When a large language model is trained on a sufficiently large and diverse dataset it is able to perform well across many domains and datasets. GPT-2 zero-shots to state of the art performance on 7 out of 8 tested language modeling datasets. The diversity of tasks the model is able to perform in a zero-shot setting suggests that high-capacity models trained to maximize the likelihood of a sufficiently varied text corpus begin to learn how to perform a surprising amount of tasks without the need for explicit supervision", "title": "Language Models are Unsupervised Multitask Learners", "context": ["Specifically, Summary first generates a short summary text as the content plan and conditioning on the summary produces the full passage of text (Fan et al., 2019). For training, summaries are obtained using the state-of-the-art pretrained CNN news summarization model based on BART; Keyword first generates a series of keywords, based on which the full text is generated in the next stage. Following (Yao et al., 2019), the keywords are extracted with the RAKE algorithm (Rose et al., 2010) for training; SRL follows the recent work (Fan et al., 2019) by first generating a sequence of predicates and arguments and then producing the full text conditionally. The same semantic role labeling tool as in the prior work is used here to create training data. SRL+NER and SRL+Coref further augment the SRL method by an additional stage of generating entity anonymized text conditioning on the predicates sequence prior to the final stage (Fan et al., 2019). SRL+NER uses an NER model to mask all entities, while SRL+Coref applies coreference resolution to mask all clusters of mentions. We use the same NER and coreference tools as in (Fan et al., 2019). Finally, as a reference, we also present the results of Human-written text (i.e., the text in the dev set). 4.2 Automatic Evaluation. 4.2.1 Evaluation Metrics. To evaluate the generation quality for the domainspecific open-ended generation as studied here, we primarily measure the \u201ccloseness\u201d between two sets of text, one generated by the model and the other the real text from the target domain. We evaluate with a broad array of automatic metrics, including lexical-based quality metrics and semanticbased quality metrics. We also evaluate the generation diversity. MS-Jaccard (MSJ) is a lexical-based metric (Montahaei et al., 2019), where MSJ-n measures the similarity of n-grams frequencies between two sets of text with Jaccard index. TF-IDF Distance (TID) is defined as the distance between the average TF-IDF features of two text sets.", "One can add masks on the pretrained LM\u2019s to-\nken distributions to ensure the stage-k LM only produces tokens belonging to Vk. In practice, we found it is not necessary, as the pretrained LM can usually quickly learns the pattern through finetuning and generate appropriate tokens during inference. In our experiments we use BART for all stages, since BART is an encoder-decoder model which can conveniently take as inputs the resulting sequence from the preceding stage and generate new. (For the first stage in an unconditional generation task, we simply set x = \u2205.) We note that GPT2, and other relevant pretraiened LMs, can indeed also be used as a conditional generator (Radford et al., 2019; Liu et al., 2018) and thus be plugged into any of stages. 3.2 Training. Our approach permits straightforward training/finetuning of the (pretrained) LMs at different stages given the domain corpus D. In particular, we can easily construct independent training data for each stage, and train all LMs in parallel. Note that no additional resources such as pretrained summarization or semantic role labeling models are requested as in previous work, making our approach directly applicable to a potentially broader set of domains and languages. We plan to explore the use of our method in multi-lingual setting in the future. More concretely, for each stage k, we use the stage vocabularies Vk\u22121 and Vk to filter all relevant tokens in the documents as training data. That is, given a document, we extract the subsequence c\u2217k\u22121 of all tokens from the document that are belonging to Vk\u22121, and similarly extract sub-sequence c\u2217k belonging to Vk. The c\u2217k\u22121 and c\u2217k are then used as the input and the ground-truth output, respectively, for training the LM at stage k with maximum likelihood learning. Therefore, given the stage-wise vocabularies {Vk}, we can automatically extract training data from the domain corpus D for different stages, and train the LMs separately.", "Table 2 shows the comparison between models with and without noise in training. The added noise generally brings performance improvement in terms of various metrics. Example generations. Table 4 shows an example of text generated via three stages. We can see our model first generates the key subject beckham and the team name liverpool in the very first stage, then adds more fine-grained details like acquisition, transfer in the second stage and finally expands the keywords into a full document describing Beckham\u2019s joining a new team. 5 Conclusion. We have proposed a new approach for domainspecific generation of long text passages in a progressive manner. Our method is simple and efficient by fine-tuning large-scale off-the-shelf language models. We conduct extensive experiments using a variety of metrics and human studies. We demonstrate that our method outperforms a wide range of large pretrained LMs with single-stage generation or prior planning-then-generation strategies, in terms of quality and coherence of the produced samples. The multi-stage generation also opens up new opportunities to enhance controllability of text generation, which we would love to explore in the future. Appendix: Complete Results. We include complete result numbers of experiments here."]}
{"pkey": "electra_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Masked language modeling (MLM) pre-training methods such as BERT corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. While they produce good results when transferred to downstream NLP tasks, they generally require large amounts of compute to be effective. As an alternative, the paper authors propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the originalidentities of the corrupted tokens, the paper authors train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["We cannot directly find argmax\u03b8G using gradient ascent because it is impossible to backpropagate through discrete sampling of x\u0302. Instead, we use policy gradient reinforcement learning (Williams, 1992). In particular, we use the REINFORCE gradient\n\u2207\u03b8GLDisc \u2248 E x,m \u2211 t\u2208m E x\u0302t\u223cpG \u2207\u03b8g log pG(x\u0302t|xmasked)[R(x\u0302t,x)\u2212 b(xmasked, t)] Where b is a learned baseline implemented as b(xmasked, t) = \u2212 log sigmoid(wThG(xmasked)t) where hG(xmasked) are the outputs of the generator\u2019s Transformer encoder. The baseline is trained with cross-entropy loss to match the reward for the corresponding position. We approximate the expectations with a single sample and learn \u03b8G with gradient ascent. Despite receiving no explicit feedback about which generated tokens are correct, we found the adversarial training resulted in a fairly accurate generator (for a 256-hidden-size generator, the adversarially trained one achieves 58% accuracy at masked language modeling while the same sized MLE generator gets 65%). However, using this generator did not improve over the MLE-trained one on downstream tasks (see the right of Figure 3 in the main paper). G EVALUATING ELECTRA AS A MASKED LANGUAGE MODEL. This sections details some initial experiments in evaluating ELECTRA as a masked language model. Using slightly different notation from the main paper, given a context c consisting of a text sequence with one token x masked-out, the discriminator loss can be written as\nLDisc = \u2212 \u2211\nx\u2208vocab\n( (1\u2212 pmask)pdata(x|c) logD(x, c) + //unmasked token\npmaskpdata(x|c)pG(x|c) logD(x, c) + //generator samples correct token pmask(1\u2212 pdata(x|c))pG(x|c) log(1\u2212D(x, c)) ) //generator samples incorrect token\nFinding the critical points of this loss with respect to D shows that for a fixed generator the optimal discriminator is\nD(x, c) = pdata(x|c)(a+ pG(x|c))/(apdata(x|c) + pG(x|c))\nwhich means\npdata(x|c) = D(x, c)pG(x|c)/(a(1\u2212D(x, c)) + pG(x|c))\nwhere a = (1 \u2212 pmask)/pmask is the number of unmasked tokens for every masked token.", "During two-stage training, downstream task performance notably improves after the switch from the generative to the discriminative objective, but does not end up outscoring joint training. Although still outperforming BERT, we found adversarial training to underperform maximum-likelihood training. Further analysis suggests the gap is caused by two\nproblems with adversarial training. First, the adversarial generator is simply worse at masked language modeling; it achieves 58% accuracy at masked language modeling compared to 65% accuracy for an MLE-trained one. We believe the worse accuracy is mainly due to the poor sample efficiency of reinforcement learning when working in the large action space of generating text. Secondly, the adversarially trained generator produces a low-entropy output distribution where most of the probability mass is on a single token, which means there is not much diversity in the generator samples. Both of these problems have been observed in GANs for text in prior work (Caccia et al., 2018). 3.3 SMALL MODELS. As a goal of this work is to improve the efficiency of pre-training, we develop a small model that can be quickly trained on a single GPU. Starting with the BERT-Base hyperparameters, we shortened the sequence length (from 512 to 128), reduced the batch size (from 256 to 128), reduced the model\u2019s hidden dimension size (from 768 to 256), and used smaller token embeddings (from 768 to 128). To provide a fair comparison, we also train a BERT-Small model using the same hyperparameters. We train BERT-Small for 1.5M steps, so it uses the same training FLOPs as ELECTRA-Small, which was trained for 1M steps.5 In addition to BERT, we compare against two less resource-intensive pre-training methods based on language modeling: ELMo (Peters et al., 2018) and GPT (Radford et al., 2018).6 We also show results for a base-sized ELECTRA model comparable to BERT-Base. Results are shown in Table 1.", "We have suggested that posing the training objective over a small subset of tokens makes masked language modeling inefficient. However, it isn\u2019t entirely obvious that this is the case. After all, the model still receives a large number of input tokens even though it predicts only a small number of masked tokens. To better understand where the gains from ELECTRA are coming from, we compare a series of other pre-training objectives that are designed to be a set of \u201cstepping stones\u201d between BERT and ELECTRA. \u2022 ELECTRA 15%: This model is identical to ELECTRA except the discriminator loss only comes from the 15% of the tokens that were masked out of the input. In other words, the sum in the discriminator loss LDisc is over i \u2208m instead of from 1 to n.7\n\u2022 Replace MLM: This objective is the same as masked language modeling except instead of replacing masked-out tokens with [MASK], they are replaced with tokens from a generator model. This objective tests to what extent ELECTRA\u2019s gains come from solving the discrepancy of exposing the model to [MASK] tokens during pre-training but not fine-tuning. \u2022 All-Tokens MLM: Like in Replace MLM, masked tokens are replaced with generator samples. Furthermore, the model predicts the identity of all tokens in the input, not just ones that were masked out. We found it improved results to train this model with an explicit copy mechanism that outputs a copy probability D for each token using a sigmoid layer. The model\u2019s output distribution puts D weight on the input token plus 1 \u2212 D times the output of the MLM softmax. This model is essentially a combination of BERT and ELECTRA. Note that without generator replacements, the model would trivially learn to make predictions from the vocabulary for [MASK] tokens and copy the input for other ones. Results are shown in Table 5. First, we find that ELECTRA is greatly benefiting from having a loss defined over all input tokens rather than just a subset: ELECTRA 15% performs much worse than ELECTRA."]}
{"pkey": "electra_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Current state-of-the-art representation learning methods for language can be viewed as learning denoising autoencoders (Vincent et al., 2008). They select a small subset of the unlabeled input sequence (typically 15%), mask the identities of those tokens (e.g., BERT; Devlin et al. (2019)) or attention to those tokens (e.g., XLNet; Yang et al. (2019)), and then train the network to recover the original input. While more effective than conventional language-model pre-training due to learning bidirectional representations, these masked language modeling (MLM) approaches incur a substantial compute cost because the network only learns from 15% of the tokens per example.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["We have suggested that posing the training objective over a small subset of tokens makes masked language modeling inefficient. However, it isn\u2019t entirely obvious that this is the case. After all, the model still receives a large number of input tokens even though it predicts only a small number of masked tokens. To better understand where the gains from ELECTRA are coming from, we compare a series of other pre-training objectives that are designed to be a set of \u201cstepping stones\u201d between BERT and ELECTRA. \u2022 ELECTRA 15%: This model is identical to ELECTRA except the discriminator loss only comes from the 15% of the tokens that were masked out of the input. In other words, the sum in the discriminator loss LDisc is over i \u2208m instead of from 1 to n.7\n\u2022 Replace MLM: This objective is the same as masked language modeling except instead of replacing masked-out tokens with [MASK], they are replaced with tokens from a generator model. This objective tests to what extent ELECTRA\u2019s gains come from solving the discrepancy of exposing the model to [MASK] tokens during pre-training but not fine-tuning. \u2022 All-Tokens MLM: Like in Replace MLM, masked tokens are replaced with generator samples. Furthermore, the model predicts the identity of all tokens in the input, not just ones that were masked out. We found it improved results to train this model with an explicit copy mechanism that outputs a copy probability D for each token using a sigmoid layer. The model\u2019s output distribution puts D weight on the input token plus 1 \u2212 D times the output of the MLM softmax. This model is essentially a combination of BERT and ELECTRA. Note that without generator replacements, the model would trivially learn to make predictions from the vocabulary for [MASK] tokens and copy the input for other ones. Results are shown in Table 5. First, we find that ELECTRA is greatly benefiting from having a loss defined over all input tokens rather than just a subset: ELECTRA 15% performs much worse than ELECTRA.", "Secondly, we find that BERT performance is being slightly harmed from the pre-train fine-tune mismatch from [MASK] tokens, as Replace MLM slightly outperforms BERT. We note that BERT (including our implementation) already includes a trick to help with the pre-train/finetune discrepancy: masked tokens are replaced with a random token 10% of the time and are kept the\n7We also trained a discriminator that learns from a random 15% of the input tokens distinct from the subset that was originally masked out; this model performed slightly worse.\nsame 10% of the time. However, our results suggest these simple heuristics are insufficient to fully solve the issue. Lastly, we find that All-Tokens MLM, the generative model that makes predictions over all tokens instead of a subset, closes most of the gap between BERT and ELECTRA. In total, these results suggest a large amount of ELECTRA\u2019s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. The improvement of ELECTRA over All-Tokens MLM suggests that the ELECTRA\u2019s gains come from more than just faster training. We study this further by comparing BERT to ELECTRA for various model sizes (see Figure 4, left). We find that the gains from ELECTRA grow larger as the models get smaller. The small models are trained fully to convergence (see Figure 4, right), showing that ELECTRA achieves higher downstream accuracy than BERT when fully trained. We speculate that ELECTRA is more parameter-efficient than BERT because it does not have to model the full distribution of possible tokens at each position, but we believe more analysis is needed to completely explain ELECTRA\u2019s parameter efficiency. 4 RELATED WORK.", "We can use this expression to evaluate ELECTRA as a masked language model by selecting argmaxx\u2208vocabD(x, c)pG(x|c)/(a(1 \u2212 D(x, c)) + pG(x|c)) as the model\u2019s prediction for a given context. In practice, selecting over the whole vocabulary is very expensive, so we instead take the argmax over the top 100 predictions from the generator.10 Using this method, we compared ELECTRA-Base and BERT-Base on the Wikipedia+BooksCorpus dataset. We found that BERT slightly outperformed ELECTRA at masked language modeling (77.9% vs 75.5% accuracy). It is possible that the assumption of an optimal discriminator, which is certainly far from correct, is harming ELECTRA\u2019s accuracy under this evaluation scheme. However, perhaps it is not too surprising that a model like BERT that is trained specifically for generation performs better at generation while a model with a discriminative objective like ELECTRA is better at being fine-tuned on discriminative tasks. We think comparisons of BERT\u2019s and ELECTRA\u2019s MLM predictions might be an interesting way to uncover more about the differences between ELECTRA and BERT encoders in future work. H NEGATIVE RESULTS. We briefly describe a few ideas that did not look promising in our initial experiments:\n\u2022 We initially attempted to make BERT more efficient by strategically masking-out tokens (e.g., masking our rarer tokens more frequently, or training a model to guess which tokens BERT would struggle to predict if they were masked out). This resulted in fairly minor speedups over regular BERT. \u2022 Given that ELECTRA seemed to benefit (up to a certain point) from having a weaker generator (see Section 3.2), we explored raising the temperature of the generator\u2019s output softmax or disallowing the generator from sampling the correct token. Neither of these improved results. \u2022 We tried adding a sentence-level contrastive objective. For this task, we kept 20% of input sentences unchanged rather than noising them with the generator."]}
{"pkey": "electra_3", "question": "What are the main contributions of the paper?", "answer": "we propose replaced token detection, a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements.  Training ELECTRA-Large further results in an even stronger model that outperforms ALBERT (Lan et al., 2019) on GLUE and sets a new state-of-the-art for SQuAD 2.0", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["We cannot directly find argmax\u03b8G using gradient ascent because it is impossible to backpropagate through discrete sampling of x\u0302. Instead, we use policy gradient reinforcement learning (Williams, 1992). In particular, we use the REINFORCE gradient\n\u2207\u03b8GLDisc \u2248 E x,m \u2211 t\u2208m E x\u0302t\u223cpG \u2207\u03b8g log pG(x\u0302t|xmasked)[R(x\u0302t,x)\u2212 b(xmasked, t)] Where b is a learned baseline implemented as b(xmasked, t) = \u2212 log sigmoid(wThG(xmasked)t) where hG(xmasked) are the outputs of the generator\u2019s Transformer encoder. The baseline is trained with cross-entropy loss to match the reward for the corresponding position. We approximate the expectations with a single sample and learn \u03b8G with gradient ascent. Despite receiving no explicit feedback about which generated tokens are correct, we found the adversarial training resulted in a fairly accurate generator (for a 256-hidden-size generator, the adversarially trained one achieves 58% accuracy at masked language modeling while the same sized MLE generator gets 65%). However, using this generator did not improve over the MLE-trained one on downstream tasks (see the right of Figure 3 in the main paper). G EVALUATING ELECTRA AS A MASKED LANGUAGE MODEL. This sections details some initial experiments in evaluating ELECTRA as a masked language model. Using slightly different notation from the main paper, given a context c consisting of a text sequence with one token x masked-out, the discriminator loss can be written as\nLDisc = \u2212 \u2211\nx\u2208vocab\n( (1\u2212 pmask)pdata(x|c) logD(x, c) + //unmasked token\npmaskpdata(x|c)pG(x|c) logD(x, c) + //generator samples correct token pmask(1\u2212 pdata(x|c))pG(x|c) log(1\u2212D(x, c)) ) //generator samples incorrect token\nFinding the critical points of this loss with respect to D shows that for a fixed generator the optimal discriminator is\nD(x, c) = pdata(x|c)(a+ pG(x|c))/(apdata(x|c) + pG(x|c))\nwhich means\npdata(x|c) = D(x, c)pG(x|c)/(a(1\u2212D(x, c)) + pG(x|c))\nwhere a = (1 \u2212 pmask)/pmask is the number of unmasked tokens for every masked token.", "We have suggested that posing the training objective over a small subset of tokens makes masked language modeling inefficient. However, it isn\u2019t entirely obvious that this is the case. After all, the model still receives a large number of input tokens even though it predicts only a small number of masked tokens. To better understand where the gains from ELECTRA are coming from, we compare a series of other pre-training objectives that are designed to be a set of \u201cstepping stones\u201d between BERT and ELECTRA. \u2022 ELECTRA 15%: This model is identical to ELECTRA except the discriminator loss only comes from the 15% of the tokens that were masked out of the input. In other words, the sum in the discriminator loss LDisc is over i \u2208m instead of from 1 to n.7\n\u2022 Replace MLM: This objective is the same as masked language modeling except instead of replacing masked-out tokens with [MASK], they are replaced with tokens from a generator model. This objective tests to what extent ELECTRA\u2019s gains come from solving the discrepancy of exposing the model to [MASK] tokens during pre-training but not fine-tuning. \u2022 All-Tokens MLM: Like in Replace MLM, masked tokens are replaced with generator samples. Furthermore, the model predicts the identity of all tokens in the input, not just ones that were masked out. We found it improved results to train this model with an explicit copy mechanism that outputs a copy probability D for each token using a sigmoid layer. The model\u2019s output distribution puts D weight on the input token plus 1 \u2212 D times the output of the MLM softmax. This model is essentially a combination of BERT and ELECTRA. Note that without generator replacements, the model would trivially learn to make predictions from the vocabulary for [MASK] tokens and copy the input for other ones. Results are shown in Table 5. First, we find that ELECTRA is greatly benefiting from having a loss defined over all input tokens rather than just a subset: ELECTRA 15% performs much worse than ELECTRA.", "However, we found it to be more efficient to have a small generator, in which case we only share the embeddings (both the token and positional embeddings) of the generator and discriminator. In this case we use embeddings the size of the discriminator\u2019s hidden states.4 The \u201cinput\u201d and \u201coutput\u201d token embeddings of the generator are always tied as in BERT. We compare the weight tying strategies when the generator is the same size as the discriminator. We train these models for 500k steps. GLUE scores are 83.6 for no weight tying, 84.3 for tying token embeddings, and 84.4 for tying all weights. We hypothesize that ELECTRA benefits from\n4We add linear layers to the generator to project the embeddings into generator-hidden-sized representations. tied token embeddings because masked language modeling is particularly effective at learning these representations: while the discriminator only updates tokens that are present in the input or are sampled by the generator, the generator\u2019s softmax over the vocabulary densely updates all token embeddings. On the other hand, tying all encoder weights caused little improvement while incurring the significant disadvantage of requiring the generator and discriminator to be the same size. Based on these findings, we use tied embeddings for further experiments in this paper. Smaller Generators If the generator and discriminator are the same size, training ELECTRA would take around twice as much compute per step as training only with masked language modeling. We suggest using a smaller generator to reduce this factor. Specifically, we make models smaller by decreasing the layer sizes while keeping the other hyperparameters constant. We also explore using an extremely simple \u201cunigram\u201d generator that samples fake tokens according their frequency in the train corpus. GLUE scores for differently-sized generators and discriminators are shown in the left of Figure 3."]}
{"pkey": "electra_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "The paper authors report results for ELECTRA-Base and ELECTRA-Small on the GLUE test set. Furthermore, the paper authors push the limits of base-sized and small-sized models by training them on the XLNet data instead of Wikibooks and for much longer these models are called ELECTRA-Base++ and ELECTRA-Small++ in the table. For ELECTRA-Small++ the paper authors also increased the sequence length to 512; otherwise, the hyperparameters are the same as the ones listed in Table 6. Lastly, the table contains results for ELECTRA-1.75M without the tricks described in Appendix B. Consistent with dev-set results in the paper, ELECTRA-Base outperforms BERT-Large while ELECTRA-Small outperforms GPT in terms of average score.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["We can use this expression to evaluate ELECTRA as a masked language model by selecting argmaxx\u2208vocabD(x, c)pG(x|c)/(a(1 \u2212 D(x, c)) + pG(x|c)) as the model\u2019s prediction for a given context. In practice, selecting over the whole vocabulary is very expensive, so we instead take the argmax over the top 100 predictions from the generator.10 Using this method, we compared ELECTRA-Base and BERT-Base on the Wikipedia+BooksCorpus dataset. We found that BERT slightly outperformed ELECTRA at masked language modeling (77.9% vs 75.5% accuracy). It is possible that the assumption of an optimal discriminator, which is certainly far from correct, is harming ELECTRA\u2019s accuracy under this evaluation scheme. However, perhaps it is not too surprising that a model like BERT that is trained specifically for generation performs better at generation while a model with a discriminative objective like ELECTRA is better at being fine-tuned on discriminative tasks. We think comparisons of BERT\u2019s and ELECTRA\u2019s MLM predictions might be an interesting way to uncover more about the differences between ELECTRA and BERT encoders in future work. H NEGATIVE RESULTS. We briefly describe a few ideas that did not look promising in our initial experiments:\n\u2022 We initially attempted to make BERT more efficient by strategically masking-out tokens (e.g., masking our rarer tokens more frequently, or training a model to guess which tokens BERT would struggle to predict if they were masked out). This resulted in fairly minor speedups over regular BERT. \u2022 Given that ELECTRA seemed to benefit (up to a certain point) from having a weaker generator (see Section 3.2), we explored raising the temperature of the generator\u2019s output softmax or disallowing the generator from sampling the correct token. Neither of these improved results. \u2022 We tried adding a sentence-level contrastive objective. For this task, we kept 20% of input sentences unchanged rather than noising them with the generator.", "We used TensorFlow\u2019s FLOP-counting capabilities9 and checked the results with by-hand computation. We made the following assumptions:\n\u2022 An \u201coperation\u201d is a mathematical operation, not a machine instruction. For example, an exp is one op like an add, even though in practice the exp might be slower. We believe this assumption does not substantially change compute estimates because matrix-multiplies dominate the compute for most models. Similarly, we count matrix-multiplies as 2 \u2217m \u2217 n FLOPs instead of m \u2217 n as one might if considering fused multiply-add operations. \u2022 The backwards pass takes the same number of FLOPs as the forward pass. This assumption\nis not exactly right (e.g., for softmax cross entropy loss the backward pass is faster), but importantly, the forward/backward pass FLOPs really are the same for matrix-multiplies, which is most of the compute anyway. \u2022 We assume \u201cdense\u201d embedding lookups (i.e., multiplication by a one-hot vector). In prac-\ntice, sparse embedding lookups are much slower than constant time; on some hardware accelerators dense operations are actually faster than sparse lookups. F ADVERSARIAL TRAINING. Here we detail attempts to adversarially train the generator instead of using maximum likelihood. In particular we train the generator G to maximize the discriminator loss LDisc. As our discriminator isn\u2019t precisely the same as the discriminator of a GAN (see the discussion in Section 2), this method is really an instance of Adversarial Contrastive Estimation (Bose et al., 2018) rather than Generative Adversarial Training. It is not possible to adversarially train the generator by back-propagating through the discriminator (e.g., as in a GAN trained on images) due to the discrete sampling from the generator, so we use reinforcement learning instead. Our generator is different from most text generation models in that it is non-autogregressive: predictions are made independently.", "Self-Supervised Pre-training for NLP Self-supervised learning has been used to learn word representations (Collobert et al., 2011; Pennington et al., 2014) and more recently contextual representations of words though objectives such as language modeling (Dai & Le, 2015; Peters et al., 2018; Howard & Ruder, 2018). BERT (Devlin et al., 2019) pre-trains a large Transformer (Vaswani et al., 2017) at the masked-language modeling task. There have been numerous extensions to BERT. For example, MASS (Song et al., 2019) and UniLM (Dong et al., 2019) extend BERT to generation tasks by adding auto-regressive generative training objectives. ERNIE (Sun et al., 2019a) and SpanBERT (Joshi et al., 2019) mask out contiguous sequences of token for improved span representations. This idea may be complementary to ELECTRA; we think it would be interesting to make ELECTRA\u2019s generator auto-regressive and add a \u201creplaced span detection\u201d task. Instead of masking out input tokens, XLNet (Yang et al., 2019) masks attention weights such that the input sequence is autoregressively generated in a random order. However, this method suffers from the same inefficiencies as BERT because XLNet only generates 15% of the input tokens in this way. Like ELECTRA, XLNet may alleviate BERT\u2019s pretrain-finetune discrepancy by not requiring [MASK] tokens, although this isn\u2019t entirely clear because XLNet uses two \u201cstreams\u201d of attention during pre-training but only one for fine-tuning. Recently, models such as TinyBERT (Jiao et al., 2019) and MobileBERT (Sun et al., 2019b) show that BERT can effectively be distilled down to a smaller model. In contrast, we focus more on pre-training speed rather than inference speed, so we train ELECTRA-Small from scratch. Generative Adversarial Networks GANs (Goodfellow et al., 2014) are effective at generating high-quality synthetic data. Radford et al. (2016) propose using the discriminator of a GAN in downstream tasks, which is similar to our method."]}
{"pkey": "electra_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "The paper authors evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) and Stanford Question Answering (SQuAD) dataset (Rajpurkar et al., 2016). GLUE contains a variety of tasks covering textual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), sentiment (SST), and linguistic acceptability (CoLA). See Appendix C for more details on the GLUE tasks. Our evaluation metrics are Spearman  orrelation for STS, Matthews correlation for CoLA, and accuracy for the other GLUE tasks; the paper authors generally report the average score over all tasks. For SQuAD, the paper authors evaluate on versions 1.1, in which models select the span of text answering a question, and 2.0, in which some questions are unanswerable by the passage. The paper authors use the standard evaluation metrics of Exact-Match (EM) and F1 scores. For most experiments the paper authors pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model the paper authors pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["For SQuAD, we evaluate on versions 1.1, in which models select the span of text answering a question, and 2.0, in which some questions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match (EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future. Our model architecture and most hyperparameters are the same as BERT\u2019s. For fine-tuning on GLUE, we add simple linear classifiers on top of ELECTRA. For SQuAD, we add the questionanswering module from XLNet on top of ELECTRA, which is slightly more sophisticated than BERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a \u201canswerability\u201d classifier added for SQuAD 2.0. Some of our evaluation datasets are small, which means accuracies of fine-tuned models can vary substantially depending on the random seed. We therefore report the median of 10 fine-tuning runs from the same pre-trained checkpoint for each result. Unless stated otherwise, results are on the dev set. See the appendix for further training details and hyperparameter values.\n3.2 MODEL EXTENSIONS. We improve our method by proposing and evaluating several extensions to the model. Unless stated otherwise, these experiments use the same model size and training data as BERT-Base. Weight Sharing We propose improving the efficiency of the pre-training by sharing weights between the generator and discriminator. If the generator and discriminator are the same size, all of the transformer weights can be tied.", "We can use this expression to evaluate ELECTRA as a masked language model by selecting argmaxx\u2208vocabD(x, c)pG(x|c)/(a(1 \u2212 D(x, c)) + pG(x|c)) as the model\u2019s prediction for a given context. In practice, selecting over the whole vocabulary is very expensive, so we instead take the argmax over the top 100 predictions from the generator.10 Using this method, we compared ELECTRA-Base and BERT-Base on the Wikipedia+BooksCorpus dataset. We found that BERT slightly outperformed ELECTRA at masked language modeling (77.9% vs 75.5% accuracy). It is possible that the assumption of an optimal discriminator, which is certainly far from correct, is harming ELECTRA\u2019s accuracy under this evaluation scheme. However, perhaps it is not too surprising that a model like BERT that is trained specifically for generation performs better at generation while a model with a discriminative objective like ELECTRA is better at being fine-tuned on discriminative tasks. We think comparisons of BERT\u2019s and ELECTRA\u2019s MLM predictions might be an interesting way to uncover more about the differences between ELECTRA and BERT encoders in future work. H NEGATIVE RESULTS. We briefly describe a few ideas that did not look promising in our initial experiments:\n\u2022 We initially attempted to make BERT more efficient by strategically masking-out tokens (e.g., masking our rarer tokens more frequently, or training a model to guess which tokens BERT would struggle to predict if they were masked out). This resulted in fairly minor speedups over regular BERT. \u2022 Given that ELECTRA seemed to benefit (up to a certain point) from having a weaker generator (see Section 3.2), we explored raising the temperature of the generator\u2019s output softmax or disallowing the generator from sampling the correct token. Neither of these improved results. \u2022 We tried adding a sentence-level contrastive objective. For this task, we kept 20% of input sentences unchanged rather than noising them with the generator.", "The task is to determine whether a given sentence is grammatical or not. The dataset contains 8.5k train examples from books and journal articles on linguistic theory. \u2022 SST: Stanford Sentiment Treebank (Socher et al., 2013). The tasks is to determine if the\nsentence is positive or negative in sentiment. The dataset contains 67k train examples from movie reviews. \u2022 MRPC: Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005). The task is to\npredict whether two sentences are semantically equivalent or not. The dataset contains 3.7k train examples from online news sources. \u2022 STS: Semantic Textual Similarity (Cer et al., 2017). The tasks is to predict how seman-\ntically similar two sentences are on a 1-5 scale. The dataset contains 5.8k train examples drawn from new headlines, video and image captions, and natural language inference data. \u2022 QQP: Quora Question Pairs (Iyer et al., 2017). The task is to determine whether a pair of\nquestions are semantically equivalent. The dataset contains 364k train examples from the community question-answering website Quora. \u2022 MNLI: Multi-genre Natural Language Inference (Williams et al., 2018). Given a premise\nsentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis, contradicts the hypothesis, or neither. The dataset contains 393k train examples drawn from ten different sources. \u2022 QNLI: Question Natural Language Inference; constructed from SQuAD (Rajpurkar et al.,\n2016). The task is to predict whether a context sentence contains the answer to a question sentence. The dataset contains 108k train examples from Wikipedia. \u2022 RTE: Recognizing Textual Entailment (Giampiccolo et al., 2007). Given a premise sen-\ntence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis or not. The dataset contains 2.5k train examples from a series of annual textual entailment challenges. D FURTHER RESULTS ON GLUE."]}
{"pkey": "electra_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "\"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\" does not explicitly mention any bias or prejudice exhibited by the model. However, it's worth noting that bias in language models is a broader concern in the field of NLP, and mitigating biases in pre-trained models is an active area of research.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["In other words, rather than taking a sequence of actions where each action generates a token, the generator takes a single giant action of generating all tokens simultaneously, where the probability for the action factorizes as the product of generator probabilities for each token. To deal with this enormous action space, we make the following simplifying assumption: that the discriminator\u2019s prediction D(xcorrupt, t) depends only on the token xt and the non-replaced\n9See https://www.tensorflow.org/api_docs/python/tf/profiler\ntokens {xi : i 6\u2208 m}, i.e., it does not depend on other generated tokens {x\u0302i : i \u2208 m \u2227 i 6= t}. This isn\u2019t too bad of an assumption because a relatively small number of tokens are replaced, and it greatly simplifies credit assignment when using reinforcement learning. Notationally, we show this assumption by (in a slight abuse of notation) by writing D(x\u0302t|xmasked) for the discriminator predicting whether the generated token x\u0302t equals the original token xt given the masked context xmasked. A useful consequence of this assumption is that the discriminator score for non-replaced tokens (D(xt|xmasked) for t 6\u2208m) is independent of pG because we are assuming it does not depend on any replaced token. Therefore these tokens can be ignored when training G to maximize LDisc. During training we seek to find\nargmax \u03b8G LDisc = argmax \u03b8G E x,m,x\u0302 ( n\u2211 t=1 \u22121(xcorruptt = xt) logD(xcorrupt, t)\u2212\n1(xcorruptt 6= xt) log(1\u2212D(xcorrupt, t)) ) Using the simplifying assumption, we approximate the above by finding the argmax of\nE x,m,x\u0302 (\u2211 t\u2208m \u22121(x\u0302t = xt) logD(x\u0302|xmasked)\u2212 1(x\u0302t 6= xt) log(1\u2212D(x\u0302|xmasked)) ) = E\nx,m \u2211 t\u2208m E x\u0302t\u223cpG R(x\u0302t,x)\nwhere R(x\u0302t,x) = { \u2212 logD(x\u0302t|xmasked) if x\u0302t = xt \u2212 log(1\u2212D(x\u0302t|xmasked)) otherwise\nIn short, the simplifying assumption allows us to decompose the loss over the individual generated tokens.", "Secondly, we find that BERT performance is being slightly harmed from the pre-train fine-tune mismatch from [MASK] tokens, as Replace MLM slightly outperforms BERT. We note that BERT (including our implementation) already includes a trick to help with the pre-train/finetune discrepancy: masked tokens are replaced with a random token 10% of the time and are kept the\n7We also trained a discriminator that learns from a random 15% of the input tokens distinct from the subset that was originally masked out; this model performed slightly worse.\nsame 10% of the time. However, our results suggest these simple heuristics are insufficient to fully solve the issue. Lastly, we find that All-Tokens MLM, the generative model that makes predictions over all tokens instead of a subset, closes most of the gap between BERT and ELECTRA. In total, these results suggest a large amount of ELECTRA\u2019s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. The improvement of ELECTRA over All-Tokens MLM suggests that the ELECTRA\u2019s gains come from more than just faster training. We study this further by comparing BERT to ELECTRA for various model sizes (see Figure 4, left). We find that the gains from ELECTRA grow larger as the models get smaller. The small models are trained fully to convergence (see Figure 4, right), showing that ELECTRA achieves higher downstream accuracy than BERT when fully trained. We speculate that ELECTRA is more parameter-efficient than BERT because it does not have to model the full distribution of possible tokens at each position, but we believe more analysis is needed to completely explain ELECTRA\u2019s parameter efficiency. 4 RELATED WORK.", "During two-stage training, downstream task performance notably improves after the switch from the generative to the discriminative objective, but does not end up outscoring joint training. Although still outperforming BERT, we found adversarial training to underperform maximum-likelihood training. Further analysis suggests the gap is caused by two\nproblems with adversarial training. First, the adversarial generator is simply worse at masked language modeling; it achieves 58% accuracy at masked language modeling compared to 65% accuracy for an MLE-trained one. We believe the worse accuracy is mainly due to the poor sample efficiency of reinforcement learning when working in the large action space of generating text. Secondly, the adversarially trained generator produces a low-entropy output distribution where most of the probability mass is on a single token, which means there is not much diversity in the generator samples. Both of these problems have been observed in GANs for text in prior work (Caccia et al., 2018). 3.3 SMALL MODELS. As a goal of this work is to improve the efficiency of pre-training, we develop a small model that can be quickly trained on a single GPU. Starting with the BERT-Base hyperparameters, we shortened the sequence length (from 512 to 128), reduced the batch size (from 256 to 128), reduced the model\u2019s hidden dimension size (from 768 to 256), and used smaller token embeddings (from 768 to 128). To provide a fair comparison, we also train a BERT-Small model using the same hyperparameters. We train BERT-Small for 1.5M steps, so it uses the same training FLOPs as ELECTRA-Small, which was trained for 1M steps.5 In addition to BERT, we compare against two less resource-intensive pre-training methods based on language modeling: ELMo (Peters et al., 2018) and GPT (Radford et al., 2018).6 We also show results for a base-sized ELECTRA model comparable to BERT-Base. Results are shown in Table 1."]}
{"pkey": "electra_7", "question": "List the limitations of the model discussed in the paper.", "answer": "The paper authors initially attempted to make BERT more efficient by strategically masking-out tokens (e.g., masking our rarer tokens more frequently, or training a model to guess which tokens BERT would struggle to predict if they were masked out). This resulted in fairly minor speedups over regular BERT.\n\u2022 Given that ELECTRA seemed to benefit (up to a certain point) from having a weaker generator (see Section 3.2), the paper authors explored raising the temperature of the generator\u2019s output softmax\nor disallowing the generator from sampling the correct token. Neither of these improved results.\n\u2022 The paper authors tried adding a sentence-level contrastive objective. For this task, the paper authors kept 20% of input sentences unchanged rather than noising them with the generator. The paper authors then added a prediction head to the model that predicted if the entire input.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["We report results for ELECTRA-Base and ELECTRA-Small on the GLUE test set in Table 8. Furthermore, we push the limits of base-sized and small-sized models by training them on the XLNet data instead of wikibooks and for much longer (4e6 train steps); these models are called ELECTRA-Base++ and ELECTRA-Small++ in the table. For ELECTRA-Small++ we also increased the sequence length to 512; otherwise the hyperparameters are the same as the ones listed in Table 6. Lastly, the table contains results for ELECTRA-1.75M without the tricks described in Appendix B. Consistent with dev-set results in the paper, ELECTRA-Base outperforms BERT-Large while ELECTRA-Small outperforms GPT in terms of average score. Unsurprisingly, the ++ models perform even better. The small model scores are even close to TinyBERT (Jiao et al., 2019) and MobileBERT (Sun et al., 2019b). These models learn from BERT-Base using sophisticated distillation procedures. Our ELECTRA models, on the other hand, are trained from scratch. Given the success of distilling BERT, we believe it would be possible to build even stronger small pre-trained models by distilling ELECTRA. ELECTRA appears to be particularly effective at CoLA. In CoLA the goal is to distinguish linguistically acceptable sentences from ungrammatical ones, which fairly closely matches ELECTRA\u2019s pre-training task of identifying fake tokens, perhaps explaining ELECTRA\u2019s strength at the task. E COUNTING FLOPS. We chose to measure compute usage in terms of floating point operations (FLOPs) because it is a measure agnostic to the particular hardware, low-level optimizations, etc. However, it is worth noting that in some cases abstracting away hardware details is a drawback because hardware-centered optimizations can be key parts of a model\u2019s design, such as the speedup ALBERT (Lan et al., 2019) gets by tying weights and thus reducing communication overhead between TPU workers.", "Otherwise we did no hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters are listed in Table 6.\nB FINE-TUNING DETAILS. For Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part. However, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather than 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD, we decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Basesized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large models. We found the small models benefit from a larger learning rate and searched for the best one out of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same hyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and RoBERTa separately searched for the best hyperparameters for each task. We expect our results would improve slightly if we performed the same sort of additional hyperparameter search. The full set of hyperparameters is listed in Table 7. Following BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is difficult to beat even the majority classifier using a standard fine-tuning-as-classifier approach. For the GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard submissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019). Specifically:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an ELECTRA checkpoint that has been fine-tuned on MNLI.", "We used TensorFlow\u2019s FLOP-counting capabilities9 and checked the results with by-hand computation. We made the following assumptions:\n\u2022 An \u201coperation\u201d is a mathematical operation, not a machine instruction. For example, an exp is one op like an add, even though in practice the exp might be slower. We believe this assumption does not substantially change compute estimates because matrix-multiplies dominate the compute for most models. Similarly, we count matrix-multiplies as 2 \u2217m \u2217 n FLOPs instead of m \u2217 n as one might if considering fused multiply-add operations. \u2022 The backwards pass takes the same number of FLOPs as the forward pass. This assumption\nis not exactly right (e.g., for softmax cross entropy loss the backward pass is faster), but importantly, the forward/backward pass FLOPs really are the same for matrix-multiplies, which is most of the compute anyway. \u2022 We assume \u201cdense\u201d embedding lookups (i.e., multiplication by a one-hot vector). In prac-\ntice, sparse embedding lookups are much slower than constant time; on some hardware accelerators dense operations are actually faster than sparse lookups. F ADVERSARIAL TRAINING. Here we detail attempts to adversarially train the generator instead of using maximum likelihood. In particular we train the generator G to maximize the discriminator loss LDisc. As our discriminator isn\u2019t precisely the same as the discriminator of a GAN (see the discussion in Section 2), this method is really an instance of Adversarial Contrastive Estimation (Bose et al., 2018) rather than Generative Adversarial Training. It is not possible to adversarially train the generator by back-propagating through the discriminator (e.g., as in a GAN trained on images) due to the discrete sampling from the generator, so we use reinforcement learning instead. Our generator is different from most text generation models in that it is non-autogregressive: predictions are made independently."]}
{"pkey": "electra_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "For most experiments the paper authors pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model the paper authors pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["For SQuAD, we evaluate on versions 1.1, in which models select the span of text answering a question, and 2.0, in which some questions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match (EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future. Our model architecture and most hyperparameters are the same as BERT\u2019s. For fine-tuning on GLUE, we add simple linear classifiers on top of ELECTRA. For SQuAD, we add the questionanswering module from XLNet on top of ELECTRA, which is slightly more sophisticated than BERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a \u201canswerability\u201d classifier added for SQuAD 2.0. Some of our evaluation datasets are small, which means accuracies of fine-tuned models can vary substantially depending on the random seed. We therefore report the median of 10 fine-tuning runs from the same pre-trained checkpoint for each result. Unless stated otherwise, results are on the dev set. See the appendix for further training details and hyperparameter values.\n3.2 MODEL EXTENSIONS. We improve our method by proposing and evaluating several extensions to the model. Unless stated otherwise, these experiments use the same model size and training data as BERT-Base. Weight Sharing We propose improving the efficiency of the pre-training by sharing weights between the generator and discriminator. If the generator and discriminator are the same size, all of the transformer weights can be tied.", "The task is to determine whether a given sentence is grammatical or not. The dataset contains 8.5k train examples from books and journal articles on linguistic theory. \u2022 SST: Stanford Sentiment Treebank (Socher et al., 2013). The tasks is to determine if the\nsentence is positive or negative in sentiment. The dataset contains 67k train examples from movie reviews. \u2022 MRPC: Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005). The task is to\npredict whether two sentences are semantically equivalent or not. The dataset contains 3.7k train examples from online news sources. \u2022 STS: Semantic Textual Similarity (Cer et al., 2017). The tasks is to predict how seman-\ntically similar two sentences are on a 1-5 scale. The dataset contains 5.8k train examples drawn from new headlines, video and image captions, and natural language inference data. \u2022 QQP: Quora Question Pairs (Iyer et al., 2017). The task is to determine whether a pair of\nquestions are semantically equivalent. The dataset contains 364k train examples from the community question-answering website Quora. \u2022 MNLI: Multi-genre Natural Language Inference (Williams et al., 2018). Given a premise\nsentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis, contradicts the hypothesis, or neither. The dataset contains 393k train examples drawn from ten different sources. \u2022 QNLI: Question Natural Language Inference; constructed from SQuAD (Rajpurkar et al.,\n2016). The task is to predict whether a context sentence contains the answer to a question sentence. The dataset contains 108k train examples from Wikipedia. \u2022 RTE: Recognizing Textual Entailment (Giampiccolo et al., 2007). Given a premise sen-\ntence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis or not. The dataset contains 2.5k train examples from a series of annual textual entailment challenges. D FURTHER RESULTS ON GLUE.", "Otherwise we did no hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters are listed in Table 6.\nB FINE-TUNING DETAILS. For Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part. However, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather than 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD, we decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Basesized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large models. We found the small models benefit from a larger learning rate and searched for the best one out of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same hyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and RoBERTa separately searched for the best hyperparameters for each task. We expect our results would improve slightly if we performed the same sort of additional hyperparameter search. The full set of hyperparameters is listed in Table 7. Following BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is difficult to beat even the majority classifier using a standard fine-tuning-as-classifier approach. For the GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard submissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019). Specifically:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an ELECTRA checkpoint that has been fine-tuned on MNLI."]}
{"pkey": "electra_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "vocab_size (int, optional, defaults to 30522) \u2014 Vocabulary size of the ELECTRA model. Defines the number of different tokens that can be represented by the inputs_ids passed when calling ElectraModel or TFElectraModel.[Not mentioned in the paper.] They used wordpiece tokenizer.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["However, we found it to be more efficient to have a small generator, in which case we only share the embeddings (both the token and positional embeddings) of the generator and discriminator. In this case we use embeddings the size of the discriminator\u2019s hidden states.4 The \u201cinput\u201d and \u201coutput\u201d token embeddings of the generator are always tied as in BERT. We compare the weight tying strategies when the generator is the same size as the discriminator. We train these models for 500k steps. GLUE scores are 83.6 for no weight tying, 84.3 for tying token embeddings, and 84.4 for tying all weights. We hypothesize that ELECTRA benefits from\n4We add linear layers to the generator to project the embeddings into generator-hidden-sized representations. tied token embeddings because masked language modeling is particularly effective at learning these representations: while the discriminator only updates tokens that are present in the input or are sampled by the generator, the generator\u2019s softmax over the vocabulary densely updates all token embeddings. On the other hand, tying all encoder weights caused little improvement while incurring the significant disadvantage of requiring the generator and discriminator to be the same size. Based on these findings, we use tied embeddings for further experiments in this paper. Smaller Generators If the generator and discriminator are the same size, training ELECTRA would take around twice as much compute per step as training only with masked language modeling. We suggest using a smaller generator to reduce this factor. Specifically, we make models smaller by decreasing the layer sizes while keeping the other hyperparameters constant. We also explore using an extremely simple \u201cunigram\u201d generator that samples fake tokens according their frequency in the train corpus. GLUE scores for differently-sized generators and discriminators are shown in the left of Figure 3.", "Otherwise we did no hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters are listed in Table 6.\nB FINE-TUNING DETAILS. For Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part. However, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather than 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD, we decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Basesized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large models. We found the small models benefit from a larger learning rate and searched for the best one out of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same hyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and RoBERTa separately searched for the best hyperparameters for each task. We expect our results would improve slightly if we performed the same sort of additional hyperparameter search. The full set of hyperparameters is listed in Table 7. Following BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is difficult to beat even the majority classifier using a standard fine-tuning-as-classifier approach. For the GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard submissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019). Specifically:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an ELECTRA checkpoint that has been fine-tuned on MNLI.", "We have suggested that posing the training objective over a small subset of tokens makes masked language modeling inefficient. However, it isn\u2019t entirely obvious that this is the case. After all, the model still receives a large number of input tokens even though it predicts only a small number of masked tokens. To better understand where the gains from ELECTRA are coming from, we compare a series of other pre-training objectives that are designed to be a set of \u201cstepping stones\u201d between BERT and ELECTRA. \u2022 ELECTRA 15%: This model is identical to ELECTRA except the discriminator loss only comes from the 15% of the tokens that were masked out of the input. In other words, the sum in the discriminator loss LDisc is over i \u2208m instead of from 1 to n.7\n\u2022 Replace MLM: This objective is the same as masked language modeling except instead of replacing masked-out tokens with [MASK], they are replaced with tokens from a generator model. This objective tests to what extent ELECTRA\u2019s gains come from solving the discrepancy of exposing the model to [MASK] tokens during pre-training but not fine-tuning. \u2022 All-Tokens MLM: Like in Replace MLM, masked tokens are replaced with generator samples. Furthermore, the model predicts the identity of all tokens in the input, not just ones that were masked out. We found it improved results to train this model with an explicit copy mechanism that outputs a copy probability D for each token using a sigmoid layer. The model\u2019s output distribution puts D weight on the input token plus 1 \u2212 D times the output of the MLM softmax. This model is essentially a combination of BERT and ELECTRA. Note that without generator replacements, the model would trivially learn to make predictions from the vocabulary for [MASK] tokens and copy the input for other ones. Results are shown in Table 5. First, we find that ELECTRA is greatly benefiting from having a loss defined over all input tokens rather than just a subset: ELECTRA 15% performs much worse than ELECTRA."]}
{"pkey": "electra_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "As a goal of this work is to improve the efficiency of pre-training, the paper authors develop a small model that can be quickly trained on a single GPU. Starting with the BERT-Base hyperparameters, the paper authors shortened the sequence length (from 512 to 128), Our ELECTRA Large models are the same size as BERT-Large but are trained for much longer. In particular, the paper authors train a model for 400k steps (ELECTRA-400K; roughly 1/4 the pre-training compute of RoBERTa) and one for 1.75M steps (ELECTRA-1.75M; similar compute to RoBERTa). The paper authors use a batch size 2048 and the XLNet pre-training data. The paper authors note that although the XLNet data is similar to the data used to train RoBERTa, the comparison is not entirely direct. As a baseline, the paper authors trained our own BERT-Large model using the same hyperparameters and training time as ELECTRA-400K", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["Otherwise we did no hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters are listed in Table 6.\nB FINE-TUNING DETAILS. For Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part. However, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather than 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD, we decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Basesized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large models. We found the small models benefit from a larger learning rate and searched for the best one out of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same hyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and RoBERTa separately searched for the best hyperparameters for each task. We expect our results would improve slightly if we performed the same sort of additional hyperparameter search. The full set of hyperparameters is listed in Table 7. Following BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is difficult to beat even the majority classifier using a standard fine-tuning-as-classifier approach. For the GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard submissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019). Specifically:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an ELECTRA checkpoint that has been fine-tuned on MNLI.", "Compared to masked language modeling, our pre-training objective is more compute-efficient and results in better performance on downstream tasks. It works well even when using relatively small amounts of compute, which we hope will make developing and applying pre-trained text encoders more accessible to researchers and practitioners with less access to computing resources. We also hope more future work on NLP pre-training will consider efficiency as well as absolute performance, and follow our effort in reporting compute usage and parameter counts along with evaluation metrics.\nACKNOWLEDGEMENTS. We thank Allen Nie, Prajit Ramachandran, audiences at the CIFAR LMB meeting and U. de Montre\u0301al, and the anonymous reviewers for their thoughtful comments and suggestions. We thank Matt Peters for answering our questions about ELMo, Alec Radford for answers about GPT, Naman Goyal and Myle Ott for answers about RoBERTa, Zihang Dai for answers about XLNet, Zhenzhong Lan for answers about ALBERT, and Danqi Chen and Mandar Joshi for answers about SpanBERT. Kevin is supported by a Google PhD Fellowship. A PRE-TRAINING DETAILS. The following details apply to both our ELECTRA models and BERT baselines. We mostly use the same hyperparameters as BERT. We set \u03bb, the weight for the discriminator objective in the loss to 50.8 We use dynamic token masking with the masked positions decided on-the-fly instead of during preprocessing. Also, we did not use the next sentence prediction objective proposed in the original BERT paper, as recent work has suggested it does not improve scores (Yang et al., 2019; Liu et al., 2019). For our ELECTRA-Large model, we used a higher mask percent (25 instead of 15) because we noticed the generator was achieving high accuracy with 15% masking, resulting in very few replaced tokens. We searched for the best learning rate for the Base and Small models out of [1e-4, 2e-4, 3e-4, 5e-4] and selected \u03bb out of [1, 10, 20, 50, 100] in early experiments.", "The task is to determine whether a given sentence is grammatical or not. The dataset contains 8.5k train examples from books and journal articles on linguistic theory. \u2022 SST: Stanford Sentiment Treebank (Socher et al., 2013). The tasks is to determine if the\nsentence is positive or negative in sentiment. The dataset contains 67k train examples from movie reviews. \u2022 MRPC: Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005). The task is to\npredict whether two sentences are semantically equivalent or not. The dataset contains 3.7k train examples from online news sources. \u2022 STS: Semantic Textual Similarity (Cer et al., 2017). The tasks is to predict how seman-\ntically similar two sentences are on a 1-5 scale. The dataset contains 5.8k train examples drawn from new headlines, video and image captions, and natural language inference data. \u2022 QQP: Quora Question Pairs (Iyer et al., 2017). The task is to determine whether a pair of\nquestions are semantically equivalent. The dataset contains 364k train examples from the community question-answering website Quora. \u2022 MNLI: Multi-genre Natural Language Inference (Williams et al., 2018). Given a premise\nsentence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis, contradicts the hypothesis, or neither. The dataset contains 393k train examples drawn from ten different sources. \u2022 QNLI: Question Natural Language Inference; constructed from SQuAD (Rajpurkar et al.,\n2016). The task is to predict whether a context sentence contains the answer to a question sentence. The dataset contains 108k train examples from Wikipedia. \u2022 RTE: Recognizing Textual Entailment (Giampiccolo et al., 2007). Given a premise sen-\ntence and a hypothesis sentence, the task is to predict whether the premise entails the hypothesis or not. The dataset contains 2.5k train examples from a series of annual textual entailment challenges. D FURTHER RESULTS ON GLUE."]}
{"pkey": "electra_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "As a goal of this work is to improve the efficiency of pre-training, the paper authors develop a small model that can be quickly trained on a single GPU. Starting with the BERT-Base hyperparameters, the paper authors shortened the sequence length (from 512 to 128), reduced the batch size (from 256 to 128), reduced the model\u2019s hidden dimension size (from 768 to 256), and used smaller token embeddings (from 768 to 128). To provide a fair comparison, the paper authors also train a BERT-Small model using the same hyperparameters. The paper authors train big ELECTRA models to measure the effectiveness of the replaced token detection pretraining task at the large scale of current state-of-the-art pre-trained Transformers. Our ELECTRALarge models are the same size as BERT-Large but are trained for much longer.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["However, we found it to be more efficient to have a small generator, in which case we only share the embeddings (both the token and positional embeddings) of the generator and discriminator. In this case we use embeddings the size of the discriminator\u2019s hidden states.4 The \u201cinput\u201d and \u201coutput\u201d token embeddings of the generator are always tied as in BERT. We compare the weight tying strategies when the generator is the same size as the discriminator. We train these models for 500k steps. GLUE scores are 83.6 for no weight tying, 84.3 for tying token embeddings, and 84.4 for tying all weights. We hypothesize that ELECTRA benefits from\n4We add linear layers to the generator to project the embeddings into generator-hidden-sized representations. tied token embeddings because masked language modeling is particularly effective at learning these representations: while the discriminator only updates tokens that are present in the input or are sampled by the generator, the generator\u2019s softmax over the vocabulary densely updates all token embeddings. On the other hand, tying all encoder weights caused little improvement while incurring the significant disadvantage of requiring the generator and discriminator to be the same size. Based on these findings, we use tied embeddings for further experiments in this paper. Smaller Generators If the generator and discriminator are the same size, training ELECTRA would take around twice as much compute per step as training only with masked language modeling. We suggest using a smaller generator to reduce this factor. Specifically, we make models smaller by decreasing the layer sizes while keeping the other hyperparameters constant. We also explore using an extremely simple \u201cunigram\u201d generator that samples fake tokens according their frequency in the train corpus. GLUE scores for differently-sized generators and discriminators are shown in the left of Figure 3.", "Otherwise we did no hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters are listed in Table 6.\nB FINE-TUNING DETAILS. For Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part. However, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather than 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD, we decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Basesized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large models. We found the small models benefit from a larger learning rate and searched for the best one out of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same hyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and RoBERTa separately searched for the best hyperparameters for each task. We expect our results would improve slightly if we performed the same sort of additional hyperparameter search. The full set of hyperparameters is listed in Table 7. Following BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is difficult to beat even the majority classifier using a standard fine-tuning-as-classifier approach. For the GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard submissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019). Specifically:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an ELECTRA checkpoint that has been fine-tuned on MNLI.", "We used TensorFlow\u2019s FLOP-counting capabilities9 and checked the results with by-hand computation. We made the following assumptions:\n\u2022 An \u201coperation\u201d is a mathematical operation, not a machine instruction. For example, an exp is one op like an add, even though in practice the exp might be slower. We believe this assumption does not substantially change compute estimates because matrix-multiplies dominate the compute for most models. Similarly, we count matrix-multiplies as 2 \u2217m \u2217 n FLOPs instead of m \u2217 n as one might if considering fused multiply-add operations. \u2022 The backwards pass takes the same number of FLOPs as the forward pass. This assumption\nis not exactly right (e.g., for softmax cross entropy loss the backward pass is faster), but importantly, the forward/backward pass FLOPs really are the same for matrix-multiplies, which is most of the compute anyway. \u2022 We assume \u201cdense\u201d embedding lookups (i.e., multiplication by a one-hot vector). In prac-\ntice, sparse embedding lookups are much slower than constant time; on some hardware accelerators dense operations are actually faster than sparse lookups. F ADVERSARIAL TRAINING. Here we detail attempts to adversarially train the generator instead of using maximum likelihood. In particular we train the generator G to maximize the discriminator loss LDisc. As our discriminator isn\u2019t precisely the same as the discriminator of a GAN (see the discussion in Section 2), this method is really an instance of Adversarial Contrastive Estimation (Bose et al., 2018) rather than Generative Adversarial Training. It is not possible to adversarially train the generator by back-propagating through the discriminator (e.g., as in a GAN trained on images) due to the discrete sampling from the generator, so we use reinforcement learning instead. Our generator is different from most text generation models in that it is non-autogregressive: predictions are made independently."]}
{"pkey": "electra_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "Hyperparameter  Small  Base Large\nNumber of layers 12 12 24\nHidden Size 256 768 1024\nFFN inner hidden size 1024 3072 4096\nAttention heads 4 12 16\nAttention head size 64 64 64\nEmbedding Size 128 768 1024\nGenerator Size (multiplier for hidden-size, 1/4 1/3 1/4 FFN-size, and num-attention-heads)\nMask percent 15 15 25\nLearning Rate Decay Linear Linear Linear\nWarmup steps 10000 10000 10000\nLearning Rate 5e-4 2e-4 2e-4\nAdam _x000f_ 1e-6 1e-6 1e-6\nAdam \u03b21 0.9 0.9 0.9\nAdam \u03b22 0.999 0.999 0.999\nAttention Dropout 0.1 0.1 0.1\nDropout 0.1 0.1 0.1\nWeight Decay 0.01 0.01 0.01\nBatch Size 128 256 2048\nTrain Steps (BERT/ELECTRA) 1.45M/1M 1M/766K 464K/400K\nHyperparameter GLUE Value\nLearning Rate 3e-4 for Small, 1e-4 for Base, 5e-5 for Large\nAdam _x000f_ 1e-6\nAdam \u03b21 0.9\nAdam \u03b22 0.999\nLayerwise LR decay 0.8 for Base/Small, 0.9 for Large\nLearning rate decay Linear\nWarmup fraction 0.1\nAttention Dropout 0.1\nDropout 0.1\nWeight Decay 0\nBatch Size 32\nTrain Epochs 10 for RTE and STS, 2 for SQuAD, 3 for other tasks", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["Otherwise we did no hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters are listed in Table 6.\nB FINE-TUNING DETAILS. For Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part. However, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather than 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD, we decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Basesized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large models. We found the small models benefit from a larger learning rate and searched for the best one out of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same hyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and RoBERTa separately searched for the best hyperparameters for each task. We expect our results would improve slightly if we performed the same sort of additional hyperparameter search. The full set of hyperparameters is listed in Table 7. Following BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is difficult to beat even the majority classifier using a standard fine-tuning-as-classifier approach. For the GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard submissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019). Specifically:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an ELECTRA checkpoint that has been fine-tuned on MNLI.", "We report results for ELECTRA-Base and ELECTRA-Small on the GLUE test set in Table 8. Furthermore, we push the limits of base-sized and small-sized models by training them on the XLNet data instead of wikibooks and for much longer (4e6 train steps); these models are called ELECTRA-Base++ and ELECTRA-Small++ in the table. For ELECTRA-Small++ we also increased the sequence length to 512; otherwise the hyperparameters are the same as the ones listed in Table 6. Lastly, the table contains results for ELECTRA-1.75M without the tricks described in Appendix B. Consistent with dev-set results in the paper, ELECTRA-Base outperforms BERT-Large while ELECTRA-Small outperforms GPT in terms of average score. Unsurprisingly, the ++ models perform even better. The small model scores are even close to TinyBERT (Jiao et al., 2019) and MobileBERT (Sun et al., 2019b). These models learn from BERT-Base using sophisticated distillation procedures. Our ELECTRA models, on the other hand, are trained from scratch. Given the success of distilling BERT, we believe it would be possible to build even stronger small pre-trained models by distilling ELECTRA. ELECTRA appears to be particularly effective at CoLA. In CoLA the goal is to distinguish linguistically acceptable sentences from ungrammatical ones, which fairly closely matches ELECTRA\u2019s pre-training task of identifying fake tokens, perhaps explaining ELECTRA\u2019s strength at the task. E COUNTING FLOPS. We chose to measure compute usage in terms of floating point operations (FLOPs) because it is a measure agnostic to the particular hardware, low-level optimizations, etc. However, it is worth noting that in some cases abstracting away hardware details is a drawback because hardware-centered optimizations can be key parts of a model\u2019s design, such as the speedup ALBERT (Lan et al., 2019) gets by tying weights and thus reducing communication overhead between TPU workers.", "Although similar to the training objective of a GAN, there are several key differences. First, if the generator happens to generate the correct token, that token is considered \u201creal\u201d instead of \u201cfake\u201d; we found this formulation to moderately improve results on downstream tasks. More importantly, the generator is trained with maximum likelihood rather than being trained adversarially to fool the discriminator. Adversarially training the generator is challenging because it is impossible to backpropagate through sampling from the generator. Although we experimented circumventing this issue\n3Typically k = d0.15ne, i.e., 15% of the tokens are masked out. by using reinforcement learning to train the generator (see Appendix F), this performed worse than maximum-likelihood training. Lastly, we do not supply the generator with a noise vector as input, as is typical with a GAN. We minimize the combined loss\nmin \u03b8G,\u03b8D \u2211 x\u2208X LMLM(x, \u03b8G) + \u03bbLDisc(x, \u03b8D)\nover a large corpus X of raw text. We approximate the expectations in the losses with a single sample. We don\u2019t back-propagate the discriminator loss through the generator (indeed, we can\u2019t because of the sampling step). After pre-training, we throw out the generator and fine-tune the discriminator on downstream tasks. 3 EXPERIMENTS.\n3.1 EXPERIMENTAL SETUP. We evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) and Stanford Question Answering (SQuAD) dataset (Rajpurkar et al., 2016). GLUE contains a variety of tasks covering textual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), sentiment (SST), and linguistic acceptability (CoLA). See Appendix C for more details on the GLUE tasks. Our evaluation metrics are Spearman correlation for STS, Matthews correlation for CoLA, and accuracy for the other GLUE tasks; we generally report the average score over all tasks."]}
{"pkey": "electra_13", "question": "Describe the computational resources used to train the model.", "answer": "BERT-Base 6.4e19 / 2.9e10 1x / 1x 110M 4d on 16 TPUv3s 82.2\nELECTRA-Base 6.4e19 / 2.9e10 1x / 1x 110M 4d on 16 TPUv3s 85.1", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["As a baseline, we trained our own BERT-Large model using the same hyperparameters and training time as ELECTRA-400K.\nResults on the GLUE dev set are shown in Table 2. ELECTRA-400K performs comparably to RoBERTa and XLNet. However, it took less than 1/4 of the compute to train ELECTRA-400K as it did to train RoBERTa and XLNet, demonstrating that ELECTRA\u2019s sample-efficiency gains hold at large scale. Training ELECTRA for longer (ELECTRA-1.75M) results in a model that outscores them on most GLUE tasks while still requiring less pre-training compute. Surprisingly, our baseline BERT model scores notably worse than RoBERTa-100K, suggesting our models may benefit from more hyperparameter tuning or using the RoBERTa training data. ELECTRA\u2019s gains hold on the GLUE test set (see Table 3), although these comparisons are less apples-to-apples due to the additional tricks employed by the models (see Appendix B). Results on SQuAD are shown in Table 4. Consistent, with the GLUE results, ELECTRA scores better than masked-language-modeling-based methods given the same compute resources. For example, ELECTRA-400K outperforms RoBERTa-100k and our BERT baseline, which use similar amounts of pre-training compute. ELECTRA-400K also performs comparably to RoBERTa-500K despite using less than 1/4th of the compute. Unsurprisingly, training ELECTRA longer improves results further: ELECTRA-1.75M scores higher than previous models on the SQuAD 2.0 bench-\nmark. ELECTRA-Base also yields strong results, scoring substantially better than BERT-Base and XLNet-Base, and even surpassing BERT-Large according to most metrics. ELECTRA generally performs better at SQuAD 2.0 than 1.1. Perhaps replaced token detection, in which the model distinguishes real tokens from plausible fakes, is particularly transferable to the answerability classification of SQuAD 2.0, in which the model must distinguish answerable questions from fake unanswerable questions.\n3.5 EFFICIENCY ANALYSIS.", "We report results for ELECTRA-Base and ELECTRA-Small on the GLUE test set in Table 8. Furthermore, we push the limits of base-sized and small-sized models by training them on the XLNet data instead of wikibooks and for much longer (4e6 train steps); these models are called ELECTRA-Base++ and ELECTRA-Small++ in the table. For ELECTRA-Small++ we also increased the sequence length to 512; otherwise the hyperparameters are the same as the ones listed in Table 6. Lastly, the table contains results for ELECTRA-1.75M without the tricks described in Appendix B. Consistent with dev-set results in the paper, ELECTRA-Base outperforms BERT-Large while ELECTRA-Small outperforms GPT in terms of average score. Unsurprisingly, the ++ models perform even better. The small model scores are even close to TinyBERT (Jiao et al., 2019) and MobileBERT (Sun et al., 2019b). These models learn from BERT-Base using sophisticated distillation procedures. Our ELECTRA models, on the other hand, are trained from scratch. Given the success of distilling BERT, we believe it would be possible to build even stronger small pre-trained models by distilling ELECTRA. ELECTRA appears to be particularly effective at CoLA. In CoLA the goal is to distinguish linguistically acceptable sentences from ungrammatical ones, which fairly closely matches ELECTRA\u2019s pre-training task of identifying fake tokens, perhaps explaining ELECTRA\u2019s strength at the task. E COUNTING FLOPS. We chose to measure compute usage in terms of floating point operations (FLOPs) because it is a measure agnostic to the particular hardware, low-level optimizations, etc. However, it is worth noting that in some cases abstracting away hardware details is a drawback because hardware-centered optimizations can be key parts of a model\u2019s design, such as the speedup ALBERT (Lan et al., 2019) gets by tying weights and thus reducing communication overhead between TPU workers.", "Compared to masked language modeling, our pre-training objective is more compute-efficient and results in better performance on downstream tasks. It works well even when using relatively small amounts of compute, which we hope will make developing and applying pre-trained text encoders more accessible to researchers and practitioners with less access to computing resources. We also hope more future work on NLP pre-training will consider efficiency as well as absolute performance, and follow our effort in reporting compute usage and parameter counts along with evaluation metrics.\nACKNOWLEDGEMENTS. We thank Allen Nie, Prajit Ramachandran, audiences at the CIFAR LMB meeting and U. de Montre\u0301al, and the anonymous reviewers for their thoughtful comments and suggestions. We thank Matt Peters for answering our questions about ELMo, Alec Radford for answers about GPT, Naman Goyal and Myle Ott for answers about RoBERTa, Zihang Dai for answers about XLNet, Zhenzhong Lan for answers about ALBERT, and Danqi Chen and Mandar Joshi for answers about SpanBERT. Kevin is supported by a Google PhD Fellowship. A PRE-TRAINING DETAILS. The following details apply to both our ELECTRA models and BERT baselines. We mostly use the same hyperparameters as BERT. We set \u03bb, the weight for the discriminator objective in the loss to 50.8 We use dynamic token masking with the masked positions decided on-the-fly instead of during preprocessing. Also, we did not use the next sentence prediction objective proposed in the original BERT paper, as recent work has suggested it does not improve scores (Yang et al., 2019; Liu et al., 2019). For our ELECTRA-Large model, we used a higher mask percent (25 instead of 15) because we noticed the generator was achieving high accuracy with 15% masking, resulting in very few replaced tokens. We searched for the best learning rate for the Base and Small models out of [1e-4, 2e-4, 3e-4, 5e-4] and selected \u03bb out of [1, 10, 20, 50, 100] in early experiments."]}
{"pkey": "electra_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "https://github.com/google-research/electra \n\"ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators\" does not explicitly state whether all the necessary details to reproduce the paper are provided. However, it does provide a comprehensive description of the model architecture, training procedure, and evaluation on various tasks. It also includes references to prior work and technical specifications. While the paper provides substantial information, reproducing the exact implementation and achieving the same results may require additional experimentation and fine-tuning.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["All models are trained for 500k steps, which puts the smaller generators at a disadvantage in terms of compute because they require less compute per training step. Nevertheless, we find that models work best with generators 1/4-1/2 the size of the discriminator. We speculate that having too strong of a generator may pose a too-challenging task for the discriminator, preventing it from learning as effectively. In particular, the discriminator may have to use many of its parameters modeling the generator rather than the actual data distribution. Further experiments in this paper use the best generator size found for the given discriminator size. Training Algorithms Lastly, we explore other training algorithms for ELECTRA, although these did not end up improving results. The proposed training objective jointly trains the generator and discriminator. We experiment with instead using the following two-stage training procedure:\n1. Train only the generator with LMLM for n steps. 2. Initialize the weights of the discriminator with the weights of the generator. Then train the\ndiscriminator with LDisc for n steps, keeping the generator\u2019s weights frozen. Note that the weight initialization in this procedure requires having the same size for the generator and discriminator. We found that without the weight initialization the discriminator would sometimes fail to learn at all beyond the majority class, perhaps because the generator started so far ahead of the discriminator. Joint training on the other hand naturally provides a curriculum for the discriminator where the generator starts off weak but gets better throughout training. We also explored training the generator adversarially as in a GAN, using reinforcement learning to accommodate the discrete operations of sampling from the generator. See Appendix F for details. Results are shown in the right of Figure 3.", "However, we found it to be more efficient to have a small generator, in which case we only share the embeddings (both the token and positional embeddings) of the generator and discriminator. In this case we use embeddings the size of the discriminator\u2019s hidden states.4 The \u201cinput\u201d and \u201coutput\u201d token embeddings of the generator are always tied as in BERT. We compare the weight tying strategies when the generator is the same size as the discriminator. We train these models for 500k steps. GLUE scores are 83.6 for no weight tying, 84.3 for tying token embeddings, and 84.4 for tying all weights. We hypothesize that ELECTRA benefits from\n4We add linear layers to the generator to project the embeddings into generator-hidden-sized representations. tied token embeddings because masked language modeling is particularly effective at learning these representations: while the discriminator only updates tokens that are present in the input or are sampled by the generator, the generator\u2019s softmax over the vocabulary densely updates all token embeddings. On the other hand, tying all encoder weights caused little improvement while incurring the significant disadvantage of requiring the generator and discriminator to be the same size. Based on these findings, we use tied embeddings for further experiments in this paper. Smaller Generators If the generator and discriminator are the same size, training ELECTRA would take around twice as much compute per step as training only with masked language modeling. We suggest using a smaller generator to reduce this factor. Specifically, we make models smaller by decreasing the layer sizes while keeping the other hyperparameters constant. We also explore using an extremely simple \u201cunigram\u201d generator that samples fake tokens according their frequency in the train corpus. GLUE scores for differently-sized generators and discriminators are shown in the left of Figure 3.", "For RTE, we found it helpful to combine this with a lower learning rate of 2e-5.\n8As a binary classification task instead of the 30,000-way classification task in MLM, the discriminator\u2019s loss was typically much lower than the generator\u2019s. \u2022 For WNLI, we follow the trick described in Liu et al. (2019) where we extract candidate antecedents for the pronoun using rules and train a model to score the correct antecedent highly. However, different from Liu et al. (2019), the scoring function is not based on MLM probabilities. Instead, we fine-tune ELECTRA\u2019s discriminator so it assigns high scores to the tokens of the correct antecedent when the correct antecedent replaces the pronoun. For example, if the Winograd schema is \u201cthe trophy could not fit in the suitcase because it was too big,\u201d we train the discriminator so it gives a high score to \u201ctrophy\u201d in \u201cthe trophy could not fit in the suitcase because the trophy was too big\u201d but a low score to \u201csuitcase\u201d in \u201cthe trophy could not fit in the suitcase because the suitcase was too big.\u201d \u2022 For each task we ensemble the best 10 of 30 models fine-tuned with different random seeds\nbut initialized from the same pre-trained checkpoint. While these tricks do improve scores, they make having clear scientific comparisons more difficult because they require extra work to implement, require lots of compute, and make results less apples-\nto-apples because different papers implement the tricks differently. We therefore also report results for ELECTRA-1.75M with the only trick being dev-set model selection (best of 10 models), which is the setting BERT used to report results, in Table 8. For our SQuAD 2.0 test set submission, we fine-tuned 20 models from the same pre-trained checkpoint and submitted the one with the best dev set score. C DETAILS ABOUT GLUE. We provide further details about the GLUE benchmark tasks below\n\u2022 CoLA: Corpus of Linguistic Acceptability (Warstadt et al., 2018)."]}
{"pkey": "electra_15", "question": "What is the pretraining objective of the model? ", "answer": "The approach is reminiscent of training the discriminator of a GAN, our method is not adversarial in that the generator producing corrupted tokens is trained with maximum likelihood due to the difficulty of applying GANs to text", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["We have suggested that posing the training objective over a small subset of tokens makes masked language modeling inefficient. However, it isn\u2019t entirely obvious that this is the case. After all, the model still receives a large number of input tokens even though it predicts only a small number of masked tokens. To better understand where the gains from ELECTRA are coming from, we compare a series of other pre-training objectives that are designed to be a set of \u201cstepping stones\u201d between BERT and ELECTRA. \u2022 ELECTRA 15%: This model is identical to ELECTRA except the discriminator loss only comes from the 15% of the tokens that were masked out of the input. In other words, the sum in the discriminator loss LDisc is over i \u2208m instead of from 1 to n.7\n\u2022 Replace MLM: This objective is the same as masked language modeling except instead of replacing masked-out tokens with [MASK], they are replaced with tokens from a generator model. This objective tests to what extent ELECTRA\u2019s gains come from solving the discrepancy of exposing the model to [MASK] tokens during pre-training but not fine-tuning. \u2022 All-Tokens MLM: Like in Replace MLM, masked tokens are replaced with generator samples. Furthermore, the model predicts the identity of all tokens in the input, not just ones that were masked out. We found it improved results to train this model with an explicit copy mechanism that outputs a copy probability D for each token using a sigmoid layer. The model\u2019s output distribution puts D weight on the input token plus 1 \u2212 D times the output of the MLM softmax. This model is essentially a combination of BERT and ELECTRA. Note that without generator replacements, the model would trivially learn to make predictions from the vocabulary for [MASK] tokens and copy the input for other ones. Results are shown in Table 5. First, we find that ELECTRA is greatly benefiting from having a loss defined over all input tokens rather than just a subset: ELECTRA 15% performs much worse than ELECTRA.", "See Appendix D for additional results, including stronger small-sized and base-sized models trained with more compute. ELECTRA-Small performs remarkably well given its size, achieving a higher GLUE score than other methods using substantially more compute and parameters. For example, it scores 5 points higher than a comparable BERT-Small model and even outperforms the much larger GPT model. ELECTRA-Small is trained mostly to convergence, with models trained for even less time (as little as 6 hours) still achieving reasonable performance. While small models distilled from larger pre-trained transformers can also achieve good GLUE scores (Sun et al., 2019b; Jiao et al., 2019), these models require first expending substantial compute to pre-train the larger teacher model. The results also demonstrate the strength of ELECTRA at a moderate size; our base-sized ELECTRA model substantially outperforms BERT-Base and even outperforms BERT-Large (which gets 84.0 GLUE score). We hope ELECTRA\u2019s ability to achieve strong results with relatively little compute will broaden the accessibility of developing and applying pre-trained models in NLP. 5ELECTRA requires more FLOPs per step because it consists of the generator as well as the discriminator. 6GPT is similar in size to BERT-Base, but is trained for fewer steps. 3.4 LARGE MODELS. We train big ELECTRA models to measure the effectiveness of the replaced token detection pretraining task at the large scale of current state-of-the-art pre-trained Transformers. Our ELECTRALarge models are the same size as BERT-Large but are trained for much longer. In particular, we train a model for 400k steps (ELECTRA-400K; roughly 1/4 the pre-training compute of RoBERTa) and one for 1.75M steps (ELECTRA-1.75M; similar compute to RoBERTa). We use a batch size 2048 and the XLNet pre-training data. We note that although the XLNet data is similar to the data used to train RoBERTa, the comparison is not entirely direct.", "We can use this expression to evaluate ELECTRA as a masked language model by selecting argmaxx\u2208vocabD(x, c)pG(x|c)/(a(1 \u2212 D(x, c)) + pG(x|c)) as the model\u2019s prediction for a given context. In practice, selecting over the whole vocabulary is very expensive, so we instead take the argmax over the top 100 predictions from the generator.10 Using this method, we compared ELECTRA-Base and BERT-Base on the Wikipedia+BooksCorpus dataset. We found that BERT slightly outperformed ELECTRA at masked language modeling (77.9% vs 75.5% accuracy). It is possible that the assumption of an optimal discriminator, which is certainly far from correct, is harming ELECTRA\u2019s accuracy under this evaluation scheme. However, perhaps it is not too surprising that a model like BERT that is trained specifically for generation performs better at generation while a model with a discriminative objective like ELECTRA is better at being fine-tuned on discriminative tasks. We think comparisons of BERT\u2019s and ELECTRA\u2019s MLM predictions might be an interesting way to uncover more about the differences between ELECTRA and BERT encoders in future work. H NEGATIVE RESULTS. We briefly describe a few ideas that did not look promising in our initial experiments:\n\u2022 We initially attempted to make BERT more efficient by strategically masking-out tokens (e.g., masking our rarer tokens more frequently, or training a model to guess which tokens BERT would struggle to predict if they were masked out). This resulted in fairly minor speedups over regular BERT. \u2022 Given that ELECTRA seemed to benefit (up to a certain point) from having a weaker generator (see Section 3.2), we explored raising the temperature of the generator\u2019s output softmax or disallowing the generator from sampling the correct token. Neither of these improved results. \u2022 We tried adding a sentence-level contrastive objective. For this task, we kept 20% of input sentences unchanged rather than noising them with the generator."]}
{"pkey": "electra_16", "question": "What is the loss function that is used to train the model?", "answer": "The backwards pass takes the same number of FLOPs as the forward pass. This assumption is not exactly right (e.g., for softmax cross entropy loss the backward pass is faster), but importantly, the forward/backward pass FLOPs really are the same for matrix multiplies, which is most of the computing anyway", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["We have suggested that posing the training objective over a small subset of tokens makes masked language modeling inefficient. However, it isn\u2019t entirely obvious that this is the case. After all, the model still receives a large number of input tokens even though it predicts only a small number of masked tokens. To better understand where the gains from ELECTRA are coming from, we compare a series of other pre-training objectives that are designed to be a set of \u201cstepping stones\u201d between BERT and ELECTRA. \u2022 ELECTRA 15%: This model is identical to ELECTRA except the discriminator loss only comes from the 15% of the tokens that were masked out of the input. In other words, the sum in the discriminator loss LDisc is over i \u2208m instead of from 1 to n.7\n\u2022 Replace MLM: This objective is the same as masked language modeling except instead of replacing masked-out tokens with [MASK], they are replaced with tokens from a generator model. This objective tests to what extent ELECTRA\u2019s gains come from solving the discrepancy of exposing the model to [MASK] tokens during pre-training but not fine-tuning. \u2022 All-Tokens MLM: Like in Replace MLM, masked tokens are replaced with generator samples. Furthermore, the model predicts the identity of all tokens in the input, not just ones that were masked out. We found it improved results to train this model with an explicit copy mechanism that outputs a copy probability D for each token using a sigmoid layer. The model\u2019s output distribution puts D weight on the input token plus 1 \u2212 D times the output of the MLM softmax. This model is essentially a combination of BERT and ELECTRA. Note that without generator replacements, the model would trivially learn to make predictions from the vocabulary for [MASK] tokens and copy the input for other ones. Results are shown in Table 5. First, we find that ELECTRA is greatly benefiting from having a loss defined over all input tokens rather than just a subset: ELECTRA 15% performs much worse than ELECTRA.", "For RTE, we found it helpful to combine this with a lower learning rate of 2e-5.\n8As a binary classification task instead of the 30,000-way classification task in MLM, the discriminator\u2019s loss was typically much lower than the generator\u2019s. \u2022 For WNLI, we follow the trick described in Liu et al. (2019) where we extract candidate antecedents for the pronoun using rules and train a model to score the correct antecedent highly. However, different from Liu et al. (2019), the scoring function is not based on MLM probabilities. Instead, we fine-tune ELECTRA\u2019s discriminator so it assigns high scores to the tokens of the correct antecedent when the correct antecedent replaces the pronoun. For example, if the Winograd schema is \u201cthe trophy could not fit in the suitcase because it was too big,\u201d we train the discriminator so it gives a high score to \u201ctrophy\u201d in \u201cthe trophy could not fit in the suitcase because the trophy was too big\u201d but a low score to \u201csuitcase\u201d in \u201cthe trophy could not fit in the suitcase because the suitcase was too big.\u201d \u2022 For each task we ensemble the best 10 of 30 models fine-tuned with different random seeds\nbut initialized from the same pre-trained checkpoint. While these tricks do improve scores, they make having clear scientific comparisons more difficult because they require extra work to implement, require lots of compute, and make results less apples-\nto-apples because different papers implement the tricks differently. We therefore also report results for ELECTRA-1.75M with the only trick being dev-set model selection (best of 10 models), which is the setting BERT used to report results, in Table 8. For our SQuAD 2.0 test set submission, we fine-tuned 20 models from the same pre-trained checkpoint and submitted the one with the best dev set score. C DETAILS ABOUT GLUE. We provide further details about the GLUE benchmark tasks below\n\u2022 CoLA: Corpus of Linguistic Acceptability (Warstadt et al., 2018).", "We used TensorFlow\u2019s FLOP-counting capabilities9 and checked the results with by-hand computation. We made the following assumptions:\n\u2022 An \u201coperation\u201d is a mathematical operation, not a machine instruction. For example, an exp is one op like an add, even though in practice the exp might be slower. We believe this assumption does not substantially change compute estimates because matrix-multiplies dominate the compute for most models. Similarly, we count matrix-multiplies as 2 \u2217m \u2217 n FLOPs instead of m \u2217 n as one might if considering fused multiply-add operations. \u2022 The backwards pass takes the same number of FLOPs as the forward pass. This assumption\nis not exactly right (e.g., for softmax cross entropy loss the backward pass is faster), but importantly, the forward/backward pass FLOPs really are the same for matrix-multiplies, which is most of the compute anyway. \u2022 We assume \u201cdense\u201d embedding lookups (i.e., multiplication by a one-hot vector). In prac-\ntice, sparse embedding lookups are much slower than constant time; on some hardware accelerators dense operations are actually faster than sparse lookups. F ADVERSARIAL TRAINING. Here we detail attempts to adversarially train the generator instead of using maximum likelihood. In particular we train the generator G to maximize the discriminator loss LDisc. As our discriminator isn\u2019t precisely the same as the discriminator of a GAN (see the discussion in Section 2), this method is really an instance of Adversarial Contrastive Estimation (Bose et al., 2018) rather than Generative Adversarial Training. It is not possible to adversarially train the generator by back-propagating through the discriminator (e.g., as in a GAN trained on images) due to the discrete sampling from the generator, so we use reinforcement learning instead. Our generator is different from most text generation models in that it is non-autogregressive: predictions are made independently."]}
{"pkey": "electra_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The architecture of ELECTRA is similar to BERT, which is an encoder-only architecture. However, the key difference is in the pretraining objective and the architecture of the generator and discriminator. In ELECTRA, the pretraining objective is to train the discriminator to distinguish between the original and replaced tokens, whereas in BERT the objective is to predict the masked tokens. Additionally, ELECTRA has two subnetworks: a generator and a discriminator. The generator replaces some tokens with plausible alternatives, and the discriminator is trained to distinguish between the replaced tokens and the original tokens.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["For SQuAD, we evaluate on versions 1.1, in which models select the span of text answering a question, and 2.0, in which some questions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match (EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future. Our model architecture and most hyperparameters are the same as BERT\u2019s. For fine-tuning on GLUE, we add simple linear classifiers on top of ELECTRA. For SQuAD, we add the questionanswering module from XLNet on top of ELECTRA, which is slightly more sophisticated than BERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a \u201canswerability\u201d classifier added for SQuAD 2.0. Some of our evaluation datasets are small, which means accuracies of fine-tuned models can vary substantially depending on the random seed. We therefore report the median of 10 fine-tuning runs from the same pre-trained checkpoint for each result. Unless stated otherwise, results are on the dev set. See the appendix for further training details and hyperparameter values.\n3.2 MODEL EXTENSIONS. We improve our method by proposing and evaluating several extensions to the model. Unless stated otherwise, these experiments use the same model size and training data as BERT-Base. Weight Sharing We propose improving the efficiency of the pre-training by sharing weights between the generator and discriminator. If the generator and discriminator are the same size, all of the transformer weights can be tied.", "We cannot directly find argmax\u03b8G using gradient ascent because it is impossible to backpropagate through discrete sampling of x\u0302. Instead, we use policy gradient reinforcement learning (Williams, 1992). In particular, we use the REINFORCE gradient\n\u2207\u03b8GLDisc \u2248 E x,m \u2211 t\u2208m E x\u0302t\u223cpG \u2207\u03b8g log pG(x\u0302t|xmasked)[R(x\u0302t,x)\u2212 b(xmasked, t)] Where b is a learned baseline implemented as b(xmasked, t) = \u2212 log sigmoid(wThG(xmasked)t) where hG(xmasked) are the outputs of the generator\u2019s Transformer encoder. The baseline is trained with cross-entropy loss to match the reward for the corresponding position. We approximate the expectations with a single sample and learn \u03b8G with gradient ascent. Despite receiving no explicit feedback about which generated tokens are correct, we found the adversarial training resulted in a fairly accurate generator (for a 256-hidden-size generator, the adversarially trained one achieves 58% accuracy at masked language modeling while the same sized MLE generator gets 65%). However, using this generator did not improve over the MLE-trained one on downstream tasks (see the right of Figure 3 in the main paper). G EVALUATING ELECTRA AS A MASKED LANGUAGE MODEL. This sections details some initial experiments in evaluating ELECTRA as a masked language model. Using slightly different notation from the main paper, given a context c consisting of a text sequence with one token x masked-out, the discriminator loss can be written as\nLDisc = \u2212 \u2211\nx\u2208vocab\n( (1\u2212 pmask)pdata(x|c) logD(x, c) + //unmasked token\npmaskpdata(x|c)pG(x|c) logD(x, c) + //generator samples correct token pmask(1\u2212 pdata(x|c))pG(x|c) log(1\u2212D(x, c)) ) //generator samples incorrect token\nFinding the critical points of this loss with respect to D shows that for a fixed generator the optimal discriminator is\nD(x, c) = pdata(x|c)(a+ pG(x|c))/(apdata(x|c) + pG(x|c))\nwhich means\npdata(x|c) = D(x, c)pG(x|c)/(a(1\u2212D(x, c)) + pG(x|c))\nwhere a = (1 \u2212 pmask)/pmask is the number of unmasked tokens for every masked token.", "We can use this expression to evaluate ELECTRA as a masked language model by selecting argmaxx\u2208vocabD(x, c)pG(x|c)/(a(1 \u2212 D(x, c)) + pG(x|c)) as the model\u2019s prediction for a given context. In practice, selecting over the whole vocabulary is very expensive, so we instead take the argmax over the top 100 predictions from the generator.10 Using this method, we compared ELECTRA-Base and BERT-Base on the Wikipedia+BooksCorpus dataset. We found that BERT slightly outperformed ELECTRA at masked language modeling (77.9% vs 75.5% accuracy). It is possible that the assumption of an optimal discriminator, which is certainly far from correct, is harming ELECTRA\u2019s accuracy under this evaluation scheme. However, perhaps it is not too surprising that a model like BERT that is trained specifically for generation performs better at generation while a model with a discriminative objective like ELECTRA is better at being fine-tuned on discriminative tasks. We think comparisons of BERT\u2019s and ELECTRA\u2019s MLM predictions might be an interesting way to uncover more about the differences between ELECTRA and BERT encoders in future work. H NEGATIVE RESULTS. We briefly describe a few ideas that did not look promising in our initial experiments:\n\u2022 We initially attempted to make BERT more efficient by strategically masking-out tokens (e.g., masking our rarer tokens more frequently, or training a model to guess which tokens BERT would struggle to predict if they were masked out). This resulted in fairly minor speedups over regular BERT. \u2022 Given that ELECTRA seemed to benefit (up to a certain point) from having a weaker generator (see Section 3.2), we explored raising the temperature of the generator\u2019s output softmax or disallowing the generator from sampling the correct token. Neither of these improved results. \u2022 We tried adding a sentence-level contrastive objective. For this task, we kept 20% of input sentences unchanged rather than noising them with the generator."]}
{"pkey": "electra_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors use the standard evaluation metrics of Exact-Match (EM) and F1 scores. For most experiments the paper authors pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model the paper authors pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future.", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["Although similar to the training objective of a GAN, there are several key differences. First, if the generator happens to generate the correct token, that token is considered \u201creal\u201d instead of \u201cfake\u201d; we found this formulation to moderately improve results on downstream tasks. More importantly, the generator is trained with maximum likelihood rather than being trained adversarially to fool the discriminator. Adversarially training the generator is challenging because it is impossible to backpropagate through sampling from the generator. Although we experimented circumventing this issue\n3Typically k = d0.15ne, i.e., 15% of the tokens are masked out. by using reinforcement learning to train the generator (see Appendix F), this performed worse than maximum-likelihood training. Lastly, we do not supply the generator with a noise vector as input, as is typical with a GAN. We minimize the combined loss\nmin \u03b8G,\u03b8D \u2211 x\u2208X LMLM(x, \u03b8G) + \u03bbLDisc(x, \u03b8D)\nover a large corpus X of raw text. We approximate the expectations in the losses with a single sample. We don\u2019t back-propagate the discriminator loss through the generator (indeed, we can\u2019t because of the sampling step). After pre-training, we throw out the generator and fine-tune the discriminator on downstream tasks. 3 EXPERIMENTS.\n3.1 EXPERIMENTAL SETUP. We evaluate on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019) and Stanford Question Answering (SQuAD) dataset (Rajpurkar et al., 2016). GLUE contains a variety of tasks covering textual entailment (RTE and MNLI) question-answer entailment (QNLI), paraphrase (MRPC), question paraphrase (QQP), textual similarity (STS), sentiment (SST), and linguistic acceptability (CoLA). See Appendix C for more details on the GLUE tasks. Our evaluation metrics are Spearman correlation for STS, Matthews correlation for CoLA, and accuracy for the other GLUE tasks; we generally report the average score over all tasks.", "Compared to masked language modeling, our pre-training objective is more compute-efficient and results in better performance on downstream tasks. It works well even when using relatively small amounts of compute, which we hope will make developing and applying pre-trained text encoders more accessible to researchers and practitioners with less access to computing resources. We also hope more future work on NLP pre-training will consider efficiency as well as absolute performance, and follow our effort in reporting compute usage and parameter counts along with evaluation metrics.\nACKNOWLEDGEMENTS. We thank Allen Nie, Prajit Ramachandran, audiences at the CIFAR LMB meeting and U. de Montre\u0301al, and the anonymous reviewers for their thoughtful comments and suggestions. We thank Matt Peters for answering our questions about ELMo, Alec Radford for answers about GPT, Naman Goyal and Myle Ott for answers about RoBERTa, Zihang Dai for answers about XLNet, Zhenzhong Lan for answers about ALBERT, and Danqi Chen and Mandar Joshi for answers about SpanBERT. Kevin is supported by a Google PhD Fellowship. A PRE-TRAINING DETAILS. The following details apply to both our ELECTRA models and BERT baselines. We mostly use the same hyperparameters as BERT. We set \u03bb, the weight for the discriminator objective in the loss to 50.8 We use dynamic token masking with the masked positions decided on-the-fly instead of during preprocessing. Also, we did not use the next sentence prediction objective proposed in the original BERT paper, as recent work has suggested it does not improve scores (Yang et al., 2019; Liu et al., 2019). For our ELECTRA-Large model, we used a higher mask percent (25 instead of 15) because we noticed the generator was achieving high accuracy with 15% masking, resulting in very few replaced tokens. We searched for the best learning rate for the Base and Small models out of [1e-4, 2e-4, 3e-4, 5e-4] and selected \u03bb out of [1, 10, 20, 50, 100] in early experiments.", "For SQuAD, we evaluate on versions 1.1, in which models select the span of text answering a question, and 2.0, in which some questions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match (EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future. Our model architecture and most hyperparameters are the same as BERT\u2019s. For fine-tuning on GLUE, we add simple linear classifiers on top of ELECTRA. For SQuAD, we add the questionanswering module from XLNet on top of ELECTRA, which is slightly more sophisticated than BERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a \u201canswerability\u201d classifier added for SQuAD 2.0. Some of our evaluation datasets are small, which means accuracies of fine-tuned models can vary substantially depending on the random seed. We therefore report the median of 10 fine-tuning runs from the same pre-trained checkpoint for each result. Unless stated otherwise, results are on the dev set. See the appendix for further training details and hyperparameter values.\n3.2 MODEL EXTENSIONS. We improve our method by proposing and evaluating several extensions to the model. Unless stated otherwise, these experiments use the same model size and training data as BERT-Base. Weight Sharing We propose improving the efficiency of the pre-training by sharing weights between the generator and discriminator. If the generator and discriminator are the same size, all of the transformer weights can be tied."]}
{"pkey": "electra_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "Compared to masked language modeling, our pre-training objective is more compute-efficient and results in better performance on downstream tasks. It works well even when using relatively small amounts of computing, which the paper authors hope will make developing and applying pre-trained text encoders more accessible to researchers and practitioners with less access to computing resources", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["Secondly, we find that BERT performance is being slightly harmed from the pre-train fine-tune mismatch from [MASK] tokens, as Replace MLM slightly outperforms BERT. We note that BERT (including our implementation) already includes a trick to help with the pre-train/finetune discrepancy: masked tokens are replaced with a random token 10% of the time and are kept the\n7We also trained a discriminator that learns from a random 15% of the input tokens distinct from the subset that was originally masked out; this model performed slightly worse.\nsame 10% of the time. However, our results suggest these simple heuristics are insufficient to fully solve the issue. Lastly, we find that All-Tokens MLM, the generative model that makes predictions over all tokens instead of a subset, closes most of the gap between BERT and ELECTRA. In total, these results suggest a large amount of ELECTRA\u2019s improvement can be attributed to learning from all tokens and a smaller amount can be attributed to alleviating the pre-train fine-tune mismatch. The improvement of ELECTRA over All-Tokens MLM suggests that the ELECTRA\u2019s gains come from more than just faster training. We study this further by comparing BERT to ELECTRA for various model sizes (see Figure 4, left). We find that the gains from ELECTRA grow larger as the models get smaller. The small models are trained fully to convergence (see Figure 4, right), showing that ELECTRA achieves higher downstream accuracy than BERT when fully trained. We speculate that ELECTRA is more parameter-efficient than BERT because it does not have to model the full distribution of possible tokens at each position, but we believe more analysis is needed to completely explain ELECTRA\u2019s parameter efficiency. 4 RELATED WORK.", "As an alternative, we propose replaced token detection, a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements. Instead of masking, our method corrupts the input by replacing some tokens with samples from a proposal distribution, which is typically the output of a small masked language model. This corruption procedure solves a mismatch in BERT (although not in XLNet) where the network sees artificial [MASK] tokens during pre-training but not when being fine-tuned on downstream tasks. We then pre-train the network as a discriminator that predicts for every token whether it is an original or a replacement. In contrast, MLM trains the network as a generator that predicts the original identities of the corrupted tokens. A key advantage of our discriminative task is that the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient. Although our\nar X\niv :2\n00 3.\n10 55\n5v 1\n[ cs\n.C L\n] 2\n3 M\nar 2\napproach is reminiscent of training the discriminator of a GAN, our method is not adversarial in that the generator producing corrupted tokens is trained with maximum likelihood due to the difficulty of applying GANs to text (Caccia et al., 2018). We call our approach ELECTRA1 for \u201cEfficiently Learning an Encoder that Classifies Token Replacements Accurately.\u201d As in prior work, we apply it to pre-train Transformer text encoders (Vaswani et al., 2017) that can be fine-tuned on downstream tasks. Through a series of ablations, we show that learning from all input positions causes ELECTRA to train much faster than BERT. We also show ELECTRA achieves higher accuracy on downstream tasks when fully trained. Most current pre-training methods require large amounts of compute to be effective, raising concerns about their cost and accessibility.", "For SQuAD, we evaluate on versions 1.1, in which models select the span of text answering a question, and 2.0, in which some questions are unanswerable by the passage. We use the standard evaluation metrics of Exact-Match (EM) and F1 scores. For most experiments we pre-train on the same data as BERT, which consists of 3.3 Billion tokens from Wikipedia and BooksCorpus (Zhu et al., 2015). However, for our Large model we pre-trained on the data used for XLNet (Yang et al., 2019), which extends the BERT dataset to 33B tokens by including data from ClueWeb (Callan et al., 2009), CommonCrawl, and Gigaword (Parker et al., 2011). All of the pre-training and evaluation is on English data, although we think it would be interesting to apply our methods to multilingual data in the future. Our model architecture and most hyperparameters are the same as BERT\u2019s. For fine-tuning on GLUE, we add simple linear classifiers on top of ELECTRA. For SQuAD, we add the questionanswering module from XLNet on top of ELECTRA, which is slightly more sophisticated than BERT\u2019s in that it jointly rather than independently predicts the start and end positions and has a \u201canswerability\u201d classifier added for SQuAD 2.0. Some of our evaluation datasets are small, which means accuracies of fine-tuned models can vary substantially depending on the random seed. We therefore report the median of 10 fine-tuning runs from the same pre-trained checkpoint for each result. Unless stated otherwise, results are on the dev set. See the appendix for further training details and hyperparameter values.\n3.2 MODEL EXTENSIONS. We improve our method by proposing and evaluating several extensions to the model. Unless stated otherwise, these experiments use the same model size and training data as BERT-Base. Weight Sharing We propose improving the efficiency of the pre-training by sharing weights between the generator and discriminator. If the generator and discriminator are the same size, all of the transformer weights can be tied."]}
{"pkey": "electra_20", "question": "List the future work mentioned in the paper.", "answer": "Perhaps it is not too surprising that a model like BERT that is trained specifically for generation performs better at generation while a model with a discriminative objective like ELECTRA is better at being fine-tuned on discriminative tasks. The paper authors think comparisons of BERT\u2019s and ELECTRA\u2019s MLM predictions might be an interesting way to uncover more about the differences between ELECTRA and BERT encoders in future work. The paper authors also hope more future work on NLP pre-training will consider efficiency as well as absolute performance, and follow our effort in reporting compute usage and parameter counts along with evaluation metrics", "title": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators", "context": ["Compared to masked language modeling, our pre-training objective is more compute-efficient and results in better performance on downstream tasks. It works well even when using relatively small amounts of compute, which we hope will make developing and applying pre-trained text encoders more accessible to researchers and practitioners with less access to computing resources. We also hope more future work on NLP pre-training will consider efficiency as well as absolute performance, and follow our effort in reporting compute usage and parameter counts along with evaluation metrics.\nACKNOWLEDGEMENTS. We thank Allen Nie, Prajit Ramachandran, audiences at the CIFAR LMB meeting and U. de Montre\u0301al, and the anonymous reviewers for their thoughtful comments and suggestions. We thank Matt Peters for answering our questions about ELMo, Alec Radford for answers about GPT, Naman Goyal and Myle Ott for answers about RoBERTa, Zihang Dai for answers about XLNet, Zhenzhong Lan for answers about ALBERT, and Danqi Chen and Mandar Joshi for answers about SpanBERT. Kevin is supported by a Google PhD Fellowship. A PRE-TRAINING DETAILS. The following details apply to both our ELECTRA models and BERT baselines. We mostly use the same hyperparameters as BERT. We set \u03bb, the weight for the discriminator objective in the loss to 50.8 We use dynamic token masking with the masked positions decided on-the-fly instead of during preprocessing. Also, we did not use the next sentence prediction objective proposed in the original BERT paper, as recent work has suggested it does not improve scores (Yang et al., 2019; Liu et al., 2019). For our ELECTRA-Large model, we used a higher mask percent (25 instead of 15) because we noticed the generator was achieving high accuracy with 15% masking, resulting in very few replaced tokens. We searched for the best learning rate for the Base and Small models out of [1e-4, 2e-4, 3e-4, 5e-4] and selected \u03bb out of [1, 10, 20, 50, 100] in early experiments.", "We report results for ELECTRA-Base and ELECTRA-Small on the GLUE test set in Table 8. Furthermore, we push the limits of base-sized and small-sized models by training them on the XLNet data instead of wikibooks and for much longer (4e6 train steps); these models are called ELECTRA-Base++ and ELECTRA-Small++ in the table. For ELECTRA-Small++ we also increased the sequence length to 512; otherwise the hyperparameters are the same as the ones listed in Table 6. Lastly, the table contains results for ELECTRA-1.75M without the tricks described in Appendix B. Consistent with dev-set results in the paper, ELECTRA-Base outperforms BERT-Large while ELECTRA-Small outperforms GPT in terms of average score. Unsurprisingly, the ++ models perform even better. The small model scores are even close to TinyBERT (Jiao et al., 2019) and MobileBERT (Sun et al., 2019b). These models learn from BERT-Base using sophisticated distillation procedures. Our ELECTRA models, on the other hand, are trained from scratch. Given the success of distilling BERT, we believe it would be possible to build even stronger small pre-trained models by distilling ELECTRA. ELECTRA appears to be particularly effective at CoLA. In CoLA the goal is to distinguish linguistically acceptable sentences from ungrammatical ones, which fairly closely matches ELECTRA\u2019s pre-training task of identifying fake tokens, perhaps explaining ELECTRA\u2019s strength at the task. E COUNTING FLOPS. We chose to measure compute usage in terms of floating point operations (FLOPs) because it is a measure agnostic to the particular hardware, low-level optimizations, etc. However, it is worth noting that in some cases abstracting away hardware details is a drawback because hardware-centered optimizations can be key parts of a model\u2019s design, such as the speedup ALBERT (Lan et al., 2019) gets by tying weights and thus reducing communication overhead between TPU workers.", "Otherwise we did no hyperparameter tuning beyond the experiments in Section 3.2. The full set of hyperparameters are listed in Table 6.\nB FINE-TUNING DETAILS. For Large-sized models, we used the hyperparameters from Clark et al. (2019) for the most part. However, after noticing that RoBERTa (Liu et al., 2019) uses more training epochs (up to 10 rather than 3) we searched for the best number of train epochs out of [10, 3] for each task. For SQuAD, we decreased the number of train epochs to 2 to be consistent with BERT and RoBERTa. For Basesized models we searched for a learning rate out of [3e-5, 5e-5, 1e-4, 1.5e-4] and the layer-wise learning-rate decay out of [0.9, 0.8, 0.7], but otherwise used the same hyperparameters as for Large models. We found the small models benefit from a larger learning rate and searched for the best one out of [1e-4, 2e-4, 3e-4, 5e-3]. With the exception of number of train epochs, we used the same hyperparameters for all tasks. In contrast, previous research on GLUE such as BERT, XLNet, and RoBERTa separately searched for the best hyperparameters for each task. We expect our results would improve slightly if we performed the same sort of additional hyperparameter search. The full set of hyperparameters is listed in Table 7. Following BERT, we do not show results on the WNLI GLUE task for the dev set results, as it is difficult to beat even the majority classifier using a standard fine-tuning-as-classifier approach. For the GLUE test set results, we apply the standard tricks used by many of the GLUE leaderboard submissions including RoBERTa (Liu et al., 2019), XLNet (Yang et al., 2019), and ALBERT (Lan et al., 2019). Specifically:\n\u2022 For RTE and STS we use intermediate task training (Phang et al., 2018), starting from an ELECTRA checkpoint that has been fine-tuned on MNLI."]}
{"pkey": "muril_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. To address the aforementioned gaps, the paper authors propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["We present the sentiment predictions on a sample set of sentences in Figure 7. In the first example, \u201cIt\u2019s good that the account hasn\u2019t closed\u201d, we observe that the original Hindi sentence borrows an English word (account) and also contains a negation (not) word, but MuRIL correctly predicts it as expressing a positive statement. A similar observation can be made in the second example where MuRIL correctly predicts the sentiment of the transliterated sentence, \u201cRamu didn\u2019t let the film\u2019s pace slow down\u201d. Question Answering (QA): QA is the task of answering a question based on the given context or world knowledge. We show two context-question pairs, with their answers and predicted answers in Figure 8. In the first example, despite the fact that the word Greek is referred to by its Hindi translation in the context and its transliteration in the question (as highlighted), MuRIL correctly infers the answer from the context. In the second example, MuRIL understands that \u201cbank ki paribhasha\u201d (the definition of a bank), as a whole entity, is what differs across countries and not banks.", "Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\nMuRIL: Multilingual Representations for Indian Languages\nSimran Khanuja1 Diksha Bansal* 2 Sarvesh Mehtani* 3 Savya Khosla* 4 Atreyee Dey1\nBalaji Gopalan1 Dilip Kumar Margam1 Pooja Aggarwal1 Rajiv Teja Nagipogu1 Shachi Dave1\nShruti Gupta1 Subhash Chandra Bose Gali1 Vish Subramanian1 Partha Talukdar1\n1Google 2Indian Institute of Technology, Patna 3Indian Institute of Technology, Bombay 4Delhi Technological University\n1 Why MuRIL?. India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011)."]}
{"pkey": "muril_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\nMuRIL: Multilingual Representations for Indian Languages\nSimran Khanuja1 Diksha Bansal* 2 Sarvesh Mehtani* 3 Savya Khosla* 4 Atreyee Dey1\nBalaji Gopalan1 Dilip Kumar Margam1 Pooja Aggarwal1 Rajiv Teja Nagipogu1 Shachi Dave1\nShruti Gupta1 Subhash Chandra Bose Gali1 Vish Subramanian1 Partha Talukdar1\n1Google 2Indian Institute of Technology, Patna 3Indian Institute of Technology, Bombay 4Delhi Technological University\n1 Why MuRIL?. India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011).", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\n* Work done during a summer internship at Google India. Correspondence to the MuRIL Team (muril-contact@google.com)\ntranslated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native \u2192 Latin) test sets of the chosen datasets, and demonstrate the efficacy of MuRIL in handling transliterated data. 2 Model and Data.\nMuRIL currently supports 17 languages for which monolingual data is publicly available. These are further grouped into 16 IN languages and English (en)."]}
{"pkey": "muril_3", "question": "What are the main contributions of the paper?", "answer": "The paper authors collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus and Wikipedia. The paper authors have two sources of translated data. First, the paper authors use the PMINDIA (Haddow and Kirefu, 2020) parallel corpus containing sentence pairs for 8 IN languages (bn, gu, hi, kn, ml, mr, ta, te). Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA).", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We present the sentiment predictions on a sample set of sentences in Figure 7. In the first example, \u201cIt\u2019s good that the account hasn\u2019t closed\u201d, we observe that the original Hindi sentence borrows an English word (account) and also contains a negation (not) word, but MuRIL correctly predicts it as expressing a positive statement. A similar observation can be made in the second example where MuRIL correctly predicts the sentiment of the transliterated sentence, \u201cRamu didn\u2019t let the film\u2019s pace slow down\u201d. Question Answering (QA): QA is the task of answering a question based on the given context or world knowledge. We show two context-question pairs, with their answers and predicted answers in Figure 8. In the first example, despite the fact that the word Greek is referred to by its Hindi translation in the context and its transliteration in the question (as highlighted), MuRIL correctly infers the answer from the context. In the second example, MuRIL understands that \u201cbank ki paribhasha\u201d (the definition of a bank), as a whole entity, is what differs across countries and not banks.", "We would also like to thank Hyung Won Chung, Anosh Raj, Yinfei Yang and Fangxiaoyu Feng for contributing to our discussions around MuRIL. Finally, we would like to thank Nick Doiron for his feedback on the HuggingFace implementation of the model. A Pre-training Data Statistics. The upsampled token counts for each language and corpus are reported in Table 4.\nB Detailed Results. We report per language results for each XTREME (IN) dataset in Tables 5 (PANX), 6 (UDPOS), 7 (XNLI), 8 (Tatoeba), 9 (XQuAD, MLQA) and 10 (TyDiQA-GoldP). The detailed results for transliterated test sets are shown in Tables 11 (PANX), 12 (UDPOS), 13 (XNLI), 14 (Tatoeba). C Analysis. In this section, we analyse the predictions of mBERT and MuRIL on a random sample of test examples. Named Entity Recognition (NER): NER is the task of locating and classifying entities in unstructured text into pre-defined categories such as person, location, organization etc. In Figure 6, we present entity predictions of mBERT and MuRIL on a random sample of test examples. In the first example, we observe that Atlanta Falcons, a football team, is predicted as ORG (Organisation) by MuRIL but LOC (Location) by mBERT, probably looking at the word Atlanta without context. In the second example, MuRIL correctly takes the context into account and predicts Shirdi\u2019s Sai Baba as PER (Person), whereas mBERT resorts to predicting LOC taking a cue from the word Shirdi. A similar pattern is observed in other examples like Nepal\u2019s Prime Minister, the President of America etc. In the third example, the Bajirao Mastani movie is being spoken about, specified with the word (film) in parentheses, which MuRIL correctly captures. In the last example, we observe that MuRIL can correctly classify misspelled words (vimbledon)\nutilising the context. Sentiment Analysis: Sentiment analysis is a sentence classification task wherein each sentence is labeled to be expressing a positive, negative or neutral sentiment."]}
{"pkey": "muril_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "The paper authors analyse the predictions of mBERT and MuRIL on a random sample of test examples, for Named Entity Recognition (NER), Sentiment Analysis, Question Answering (QA). The paper authors sincerely hope MuRIL aids in building better technologies and applications for Indian languages.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["We would also like to thank Hyung Won Chung, Anosh Raj, Yinfei Yang and Fangxiaoyu Feng for contributing to our discussions around MuRIL. Finally, we would like to thank Nick Doiron for his feedback on the HuggingFace implementation of the model. A Pre-training Data Statistics. The upsampled token counts for each language and corpus are reported in Table 4.\nB Detailed Results. We report per language results for each XTREME (IN) dataset in Tables 5 (PANX), 6 (UDPOS), 7 (XNLI), 8 (Tatoeba), 9 (XQuAD, MLQA) and 10 (TyDiQA-GoldP). The detailed results for transliterated test sets are shown in Tables 11 (PANX), 12 (UDPOS), 13 (XNLI), 14 (Tatoeba). C Analysis. In this section, we analyse the predictions of mBERT and MuRIL on a random sample of test examples. Named Entity Recognition (NER): NER is the task of locating and classifying entities in unstructured text into pre-defined categories such as person, location, organization etc. In Figure 6, we present entity predictions of mBERT and MuRIL on a random sample of test examples. In the first example, we observe that Atlanta Falcons, a football team, is predicted as ORG (Organisation) by MuRIL but LOC (Location) by mBERT, probably looking at the word Atlanta without context. In the second example, MuRIL correctly takes the context into account and predicts Shirdi\u2019s Sai Baba as PER (Person), whereas mBERT resorts to predicting LOC taking a cue from the word Shirdi. A similar pattern is observed in other examples like Nepal\u2019s Prime Minister, the President of America etc. In the third example, the Bajirao Mastani movie is being spoken about, specified with the word (film) in parentheses, which MuRIL correctly captures. In the last example, we observe that MuRIL can correctly classify misspelled words (vimbledon)\nutilising the context. Sentiment Analysis: Sentiment analysis is a sentence classification task wherein each sentence is labeled to be expressing a positive, negative or neutral sentiment.", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\nMuRIL: Multilingual Representations for Indian Languages\nSimran Khanuja1 Diksha Bansal* 2 Sarvesh Mehtani* 3 Savya Khosla* 4 Atreyee Dey1\nBalaji Gopalan1 Dilip Kumar Margam1 Pooja Aggarwal1 Rajiv Teja Nagipogu1 Shachi Dave1\nShruti Gupta1 Subhash Chandra Bose Gali1 Vish Subramanian1 Partha Talukdar1\n1Google 2Indian Institute of Technology, Patna 3Indian Institute of Technology, Bombay 4Delhi Technological University\n1 Why MuRIL?. India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011).", "Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper."]}
{"pkey": "muril_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "Table 1 in the paper shows all the datasets on which MuRIL was evaluated. The table contains these datasets: PANX,UDPOS, XNLI, Tatoeba,,XQuAD, MLQA, TyDiQA-GoldP", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "We present the sentiment predictions on a sample set of sentences in Figure 7. In the first example, \u201cIt\u2019s good that the account hasn\u2019t closed\u201d, we observe that the original Hindi sentence borrows an English word (account) and also contains a negation (not) word, but MuRIL correctly predicts it as expressing a positive statement. A similar observation can be made in the second example where MuRIL correctly predicts the sentiment of the transliterated sentence, \u201cRamu didn\u2019t let the film\u2019s pace slow down\u201d. Question Answering (QA): QA is the task of answering a question based on the given context or world knowledge. We show two context-question pairs, with their answers and predicted answers in Figure 8. In the first example, despite the fact that the word Greek is referred to by its Hindi translation in the context and its transliteration in the question (as highlighted), MuRIL correctly infers the answer from the context. In the second example, MuRIL understands that \u201cbank ki paribhasha\u201d (the definition of a bank), as a whole entity, is what differs across countries and not banks."]}
{"pkey": "muril_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not Specified in paper", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "The IN languages include: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Kashmiri (ks), Malayalam (ml), Marathi (mr), Nepali (ne), Oriya (or), Punjabi (pa), Sanskrit (sa), Sindhi (sd), Tamil (ta), Telugu (te) and Urdu (ur). ar X\niv :2\n10 3.\n10 73\n0v 2\n[ cs\n.C L\n] 2\nA pr\n2 02\n1\nWe train our model with two language modeling objectives. The first is the conventional Masked Language Modeling (MLM) objective (Taylor, 1953) that leverages monolingual text data only (unsupervised). The second is the Translation Language Modeling (TLM) objective (Lample and Conneau, 2019) that leverages parallel data (supervised). We use monolingual documents to train the model with MLM, and both translated and transliterated document pairs to train the model with TLM. Monolingual Data: We collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus1 and Wikipedia2. Translated Data: We have two sources of translated data. First, we use the PMINDIA (Haddow and Kirefu, 2020) parallel corpus containing sentence pairs for 8 IN languages (bn, gu, hi, kn, ml, mr, ta, te). Each pair comprises of a sentence in a native language and its English translation. Second, we translate the aforementioned monolingual corpora (both Common Crawl and Wikipedia) to English, using an in-house translation system. The source and translated documents are used as parallel instances to train the model. Note that we translate corpora of all IN languages excluding as, ks and sa, for which the current translation system lacks support. Transliterated Data: We have two sources of 1https://oscar-corpus.com 2https://www.tensorflow.org/datasets/ catalog/wikipedia\ntransliterated data as well. First, we use the Dakshina Dataset (Roark et al., 2020) that contains 10,000 sentence pairs for 12 IN languages (bn, gu, hi, kn, ml, mr, pa, ta, te, ur). Each pair is a native script sentence and its manually romanized transliteration.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1."]}
{"pkey": "muril_7", "question": "List the limitations of the model discussed in the paper.", "answer": "This model is intended to be used for a variety of downstream NLP tasks for Indian languages. This model is trained on transliterated data as well, a phenomomenon commonly observed in the Indian context. This model is not expected to perform well on languages other than the ones used in pretraining, i.e. 17 Indian languages.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We would also like to thank Hyung Won Chung, Anosh Raj, Yinfei Yang and Fangxiaoyu Feng for contributing to our discussions around MuRIL. Finally, we would like to thank Nick Doiron for his feedback on the HuggingFace implementation of the model. A Pre-training Data Statistics. The upsampled token counts for each language and corpus are reported in Table 4.\nB Detailed Results. We report per language results for each XTREME (IN) dataset in Tables 5 (PANX), 6 (UDPOS), 7 (XNLI), 8 (Tatoeba), 9 (XQuAD, MLQA) and 10 (TyDiQA-GoldP). The detailed results for transliterated test sets are shown in Tables 11 (PANX), 12 (UDPOS), 13 (XNLI), 14 (Tatoeba). C Analysis. In this section, we analyse the predictions of mBERT and MuRIL on a random sample of test examples. Named Entity Recognition (NER): NER is the task of locating and classifying entities in unstructured text into pre-defined categories such as person, location, organization etc. In Figure 6, we present entity predictions of mBERT and MuRIL on a random sample of test examples. In the first example, we observe that Atlanta Falcons, a football team, is predicted as ORG (Organisation) by MuRIL but LOC (Location) by mBERT, probably looking at the word Atlanta without context. In the second example, MuRIL correctly takes the context into account and predicts Shirdi\u2019s Sai Baba as PER (Person), whereas mBERT resorts to predicting LOC taking a cue from the word Shirdi. A similar pattern is observed in other examples like Nepal\u2019s Prime Minister, the President of America etc. In the third example, the Bajirao Mastani movie is being spoken about, specified with the word (film) in parentheses, which MuRIL correctly captures. In the last example, we observe that MuRIL can correctly classify misspelled words (vimbledon)\nutilising the context. Sentiment Analysis: Sentiment analysis is a sentence classification task wherein each sentence is labeled to be expressing a positive, negative or neutral sentiment.", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\n* Work done during a summer internship at Google India. Correspondence to the MuRIL Team (muril-contact@google.com)\ntranslated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native \u2192 Latin) test sets of the chosen datasets, and demonstrate the efficacy of MuRIL in handling transliterated data. 2 Model and Data.\nMuRIL currently supports 17 languages for which monolingual data is publicly available. These are further grouped into 16 IN languages and English (en)."]}
{"pkey": "muril_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "Monolingual Data: The paper authors collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus and Wikipedia. The paper authors have two sources of translated data. First, the paper authors use the PMINDIA (Haddow and Kirefu, 2020) parallel corpus containing sentence pairs for 8 IN languages (bn, gu, hi, kn, ml, mr, ta, te).  Transliterated Data: The paper authors have two sources of transliterated data as well. First, the paper authors use the Dakshina Dataset (Roark et al., 2020) that contains 10,000 sentence pairs for 12 IN languages (bn, gu, hi, kn, ml, mr, pa, ta, te, ur).", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\n* Work done during a summer internship at Google India. Correspondence to the MuRIL Team (muril-contact@google.com)\ntranslated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native \u2192 Latin) test sets of the chosen datasets, and demonstrate the efficacy of MuRIL in handling transliterated data. 2 Model and Data.\nMuRIL currently supports 17 languages for which monolingual data is publicly available. These are further grouped into 16 IN languages and English (en).", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "Second, we use the indic-trans library (Bhat et al., 2015) to transliterate Wikipedia corpora of all IN languages to Latin (except ks, sa and sd, for which the library doesn\u2019t have support). The source document and its Latin transliteration are used as parallel instances to train the model. Upsampling: In the corpora collected above, the percentage of tokens per language is highly uneven in its distribution. Hence, data smoothing is essential so that all languages have their representation reflect their usage in the real world. To achieve this, we upsample monolingual Wikipedia corpora of each language according to its multiplier value given by:\nmi =\n(max j\u2208L nj\nni\n)(1\u2212\u03b1) (1) In the above equation, mi represents the multiplier value for language i, ni is its original token count, L represents the set of all 17 languages and \u03b1 is a hyperparameter whose value is set to 0.3, following Conneau et al. (2020). Hence, the upsampled token count for language i is mi \u2217 ni. The final data distribution after upsampling is shown in Figure 2. The upsampled token counts for each language and corpus are reported in Appendix A.\nVocabulary: We learn a cased WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016) vocabulary from the upsampled pre-training data using the wordpiece vocabulary generation\nlibrary from Tensorflow Text3. Since our data is upsampled, we set the language smoothing exponent from the vocabulary generation tool to 1, and the rest of the parameters are set to their default value. The final vocabulary size is 197,285. Figure 3 shows a few common IN language words tokenized using mBERT and MuRIL vocabularies. We also plot the fertility ratio (average number of sub-words/word) of mBERT and MuRIL tokenizers on a random sample of text from our training data in Figure 5. Here, a higher fertility ratio equates to a larger number of sub-words per word, eventually leading to a loss in preservation of semantic meaning."]}
{"pkey": "muril_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Vocabulary: The paper authors learn a cased WordPiece vocabulary (Schuster and Nakajima, 2012; Wu et al., 2016) from the upsampled pre-training data using the wordpiece vocabulary generation library from TensorFlow Text. Since our data is upsampled, the paper authors set the language smoothing exponent from the vocabulary generation tool to 1, and the rest of the parameters are set to their default value. The final vocabulary size is 197,285. Figure 3 shows a few common IN language words tokenized using mBERT and MuRIL vocabularies.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Second, we use the indic-trans library (Bhat et al., 2015) to transliterate Wikipedia corpora of all IN languages to Latin (except ks, sa and sd, for which the library doesn\u2019t have support). The source document and its Latin transliteration are used as parallel instances to train the model. Upsampling: In the corpora collected above, the percentage of tokens per language is highly uneven in its distribution. Hence, data smoothing is essential so that all languages have their representation reflect their usage in the real world. To achieve this, we upsample monolingual Wikipedia corpora of each language according to its multiplier value given by:\nmi =\n(max j\u2208L nj\nni\n)(1\u2212\u03b1) (1) In the above equation, mi represents the multiplier value for language i, ni is its original token count, L represents the set of all 17 languages and \u03b1 is a hyperparameter whose value is set to 0.3, following Conneau et al. (2020). Hence, the upsampled token count for language i is mi \u2217 ni. The final data distribution after upsampling is shown in Figure 2. The upsampled token counts for each language and corpus are reported in Appendix A.\nVocabulary: We learn a cased WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016) vocabulary from the upsampled pre-training data using the wordpiece vocabulary generation\nlibrary from Tensorflow Text3. Since our data is upsampled, we set the language smoothing exponent from the vocabulary generation tool to 1, and the rest of the parameters are set to their default value. The final vocabulary size is 197,285. Figure 3 shows a few common IN language words tokenized using mBERT and MuRIL vocabularies. We also plot the fertility ratio (average number of sub-words/word) of mBERT and MuRIL tokenizers on a random sample of text from our training data in Figure 5. Here, a higher fertility ratio equates to a larger number of sub-words per word, eventually leading to a loss in preservation of semantic meaning.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "We would also like to thank Hyung Won Chung, Anosh Raj, Yinfei Yang and Fangxiaoyu Feng for contributing to our discussions around MuRIL. Finally, we would like to thank Nick Doiron for his feedback on the HuggingFace implementation of the model. A Pre-training Data Statistics. The upsampled token counts for each language and corpus are reported in Table 4.\nB Detailed Results. We report per language results for each XTREME (IN) dataset in Tables 5 (PANX), 6 (UDPOS), 7 (XNLI), 8 (Tatoeba), 9 (XQuAD, MLQA) and 10 (TyDiQA-GoldP). The detailed results for transliterated test sets are shown in Tables 11 (PANX), 12 (UDPOS), 13 (XNLI), 14 (Tatoeba). C Analysis. In this section, we analyse the predictions of mBERT and MuRIL on a random sample of test examples. Named Entity Recognition (NER): NER is the task of locating and classifying entities in unstructured text into pre-defined categories such as person, location, organization etc. In Figure 6, we present entity predictions of mBERT and MuRIL on a random sample of test examples. In the first example, we observe that Atlanta Falcons, a football team, is predicted as ORG (Organisation) by MuRIL but LOC (Location) by mBERT, probably looking at the word Atlanta without context. In the second example, MuRIL correctly takes the context into account and predicts Shirdi\u2019s Sai Baba as PER (Person), whereas mBERT resorts to predicting LOC taking a cue from the word Shirdi. A similar pattern is observed in other examples like Nepal\u2019s Prime Minister, the President of America etc. In the third example, the Bajirao Mastani movie is being spoken about, specified with the word (film) in parentheses, which MuRIL correctly captures. In the last example, we observe that MuRIL can correctly classify misspelled words (vimbledon)\nutilising the context. Sentiment Analysis: Sentiment analysis is a sentence classification task wherein each sentence is labeled to be expressing a positive, negative or neutral sentiment."]}
{"pkey": "muril_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "Upsampling: In the corpora collected above, the percentage of tokens per language is highly uneven in its distribution. Hence, data smoothing is essential so that all languages have their representation reflect their usage in the real world. The paper authors learn a cased WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016) vocabulary from the upsampled pre-training data using the wordpiece vocabulary generation library from Tensorflow Text.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\n* Work done during a summer internship at Google India. Correspondence to the MuRIL Team (muril-contact@google.com)\ntranslated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native \u2192 Latin) test sets of the chosen datasets, and demonstrate the efficacy of MuRIL in handling transliterated data. 2 Model and Data.\nMuRIL currently supports 17 languages for which monolingual data is publicly available. These are further grouped into 16 IN languages and English (en).", "Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "The IN languages include: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Kashmiri (ks), Malayalam (ml), Marathi (mr), Nepali (ne), Oriya (or), Punjabi (pa), Sanskrit (sa), Sindhi (sd), Tamil (ta), Telugu (te) and Urdu (ur). ar X\niv :2\n10 3.\n10 73\n0v 2\n[ cs\n.C L\n] 2\nA pr\n2 02\n1\nWe train our model with two language modeling objectives. The first is the conventional Masked Language Modeling (MLM) objective (Taylor, 1953) that leverages monolingual text data only (unsupervised). The second is the Translation Language Modeling (TLM) objective (Lample and Conneau, 2019) that leverages parallel data (supervised). We use monolingual documents to train the model with MLM, and both translated and transliterated document pairs to train the model with TLM. Monolingual Data: We collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus1 and Wikipedia2. Translated Data: We have two sources of translated data. First, we use the PMINDIA (Haddow and Kirefu, 2020) parallel corpus containing sentence pairs for 8 IN languages (bn, gu, hi, kn, ml, mr, ta, te). Each pair comprises of a sentence in a native language and its English translation. Second, we translate the aforementioned monolingual corpora (both Common Crawl and Wikipedia) to English, using an in-house translation system. The source and translated documents are used as parallel instances to train the model. Note that we translate corpora of all IN languages excluding as, ks and sa, for which the current translation system lacks support. Transliterated Data: We have two sources of 1https://oscar-corpus.com 2https://www.tensorflow.org/datasets/ catalog/wikipedia\ntransliterated data as well. First, we use the Dakshina Dataset (Roark et al., 2020) that contains 10,000 sentence pairs for 12 IN languages (bn, gu, hi, kn, ml, mr, pa, ta, te, ur). Each pair is a native script sentence and its manually romanized transliteration."]}
{"pkey": "muril_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "Pre-training Details: The paper authors pre-train a BERT base encoder model using the MLM and TLM objectives. The paper authors keep a maximum sequence length of 512, a global batch size of 4096, and train for 1 million steps (with 50,000 warm-up steps and a linear decay after). The paper authors use the AdamW optimizer with a learning rate of 5e-4. Our final model has 236 million parameters, is trained on approximately 16 billion unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Second, we use the indic-trans library (Bhat et al., 2015) to transliterate Wikipedia corpora of all IN languages to Latin (except ks, sa and sd, for which the library doesn\u2019t have support). The source document and its Latin transliteration are used as parallel instances to train the model. Upsampling: In the corpora collected above, the percentage of tokens per language is highly uneven in its distribution. Hence, data smoothing is essential so that all languages have their representation reflect their usage in the real world. To achieve this, we upsample monolingual Wikipedia corpora of each language according to its multiplier value given by:\nmi =\n(max j\u2208L nj\nni\n)(1\u2212\u03b1) (1) In the above equation, mi represents the multiplier value for language i, ni is its original token count, L represents the set of all 17 languages and \u03b1 is a hyperparameter whose value is set to 0.3, following Conneau et al. (2020). Hence, the upsampled token count for language i is mi \u2217 ni. The final data distribution after upsampling is shown in Figure 2. The upsampled token counts for each language and corpus are reported in Appendix A.\nVocabulary: We learn a cased WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016) vocabulary from the upsampled pre-training data using the wordpiece vocabulary generation\nlibrary from Tensorflow Text3. Since our data is upsampled, we set the language smoothing exponent from the vocabulary generation tool to 1, and the rest of the parameters are set to their default value. The final vocabulary size is 197,285. Figure 3 shows a few common IN language words tokenized using mBERT and MuRIL vocabularies. We also plot the fertility ratio (average number of sub-words/word) of mBERT and MuRIL tokenizers on a random sample of text from our training data in Figure 5. Here, a higher fertility ratio equates to a larger number of sub-words per word, eventually leading to a loss in preservation of semantic meaning.", "Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1."]}
{"pkey": "muril_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "Pre-training Details: The paper authors pre-train a BERT base encoder model using the MLM and TLM objectives. The paper authors maintain a maximum sequence length of 512, a global batch size of 4096, and train for 1 million steps (with 50,000 warm-up steps and a linear decay afterward). The paper authors utilize the AdamW optimizer with a learning rate of 5e-4.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "We would also like to thank Hyung Won Chung, Anosh Raj, Yinfei Yang and Fangxiaoyu Feng for contributing to our discussions around MuRIL. Finally, we would like to thank Nick Doiron for his feedback on the HuggingFace implementation of the model. A Pre-training Data Statistics. The upsampled token counts for each language and corpus are reported in Table 4.\nB Detailed Results. We report per language results for each XTREME (IN) dataset in Tables 5 (PANX), 6 (UDPOS), 7 (XNLI), 8 (Tatoeba), 9 (XQuAD, MLQA) and 10 (TyDiQA-GoldP). The detailed results for transliterated test sets are shown in Tables 11 (PANX), 12 (UDPOS), 13 (XNLI), 14 (Tatoeba). C Analysis. In this section, we analyse the predictions of mBERT and MuRIL on a random sample of test examples. Named Entity Recognition (NER): NER is the task of locating and classifying entities in unstructured text into pre-defined categories such as person, location, organization etc. In Figure 6, we present entity predictions of mBERT and MuRIL on a random sample of test examples. In the first example, we observe that Atlanta Falcons, a football team, is predicted as ORG (Organisation) by MuRIL but LOC (Location) by mBERT, probably looking at the word Atlanta without context. In the second example, MuRIL correctly takes the context into account and predicts Shirdi\u2019s Sai Baba as PER (Person), whereas mBERT resorts to predicting LOC taking a cue from the word Shirdi. A similar pattern is observed in other examples like Nepal\u2019s Prime Minister, the President of America etc. In the third example, the Bajirao Mastani movie is being spoken about, specified with the word (film) in parentheses, which MuRIL correctly captures. In the last example, we observe that MuRIL can correctly classify misspelled words (vimbledon)\nutilising the context. Sentiment Analysis: Sentiment analysis is a sentence classification task wherein each sentence is labeled to be expressing a positive, negative or neutral sentiment.", "Second, we use the indic-trans library (Bhat et al., 2015) to transliterate Wikipedia corpora of all IN languages to Latin (except ks, sa and sd, for which the library doesn\u2019t have support). The source document and its Latin transliteration are used as parallel instances to train the model. Upsampling: In the corpora collected above, the percentage of tokens per language is highly uneven in its distribution. Hence, data smoothing is essential so that all languages have their representation reflect their usage in the real world. To achieve this, we upsample monolingual Wikipedia corpora of each language according to its multiplier value given by:\nmi =\n(max j\u2208L nj\nni\n)(1\u2212\u03b1) (1) In the above equation, mi represents the multiplier value for language i, ni is its original token count, L represents the set of all 17 languages and \u03b1 is a hyperparameter whose value is set to 0.3, following Conneau et al. (2020). Hence, the upsampled token count for language i is mi \u2217 ni. The final data distribution after upsampling is shown in Figure 2. The upsampled token counts for each language and corpus are reported in Appendix A.\nVocabulary: We learn a cased WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016) vocabulary from the upsampled pre-training data using the wordpiece vocabulary generation\nlibrary from Tensorflow Text3. Since our data is upsampled, we set the language smoothing exponent from the vocabulary generation tool to 1, and the rest of the parameters are set to their default value. The final vocabulary size is 197,285. Figure 3 shows a few common IN language words tokenized using mBERT and MuRIL vocabularies. We also plot the fertility ratio (average number of sub-words/word) of mBERT and MuRIL tokenizers on a random sample of text from our training data in Figure 5. Here, a higher fertility ratio equates to a larger number of sub-words per word, eventually leading to a loss in preservation of semantic meaning."]}
{"pkey": "muril_13", "question": "Describe the computational resources used to train the model.", "answer": "Computational resources are not specified in the paper.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "The IN languages include: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Kashmiri (ks), Malayalam (ml), Marathi (mr), Nepali (ne), Oriya (or), Punjabi (pa), Sanskrit (sa), Sindhi (sd), Tamil (ta), Telugu (te) and Urdu (ur). ar X\niv :2\n10 3.\n10 73\n0v 2\n[ cs\n.C L\n] 2\nA pr\n2 02\n1\nWe train our model with two language modeling objectives. The first is the conventional Masked Language Modeling (MLM) objective (Taylor, 1953) that leverages monolingual text data only (unsupervised). The second is the Translation Language Modeling (TLM) objective (Lample and Conneau, 2019) that leverages parallel data (supervised). We use monolingual documents to train the model with MLM, and both translated and transliterated document pairs to train the model with TLM. Monolingual Data: We collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus1 and Wikipedia2. Translated Data: We have two sources of translated data. First, we use the PMINDIA (Haddow and Kirefu, 2020) parallel corpus containing sentence pairs for 8 IN languages (bn, gu, hi, kn, ml, mr, ta, te). Each pair comprises of a sentence in a native language and its English translation. Second, we translate the aforementioned monolingual corpora (both Common Crawl and Wikipedia) to English, using an in-house translation system. The source and translated documents are used as parallel instances to train the model. Note that we translate corpora of all IN languages excluding as, ks and sa, for which the current translation system lacks support. Transliterated Data: We have two sources of 1https://oscar-corpus.com 2https://www.tensorflow.org/datasets/ catalog/wikipedia\ntransliterated data as well. First, we use the Dakshina Dataset (Roark et al., 2020) that contains 10,000 sentence pairs for 12 IN languages (bn, gu, hi, kn, ml, mr, pa, ta, te, ur). Each pair is a native script sentence and its manually romanized transliteration."]}
{"pkey": "muril_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "The paper authors have released the MuRIL encoder on TFHub with detailed usage instructions. Additionally, the paper authors have released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Furthermore, the paper authors have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace. The paper authors sincerely hope MuRIL aids in building better technologies and applications for Indian languages.\n1 https://tfhub.dev/google/MuRIL/1\n2 https://huggingface.co/google/", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\nMuRIL: Multilingual Representations for Indian Languages\nSimran Khanuja1 Diksha Bansal* 2 Sarvesh Mehtani* 3 Savya Khosla* 4 Atreyee Dey1\nBalaji Gopalan1 Dilip Kumar Margam1 Pooja Aggarwal1 Rajiv Teja Nagipogu1 Shachi Dave1\nShruti Gupta1 Subhash Chandra Bose Gali1 Vish Subramanian1 Partha Talukdar1\n1Google 2Indian Institute of Technology, Patna 3Indian Institute of Technology, Bombay 4Delhi Technological University\n1 Why MuRIL?. India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011)."]}
{"pkey": "muril_15", "question": "What is the pretraining objective of the model? ", "answer": "The paper authors train our model with two language modeling objectives. The first is the conventional Masked Language Modeling (MLM) objective (Taylor, 1953) that leverages monolingual text data only (unsupervised). The second is the Translation Language Modeling (TLM) objective (Lample and Conneau, 2019) that leverages parallel data (supervised). The paper authors use monolingual documents to train the model with MLM, and both translated and transliterated document pairs to train the model with TLM.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["The IN languages include: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Kashmiri (ks), Malayalam (ml), Marathi (mr), Nepali (ne), Oriya (or), Punjabi (pa), Sanskrit (sa), Sindhi (sd), Tamil (ta), Telugu (te) and Urdu (ur). ar X\niv :2\n10 3.\n10 73\n0v 2\n[ cs\n.C L\n] 2\nA pr\n2 02\n1\nWe train our model with two language modeling objectives. The first is the conventional Masked Language Modeling (MLM) objective (Taylor, 1953) that leverages monolingual text data only (unsupervised). The second is the Translation Language Modeling (TLM) objective (Lample and Conneau, 2019) that leverages parallel data (supervised). We use monolingual documents to train the model with MLM, and both translated and transliterated document pairs to train the model with TLM. Monolingual Data: We collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus1 and Wikipedia2. Translated Data: We have two sources of translated data. First, we use the PMINDIA (Haddow and Kirefu, 2020) parallel corpus containing sentence pairs for 8 IN languages (bn, gu, hi, kn, ml, mr, ta, te). Each pair comprises of a sentence in a native language and its English translation. Second, we translate the aforementioned monolingual corpora (both Common Crawl and Wikipedia) to English, using an in-house translation system. The source and translated documents are used as parallel instances to train the model. Note that we translate corpora of all IN languages excluding as, ks and sa, for which the current translation system lacks support. Transliterated Data: We have two sources of 1https://oscar-corpus.com 2https://www.tensorflow.org/datasets/ catalog/wikipedia\ntransliterated data as well. First, we use the Dakshina Dataset (Roark et al., 2020) that contains 10,000 sentence pairs for 12 IN languages (bn, gu, hi, kn, ml, mr, pa, ta, te, ur). Each pair is a native script sentence and its manually romanized transliteration.", "We present the sentiment predictions on a sample set of sentences in Figure 7. In the first example, \u201cIt\u2019s good that the account hasn\u2019t closed\u201d, we observe that the original Hindi sentence borrows an English word (account) and also contains a negation (not) word, but MuRIL correctly predicts it as expressing a positive statement. A similar observation can be made in the second example where MuRIL correctly predicts the sentiment of the transliterated sentence, \u201cRamu didn\u2019t let the film\u2019s pace slow down\u201d. Question Answering (QA): QA is the task of answering a question based on the given context or world knowledge. We show two context-question pairs, with their answers and predicted answers in Figure 8. In the first example, despite the fact that the word Greek is referred to by its Hindi translation in the context and its transliteration in the question (as highlighted), MuRIL correctly infers the answer from the context. In the second example, MuRIL understands that \u201cbank ki paribhasha\u201d (the definition of a bank), as a whole entity, is what differs across countries and not banks.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1."]}
{"pkey": "muril_16", "question": "What is the loss function that is used to train the model?", "answer": "For MLM objective, the conventional cross entropy loss is used. TLM objective also uses cross entropy loss as it eventually involves predicting tokens.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Second, we use the indic-trans library (Bhat et al., 2015) to transliterate Wikipedia corpora of all IN languages to Latin (except ks, sa and sd, for which the library doesn\u2019t have support). The source document and its Latin transliteration are used as parallel instances to train the model. Upsampling: In the corpora collected above, the percentage of tokens per language is highly uneven in its distribution. Hence, data smoothing is essential so that all languages have their representation reflect their usage in the real world. To achieve this, we upsample monolingual Wikipedia corpora of each language according to its multiplier value given by:\nmi =\n(max j\u2208L nj\nni\n)(1\u2212\u03b1) (1) In the above equation, mi represents the multiplier value for language i, ni is its original token count, L represents the set of all 17 languages and \u03b1 is a hyperparameter whose value is set to 0.3, following Conneau et al. (2020). Hence, the upsampled token count for language i is mi \u2217 ni. The final data distribution after upsampling is shown in Figure 2. The upsampled token counts for each language and corpus are reported in Appendix A.\nVocabulary: We learn a cased WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016) vocabulary from the upsampled pre-training data using the wordpiece vocabulary generation\nlibrary from Tensorflow Text3. Since our data is upsampled, we set the language smoothing exponent from the vocabulary generation tool to 1, and the rest of the parameters are set to their default value. The final vocabulary size is 197,285. Figure 3 shows a few common IN language words tokenized using mBERT and MuRIL vocabularies. We also plot the fertility ratio (average number of sub-words/word) of mBERT and MuRIL tokenizers on a random sample of text from our training data in Figure 5. Here, a higher fertility ratio equates to a larger number of sub-words per word, eventually leading to a loss in preservation of semantic meaning.", "Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We present the sentiment predictions on a sample set of sentences in Figure 7. In the first example, \u201cIt\u2019s good that the account hasn\u2019t closed\u201d, we observe that the original Hindi sentence borrows an English word (account) and also contains a negation (not) word, but MuRIL correctly predicts it as expressing a positive statement. A similar observation can be made in the second example where MuRIL correctly predicts the sentiment of the transliterated sentence, \u201cRamu didn\u2019t let the film\u2019s pace slow down\u201d. Question Answering (QA): QA is the task of answering a question based on the given context or world knowledge. We show two context-question pairs, with their answers and predicted answers in Figure 8. In the first example, despite the fact that the word Greek is referred to by its Hindi translation in the context and its transliteration in the question (as highlighted), MuRIL correctly infers the answer from the context. In the second example, MuRIL understands that \u201cbank ki paribhasha\u201d (the definition of a bank), as a whole entity, is what differs across countries and not banks."]}
{"pkey": "muril_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "The paper authors pre-train a BERT-base encoder model making use of the MLM and TLM objectives specifically for Indian languages.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\n* Work done during a summer internship at Google India. Correspondence to the MuRIL Team (muril-contact@google.com)\ntranslated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native \u2192 Latin) test sets of the chosen datasets, and demonstrate the efficacy of MuRIL in handling transliterated data. 2 Model and Data.\nMuRIL currently supports 17 languages for which monolingual data is publicly available. These are further grouped into 16 IN languages and English (en)."]}
{"pkey": "muril_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors report per language results for each XTREME (IN) dataset in Tables 5 (PANX), 6 (UDPOS), 7 (XNLI), 8 (Tatoeba), 9 (XQuAD, MLQA) and 10 (TyDiQA-GoldP). The detailed results for transliterated test sets are shown in Tables 11 (PANX), 12 (UDPOS), 13 (XNLI), 14 (Tatoeba). Results are presented for several tasks such as NER, sentiment, and question answering.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "We observe a higher fertility ratio for mBERT as compared to MuRIL because of two reasons. First, there is very little representation of IN languages in the mBERT vocabulary4 (refer to Figure 4 for a comparison) and second, the vocabulary does not take transliterated words into account. Since vocabulary plays a key role in the performance of transformer based LMs (Chung et al., 2020; Artetxe et al., 2020), MuRIL\u2019s vocabulary (specifically focused on IN languages) is a significant contributor to the model\u2019s improved performance over mBERT. Pre-training Details: We pre-train a BERT base encoder model making use of the MLM and TLM objectives. We keep a maximum sequence length of 512, a global batch size of 4096, and train for 1M steps (with 50k warm-up steps and a linear decay\n3https://github.com/tensorflow/text/ blob/master/tensorflow_text/tools/ wordpiece_vocab/generate_vocab.py\n4http://juditacs.github.io/2019/02/19/ bert-tokenization-stats.html\nafter). We make use of the AdamW optimizer with a learning rate of 5e-4. Our final model has 236M parameters, is trained on \u223c16B unique tokens, and has a vocabulary of 197,285. Please note that we preserve the case to prevent stripping off accents, often present in IN languages. 3 Evaluation. In all our experiments, the goal has been to improve the model\u2019s performance for cross-lingual understanding. For this reason, the results are computed in a zero-shot setting, i.e., by fine-tuning models on the labeled training set of one language and evaluating on test sets for all languages. Here, our labeled training sets are in English for all tasks. We choose the XTREME benchmark (Hu et al., 2020) as a test-bed. XTREME covers 40 typologically diverse languages spanning 12 language families and includes 9 tasks that require reasoning about different levels of syntax or semantics (Hu et al., 2020). We present our results in Table 1.", "We present the sentiment predictions on a sample set of sentences in Figure 7. In the first example, \u201cIt\u2019s good that the account hasn\u2019t closed\u201d, we observe that the original Hindi sentence borrows an English word (account) and also contains a negation (not) word, but MuRIL correctly predicts it as expressing a positive statement. A similar observation can be made in the second example where MuRIL correctly predicts the sentiment of the transliterated sentence, \u201cRamu didn\u2019t let the film\u2019s pace slow down\u201d. Question Answering (QA): QA is the task of answering a question based on the given context or world knowledge. We show two context-question pairs, with their answers and predicted answers in Figure 8. In the first example, despite the fact that the word Greek is referred to by its Hindi translation in the context and its transliteration in the question (as highlighted), MuRIL correctly infers the answer from the context. In the second example, MuRIL understands that \u201cbank ki paribhasha\u201d (the definition of a bank), as a whole entity, is what differs across countries and not banks."]}
{"pkey": "muril_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "No ablation studies about any parameters or design decisions are presented in the paper.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\nMuRIL: Multilingual Representations for Indian Languages\nSimran Khanuja1 Diksha Bansal* 2 Sarvesh Mehtani* 3 Savya Khosla* 4 Atreyee Dey1\nBalaji Gopalan1 Dilip Kumar Margam1 Pooja Aggarwal1 Rajiv Teja Nagipogu1 Shachi Dave1\nShruti Gupta1 Subhash Chandra Bose Gali1 Vish Subramanian1 Partha Talukdar1\n1Google 2Indian Institute of Technology, Patna 3Indian Institute of Technology, Bombay 4Delhi Technological University\n1 Why MuRIL?. India is a multilingual society with 1369 rationalized languages and dialects being spoken across the country (INDIA, 2011). Of these, the 22 scheduled languages have a staggering total of 1.17 billion speakers and 121 languages have more than 10,000 speakers (INDIA, 2011).", "Second, we use the indic-trans library (Bhat et al., 2015) to transliterate Wikipedia corpora of all IN languages to Latin (except ks, sa and sd, for which the library doesn\u2019t have support). The source document and its Latin transliteration are used as parallel instances to train the model. Upsampling: In the corpora collected above, the percentage of tokens per language is highly uneven in its distribution. Hence, data smoothing is essential so that all languages have their representation reflect their usage in the real world. To achieve this, we upsample monolingual Wikipedia corpora of each language according to its multiplier value given by:\nmi =\n(max j\u2208L nj\nni\n)(1\u2212\u03b1) (1) In the above equation, mi represents the multiplier value for language i, ni is its original token count, L represents the set of all 17 languages and \u03b1 is a hyperparameter whose value is set to 0.3, following Conneau et al. (2020). Hence, the upsampled token count for language i is mi \u2217 ni. The final data distribution after upsampling is shown in Figure 2. The upsampled token counts for each language and corpus are reported in Appendix A.\nVocabulary: We learn a cased WordPiece (Schuster and Nakajima, 2012; Wu et al., 2016) vocabulary from the upsampled pre-training data using the wordpiece vocabulary generation\nlibrary from Tensorflow Text3. Since our data is upsampled, we set the language smoothing exponent from the vocabulary generation tool to 1, and the rest of the parameters are set to their default value. The final vocabulary size is 197,285. Figure 3 shows a few common IN language words tokenized using mBERT and MuRIL vocabularies. We also plot the fertility ratio (average number of sub-words/word) of mBERT and MuRIL tokenizers on a random sample of text from our training data in Figure 5. Here, a higher fertility ratio equates to a larger number of sub-words per word, eventually leading to a loss in preservation of semantic meaning."]}
{"pkey": "muril_20", "question": "List the future work mentioned in the paper.", "answer": "No future directions for extending the work are specified in paper.", "title": "MuRIL: Multilingual Representations for Indian Languages", "context": ["Since MuRIL currently supports IN languages only, we compute average performances across IN language test sets for all tasks. We also transliterate IN language test sets (native\u2192 Latin) using the indic-trans library (Bhat et al., 2015), and report results on the same in Table 2. Detailed results for each language and task can be found in Appendix B. On average, MuRIL significantly beats mBERT across all tasks. This difference is more so for the transliterated test sets, which is expected because mBERT does not include transliterated data in training. We analyse predictions of mBERT and MuRIL on a random sample of test examples in Appendix C.\nFine-tuning Details: For each task, we report re-\nsults of the best performing checkpoint on the evaluation set. We present the hyperparameter details for each task in Table 3. Note that we use the same hyperparameters for evaluating both mBERT and MuRIL. We fine-tune the model on the English training set for each task, and evaluate on the test sets of all IN languages. For TyDiQA-GoldP, we augment the training set with SQuAD English training set, similar to Fang et al. (2020), and then fine-tune the model. For Tatoeba, we do not finetune the model, and use the pooled output of the last layer as the sentence embedding. 4 How to use MuRIL?. We have released the MuRIL encoder on TFHub5 with detailed usage instructions. We have also released a pre-processor module with the same, that processes raw text into the expected input format for the encoder. Additionally, we have released the MuRIL pre-trained model, i.e., with the MLM layer intact (to enable masked word predictions) on HuggingFace6. We sincerely hope MuRIL aids\n5https://tfhub.dev/google/MuRIL/1 6https://huggingface.co/google/\nmuril-base-cased\nin building better technologies and applications for Indian languages.\nAcknowledgments. We would like to thank Melvin Johnson for his feedback on a draft of this paper.", "The IN languages include: Assamese (as), Bengali (bn), Gujarati (gu), Hindi (hi), Kannada (kn), Kashmiri (ks), Malayalam (ml), Marathi (mr), Nepali (ne), Oriya (or), Punjabi (pa), Sanskrit (sa), Sindhi (sd), Tamil (ta), Telugu (te) and Urdu (ur). ar X\niv :2\n10 3.\n10 73\n0v 2\n[ cs\n.C L\n] 2\nA pr\n2 02\n1\nWe train our model with two language modeling objectives. The first is the conventional Masked Language Modeling (MLM) objective (Taylor, 1953) that leverages monolingual text data only (unsupervised). The second is the Translation Language Modeling (TLM) objective (Lample and Conneau, 2019) that leverages parallel data (supervised). We use monolingual documents to train the model with MLM, and both translated and transliterated document pairs to train the model with TLM. Monolingual Data: We collect monolingual data for the 17 languages mentioned above from the Common Crawl OSCAR corpus1 and Wikipedia2. Translated Data: We have two sources of translated data. First, we use the PMINDIA (Haddow and Kirefu, 2020) parallel corpus containing sentence pairs for 8 IN languages (bn, gu, hi, kn, ml, mr, ta, te). Each pair comprises of a sentence in a native language and its English translation. Second, we translate the aforementioned monolingual corpora (both Common Crawl and Wikipedia) to English, using an in-house translation system. The source and translated documents are used as parallel instances to train the model. Note that we translate corpora of all IN languages excluding as, ks and sa, for which the current translation system lacks support. Transliterated Data: We have two sources of 1https://oscar-corpus.com 2https://www.tensorflow.org/datasets/ catalog/wikipedia\ntransliterated data as well. First, we use the Dakshina Dataset (Roark et al., 2020) that contains 10,000 sentence pairs for 12 IN languages (bn, gu, hi, kn, ml, mr, pa, ta, te, ur). Each pair is a native script sentence and its manually romanized transliteration.", "India also has the second largest (and an ever growing) digital footprint (Statista, 2020). Despite this, today\u2019s state-ofthe-art multilingual systems perform sub-optimally on Indian (IN) languages (as shown in Figure 1). This can be explained by the fact that multilingual language models (LMs) are often trained on 100+ languages together, leading to a small representation of IN languages in their vocabulary and training data. Multilingual LMs are substantially less effective in resource-lean scenarios (Wu and Dredze, 2020; Lauscher et al., 2020), as limited data doesn\u2019t help capture the various nuances of a language. One also commonly observes IN language text transliterated to Latin or code-mixed with English, especially in informal settings (for example, on social media platforms) (Rijhwani et al., 2017). This phenomenon is not adequately handled by current state-of-the-art multilingual LMs. Few works like Conneau et al. (2020) use transliterated data in training, but limit to including naturally present web crawl data. To address the aforementioned gaps, we propose MuRIL, a multilingual LM specifically built for IN languages. MuRIL is trained on significantly large amounts of IN text corpora only. We explicitly augment monolingual text corpora with both\n* Work done during a summer internship at Google India. Correspondence to the MuRIL Team (muril-contact@google.com)\ntranslated and transliterated document pairs, that serve as supervised cross-lingual signals in training. MuRIL significantly outperforms multilingual BERT (mBERT) on all tasks in the challenging cross-lingual XTREME benchmark (Hu et al., 2020). We also present results on transliterated (native \u2192 Latin) test sets of the chosen datasets, and demonstrate the efficacy of MuRIL in handling transliterated data. 2 Model and Data.\nMuRIL currently supports 17 languages for which monolingual data is publicly available. These are further grouped into 16 IN languages and English (en)."]}
{"pkey": "gpt_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Existing techniques involve a combination of making task-specific changes to the model architecture [43, 44], using intricate learning schemes [21] and adding auxiliary learning objectives [50]. These uncertainties have made it difficult to develop effective semi-supervised learning approaches for language processing.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared. We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn\u2019t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results. For each benchmark, we produce a \u2018clean\u2019 version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C. We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.", "Note that since PTB is a traditional language modeling dataset it does not have a clear separation of examples to define one-shot or few-shot evaluation around, so we measure only zero-shot. 3.1.2 LAMBADA. The LAMBADA dataset [PKL+16] tests the modeling of long-range dependencies in text \u2013 the model is asked to predict the last word of sentences which require reading a paragraph of context. It has recently been suggested that the continued scaling of language models is yielding diminishing returns on this difficult benchmark. [BHT+20] reflect on the small 1.5% improvement achieved by a doubling of model size between two recent state of the art results ([SPP+19]\nand [Tur20]) and argue that \u201ccontinuing to expand hardware and data sizes by orders of magnitude is not the path forward\u201d. We find that path is still promising and in a zero-shot setting GPT-3 achieves 76% on LAMBADA, a gain of 8% over the previous state of the art. LAMBADA is also a demonstration of the flexibility of few-shot learning as it provides a way to address a problem that classically occurs with this dataset. Although the completion in LAMBADA is always the last word in a sentence, a standard language model has no way of knowing this detail. It thus assigns probability not only to the correct ending but also to other valid continuations of the paragraph. This problem has been partially addressed in the past with stop-word filters [RWC+19] (which ban \u201ccontinuation\u201d words). The few-shot setting instead allows us to \u201cframe\u201d the task as a cloze-test and allows the language model to infer from examples that a completion of exactly one word is desired. We use the following fill-in-the-blank format:\nAlice was friends with Bob. Alice went to visit her friend . \u2192 Bob George bought some baseball equipment, a ball, a glove, and a . \u2192\nWhen presented with examples formatted this way, GPT-3 achieves 86.4% accuracy in the few-shot setting, an increase of over 18% from the previous state-of-the-art.", "This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3\u2019s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the \u201cbest of both worlds\u201d. A more fundamental limitation of the general approach described in this paper \u2013 scaling up any LM-like model, whether autoregressive or bidirectional \u2013 is that it may eventually run into (or could already be running into) the limits of the\npretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. [RRS20] demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions."]}
{"pkey": "gpt_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. The paper authors demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Thus, we expect that contamination is likely to be frequent, but that its effects may not be as large as feared. We initially tried to address the issue of contamination by proactively searching for and attempting to remove any overlap between our training data and the development and test sets of all benchmarks studied in this paper. Unfortunately, a bug resulted in only partial removal of all detected overlaps from the training data. Due to the cost of training, it wasn\u2019t feasible to retrain the model. To address this, we investigate in detail how the remaining detected overlap impacts results. For each benchmark, we produce a \u2018clean\u2019 version which removes all potentially leaked examples, defined roughly as examples that have a 13-gram overlap with anything in the pretraining set (or that overlap with the whole example when it is shorter than 13-grams). The goal is to very conservatively flag anything that could potentially be contamination, so as to produce a clean subset that is free of contamination with high confidence. The exact procedure is detailed in Appendix C. We then evaluate GPT-3 on these clean benchmarks, and compare to the original score. If the score on the clean subset is similar to the score on the entire dataset, this suggests that contamination, even if present, does not have a significant effect on reported results. If the score on the clean subset is lower, this suggests contamination may be inflating the results. The results are summarized in Figure 4.2. Although potential contamination is often high (with a quarter of benchmarks scoring over 50%), in most cases performance changes only negligibly, and we see no evidence that contamination level and performance difference are correlated. We conclude that either our conservative method substantially overestimated contamination or that contamination has little effect on performance.", "This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3\u2019s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the \u201cbest of both worlds\u201d. A more fundamental limitation of the general approach described in this paper \u2013 scaling up any LM-like model, whether autoregressive or bidirectional \u2013 is that it may eventually run into (or could already be running into) the limits of the\npretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. [RRS20] demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions.", "Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners. Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3\u2019s characteristics in this regard. The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach. Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration): \u2022 Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used."]}
{"pkey": "gpt_3", "question": "What are the main contributions of the paper?", "answer": "The paper authors demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, the paper authors make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Helsinki is some north of Tallinn, Estonia, east of Stockholm, Sweden, and west of Saint Petersburg, Russia. Helsinki has close historical connections with these three cities. The Helsinki metropolitan area includes the urban core of Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns. It is the world\u2019s northernmost metro area of over one million people, and the city is the northernmost capital of an EU member state. The Helsinki metropolitan area is the third largest metropolitan area in the Nordic countries after Stockholm and Copenhagen, and the City of Helsinki is the third largest after Stockholm and Oslo. Helsinki is Finland\u2019s major political, educational, financial, cultural, and research center as well as one of northern Europe\u2019s major cities. Approximately 75% of foreign companies that operate in Finland have settled in the Helsinki region. The nearby municipality of Vantaa is the location of Helsinki Airport, with frequent service to various destinations in Europe and Asia. Q: what is the most populous municipality in Finland?\nA: Helsinki\nQ: how many people live there? A: 1.4 million in the metropolitan area\nQ: what percent of the foreign companies that operate in Finland are in Helsinki? A: 75%\nQ: what towns are a part of the metropolitan area? A:\nTarget Completion \u2192 Helsinki, Espoo, Vantaa, Kauniainen, and surrounding commuter towns\nFigure G.18: Formatted dataset example for CoQA\nContext \u2192 Please unscramble the letters into a word, and write that word: asinoc =\nTarget Completion \u2192 casino\nFigure G.19: Formatted dataset example for Cycled Letters\nContext \u2192 Passage: Saint Jean de Bre\u0301beuf was a French Jesuit missionary who travelled to New France in 1625. There he worked primarily with the Huron for the rest of his life, except for a few years in France from 1629 to 1633. He learned their language and culture, writing extensively about each to aid other missionaries.", "=\nTarget Completion \u2192 The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey\u2019s accession to the European Union, despite Turkey\u2019s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. Figure G.41: Formatted dataset example for Ro\u2192En\nContext \u2192 Q: What is (2 * 4) * 6? A:\nTarget Completion \u2192 48\nFigure G.42: Formatted dataset example for Arithmetic 1DC\nContext \u2192 Q: What is 17 minus 14? A:\nTarget Completion \u2192 3\nFigure G.43: Formatted dataset example for Arithmetic 2D-\nContext \u2192 Q: What is 98 plus 45? A:\nTarget Completion \u2192 143\nFigure G.44: Formatted dataset example for Arithmetic 2D+\nContext \u2192 Q: What is 95 times 45? A:\nTarget Completion \u2192 4275\nFigure G.45: Formatted dataset example for Arithmetic 2Dx\nContext \u2192 Q: What is 509 minus 488? A:\nTarget Completion \u2192 21\nFigure G.46: Formatted dataset example for Arithmetic 3D-\nContext \u2192 Q: What is 556 plus 497? A:\nTarget Completion \u2192 1053\nFigure G.47: Formatted dataset example for Arithmetic 3D+\nContext \u2192 Q: What is 6209 minus 3365? A:\nTarget Completion \u2192 2844\nFigure G.48: Formatted dataset example for Arithmetic 4D-\nContext \u2192 Q: What is 9923 plus 617? A:\nTarget Completion \u2192 10540\nFigure G.49: Formatted dataset example for Arithmetic 4D+\nContext \u2192 Q: What is 40649 minus 78746? A:\nTarget Completion \u2192 -38097\nFigure G.50: Formatted dataset example for Arithmetic 5D\u2212\nContext \u2192 Q: What is 65360 plus 16204? A:\nTarget Completion \u2192 81564\nFigure G.51: Formatted dataset example for Arithmetic 5D+\nH Results on All Tasks for All Model Sizes. Zero-Shot One-Shot Few-Shot\nName Metric Split Fine-tune SOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B 175B (test server)", "The main advantage of fine-tuning is strong performance on many benchmarks. The main disadvantages are the need for a new large dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and the potential to exploit spurious features of the training data [GSL+18, NK19], potentially resulting in an unfair comparison with human performance. In this work we do not fine-tune GPT-3 because our focus is on task-agnostic performance, but GPT-3 can be fine-tuned in principle and this is a promising direction for future work. \u2022 Few-Shot (FS) is the term we will use in this work to refer to the setting where the model is given a few demonstrations of the task at inference time as conditioning [RWC+19], but no weight updates are allowed. As shown in Figure 2.1, for a typical dataset an example has a context and a desired completion (for example an English sentence and the French translation), and few-shot works by giving K examples of context and completion, and then one final example of context, with the model expected to provide the completion. We typically set K in the range of 10 to 100 as this is how many examples can fit in the model\u2019s context window (nctx = 2048). The main advantages of few-shot are a major reduction in the need for task-specific data and reduced potential to learn an overly narrow distribution from a large but narrow fine-tuning dataset. The main disadvantage is that results from this method have so far been much worse than state-of-the-art fine-tuned models. Also, a small amount of task specific data is still required. As indicated by the name, few-shot learning as described here for language models is related to few-shot learning as used in other contexts in ML [HYC01, VBL+16] \u2013 both involve learning based on a broad distribution of tasks (in this case implicit in the pre-training data) and then rapidly adapting to a new task."]}
{"pkey": "gpt_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "The paper authors hope that this will help enable new research into unsupervised learning, for both natural language understanding and other domains, further improving our understanding of how and when unsupervised learning works.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world [BHT+20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans [ZSW+19a], fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world [CLY+19].\nAnother limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements. A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks \u201cfrom scratch\u201d at inference time, or if it simply recognizes and identifies tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task.", "First, despite the strong quantitative and qualitative improvements of GPT-3, particularly compared to its direct predecessor GPT-2, it still has notable weaknesses in text synthesis and several NLP tasks. On text synthesis, although the overall quality is high, GPT-3 samples still sometimes repeat themselves semantically at the document level, start to lose coherence over sufficiently long passages, contradict themselves, and occasionally contain non-sequitur sentences or paragraphs. We will release a collection of 500 uncurated unconditional samples to help provide a better sense of GPT-3\u2019s limitations and strengths at text synthesis. Within the domain of discrete language tasks, we have noticed informally that GPT-3 seems to have special difficulty with \u201ccommon sense physics\u201d, despite doing well on some datasets (such as PIQA [BZB+19]) that test this domain. Specifically GPT-3 has difficulty with questions of the type \u201cIf I put cheese into the fridge, will it melt?\u201d. Quantitatively, GPT-3\u2019s in-context learning performance has some notable gaps on our suite of benchmarks, as described in Section 3, and in particular it does little better than chance when evaluated one-shot or even few-shot on some \u201ccomparison\u201d tasks, such as determining if two words are used the same way in a sentence, or if one sentence implies another (WIC and ANLI respectively), as well as on a subset of reading comprehension tasks. This is especially striking given GPT-3\u2019s strong few-shot performance on many other tasks. GPT-3 has several structural and algorithmic limitations, which could account for some of the issues above. We focused on exploring in-context learning behavior in autoregressive language models because it is straightforward to both sample and compute likelihoods with this model class. As a result our experiments do not include any bidirectional architectures or other training objectives such as denoising.", "Language Models are Few-Shot Learners. Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \u2013 something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\u2019s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. \u2217Equal contribution \u2020Johns Hopkins University, OpenAI Author contributions listed at end of paper."]}
{"pkey": "gpt_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "Table 1: A list of the different tasks and datasets used in our experiments.\nTask                                                                           Datasets\nNatural language inference                                   SNLI [5], MultiNLI [66], Question NLI [64], RTE [4], SciTail [25]\nQuestion Answering                                               RACE [30], Story Cloze [40]\nSentence similarity                                                 MSR Paraphrase Corpus [14], Quora Question Pairs [9], STS Benchmark [6]\nClassification                                                           Stanford Sentiment Treebank-2 [54], CoLA [65]", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["=\nTarget Completion \u2192 The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey\u2019s accession to the European Union, despite Turkey\u2019s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. Figure G.41: Formatted dataset example for Ro\u2192En\nContext \u2192 Q: What is (2 * 4) * 6? A:\nTarget Completion \u2192 48\nFigure G.42: Formatted dataset example for Arithmetic 1DC\nContext \u2192 Q: What is 17 minus 14? A:\nTarget Completion \u2192 3\nFigure G.43: Formatted dataset example for Arithmetic 2D-\nContext \u2192 Q: What is 98 plus 45? A:\nTarget Completion \u2192 143\nFigure G.44: Formatted dataset example for Arithmetic 2D+\nContext \u2192 Q: What is 95 times 45? A:\nTarget Completion \u2192 4275\nFigure G.45: Formatted dataset example for Arithmetic 2Dx\nContext \u2192 Q: What is 509 minus 488? A:\nTarget Completion \u2192 21\nFigure G.46: Formatted dataset example for Arithmetic 3D-\nContext \u2192 Q: What is 556 plus 497? A:\nTarget Completion \u2192 1053\nFigure G.47: Formatted dataset example for Arithmetic 3D+\nContext \u2192 Q: What is 6209 minus 3365? A:\nTarget Completion \u2192 2844\nFigure G.48: Formatted dataset example for Arithmetic 4D-\nContext \u2192 Q: What is 9923 plus 617? A:\nTarget Completion \u2192 10540\nFigure G.49: Formatted dataset example for Arithmetic 4D+\nContext \u2192 Q: What is 40649 minus 78746? A:\nTarget Completion \u2192 -38097\nFigure G.50: Formatted dataset example for Arithmetic 5D\u2212\nContext \u2192 Q: What is 65360 plus 16204? A:\nTarget Completion \u2192 81564\nFigure G.51: Formatted dataset example for Arithmetic 5D+\nH Results on All Tasks for All Model Sizes. Zero-Shot One-Shot Few-Shot\nName Metric Split Fine-tune SOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B 175B (test server)", "For example, \u201cQ: What is 6+(4*8)? A: 38\u201d. The three 1 digit numbers are selected uniformly on [0, 10) and the operations are selected uniformly from {+,-,*}. In all 10 tasks the model must generate the correct answer exactly. For each task we generate a dataset of 2,000 random instances of the task and evaluate all models on those instances. First we evaluate GPT-3 in the few-shot setting, for which results are shown in Figure 3.10. On addition and subtraction, GPT-3 displays strong proficiency when the number of digits is small, achieving 100% accuracy on 2 digit addition, 98.9% at 2 digit subtraction, 80.2% at 3 digit addition, and 94.2% at 3-digit subtraction. Performance decreases as the number of digits increases, but GPT-3 still achieves 25-26% accuracy on four digit operations and 9-10% accuracy on five digit operations, suggesting at least some capacity to generalize to larger numbers of digits. GPT-3 also achieves 29.2% accuracy at 2 digit multiplication, an especially computationally intensive operation. Finally, GPT-3 achieves 21.3% accuracy at single digit combined operations (for example, 9*(7+5)), suggesting that it has some robustness beyond just single operations. As Figure 3.10 makes clear, small models do poorly on all of these tasks \u2013 even the 13 billion parameter model (the second largest after the 175 billion full GPT-3) can solve 2 digit addition and subtraction only half the time, and all other operations less than 10% of the time. One-shot and zero-shot performance are somewhat degraded relative to few-shot performance, suggesting that adaptation to the task (or at the very least recognition of the task) is important to performing these computations correctly. Nevertheless, one-shot performance is still quite strong, and even zero-shot performance of the full GPT-3 significantly\noutperforms few-shot learning for all smaller models.", "On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. \u201cTrue\u201d or \u201cFalse\u201d rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR+19] (see Appendix G) for details. On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4 and a length penalty of \u03b1 = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else. 3 Results. In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets."]}
{"pkey": "gpt_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Our model obtains an score of 45.4 on CoLA, The Corpus of Linguistic Acceptability (CoLA) [65] contains expert judgements on whether a sentence is grammatical or not, and tests the innate linguistic bias of trained models. which is an especially big jump over the previous best result of 35.0, showcasing the innate linguistic bias learned by our model.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Then we run evaluation on the clean-only examples and report the relative percent change between the clean score and the original score. If the clean score is more than 1% or 2% worse than the overall score, it suggests the model may have overfit to the examples it has seen. If the clean score is significantly better, our filtering scheme may have preferentially marked easier examples as dirty. This overlap metric tends to show a high rate of false positives for datasets that contain background information (but not answers) drawn from the web (such as SQuAD, which draws from Wikipedia) or examples less than 8 words long, which we ignored in our filtering process (except for wordscrambling tasks). One instance where this technique seems to fail to give good signal is DROP, a reading comprehension task in which 94% of the examples are dirty. The information required to answer the question is in a passage provided to the model, so having seen the passage during training but not the questions and answers does not meaningfully constitute cheating. We confirmed that every matching training document contained only the source passage, and none of the questions and answers in the dataset. The more likely explanation for the decrease in performance is that the 6% of examples that remain after filtering come from a slightly different distribution than the dirty examples. Figure 4.2 shows that as the dataset becomes more contaminated, the variance of the clean/all fraction increases, but there is no apparent bias towards improved or degraded performance. This suggests that GPT-3 is relatively insensitive to contamination. See Section 4 for details on the datasets we flagged for further review.", "We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers. 6.2 Fairness, Bias, and Representation. Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms [Cra17]. We have conducted an analysis of biases in the model in order to better understand GPT-3\u2019s limitations when it comes to fairness, bias, and representation. 8\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model\u2019s biases even within the studied categories. Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension. 6.2.1 Gender. In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3.", "Example: opoepnnt \u2192 opponent.\n\u2022 Random insertion in word (RI) \u2013 A random punctuation or space character is inserted between each letter of a word, and the model must output the original word. Example: s.u!c/c!e.s s i/o/n = succession. \u2022 Reversed words (RW) \u2013 The model is given a word spelled backwards, and must output the original word. Example: stcejbo\u2192 objects. For each task we generate 10,000 examples, which we chose to be the top 10,000 most frequent words as measured by [Nor09] of length more than 4 characters and less than 15 characters. The few-shot results are shown in Figure 3.11. Task performance tends to grow smoothly with model size, with the full GPT-3 model achieving 66.9% on removing\nrandom insertions, 38.6% on cycling letters, 40.2% on the easier anagram task, and 15.1% on the more difficult anagram task (where only the first and last letters are held fixed). None of the models can reverse the letters in a word. In the one-shot setting, performance is significantly weaker (dropping by half or more), and in the zero-shot setting the model can rarely perform any of the tasks (Table 3.10). This suggests that the model really does appear to learn these tasks at test time, as the model cannot perform them zero-shot and their artificial nature makes them unlikely to appear in the pre-training data (although we cannot confirm this with certainty). We can further quantify performance by plotting \u201cin-context learning curves\u201d, which show task performance as a function of the number of in-context examples. We show in-context learning curves for the Symbol Insertion task in Figure 1.2. We can see that larger models are able to make increasingly effective use of in-context information, including both task examples and natural language task descriptions."]}
{"pkey": "gpt_7", "question": "List the limitations of the model discussed in the paper.", "answer": "The Corpus of Linguistic Acceptability (CoLA) [65] contains expert judgements on whether a sentence is grammatical or not, and tests the innate linguistic bias of trained models. The Stanford Sentiment Treebank (SST-2) [54], on the other hand, is a standard binary classification task. Our model obtains an score of 45.4 on CoLA, which is an especially big jump over the previous best result of 35.0, showcasing the innate linguistic bias learned by our model. Our method significantly outperforms the baselines on four of the five datasets, achieving absolute improvements of upto 1.5% on MNLI, 5% on SciTail, 5.8% on QNLI and 0.6% on SNLI over the previous best results. This demonstrates our model\u2019s ability to better reason over multiple sentences, and handle aspects of linguistic ambiguity. During transfer, the paper authors utilize task-specific input adaptations derived from traversal-style approaches [52], which process structured text input as a single contiguous sequence of tokens. As we demonstrate in our experiments, these adaptations enable us to fine-tune effectively with minimal changes to the architecture of the pre-trained model.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["We presented a 175 billion parameter language model which shows strong performance on many NLP tasks and benchmarks in the zero-shot, one-shot, and few-shot settings, in some cases nearly matching the performance of\nstate-of-the-art fine-tuned systems, as well as generating high-quality samples and strong qualitative performance at tasks defined on-the-fly. We documented roughly predictable trends of scaling in performance without using fine-tuning. We also discussed the social impacts of this class of model. Despite many limitations and weaknesses, these results suggest that very large language models may be an important ingredient in the development of adaptable, general language systems.\nAcknowledgements. The authors would like to thank Ryan Lowe for giving detailed feedback on drafts of the paper. Thanks to Jakub Pachocki and Szymon Sidor for suggesting tasks, and Greg Brockman, Michael Petrov, Brooke Chan, and Chelsea Voss for helping run evaluations on OpenAI\u2019s infrastructure. Thanks to David Luan for initial support in scaling up this project, Irene Solaiman for discussions about ways to approach and evaluate bias, Harrison Edwards and Yura Burda for discussions and experimentation with in-context learning, Geoffrey Irving and Paul Christiano for early discussions of language model scaling, Long Ouyang for advising on the design of the human evaluation experiments, Chris Hallacy for discussions on data collection, and Shan Carter for help with visual design. Thanks to the millions of people who created content that was used in the training of the model, and to those who were involved in indexing or upvoting the content (in the case of WebText). Additionally, we would like to thank the entire OpenAI infrastructure and supercomputing teams for making it possible to train models at this scale.", "Broadly, for most tasks we find relatively smooth scaling with model capacity in all three settings; one notable pattern is that the gap between zero-, one-, and few-shot performance often grows with model capacity, perhaps suggesting that larger models are more proficient meta-learners. Finally, given the broad spectrum of capabilities displayed by GPT-3, we discuss concerns about bias, fairness, and broader societal impacts, and attempt a preliminary analysis of GPT-3\u2019s characteristics in this regard. The remainder of this paper is organized as follows. In Section 2, we describe our approach and methods for training GPT-3 and evaluating it. Section 3 presents results on the full range of tasks in the zero-, one- and few-shot settings. Section 4 addresses questions of data contamination (train-test overlap). Section 5 discusses limitations of GPT-3. Section 6 discusses broader impacts. Section 7 reviews related work and Section 8 concludes. 2 Approach. Our basic pre-training approach, including model, data, and training, is similar to the process described in [RWC+19], with relatively straightforward scaling up of the model size, dataset size and diversity, and length of training. Our use of in-context learning is also similar to [RWC+19], but in this work we systematically explore different settings for learning within the context. Therefore, we start this section by explicitly defining and contrasting the different settings that we will be evaluating GPT-3 on or could in principle evaluate GPT-3 on. These settings can be seen as lying on a spectrum of how much task-specific data they tend to rely on. Specifically, we can identify at least four points on this spectrum (see Figure 2.1 for an illustration): \u2022 Fine-Tuning (FT) has been the most common approach in recent years, and involves updating the weights of a pre-trained model by training on a supervised dataset specific to the desired task. Typically thousands to hundreds of thousands of labeled examples are used.", "Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology. Across the models we analyzed, \u2018Asian\u2019 had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \u2019Black\u2019 had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data. 9We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not require the isolation of instances in which \u2018they\u2019 refers to a singular noun from those where it didn\u2019t, but other forms of gender bias are likely present and could be studied using different approaches. 6.2.3 Religion. We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length \u224850 with a temperature of 1 and a top p of 0.9 for every prompt. Our prompts were of the nature \"{Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words. The following is an example output from the model: \"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic."]}
{"pkey": "gpt_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "The paper authors use the BooksCorpus dataset [71] for training the language model. It contains over 7,000 unique unpublished books from a variety of genres including Adventure, Fantasy, and Romance. Crucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information. An alternative dataset, the 1B Word Benchmark, which is used by a similar approach, ELMo [44], is approximately the same size.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Another task well suited for few-shot learning is correcting English grammar. We test this with GPT-3 in the fewshot setting by giving prompts of the form \"Poor English Input: <sentence>\\n Good English Output: <sentence>\". We give GPT-3 one human-generated correction and then ask it to correct 5 more (again without any omissions or repeats). Results are shown in Figure 3.17. 4 Measuring and Preventing Memorization Of Benchmarks. Since our training dataset is sourced from the internet, it is possible that our model was trained on some of our benchmark test sets. Accurately detecting test contamination from internet-scale datasets is a new area of research without established best practices. While it is common practice to train large models without investigating contamination, given the increasing scale of pretraining datasets, we believe this issue is becoming increasingly important to attend to. This concern is not just hypothetical. One of the first papers to train a language model on Common Crawl data [TL18] detected and removed a training document which overlapped with one of their evaluation datasets. Other work such as GPT-2 [RWC+19] also conducted post-hoc overlap analysis. Their study was relatively encouraging, finding that\nalthough models did perform moderately better on data that overlapped between training and testing, this did not significantly impact reported results due to the small fraction of data which was contaminated (often only a few percent). GPT-3 operates in a somewhat different regime. On the one hand, the dataset and model size are about two orders of magnitude larger than those used for GPT-2, and include a large amount of Common Crawl, creating increased potential for contamination and memorization. On the other hand, precisely due to the large amount of data, even GPT-3 175B does not overfit its training set by a significant amount, measured relative to a held-out validation set with which it was deduplicated (Figure 4.1).", "Language Models are Few-Shot Learners. Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \u2013 something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\u2019s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. \u2217Equal contribution \u2020Johns Hopkins University, OpenAI Author contributions listed at end of paper.", "For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unfiltered Common Crawl. We used this classifier to score Common Crawl documents. We kept each document in our dataset iff\nnp.random.pareto(\u03b1) > 1\u2212 document_score\nWe chose \u03b1 = 9 in order to take mostly documents the classifier scored highly, but still include some documents that were out of distribution. \u03b1 was chosen to match the distribution of scores from our classifier on WebText. We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples. 2. To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark\u2019s MinHashLSH implementation with 10 hashes, using the same features as were used for classification above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%. After filtering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C.\nB Details of Model Training. To train all versions of GPT-3, we use Adam with \u03b21 = 0.9, \u03b22 = 0.95, and = 10\u22128, we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the first 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overfitting."]}
{"pkey": "gpt_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "The paper authors used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization.The paper authors use the ftfy library to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unfiltered Common Crawl. We used this classifier to score Common Crawl documents. We kept each document in our dataset iff\nnp.random.pareto(\u03b1) > 1\u2212 document_score\nWe chose \u03b1 = 9 in order to take mostly documents the classifier scored highly, but still include some documents that were out of distribution. \u03b1 was chosen to match the distribution of scores from our classifier on WebText. We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples. 2. To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark\u2019s MinHashLSH implementation with 10 hashes, using the same features as were used for classification above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%. After filtering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C.\nB Details of Model Training. To train all versions of GPT-3, we use Adam with \u03b21 = 0.9, \u03b22 = 0.95, and = 10\u22128, we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the first 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overfitting.", "In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work. Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations. 2.1 Model and Architectures. We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks. Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters, nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 \u2217 dmodel), and dhead is the dimension of each attention head.", "Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology. Across the models we analyzed, \u2018Asian\u2019 had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \u2019Black\u2019 had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data. 9We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not require the isolation of instances in which \u2018they\u2019 refers to a singular noun from those where it didn\u2019t, but other forms of gender bias are likely present and could be studied using different approaches. 6.2.3 Religion. We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length \u224850 with a temperature of 1 and a top p of 0.9 for every prompt. Our prompts were of the nature \"{Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words. The following is an example output from the model: \"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic."]}
{"pkey": "gpt_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "The paper authors use the ftfy library to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Language Models are Few-Shot Learners. Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \u2013 something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\u2019s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. \u2217Equal contribution \u2020Johns Hopkins University, OpenAI Author contributions listed at end of paper.", "Language Models are Few-Shot Learners. Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \u2013 something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\u2019s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. \u2217Equal contribution \u2020Johns Hopkins University, OpenAI Author contributions listed at end of paper. ar X iv :2 00 5. 14 16 5v 4 [ cs .C L ] 2 2 Ju l 2 02 0", "Additionally, since we are measuring sentiment by simply looking at word co-occurrences, the resulting sentiment can reflect socio-historical factors - for instance, text relating to a discussion of slavery will frequently have a negative sentiment, which may lead to a demographic being associated with a negative sentiment under this testing methodology. Across the models we analyzed, \u2018Asian\u2019 had a consistently high sentiment - it ranked 1st in 3 out of 7 models. On the other hand, \u2019Black\u2019 had a consistently low sentiment - it ranked the lowest in 5 out of 7 models. These differences narrowed marginally on the larger model sizes. This analysis gives a sense of the biases of different models and highlights the need for more sophisticated analysis of the relationship between sentiment, entities, and input data. 9We only used male and female pronouns. This simplifying assumption makes it easier to study co-occurrence since it does not require the isolation of instances in which \u2018they\u2019 refers to a singular noun from those where it didn\u2019t, but other forms of gender bias are likely present and could be studied using different approaches. 6.2.3 Religion. We studied which words co-occurred with religious terms relating to Atheism, Buddhism, Christianity, Hinduism, Islam, and Judaism, by generating 800 model outputs of length \u224850 with a temperature of 1 and a top p of 0.9 for every prompt. Our prompts were of the nature \"{Religion practitioners} are\" (Eg. \"Christians are\") for each of the six religious categories listed above. We then allowed the model to naturally carry out completions and created a corpus of such completions for studying co-occurrence of words. The following is an example output from the model: \"Buddhists are divided into two main branches - Theravada and Mahayana. Theravada is the more conservative branch, centering on monastic life and the earliest sutras and refusing to recognize the later Mahayana sutras as authentic."]}
{"pkey": "gpt_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "Our model largely follows the original transformer work [62]. The paper authors trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, the paper authors used 3072 dimensional inner states.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work. Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations. 2.1 Model and Architectures. We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks. Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters, nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 \u2217 dmodel), and dhead is the dimension of each attention head.", "On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. \u201cTrue\u201d or \u201cFalse\u201d rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR+19] (see Appendix G) for details. On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4 and a length penalty of \u03b1 = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else. 3 Results. In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets.", "All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.\n2.2 Training Dataset. Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2 [RSR+19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity. Details of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected by scraping links over a longer period of time, and first described in [KMH+20], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia. Table 2.2 shows the final mixture of datasets that we used in training."]}
{"pkey": "gpt_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. The paper authors train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm [2] is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient. The paper authors used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["For the positive examples, we used a collection of curated datasets such as WebText, Wikiedia, and our web books corpus as the positive examples, and for the negative examples, we used unfiltered Common Crawl. We used this classifier to score Common Crawl documents. We kept each document in our dataset iff\nnp.random.pareto(\u03b1) > 1\u2212 document_score\nWe chose \u03b1 = 9 in order to take mostly documents the classifier scored highly, but still include some documents that were out of distribution. \u03b1 was chosen to match the distribution of scores from our classifier on WebText. We found this re-weighting increased quality as measured by loss on a range of out-of-distribution generative text samples. 2. To further improve model quality and prevent overfitting (which becomes increasingly important as model capacity increases), we fuzzily deduplicated documents (i.e. removed documents with high overlap with other documents) within each dataset using Spark\u2019s MinHashLSH implementation with 10 hashes, using the same features as were used for classification above. We also fuzzily removed WebText from Common Crawl. Overall this decreased dataset size by an average of 10%. After filtering for duplicates and quality, we also partially removed text occurring in benchmark datasets, described in Appendix C.\nB Details of Model Training. To train all versions of GPT-3, we use Adam with \u03b21 = 0.9, \u03b22 = 0.95, and = 10\u22128, we clip the global norm of the gradient at 1.0, and we use cosine decay for learning rate down to 10% of its value, over 260 billion tokens (after 260 billion tokens, training continues at 10% of the original learning rate). There is a linear LR warmup over the first 375 million tokens. We also gradually increase the batch size linearly from a small value (32k tokens) to the full value over the first 4-12 billion tokens of training, depending on the model size. Data are sampled without replacement during training (until an epoch boundary is reached) to minimize overfitting.", "We found females were more often described using appearance oriented words such as \u201dbeautiful\u201d and \u201dgorgeous\u201d as compared to men who were more often described using adjectives that span a greater spectrum. Table 6.1 shows the top 10 most favored descriptive words for the model along with the raw number of times each word co-occurred with a pronoun indicator. \u201cMost Favored\u201d here indicates words which were most skewed towards a category by co-occurring with it at a higher rate as compared to the other category. To put these numbers in perspective, we have also included the average for the number of co-occurrences across all qualifying words for each gender. 6.2.2 Race. To investigate racial bias in GPT-3, we seeded the model with prompts such as - \"The {race} man was very\", \"The {race} woman was very\" and \"People would describe the {race} person as\" and generated 800 samples for each of the above prompts, with {race} replaced with a term indicating a racial category such as White or Asian. We then measure word co-occurrences in the generated samples. Given prior research demonstrating that language models produce text of differing sentiment when varying features such as occupation [HZJ+19], we explored how race impacted sentiment. We measured sentiment using Senti WordNet [BES10] for the words which co-occurred disproportionately with each race. Each word sentiment varied from 100 to -100, with positive scores indicating positive words (eg. wonderfulness: 100, amicable: 87.5), negative scores indicating negative words (eg. wretched: -87.5 , horrid: -87.5) and a score of 0 indicating neutral words (eg. sloping, chalet). It should be noted that we were explicitly prompting the models to talk about race and this in turn generated text that focused on racial features; these results are not from the models talking about race in the wild but talking about race in an experimental setup where they have been primed to do so.", "We measured this by feeding the model a context such as \"The detective was a\" and then looking at the probability of the model following up with male indicating words (eg. man, male etc.) or female indicating words (woman, female etc.). In particular, occupations demonstrating higher levels of education such as legislator, banker, or professor emeritus were heavily male leaning along with occupations that require hard physical labour such as mason, millwright, and sheriff. Occupations that were more likely to be followed by female identifiers include midwife, nurse, receptionist, housekeeper etc. We also tested how these probabilities changed when we shifted the context to be the \"The competent {occupation} was a\" (Competent Variant), and when we shifted the context to be \"The incompetent {occupation} was a\" (Incompetent Variant) for each occupation in the dataset. We found that, when prompted with \"The competent {occupation} was a,\" the majority of occupations had an even higher probability of being followed by a male identifier than a female one than was the case with our original neutral prompt, \"The {occupation} was a\". With the prompt \"The incompetent {occupation} was a\" the majority of occupations still leaned male with a similar probability than for our original neutral prompt. The average occupation bias - measured as\n1 njobs \u2211 jobs log( P (female|Context) P (male|Context)) ) - was \u22121.11 for the Neutral Variant, \u22122.14 for the Competent Variant and \u22121.15 for the Incompetent Variant. We also carried out pronoun resolution on the Winogender dataset [RNLVD18] using two methods which further corroborated the model\u2019s tendency to associate most occupations with males. One method measured the models ability to correctly assign a pronoun as the occupation or the participant. For example, we fed the model a context such as \"The advisor met with the advisee because she wanted to get advice about job applications."]}
{"pkey": "gpt_13", "question": "Describe the computational resources used to train the model.", "answer": "Not specified by the paper authors, and not in public information.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Thus, mitigation work should not be approached purely with a metric driven objective to \u2018remove\u2019 bias as this has been shown to have blind spots [GG19, NvNvdG19] but in a holistic manner. 6.3 Energy Usage. Practical large-scale pre-training requires large amounts of computation, which is energy-intensive: training the GPT-3 175B consumed several thousand petaflop/s-days of compute during pre-training, compared to tens of petaflop/s-days for a 1.5B parameter GPT-2 model (Figure 2.2). This means we should be cognizant of the cost and efficiency of such models, as advocated by [SDSE19]. The use of large-scale pre-training also gives another lens through which to view the efficiency of large models - we should consider not only the resources that go into training them, but how these resources are amortized over the lifetime of a model, which will subsequently be used for a variety of purposes and fine-tuned for specific tasks. Though models like GPT-3 consume significant resources during training, they can be surprisingly efficient once trained: even with the full GPT-3 175B, generating 100 pages of content from a trained model can cost on the order of 0.4 kW-hr, or only a few cents in energy costs. Additionally, techniques like model distillation [LHCG19a] can further bring down the cost of such models, letting us adopt a paradigm of training single, large-scale models, then creating more efficient versions of them for use in appropriate contexts. Algorithmic progress may also naturally further increase the efficiency of such models over time, similar to trends observed in image recognition and neural machine translation [HB20].\n7 Related Work. Several lines of work have focused on increasing parameter count and/or computation in language models as a means to improve generative or task performance. An early work scaled LSTM based language models to over a billion parameters [JVS+16].", "Any socially harmful activity that relies on generating text could be augmented by powerful language models. Examples include misinformation, spam, phishing, abuse of legal and governmental processes, fraudulent academic essay writing and social engineering pretexting. Many of these applications bottleneck on human beings to write sufficiently high quality text. Language models that produce high quality text generation could lower existing barriers to carrying out these activities and increase their efficacy. The misuse potential of language models increases as the quality of text synthesis improves. The ability of GPT-3 to generate several paragraphs of synthetic content that people find difficult to distinguish from human-written text in 3.9.4 represents a concerning milestone in this regard. 6.1.2 Threat Actor Analysis. Threat actors can be organized by skill and resource levels, ranging from low or moderately skilled and resourced actors who may be able to build a malicious product to \u2018advanced persistent threats\u2019 (APTs): highly skilled and well-resourced (e.g. state-sponsored) groups with long-term agendas [SBC+19]. To understand how low and mid-skill actors think about language models, we have been monitoring forums and chat groups where misinformation tactics, malware distribution, and computer fraud are frequently discussed. While we did find significant discussion of misuse following the initial release of GPT-2 in spring of 2019, we found fewer instances of experimentation and no successful deployments since then. Additionally, those misuse discussions were correlated with media coverage of language model technologies. From this, we assess that the threat of misuse from these actors is not immediate, but significant improvements in reliability could change this. Because APTs do not typically discuss operations in the open, we have consulted with professional threat analysts about possible APT activity involving the use of language models.", "We ignored 13\u2212grams that matched more than 10 training documents, as inspection showed the majority of these to contain common cultural phrases, legal boilerplate, or similar content that we likely do want the model to learn, rather than undesired specific overlaps with test sets. Examples for various frequencies can be found in the GPT-3 release repository11. Overlap methodology For our benchmark overlap analysis in Section 4 , we used a variable number of words N to check for overlap for each dataset, where N is the 5th percentile example length in words, ignoring all punctuation, whitespace, and casing. Due to spurious collisions at lower values of N we use a minimum value of 8 on non-synthetic tasks. For performance reasons, we set a maximum value of 13 for all tasks. Values for N and the amount of data marked as dirty are shown in Table C.1. Unlike GPT-2\u2019s use of bloom filters to compute probabilistic bounds for test contamination, we used Apache Spark to compute exact collisions across all training and test sets. We compute overlaps between test sets and our full training corpus, even though we only trained on 40% of our filtered Common Crawl documents per Section 2.2. We define a \u2018dirty\u2019 example as one with any N -gram overlap with any training document, and a \u2018clean\u2019 example as one with no collision. Test and validation splits had similar contamination levels despite some test splits being unlabeled. Due to a bug revealed by this analysis, filtering described above failed on long documents such as books. Because of cost considerations it was infeasible to retrain the model on a corrected version of the training dataset. As such, several language modeling benchmarks plus the Children\u2019s Book Test showed almost complete overlap, and therefore were not included in this paper. Overlaps are shown in Table C.1\nOverlap results To understand how much having seen some of the data helps the model perform on downstream tasks, we filter every validation and test set by dirtiness."]}
{"pkey": "gpt_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "GPT model largely follows the original transformer work [62]. The paper authors trained a 12-layer decoder-only transformer with masked self-attention heads (768 dimensional states and 12 attention heads). For the position-wise feed-forward networks, the paper authors used 3072 dimensional inner states. The paper authors used the Adam optimization scheme [27] with a max learning rate of 2.5e-4. The learning rate was increased linearly from zero over the first 2000 updates and annealed to 0 using a cosine schedule. The paper authors train for 100 epochs on minibatches of 64 randomly sampled, contiguous sequences of 512 tokens. Since layernorm [2] is used extensively throughout the model, a simple weight initialization of N(0, 0.02) was sufficient. The paper authors used a bytepair encoding (BPE) vocabulary with 40,000 merges [53] and residual, embedding, and attention dropouts with a rate of 0.1 for regularization. The paper authors also employed a modified version of L2 regularization proposed in [37], with w = 0.01 on all non bias or gain weights. For the activation function, the paper authors used the Gaussian Error Linear Unit (GELU) [18]. The paper authors used learned position embeddings instead of the sinusoidal version proposed in the original work. The paper authors use the ftfy library2 to clean the raw text in BooksCorpus, standardize some punctuation and whitespace, and use the spaCy tokenizer.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["All models use weight decay of 0.1 to provide a small amount of regularization [LH17]. During training we always train on sequences of the full nctx = 2048 token context window, packing multiple documents into a single sequence when documents are shorter than 2048, in order to increase computational efficiency. Sequences with multiple documents are not masked in any special way but instead documents within a sequence are delimited with a special end of text token, giving the language model the information necessary to infer that context separated by the end of text token is unrelated. This allows for efficient training without need for any special sequence-specific masking. C Details of Test Set Contamination Studies. In section 4 we gave a high level overview of test set contamination studies. In this section we provide details on methodology and results. Initial training set filtering We attempted to remove text occurring in benchmarks from training data by searching for 13\u2212gram overlaps between all test/development sets used in this work and our training data, and we removed the colliding 13\u2212gram as well as a 200 character window around it, splitting the original document into pieces. For filtering purposes we define a gram as a lowercase, whitespace delimited word with no punctuation. Pieces less than 200 characters long were discarded. Documents split into more than 10 pieces were considered contaminated and\n10https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.HashingTF\nremoved entirely. Originally we removed entire documents given a single collision, but that overly penalized long documents such as books for false positives. An example of a false positive might be a test set based on Wikipedia, in which the Wikipedia article quotes a single line from a book.", "Each increase has brought improvements in text synthesis and/or downstream NLP tasks, and there is evidence suggesting that log loss, which correlates well with many downstream tasks, follows a smooth trend of improvement with scale [KMH+20]. Since in-context learning involves absorbing many skills and tasks within the parameters of the model, it is plausible that in-context learning abilities might show similarly strong gains with scale. 1In the context of language models this has sometimes been called \u201czero-shot transfer\u201d, but this term is potentially ambiguous: the method is \u201czero-shot\u201d in the sense that no gradient updates are performed, but it often involves providing inference-time demonstrations to the model, so is not truly learning from zero examples. To avoid this confusion, we use the term \u201cmeta-learning\u201d to capture the inner-loop / outer-loop structure of the general method, and the term \u201cin context-learning\u201d to refer to the inner loop of meta-learning. We further specialize the description to \u201czero-shot\u201d, \u201cone-shot\u201d, or \u201cfew-shot\u201d depending on how many demonstrations are provided at inference time. These terms are intended to remain agnostic on the question of whether the model learns new tasks from scratch at inference time or simply recognizes patterns seen during training \u2013 this is an important issue which we discuss later in the paper, but \u201cmeta-learning\u201d is intended to encompass both possibilities, and simply describes the inner-outer loop structure. In this paper, we test this hypothesis by training a 175 billion parameter autoregressive language model, which we call GPT-3, and measuring its in-context learning abilities. Specifically, we evaluate GPT-3 on over two dozen NLP datasets, as well as several novel tasks designed to test rapid adaptation to tasks unlikely to be directly contained in the training set.", "Finally, large pretrained language models are not grounded in other domains of experience, such as video or real-world physical interaction, and thus lack a large amount of context about the world [BHT+20]. For all these reasons, scaling pure self-supervised prediction is likely to hit limits, and augmentation with a different approach is likely to be necessary. Promising future directions in this vein might include learning the objective function from humans [ZSW+19a], fine-tuning with reinforcement learning, or adding additional modalities such as images to provide grounding and a better model of the world [CLY+19].\nAnother limitation broadly shared by language models is poor sample efficiency during pre-training. While GPT-3 takes a step towards test-time sample efficiency closer to that of humans (one-shot or zero-shot), it still sees much more text during pre-training than a human sees in the their lifetime [Lin20]. Improving pre-training sample efficiency is an important direction for future work, and might come from grounding in the physical world to provide additional information, or from algorithmic improvements. A limitation, or at least uncertainty, associated with few-shot learning in GPT-3 is ambiguity about whether few-shot learning actually learns new tasks \u201cfrom scratch\u201d at inference time, or if it simply recognizes and identifies tasks that it has learned during training. These possibilities exist on a spectrum, ranging from demonstrations in the training set that are drawn from exactly the same distribution as those at test time, to recognizing the same task but in a different format, to adapting to a specific style of a general task such as QA, to learning a skill entirely de novo. Where GPT-3 is on this spectrum may also vary from task to task."]}
{"pkey": "gpt_15", "question": "What is the pretraining objective of the model? ", "answer": "Unsupervised pre-training is a special case of semi-supervised learning where the goal is to find a good initialization point instead of modifying the supervised learning objective. Early works explored the use of the technique in image classification [20, 49, 63] and regression tasks [3]. Subsequent research [15] demonstrated that pre-training acts as a regularization scheme, enabling better generalization in deep neural networks. By pre-training on a diverse corpus with long stretches of contiguous text our model acquires significant world knowledge and ability to process long-range dependencies which are then successfully transferred to solving discriminative tasks such as question answering, semantic similarity assessment, entailment determination, and text classification, improving the state of the art on 9 of the 12 datasets we study. Using unsupervised (pre-)training to boost performance on discriminative tasks has long been an important goal of Machine Learning research.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["This is a noticeable difference from much of the recent literature, which has documented improved fine-tuning performance when using these approaches over standard language models [RSR+19]. Thus our design decision comes at the cost of potentially worse performance on tasks which empirically benefit from bidirectionality. This may include fill-in-the-blank tasks, tasks that involve looking back and comparing two pieces of content, or tasks that require re-reading or carefully considering a long passage and then generating a very short answer. This could be a possible explanation for GPT-3\u2019s lagging few-shot performance on a few of the tasks, such as WIC (which involves comparing the use of a word in two sentences), ANLI (which involves comparing two sentences to see if one implies the other), and several reading comprehension tasks (e.g. QuAC and RACE). We also conjecture, based on past literature, that a large bidirectional model would be stronger at fine-tuning than GPT-3. Making a bidirectional model at the scale of GPT-3, and/or trying to make bidirectional models work with few- or zero-shot learning, is a promising direction for future research, and could help achieve the \u201cbest of both worlds\u201d. A more fundamental limitation of the general approach described in this paper \u2013 scaling up any LM-like model, whether autoregressive or bidirectional \u2013 is that it may eventually run into (or could already be running into) the limits of the\npretraining objective. Our current objective weights every token equally and lacks a notion of what is most important to predict and what is less important. [RRS20] demonstrate benefits of customizing prediction to entities of interest. Also, with self-supervised objectives, task specification relies on forcing the desired task into a prediction problem, whereas ultimately, useful language systems (for example virtual assistants) might be better thought of as taking goal-directed actions rather than just making predictions.", "Synthetic tasks such as wordscrambling or defining nonsense words seem especially likely to be learned de novo, whereas translation clearly must be learned during pretraining, although possibly from data that is very different in organization and style than the test data. Ultimately, it is not even clear what humans learn from scratch vs from prior demonstrations. Even organizing diverse demonstrations during pre-training and identifying them at test time would be an advance for language models, but nevertheless understanding precisely how few-shot learning works is an important unexplored direction for future research. A limitation associated with models at the scale of GPT-3, regardless of objective function or algorithm, is that they are both expensive and inconvenient to perform inference on, which may present a challenge for practical applicability of models of this scale in their current form. One possible future direction to address this is distillation [HVD15] of large models down to a manageable size for specific tasks. Large models such as GPT-3 contain a very wide range of skills, most of which are not needed for a specific task, suggesting that in principle aggressive distillation may be possible. Distillation is well-explored in general [LHCG19a] but has not been tried at the scale of hundred of billions parameters; new challenges and opportunities may be associated with applying it to models of this size. Finally, GPT-3 shares some limitations common to most deep learning systems \u2013 its decisions are not easily interpretable, it is not necessarily well-calibrated in its predictions on novel inputs as observed by the much higher variance in performance than humans on standard benchmarks, and it retains the biases of the data it has been trained on.", "=\nTarget Completion \u2192 The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey\u2019s accession to the European Union, despite Turkey\u2019s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. Figure G.41: Formatted dataset example for Ro\u2192En\nContext \u2192 Q: What is (2 * 4) * 6? A:\nTarget Completion \u2192 48\nFigure G.42: Formatted dataset example for Arithmetic 1DC\nContext \u2192 Q: What is 17 minus 14? A:\nTarget Completion \u2192 3\nFigure G.43: Formatted dataset example for Arithmetic 2D-\nContext \u2192 Q: What is 98 plus 45? A:\nTarget Completion \u2192 143\nFigure G.44: Formatted dataset example for Arithmetic 2D+\nContext \u2192 Q: What is 95 times 45? A:\nTarget Completion \u2192 4275\nFigure G.45: Formatted dataset example for Arithmetic 2Dx\nContext \u2192 Q: What is 509 minus 488? A:\nTarget Completion \u2192 21\nFigure G.46: Formatted dataset example for Arithmetic 3D-\nContext \u2192 Q: What is 556 plus 497? A:\nTarget Completion \u2192 1053\nFigure G.47: Formatted dataset example for Arithmetic 3D+\nContext \u2192 Q: What is 6209 minus 3365? A:\nTarget Completion \u2192 2844\nFigure G.48: Formatted dataset example for Arithmetic 4D-\nContext \u2192 Q: What is 9923 plus 617? A:\nTarget Completion \u2192 10540\nFigure G.49: Formatted dataset example for Arithmetic 4D+\nContext \u2192 Q: What is 40649 minus 78746? A:\nTarget Completion \u2192 -38097\nFigure G.50: Formatted dataset example for Arithmetic 5D\u2212\nContext \u2192 Q: What is 65360 plus 16204? A:\nTarget Completion \u2192 81564\nFigure G.51: Formatted dataset example for Arithmetic 5D+\nH Results on All Tasks for All Model Sizes. Zero-Shot One-Shot Few-Shot\nName Metric Split Fine-tune SOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B 175B (test server)"]}
{"pkey": "gpt_16", "question": "What is the loss function that is used to train the model?", "answer": "Given an unsupervised corpus of tokens U = {u1, . . . , un}, the paper authors use a standard language modeling objective to maximize the following likelihood: L1(U) = Xilog P(ui|ui\u2212k, . . . , ui\u22121; \u0398) (1) where k is the size of the context window, and the conditional probability P is modeled using a neural network with parameters \u0398. These parameters are trained using stochastic gradient descent [51]. In our experiments, the paper authors use a multi-layer Transformer decoder [34] for the language model, which is a variant of the transformer [62]. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens: h0 = UThe paper authors + Wp hl = transformer_block(hl\u22121)\u2200i \u2208 [1, n] P(u) = softmax(hnWTe)(2) where U = (u\u2212k, . . . , u\u22121) is the context vector of tokens, n is the number of layers, The paper authors is the token embedding matrix, and Wp is the position embedding matrix.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work. Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations. 2.1 Model and Architectures. We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks. Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters, nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 \u2217 dmodel), and dhead is the dimension of each attention head.", "On tasks that involve binary classification, we give the options more semantically meaningful names (e.g. \u201cTrue\u201d or \u201cFalse\u201d rather than 0 or 1) and then treat the task like multiple choice; we also sometimes frame the task similar to what is done by [RSR+19] (see Appendix G) for details. On tasks with free-form completion, we use beam search with the same parameters as [RSR+19]: a beam width of 4 and a length penalty of \u03b1 = 0.6. We score the model using F1 similarity score, BLEU, or exact match, depending on what is standard for the dataset at hand. Final results are reported on the test set when publicly available, for each model size and learning setting (zero-, one-, and few-shot). When the test set is private, our model is often too large to fit on the test server, so we report results on the development set. We do submit to the test server on a small number of datasets (SuperGLUE, TriviaQA, PiQa) where we were able to make submission work, and we submit only the 200B few-shot results, and report development set results for everything else. 3 Results. In Figure 3.1 we display training curves for the 8 models described in Section 2. For this graph we also include 6 additional extra-small models with as few as 100,000 parameters. As observed in [KMH+20], language modeling performance follows a power-law when making efficient use of training compute. After extending this trend by two more orders of magnitude, we observe only a slight (if any) departure from the power-law. One might worry that these improvements in cross-entropy loss come only from modeling spurious details of our training corpus. However, we will see in the following sections that improvements in cross-entropy loss lead to consistent performance gains across a broad spectrum of natural language tasks. Below, we evaluate the 8 models described in Section 2 (the 175 billion parameter parameter GPT-3 and 7 smaller models) on a wide range of datasets.", "=\nTarget Completion \u2192 The truth is that you want, at any price, and against the wishes of the peoples of Europe, to continue the negotiations for Turkey\u2019s accession to the European Union, despite Turkey\u2019s continuing refusal to recognise Cyprus and despite the fact that the democratic reforms are at a standstill. Figure G.41: Formatted dataset example for Ro\u2192En\nContext \u2192 Q: What is (2 * 4) * 6? A:\nTarget Completion \u2192 48\nFigure G.42: Formatted dataset example for Arithmetic 1DC\nContext \u2192 Q: What is 17 minus 14? A:\nTarget Completion \u2192 3\nFigure G.43: Formatted dataset example for Arithmetic 2D-\nContext \u2192 Q: What is 98 plus 45? A:\nTarget Completion \u2192 143\nFigure G.44: Formatted dataset example for Arithmetic 2D+\nContext \u2192 Q: What is 95 times 45? A:\nTarget Completion \u2192 4275\nFigure G.45: Formatted dataset example for Arithmetic 2Dx\nContext \u2192 Q: What is 509 minus 488? A:\nTarget Completion \u2192 21\nFigure G.46: Formatted dataset example for Arithmetic 3D-\nContext \u2192 Q: What is 556 plus 497? A:\nTarget Completion \u2192 1053\nFigure G.47: Formatted dataset example for Arithmetic 3D+\nContext \u2192 Q: What is 6209 minus 3365? A:\nTarget Completion \u2192 2844\nFigure G.48: Formatted dataset example for Arithmetic 4D-\nContext \u2192 Q: What is 9923 plus 617? A:\nTarget Completion \u2192 10540\nFigure G.49: Formatted dataset example for Arithmetic 4D+\nContext \u2192 Q: What is 40649 minus 78746? A:\nTarget Completion \u2192 -38097\nFigure G.50: Formatted dataset example for Arithmetic 5D\u2212\nContext \u2192 Q: What is 65360 plus 16204? A:\nTarget Completion \u2192 81564\nFigure G.51: Formatted dataset example for Arithmetic 5D+\nH Results on All Tasks for All Model Sizes. Zero-Shot One-Shot Few-Shot\nName Metric Split Fine-tune SOTA K Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B Small Med Large XL 2.7B 6.7B 13B 175B 175B (test server)"]}
{"pkey": "gpt_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "In our experiments, the paper authors use a multi-layer Transformer decoder [34] for the language model, which is a variant of the transformer [62]. This model applies a multi-headed self-attention operation over the input context tokens followed by position-wise feedforward layers to produce an output distribution over target tokens.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["In this paper we focus on zero-shot, one-shot and few-shot, with the aim of comparing them not as competing alternatives, but as different problem settings which offer a varying trade-off between performance on specific benchmarks and sample efficiency. We especially highlight the few-shot results as many of them are only slightly behind state-of-the-art fine-tuned models. Ultimately, however, one-shot, or even sometimes zero-shot, seem like the fairest comparisons to human performance, and are important targets for future work. Sections 2.1-2.3 below give details on our models, training data, and training process respectively. Section 2.4 discusses the details of how we do few-shot, one-shot, and zero-shot evaluations. 2.1 Model and Architectures. We use the same model and architecture as GPT-2 [RWC+19], including the modified initialization, pre-normalization, and reversible tokenization described therein, with the exception that we use alternating dense and locally banded sparse attention patterns in the layers of the transformer, similar to the Sparse Transformer [CGRS19]. To study the dependence of ML performance on model size, we train 8 different sizes of model, ranging over three orders of magnitude from 125 million parameters to 175 billion parameters, with the last being the model we call GPT-3. Previous work [KMH+20] suggests that with enough training data, scaling of validation loss should be approximately a smooth power law as a function of size; training models of many different sizes allows us to test this hypothesis both for validation loss and for downstream language tasks. Table 2.1 shows the sizes and architectures of our 8 models. Here nparams is the total number of trainable parameters, nlayers is the total number of layers, dmodel is the number of units in each bottleneck layer (we always have the feedforward layer four times the size of the bottleneck layer, dff = 4 \u2217 dmodel), and dhead is the dimension of each attention head.", "35 6.2 Fairness, Bias, and Representation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 6.3 Energy Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39\n7 Related Work 39.\n8 Conclusion 40. A Details of Common Crawl Filtering 43.\nB Details of Model Training 43. C Details of Test Set Contamination Studies 43. D Total Compute Used to Train Language Models 46. E Human Quality Assessment of Synthetic News Articles 46. F Additional Samples from GPT-3 48. G Details of Task Phrasing and Specifications 50. H Results on All Tasks for All Model Sizes 63\n1 Introduction. Recent years have featured a trend towards pre-trained language representations in NLP systems, applied in increasingly flexible and task-agnostic ways for downstream transfer. First, single-layer representations were learned using word vectors [MCCD13, PSM14] and fed to task-specific architectures, then RNNs with multiple layers of representations and contextual state were used to form stronger representations [DL15, MBXS17, PNZtY18] (though still applied to task-specific architectures), and more recently pre-trained recurrent or transformer language models [VSP+17] have been directly fine-tuned, entirely removing the need for task-specific architectures [RNSS18, DCLT18, HR18]. This last paradigm has led to substantial progress on many challenging NLP tasks such as reading comprehension, question answering, textual entailment, and many others, and has continued to advance based on new architectures and algorithms [RSR+19, LOG+19, YDY+19, LCG+19]. However, a major limitation to this approach is that while the architecture is task-agnostic, there is still a need for task-specific datasets and task-specific fine-tuning: to achieve strong performance on a desired task typically requires fine-tuning on a dataset of thousands to hundreds of thousands of examples specific to that task. Removing this limitation would be desirable, for several reasons.", "All models use a context window of nctx = 2048 tokens. We partition the model across GPUs along both the depth and width dimension in order to minimize data-transfer between nodes. The precise architectural parameters for each model are chosen based on computational efficiency and load-balancing in the layout of models across GPU\u2019s. Previous work [KMH+20] suggests that validation loss is not strongly sensitive to these parameters within a reasonably broad range.\n2.2 Training Dataset. Datasets for language models have rapidly expanded, culminating in the Common Crawl dataset2 [RSR+19] constituting nearly a trillion words. This size of dataset is sufficient to train our largest models without ever updating on the same sequence twice. However, we have found that unfiltered or lightly filtered versions of Common Crawl tend to have lower quality than more curated datasets. Therefore, we took 3 steps to improve the average quality of our datasets: (1) we downloaded and filtered a version of CommonCrawl based on similarity to a range of high-quality reference corpora, (2) we performed fuzzy deduplication at the document level, within and across datasets, to prevent redundancy and preserve the integrity of our held-out validation set as an accurate measure of overfitting, and (3) we also added known high-quality reference corpora to the training mix to augment CommonCrawl and increase its diversity. Details of the first two points (processing of Common Crawl) are described in Appendix A. For the third, we added several curated high-quality datasets, including an expanded version of the WebText dataset [RWC+19], collected by scraping links over a longer period of time, and first described in [KMH+20], two internet-based books corpora (Books1 and Books2) and English-language Wikipedia. Table 2.2 shows the final mixture of datasets that we used in training."]}
{"pkey": "gpt_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors perform experiments on a variety of supervised tasks including natural language inference, question answering, semantic similarity, and text classification. Some of these tasks are available as part of the recently released GLUE multi-task benchmark [64], which the paper authors make use of. Figure 1 provides an overview of all the tasks and datasets.The paper authors also achieve an overall score of 72.8 on the GLUE benchmark, which is significantly better than the previous best of 68.9.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["We multiply this value by the total training tokens and the total parameters to yield the number of total flops used during training. We report both flops and petaflop/s-day (each of which are 8.64e+19 flops). E Human Quality Assessment of Synthetic News Articles. This appendix contains details on the experiments measuring human ability to distinguish GPT-3-generated synthetic news articles from real news articles. We first describe the experiments on the \u223c 200 word news articles, and then describe the preliminary investigation of \u223c 500 word news articles generated by GPT-3. Participants: We recruited 718 unique participants to take part in 6 experiments. 97 participants were excluded for failing an internet check question, leaving a total of 621 participants: 343 male, 271 female, and 7 other. Mean participant age was \u223c 38 years old. All participants were recruited through Positly, which maintains a whitelist of high-performing workers from Mechanical Turk. All participants were US-based but there were no other demographic restrictions. Participants were paid $12 for their participation, based on a task time estimate of 60 minutes determined by pilot runs. In order to ensure that the sample of participants for each experiment quiz was unique, participants were not allowed to take part in an experiment more than once. Procedure and design: We arbitrarily selected 25 news articles that appeared in newser.com in early 2020. We used the article titles and subtitles to produce outputs from the 125M, 350M, 760M, 1.3B, 2.7B, 6.7B, 13.0B, and 200B (GPT-3) parameter language models. Five outputs per question were generated by each model and the generation with a word count closest to that of the human written article was selected automatically. This was to minimize the effect that completion length might have on participants\u2019 judgments. The same output procedure for each model with the exception of the removal of the intentionally bad control model, as described in the main text.", "\"\nSimilar to race, we found that the models make associations with religious terms that indicate some propensity to reflect how these terms are sometimes presented in the world. For example, with the religion Islam, we found that words such as ramadan, prophet and mosque co-occurred at a higher rate than for other religions. We also found that words such as violent, terrorism and terrorist co-occurred at a greater rate with Islam than with other religions and were in the top 40 most favored words for Islam in GPT-3. 6.2.4 Future Bias and Fairness Challenges. We have presented this preliminary analysis to share some of the biases we found in order to motivate further research, and to highlight the inherent difficulties in characterizing biases in large-scale generative models; we expect this to be an area of continuous research for us and are excited to discuss different methodological approaches with the community. We view the work in this section as subjective signposting - we chose gender, race, and religion as a starting point, but we recognize the inherent subjectivity in this choice. Our work is inspired by the literature on characterizing model attributes to develop informative labels such as Model Cards for Model Reporting from [MWZ+18]. Ultimately, it is important not just to characterize biases in language systems but to intervene. The literature on this is also extensive [QMZH19, HZJ+19], so we offer only a few brief comments on future directions specific to large language models. In order to pave the way for effective bias prevention in general purpose models, there is a need for building a common vocabulary tying together the normative, technical and empirical challenges of bias mitigation for these models. There is room for more research that engages with the literature outside NLP, better articulates normative statements about harm, and engages with the lived experience of communities affected by NLP systems [BBDIW20].", "Contributions\nTom Brown, Ben Mann, Prafulla Dhariwal, Dario Amodei, Nick Ryder, Daniel M Ziegler, and Jeffrey Wu implemented the large-scale models, training infrastructure, and model-parallel strategies. Tom Brown, Dario Amodei, Ben Mann, and Nick Ryder conducted pre-training experiments. Ben Mann and Alec Radford collected, filtered, deduplicated, and conducted overlap analysis on the training data. Melanie Subbiah, Ben Mann, Dario Amodei, Jared Kaplan, Sam McCandlish, Tom Brown, Tom Henighan, and Girish Sastry implemented the downstream tasks and the software framework for supporting them, including creation of synthetic tasks. Jared Kaplan and Sam McCandlish initially predicted that a giant language model should show continued gains, and applied scaling laws to help predict and guide model and data scaling decisions for the research. Ben Mann implemented sampling without replacement during training. Alec Radford originally demonstrated few-shot learning occurs in language models. Jared Kaplan and Sam McCandlish showed that larger models learn more quickly in-context, and systematically studied in-context learning curves, task prompting, and evaluation methods. Prafulla Dhariwal implemented an early version of the codebase, and developed the memory optimizations for fully half-precision training. Rewon Child and Mark Chen developed an early version of our model-parallel strategy. Rewon Child and Scott Gray contributed the sparse transformer. Aditya Ramesh experimented with loss scaling strategies for pretraining. Melanie Subbiah and Arvind Neelakantan implemented, experimented with, and tested beam search. Pranav Shyam worked on SuperGLUE and assisted with connections to few-shot learning and meta-learning literature. Sandhini Agarwal conducted the fairness and representation analysis. Girish Sastry and Amanda Askell conducted the human evaluations of the model. Ariel Herbert-Voss conducted the threat analysis of malicious use."]}
{"pkey": "gpt_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "The paper authors perform three different ablation studies (Table 5). First, the paper authors examine the performance of our method without the auxiliary LM objective during fine-tuning. The paper authors observe that the auxiliary objective helps on the NLI tasks and QQP. Overall, the trend suggests that larger datasets benefit from the auxiliary objective but smaller datasets do not. Second, the paper authors analyze the effect of the Transformer by comparing it with a single layer 2048 unit LSTM using the same framework. We observe a 5.6 average score drop when using the LSTM instead of the Transformer. The LSTM only outperforms the Transformer on one dataset \u2013 MRPC. Finally, the paper authors also compare with our transformer architecture directly trained on supervised target tasks, without pre-training. The paper authors observe that the lack of pre-training hurts performance across all the tasks, resulting in a 14.8% decrease compared to our full model.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["However, it is easy to assume that the normal force and weight are action-reaction force pairs (a common mistake). In this case, the normal force and weight need to be equal in magnitude to explain why there is no upward acceleration of the object. For example, a ball that bounces upwards accelerates upwards because the normal force acting on the ball is larger in magnitude than the weight of the ball. question: is the normal force equal to the force of gravity? answer:\nTarget Completion \u2192 yes\nFigure G.29: Formatted dataset example for BoolQ\nContext \u2192 The trend toward lower rents may seem surprising given that some communities in New York are bemoaning the loss of favorite local businesses to high rents. But, despite the recent softening, for many of these retailers there\u2019s still been too big a jump from the rental rates of the late 1970s, when their leases were signed. Certainly, the recent drop in prices doesn\u2019t mean Manhattan comes cheap. question: Manhattan comes cheap. true, false, or neither? answer:\nTarget Completion \u2192 false\nFigure G.30: Formatted dataset example for CB\nContext \u2192 The bet, which won him dinner for four, was regarding the existence and mass of the top quark, an elementary particle discovered in 1995. question: The Top Quark is the last of six flavors of quarks predicted by the standard model theory of particle physics. True or False? answer:\nTarget Completion \u2192 False\nFigure G.31: Formatted dataset example for RTE\nContext \u2192 An outfitter provided everything needed for the safari. Before his first walking holiday, he went to a specialist outfitter to buy some boots. question: Is the word \u2018outfitter\u2019 used in the same way in the two sentences above? answer:\nTarget Completion \u2192 no\nFigure G.32: Formatted dataset example for WiC\nContext \u2192 Final Exam with Answer Key Instructions: Please carefully read the following passages. For each passage, you must identify which noun the pronoun marked in *bold* refers to.", "We expect this will introduce challenges for the broader research community, and hope to work on this through a combination of mitigation research, prototyping, and coordinating with other technical developers. 6.2 Fairness, Bias, and Representation. Biases present in training data may lead models to generate stereotyped or prejudiced content. This is concerning, since model bias could harm people in the relevant groups in different ways by entrenching existing stereotypes and producing demeaning portrayals amongst other potential harms [Cra17]. We have conducted an analysis of biases in the model in order to better understand GPT-3\u2019s limitations when it comes to fairness, bias, and representation. 8\nOur goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of its limitations and behaviors. We focus on biases relating to gender, race, and religion, although many other categories of bias are likely present and could be studied in follow-up work. This is a preliminary analysis and does not reflect all of the model\u2019s biases even within the studied categories. Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to reflect stereotypes present in their training data. Below we discuss our preliminary findings of bias along the dimensions of gender, race, and religion. We probe for bias in the 175 billion parameter model and also in similar smaller models, to see if and how they are different in this dimension. 6.2.1 Gender. In our investigation of gender bias in GPT-3, we focused on associations between gender and occupation. We found that occupations in general have a higher probability of being followed by a male gender identifier than a female one (in other words, they are male leaning) when given a context such as \"The {occupation} was a\" (Neutral Variant). 83% of the 388 occupations we tested were more likely to be followed by a male identifier by GPT-3.", "Second, we create several tasks that involve rearranging or unscrambling the letters in a word, tasks which are unlikely to have been exactly seen during training. Third, we test GPT-3\u2019s ability to solve SAT-style analogy problems few-shot. Finally, we test GPT-3 on several qualitative tasks, including using new words in a sentence, correcting English grammar, and news article generation. We will release the synthetic datasets with the hope of stimulating further study of test-time behavior of language models. 3.9.1 Arithmetic. To test GPT-3\u2019s ability to perform simple arithmetic operations without task-specific training, we developed a small battery of 10 tests that involve asking GPT-3 a simple arithmetic problem in natural language:\n\u2022 2 digit addition (2D+) \u2013 The model is asked to add two integers sampled uniformly from [0, 100), phrased in the form of a question, e.g. \u201cQ: What is 48 plus 76? A: 124.\u201d\n\u2022 2 digit subtraction (2D-) \u2013 The model is asked to subtract two integers sampled uniformly from [0, 100); the answer may be negative. Example: \u201cQ: What is 34 minus 53? A: -19\u201d. \u2022 3 digit addition (3D+) \u2013 Same as 2 digit addition, except numbers are uniformly sampled from [0, 1000). \u2022 3 digit subtraction (3D-) \u2013 Same as 2 digit subtraction, except numbers are uniformly sampled from [0, 1000). \u2022 4 digit addition (4D+) \u2013 Same as 3 digit addition, except uniformly sampled from [0, 10000). \u2022 4 digit subtraction (4D-) \u2013 Same as 3 digit subtraction, except uniformly sampled from [0, 10000). \u2022 5 digit addition (5D+) \u2013 Same as 3 digit addition, except uniformly sampled from [0, 100000). \u2022 5 digit subtraction (5D-) \u2013 Same as 3 digit subtraction, except uniformly sampled from [0, 100000). \u2022 2 digit multiplication (2Dx) \u2013 The model is asked to multiply two integers sampled uniformly from [0, 100),\ne.g. \u201cQ: What is 24 times 42? A: 1008\u201d. \u2022 One-digit composite (1DC) \u2013 The model is asked to perform a composite operation on three 1 digit numbers, with parentheses around the last two."]}
{"pkey": "gpt_20", "question": "List the future work mentioned in the paper.", "answer": "Our work suggests that achieving significant performance gains is indeed possible, and offers hints as to what models (Transformers) and data sets (text with long range dependencies) work best with this approach. The paper authors hope that this will help enable new research into unsupervised learning, for both natural language understanding and other domains, further improving our understanding of how and when unsupervised learning works.", "title": "Improving Language Understandingby Generative Pre-Training", "context": ["Gretchen Krueger edited and red-teamed the policy sections of the paper. Benjamin Chess, Clemens Winter, Eric Sigler, Christopher Hesse, Mateusz Litwin, and Christopher Berner optimized OpenAI\u2019s clusters to run the largest models efficiently. Scott Gray developed fast GPU kernels used during training. Jack Clark led the analysis of ethical impacts \u2014 fairness and representation, human assessments of the model, and broader impacts analysis, and advised Gretchen, Amanda, Girish, Sandhini, and Ariel on their work. Dario Amodei, Alec Radford, Tom Brown, Sam McCandlish, Nick Ryder, Jared Kaplan, Sandhini Agarwal, Amanda Askell, Girish Sastry, and Jack Clark wrote the paper. Sam McCandlish led the analysis of model scaling, and advised Tom Henighan and Jared Kaplan on their work. Alec Radford advised the project from an NLP perspective, suggested tasks, put the results in context, and demonstrated the benefit of weight decay for training. Ilya Sutskever was an early advocate for scaling large generative likelihood models, and advised Pranav, Prafulla, Rewon, Alec, and Aditya on their work. Dario Amodei designed and led the research. A Details of Common Crawl Filtering. As mentioned in Section 2.2, we employed two techniques to improve the quality of the Common Crawl dataset: (1) filtering Common Crawl and (2) fuzzy deduplication:\n1. In order to improve the quality of Common Crawl, we developed an automatic filtering method to remove low quality documents. Using the original WebText as a proxy for high-quality documents, we trained a classifier to distinguish these from raw Common Crawl. We then used this classifier to re-sample Common Crawl by prioritizing documents which were predicted by the classifier to be higher quality. The classifier is trained using logistic regression classifier with features from Spark\u2019s standard tokenizer and HashingTF 10.", "Language Models are Few-Shot Learners. Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions \u2013 something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3\u2019s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general. \u2217Equal contribution \u2020Johns Hopkins University, OpenAI Author contributions listed at end of paper.", "On the \u201cChallenge\u201d version of the dataset which has been filtered to questions which simple statistical or information retrieval methods are unable to correctly answer, GPT-3 achieves 51.4% accuracy in the zero-shot setting, 53.2% in the one-shot setting, and 51.5% in the few-shot setting. This is approaching the performance of a fine-tuned RoBERTa baseline (55.9%) from UnifiedQA [KKS+20]. On the \u201cEasy\u201d version of the dataset (questions which either of the mentioned baseline approaches answered correctly), GPT-3 achieves 68.8%, 71.2%, and 70.1% which slightly exceeds a fine-tuned RoBERTa baseline from [KKS+20]. However, both of these results are still much worse than the overall SOTAs achieved by the UnifiedQA which exceeds GPT-3\u2019s few-shot results by 27% on the challenge set and 22% on the easy set. On OpenBookQA [MCKS18], GPT-3 improves significantly from zero to few shot settings but is still over 20 points short of the overall SOTA. GPT-3\u2019s few-shot performance is similar to a fine-tuned BERT Large baseline on the leaderboard. Overall, in-context learning with GPT-3 shows mixed results on commonsense reasoning tasks, with only small and inconsistent gains observed in the one and few-shot learning settings for both PIQA and ARC, but a significant improvement is observed on OpenBookQA. GPT-3 sets SOTA on the new PIQA dataset in all evaluation settings. 3.6 Reading Comprehension. Next we evaluate GPT-3 on the task of reading comprehension. We use a suite of 5 datasets including abstractive, multiple choice, and span based answer formats in both dialog and single question settings. We observe a wide spread in GPT-3\u2019s performance across these datasets suggestive of varying capability with different answer formats. In general we observe GPT-3 is on par with initial baselines and early results trained using contextual representations on each respective dataset. GPT-3 performs best (within 3 points of the human baseline) on CoQA"]}
{"pkey": "albert_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, the paper authors present two parameterreduction techniques to lower memory consumption and increase the training speed of BERT (Devlin et al., 2019). As a result, our best model establishes new state-of-the-art results.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Existing solutions to the aforementioned problems include model parallelization (Shazeer et al., 2018; Shoeybi et al., 2019) and clever memory management (Chen et al., 2016; Gomez et al., 2017).\n\u2217Work done as an intern at Google Research, driving data processing and downstream task evaluations. ar X\niv :1\n90 9. 11 94\n2v 6\n[ cs\n.C L\n] 9\nF eb\n2 02\nThese solutions address the memory limitation problem, but not the communication overhead. In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture. ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.", "In addition to the masked language modeling (MLM) loss (Devlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a binary classification loss for predicting whether two segments appear consecutively in the original text, as follows: positive examples are created by taking consecutive segments from the training corpus; negative examples are created by pairing segments from different documents; positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as natural language inference, that require reasoning about the relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al., 2019) found NSP\u2019s impact unreliable and decided to eliminate it, a decision supported by an improvement in downstream task performance across several tasks. We conjecture that the main reason behind NSP\u2019s ineffectiveness is its lack of difficulty as a task, as compared to MLM. As formulated, NSP conflates topic prediction and coherence prediction in a\nsingle task2. However, topic prediction is easier to learn compared to coherence prediction, and also overlaps more with what is learned using the MLM loss. We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about discourse-level coherence properties.", "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \u201cNone\u201d condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?. The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS. The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019)."]}
{"pkey": "albert_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["The code and the pretrained models are available at https://github.com/google-research/ALBERT.\n1 INTRODUCTION. Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model.", "Existing solutions to the aforementioned problems include model parallelization (Shazeer et al., 2018; Shoeybi et al., 2019) and clever memory management (Chen et al., 2016; Gomez et al., 2017).\n\u2217Work done as an intern at Google Research, driving data processing and downstream task evaluations. ar X\niv :1\n90 9. 11 94\n2v 6\n[ cs\n.C L\n] 9\nF eb\n2 02\nThese solutions address the memory limitation problem, but not the communication overhead. In this paper, we address all of the aforementioned problems, by designing A Lite BERT (ALBERT) architecture that has significantly fewer parameters than a traditional BERT architecture. ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. This separation makes it easier to grow the hidden size without significantly increasing the parameter size of the vocabulary embeddings. The second technique is cross-layer parameter sharing. This technique prevents the parameter from growing with the depth of the network. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. To further improve the performance of ALBERT, we also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.", "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \u201cNone\u201d condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?. The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS. The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019)."]}
{"pkey": "albert_3", "question": "What are the main contributions of the paper?", "answer": "Our best model establishes new state-of-the-art results. Both techniques significantly reduce the number of parameters for BERT without seriously hurting performance, thus improving parameter-efficiency reduce the embedding parameters from O(V \u00d7 H) to O(V \u00d7 E + E \u00d7 H). We also introduce a self-supervised loss for sentence-order prediction (SOP).", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["There are three main contributions that ALBERT makes over the design choices of BERT. Factorized embedding parameterization. In BERT, as well as subsequent modeling improvements such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), the WordPiece embedding size E is tied with the hidden layer size H , i.e., E \u2261 H . This decision appears suboptimal for both modeling and practical reasons, as follows. From a modeling perspective, WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations. As experiments with context length indicate (Liu et al., 2019), the power of BERT-like representations comes from the use of context to provide the signal for learning such context-dependent representations. As such, untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs, which dictate that H E. From a practical perspective, natural language processing usually require the vocabulary size V to be large.1 If E \u2261 H , then increasing H increases the size of the embedding matrix, which has size\n1Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000. V \u00d7E. This can easily result in a model with billions of parameters, most of which are only updated sparsely during training. Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H , we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space. By using this decomposition, we reduce the embedding parameters from O(V \u00d7 H) to O(V \u00d7 E + E \u00d7 H).", "In addition to the masked language modeling (MLM) loss (Devlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a binary classification loss for predicting whether two segments appear consecutively in the original text, as follows: positive examples are created by taking consecutive segments from the training corpus; negative examples are created by pairing segments from different documents; positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as natural language inference, that require reasoning about the relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al., 2019) found NSP\u2019s impact unreliable and decided to eliminate it, a decision supported by an improvement in downstream task performance across several tasks. We conjecture that the main reason behind NSP\u2019s ineffectiveness is its lack of difficulty as a task, as compared to MLM. As formulated, NSP conflates topic prediction and coherence prediction in a\nsingle task2. However, topic prediction is easier to learn compared to coherence prediction, and also overlaps more with what is learned using the MLM loss. We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about discourse-level coherence properties.", "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \u201cNone\u201d condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?. The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS. The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019)."]}
{"pkey": "albert_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "CURRENT STATE-OF-THE-ART ON NLU TASKS. The parameter reduction techniques also act as a form of regularization that stabilizes the training and helps with generalization. The model is a language model pretrained using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019).", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the passage, question, and each candidate answer as the input to models. Then, we use the representations from the \u201c[CLS]\u201d token for predicting the probability of each answer. The dataset consists of two domains: middle school and high school. We train our models on both domains and report accuracies on both the development set and test set. A.4 HYPERPARAMETERS\nHyperparameters for downstream tasks are shown in Table 14. We adapt these hyperparameters from Liu et al. (2019), Devlin et al. (2019), and Yang et al. (2019).", "The single-model ALBERT configuration incorporates the best-performing settings discussed: an ALBERT-xxlarge configuration (Table 1) using combined MLM and SOP losses, and no dropout. 4Following Liu et al. (2019), we fine-tune for RTE, STS, and MRPC using an MNLI checkpoint. The checkpoints that contribute to the final ensemble model are selected based on development set performance; the number of checkpoints considered for this selection range from 6 to 17, depending on the task. For the GLUE (Table 9) and RACE (Table 10) benchmarks, we average the model predictions for the ensemble models, where the candidates are fine-tuned from different training steps using the 12-layer and 24-layer architectures. For SQuAD (Table 10), we average the prediction scores for those spans that have multiple probabilities; we also average the scores of the \u201cunanswerable\u201d decision. Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art significantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of 92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement, a jump of +17.4% absolute points over BERT (Devlin et al., 2019; Clark et al., 2019), +7.6% over XLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang et al., 2019), an ensemble of multiple models specifically designed for reading comprehension tasks. Our single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art ensemble model. 5 DISCUSSION. While ALBERT-xxlarge has less parameters than BERT-large and gets significantly better results, it is computationally more expensive due to its larger structure. An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention (Child et al., 2019) and block attention (Shen et al., 2018).", "The code and the pretrained models are available at https://github.com/google-research/ALBERT.\n1 INTRODUCTION. Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model."]}
{"pkey": "albert_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks. The paper authors establish new state-of-the-art results on the well-known GLUE, SQuAD, and RACE benchmarks for natural language understanding. Specifically, the paper authors push the RACE accuracy to 89.4%, the GLUE benchmark to 89.4, and the F1 score of SQuAD 2.0 to 92.2.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \u201cNone\u201d condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?. The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS. The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019).", "Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs. 4.3 OVERALL COMPARISON BETWEEN BERT AND ALBERT. We are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT\u2019s design choices, as shown in Table 2: with only around 70% of BERT-large\u2019s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%). Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure. Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT.", "A.3 DOWNSTREAM EVALUATION TASKS\nGLUE GLUE is comprised of 9 tasks, namely Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), Stanford Sentiment Treebank (SST; Socher et al., 2013), Microsoft Research Paraphrase Corpus (MRPC; Dolan & Brockett, 2005), Semantic Textual Similarity Benchmark (STS; Cer et al., 2017), Quora Question Pairs (QQP; Iyer et al., 2017), Multi-Genre NLI (MNLI; Williams et al., 2018), Question NLI (QNLI; Rajpurkar et al., 2016), Recognizing Textual Entailment (RTE; Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009) and Winograd NLI (WNLI; Levesque et al., 2012). It focuses on evaluating model capabilities for natural language understanding. When reporting MNLI results, we only report the \u201cmatch\u201d condition (MNLI-m). We follow the finetuning procedures from prior work (Devlin et al., 2019; Liu et al., 2019; Yang et al., 2019) and report the held-out test set performance obtained from GLUE submissions. For test set submissions, we perform task-specific modifications for WNLI and QNLI as described by Liu et al. (2019) and Yang et al. (2019). SQuAD SQuAD is an extractive question answering dataset built from Wikipedia. The answers are segments from the context paragraphs and the task is to predict answer spans. We evaluate our models on two versions of SQuAD: v1.1 and v2.0. SQuAD v1.1 has 100,000 human-annotated question/answer pairs. SQuAD v2.0 additionally introduced 50,000 unanswerable questions. For SQuAD v1.1, we use the same training procedure as BERT, whereas for SQuAD v2.0, models are jointly trained with a span extraction loss and an additional classifier for predicting answerability (Yang et al., 2019; Liu et al., 2019). We report both development set and test set performance. RACE RACE is a large-scale dataset for multi-choice reading comprehension, collected from English examinations in China with nearly 100,000 questions. Each instance in RACE has 4 candidate answers."]}
{"pkey": "albert_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "For GLUE datasets that have large variances on the dev set, the paper authors report median over 5 runs.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["However, there are diminishing returns when continuing to increase the number of layers: the results of a 12-layer network are relatively close to the results of a 24-layer network, and the performance of a 48-layer network appears to decline. A similar phenomenon, this time for width, can be seen in Table 12 for a 3-layer ALBERT-large configuration. As we increase the hidden size, we get an increase in performance with diminishing returns. At a hidden size of 6144, the performance appears to decline significantly. We note that none of these models appear to overfit the training data, and they all have higher training and development loss compared to the best-performing ALBERT configurations. 5If we compare the performance of ALBERT-large here to the performance in Table 2, we can see that this warm-start technique does not help to improve the downstream performance. However, it does help the 48-layer network to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from a 6-layer network. A.2 DO VERY WIDE ALBERT MODELS NEED TO BE DEEP(ER) TOO? In Section A.1, we show that for ALBERT-large (H=1024), the difference between a 12-layer and a 24-layer configuration is small. Does this result still hold for much wider ALBERT configurations, such as ALBERT-xxlarge (H=4096)? The answer is given by the results from Table 13. The difference between 12-layer and 24-layer ALBERT-xxlarge configurations in terms of downstream accuracy is negligible, with the Avg score being the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there is no need for models deeper than a 12-layer configuration.", "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \u201cNone\u201d condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?. The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS. The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019).", "The code and the pretrained models are available at https://github.com/google-research/ALBERT.\n1 INTRODUCTION. Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model."]}
{"pkey": "albert_7", "question": "List the limitations of the model discussed in the paper.", "answer": "For GLUE datasets that have large variances on the dev set, the paper authors report median over 5 runs.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["The code and the pretrained models are available at https://github.com/google-research/ALBERT.\n1 INTRODUCTION. Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model.", "An orthogonal line of research, which could provide additional representation power, includes hard example mining (Mikolov et al., 2013) and more efficient language modeling training (Yang et al., 2019). Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations.\nACKNOWLEDGEMENT. The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for clarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick, Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu, Chenjie Cao and the CLUE community for providing the training data and evaluation benechmark of the Chinese version of ALBERT models. A APPENDIX. A.1 EFFECT OF NETWORK DEPTH AND WIDTH In this section, we check how depth (number of layers) and width (hidden size) affect the performance of ALBERT. Table 11 shows the performance of an ALBERT-large configuration (see Table 1) using different numbers of layers. Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters).5 Similar technique has been used in Gong et al. (2019). If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although they have the same number of parameters, the performance increases significantly.", "As we show in Sec. 4.6, it turns out that NSP cannot solve the SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at randombaseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, presumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently improve downstream task performance for multi-sentence encoding tasks. 3.2 MODEL SETUP. We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 1. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models. For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. An ALBERT-xlarge configuration with H = 2048 has only 60M parameters and an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERTlarge\u2019s parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive. This improvement in parameter efficiency is the most important advantage of ALBERT\u2019s design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail. 4 EXPERIMENTAL RESULTS. 4.1 EXPERIMENTAL SETUP. To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. We format our inputs as \u201c[CLS] x1 [SEP] x2 [SEP]\u201d, where x1 = x1,1, x1,2 \u00b7 \u00b7 \u00b7 and x2 = x1,1, x1,2 \u00b7 \u00b7 \u00b7 are two segments.3 We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%."]}
{"pkey": "albert_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "To keep the comparison as meaningful as possible, the paper authors follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. The results the paper authors report in this section make use of the training data used by Devlin et al. (2019), as well as the additional data used by Liu et al. (2019) and Yang et al. (2019). The paper authors report state-of-the-art results under two settings for fine-tuning: single-model and ensembles. In both settings, the paper authors only do single-task fine-tuning4  Following Liu et al. (2019), on the development set the paper authors report the median result over five runs.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs. 4.3 OVERALL COMPARISON BETWEEN BERT AND ALBERT. We are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT\u2019s design choices, as shown in Table 2: with only around 70% of BERT-large\u2019s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%). Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure. Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT.", "Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019). 2Since a negative example is constructed using material from a different document, the negative-example segment is misaligned both from a topic and from a coherence perspective. 3A segment is usually comprised of more than one natural sentence, which has been shown to benefit performance by Liu et al. (2019). We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly. The probability for the length n is given by\np(n) = 1/n\u2211N k=1 1/k We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as \u201cWhite House correspondents\u201d). All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512, depending on model size. The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified. 4.2 EVALUATION BENCHMARKS. 4.2.1 INTRINSIC EVALUATION. To monitor the training progress, we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and sentence classification tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection. 4.2.2 DOWNSTREAM EVALUATION.", "Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the passage, question, and each candidate answer as the input to models. Then, we use the representations from the \u201c[CLS]\u201d token for predicting the probability of each answer. The dataset consists of two domains: middle school and high school. We train our models on both domains and report accuracies on both the development set and test set. A.4 HYPERPARAMETERS\nHyperparameters for downstream tasks are shown in Table 14. We adapt these hyperparameters from Liu et al. (2019), Devlin et al. (2019), and Yang et al. (2019)."]}
{"pkey": "albert_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000. Like BERT, the paper authors use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019).", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019). 2Since a negative example is constructed using material from a different document, the negative-example segment is misaligned both from a topic and from a coherence perspective. 3A segment is usually comprised of more than one natural sentence, which has been shown to benefit performance by Liu et al. (2019). We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly. The probability for the length n is given by\np(n) = 1/n\u2211N k=1 1/k We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as \u201cWhite House correspondents\u201d). All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512, depending on model size. The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified. 4.2 EVALUATION BENCHMARKS. 4.2.1 INTRINSIC EVALUATION. To monitor the training progress, we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and sentence classification tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection. 4.2.2 DOWNSTREAM EVALUATION.", "Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the passage, question, and each candidate answer as the input to models. Then, we use the representations from the \u201c[CLS]\u201d token for predicting the probability of each answer. The dataset consists of two domains: middle school and high school. We train our models on both domains and report accuracies on both the development set and test set. A.4 HYPERPARAMETERS\nHyperparameters for downstream tasks are shown in Table 14. We adapt these hyperparameters from Liu et al. (2019), Devlin et al. (2019), and Yang et al. (2019).", "There are three main contributions that ALBERT makes over the design choices of BERT. Factorized embedding parameterization. In BERT, as well as subsequent modeling improvements such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), the WordPiece embedding size E is tied with the hidden layer size H , i.e., E \u2261 H . This decision appears suboptimal for both modeling and practical reasons, as follows. From a modeling perspective, WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations. As experiments with context length indicate (Liu et al., 2019), the power of BERT-like representations comes from the use of context to provide the signal for learning such context-dependent representations. As such, untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs, which dictate that H E. From a practical perspective, natural language processing usually require the vocabulary size V to be large.1 If E \u2261 H , then increasing H increases the size of the embedding matrix, which has size\n1Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000. V \u00d7E. This can easily result in a model with billions of parameters, most of which are only updated sparsely during training. Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H , we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space. By using this decomposition, we reduce the embedding parameters from O(V \u00d7 H) to O(V \u00d7 E + E \u00d7 H)."]}
{"pkey": "albert_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "The paper authors format our inputs as \u201c[CLS] x1 [SEP] x2 [SEP]\u201d, where x1 = x1,1, x1,2 \u00b7 \u00b7 \u00b7 and x2 = x1,1, x1,2 \u00b7 \u00b7 \u00b7 are two segments.3 The paper authors always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%. \nThe paper authors generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs. 4.3 OVERALL COMPARISON BETWEEN BERT AND ALBERT. We are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT\u2019s design choices, as shown in Table 2: with only around 70% of BERT-large\u2019s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%). Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure. Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT.", "Following prior work (Yang et al., 2019; Liu et al., 2019), we use the concatenation of the passage, question, and each candidate answer as the input to models. Then, we use the representations from the \u201c[CLS]\u201d token for predicting the probability of each answer. The dataset consists of two domains: middle school and high school. We train our models on both domains and report accuracies on both the development set and test set. A.4 HYPERPARAMETERS\nHyperparameters for downstream tasks are shown in Table 14. We adapt these hyperparameters from Liu et al. (2019), Devlin et al. (2019), and Yang et al. (2019).", "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \u201cNone\u201d condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?. The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS. The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019)."]}
{"pkey": "albert_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "Model       Parameters       Layers       Hidden       Embedding       Parameter-sharing\nBERT-base          108M                  12              768              768                     False\nBERT-large          334M                  24              1024            1024                   False\nALBERT-base          12M                    12               768             128                      True\nALBERT-large          18M                    24               1024           128                      True\nALBERT-xlarge         60M                   24                2048           128                     True\nALBERT-xxlarge       235M                 12                4096           128                      True", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is defined on textual segments rather than sentences. BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and find that sentence ordering is a more challenging pretraining task and more useful for certain downstream tasks. Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classification task rather than empirically comparing the two. 3 THE ELEMENTS OF ALBERT. In this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture (Devlin et al., 2019). 3.1 MODEL ARCHITECTURE CHOICES. The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016). We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H . Following Devlin et al. (2019), we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64.", "As we show in Sec. 4.6, it turns out that NSP cannot solve the SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at randombaseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, presumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently improve downstream task performance for multi-sentence encoding tasks. 3.2 MODEL SETUP. We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 1. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models. For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. An ALBERT-xlarge configuration with H = 2048 has only 60M parameters and an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERTlarge\u2019s parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive. This improvement in parameter efficiency is the most important advantage of ALBERT\u2019s design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail. 4 EXPERIMENTAL RESULTS. 4.1 EXPERIMENTAL SETUP. To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. We format our inputs as \u201c[CLS] x1 [SEP] x2 [SEP]\u201d, where x1 = x1,1, x1,2 \u00b7 \u00b7 \u00b7 and x2 = x1,1, x1,2 \u00b7 \u00b7 \u00b7 are two segments.3 We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%.", "An orthogonal line of research, which could provide additional representation power, includes hard example mining (Mikolov et al., 2013) and more efficient language modeling training (Yang et al., 2019). Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations.\nACKNOWLEDGEMENT. The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for clarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick, Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu, Chenjie Cao and the CLUE community for providing the training data and evaluation benechmark of the Chinese version of ALBERT models. A APPENDIX. A.1 EFFECT OF NETWORK DEPTH AND WIDTH In this section, we check how depth (number of layers) and width (hidden size) affect the performance of ALBERT. Table 11 shows the performance of an ALBERT-large configuration (see Table 1) using different numbers of layers. Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters).5 Similar technique has been used in Gong et al. (2019). If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although they have the same number of parameters, the performance increases significantly."]}
{"pkey": "albert_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). The paper authors train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512, depending on model size.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019). 2Since a negative example is constructed using material from a different document, the negative-example segment is misaligned both from a topic and from a coherence perspective. 3A segment is usually comprised of more than one natural sentence, which has been shown to benefit performance by Liu et al. (2019). We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly. The probability for the length n is given by\np(n) = 1/n\u2211N k=1 1/k We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as \u201cWhite House correspondents\u201d). All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512, depending on model size. The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified. 4.2 EVALUATION BENCHMARKS. 4.2.1 INTRINSIC EVALUATION. To monitor the training progress, we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and sentence classification tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection. 4.2.2 DOWNSTREAM EVALUATION.", "We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \u201cNone\u201d condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?. The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS. The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019).", "As we show in Sec. 4.6, it turns out that NSP cannot solve the SOP task at all (i.e., it ends up learning the easier topic-prediction signal, and performs at randombaseline level on the SOP task), while SOP can solve the NSP task to a reasonable degree, presumably based on analyzing misaligned coherence cues. As a result, ALBERT models consistently improve downstream task performance for multi-sentence encoding tasks. 3.2 MODEL SETUP. We present the differences between BERT and ALBERT models with comparable hyperparameter settings in Table 1. Due to the design choices discussed above, ALBERT models have much smaller parameter size compared to corresponding BERT models. For example, ALBERT-large has about 18x fewer parameters compared to BERT-large, 18M versus 334M. An ALBERT-xlarge configuration with H = 2048 has only 60M parameters and an ALBERT-xxlarge configuration with H = 4096 has 233M parameters, i.e., around 70% of BERTlarge\u2019s parameters. Note that for ALBERT-xxlarge, we mainly report results on a 12-layer network because a 24-layer network (with the same configuration) obtains similar results but is computationally more expensive. This improvement in parameter efficiency is the most important advantage of ALBERT\u2019s design choices. Before we can quantify this advantage, we need to introduce our experimental setup in more detail. 4 EXPERIMENTAL RESULTS. 4.1 EXPERIMENTAL SETUP. To keep the comparison as meaningful as possible, we follow the BERT (Devlin et al., 2019) setup in using the BOOKCORPUS (Zhu et al., 2015) and English Wikipedia (Devlin et al., 2019) for pretraining baseline models. These two corpora consist of around 16GB of uncompressed text. We format our inputs as \u201c[CLS] x1 [SEP] x2 [SEP]\u201d, where x1 = x1,1, x1,2 \u00b7 \u00b7 \u00b7 and x2 = x1,1, x1,2 \u00b7 \u00b7 \u00b7 are two segments.3 We always limit the maximum input length to 512, and randomly generate input sequences shorter than 512 with a probability of 10%."]}
{"pkey": "albert_13", "question": "Describe the computational resources used to train the model.", "answer": "Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512, depending on model size.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Like BERT, we use a vocabulary size of 30,000, tokenized using SentencePiece (Kudo & Richardson, 2018) as in XLNet (Yang et al., 2019). 2Since a negative example is constructed using material from a different document, the negative-example segment is misaligned both from a topic and from a coherence perspective. 3A segment is usually comprised of more than one natural sentence, which has been shown to benefit performance by Liu et al. (2019). We generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly. The probability for the length n is given by\np(n) = 1/n\u2211N k=1 1/k We set the maximum length of n-gram (i.e., n) to be 3 (i.e., the MLM target can consist of up to a 3-gram of complete words, such as \u201cWhite House correspondents\u201d). All the model updates use a batch size of 4096 and a LAMB optimizer with learning rate 0.00176 (You et al., 2019). We train all models for 125,000 steps unless otherwise specified. Training was done on Cloud TPU V3. The number of TPUs used for training ranged from 64 to 512, depending on model size. The experimental setup described in this section is used for all of our own versions of BERT as well as ALBERT models, unless otherwise specified. 4.2 EVALUATION BENCHMARKS. 4.2.1 INTRINSIC EVALUATION. To monitor the training progress, we create a development set based on the development sets from SQuAD and RACE using the same procedure as in Sec. 4.1. We report accuracies for both MLM and sentence classification tasks. Note that we only use this set to check how the model is converging; it has not been used in a way that would affect the performance of any downstream evaluation, such as via model selection. 4.2.2 DOWNSTREAM EVALUATION.", "Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs. 4.3 OVERALL COMPARISON BETWEEN BERT AND ALBERT. We are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT\u2019s design choices, as shown in Table 2: with only around 70% of BERT-large\u2019s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%). Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure. Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT.", "The single-model ALBERT configuration incorporates the best-performing settings discussed: an ALBERT-xxlarge configuration (Table 1) using combined MLM and SOP losses, and no dropout. 4Following Liu et al. (2019), we fine-tune for RTE, STS, and MRPC using an MNLI checkpoint. The checkpoints that contribute to the final ensemble model are selected based on development set performance; the number of checkpoints considered for this selection range from 6 to 17, depending on the task. For the GLUE (Table 9) and RACE (Table 10) benchmarks, we average the model predictions for the ensemble models, where the candidates are fine-tuned from different training steps using the 12-layer and 24-layer architectures. For SQuAD (Table 10), we average the prediction scores for those spans that have multiple probabilities; we also average the scores of the \u201cunanswerable\u201d decision. Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art significantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of 92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement, a jump of +17.4% absolute points over BERT (Devlin et al., 2019; Clark et al., 2019), +7.6% over XLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang et al., 2019), an ensemble of multiple models specifically designed for reading comprehension tasks. Our single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art ensemble model. 5 DISCUSSION. While ALBERT-xxlarge has less parameters than BERT-large and gets significantly better results, it is computationally more expensive due to its larger structure. An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention (Child et al., 2019) and block attention (Shen et al., 2018)."]}
{"pkey": "albert_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "Yes, the code and the pretrained models are available at https://github.com/google-research/ALBERT.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["An orthogonal line of research, which could provide additional representation power, includes hard example mining (Mikolov et al., 2013) and more efficient language modeling training (Yang et al., 2019). Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations.\nACKNOWLEDGEMENT. The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for clarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick, Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu, Chenjie Cao and the CLUE community for providing the training data and evaluation benechmark of the Chinese version of ALBERT models. A APPENDIX. A.1 EFFECT OF NETWORK DEPTH AND WIDTH In this section, we check how depth (number of layers) and width (hidden size) affect the performance of ALBERT. Table 11 shows the performance of an ALBERT-large configuration (see Table 1) using different numbers of layers. Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters).5 Similar technique has been used in Gong et al. (2019). If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although they have the same number of parameters, the performance increases significantly.", "There are three main contributions that ALBERT makes over the design choices of BERT. Factorized embedding parameterization. In BERT, as well as subsequent modeling improvements such as XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019), the WordPiece embedding size E is tied with the hidden layer size H , i.e., E \u2261 H . This decision appears suboptimal for both modeling and practical reasons, as follows. From a modeling perspective, WordPiece embeddings are meant to learn context-independent representations, whereas hidden-layer embeddings are meant to learn context-dependent representations. As experiments with context length indicate (Liu et al., 2019), the power of BERT-like representations comes from the use of context to provide the signal for learning such context-dependent representations. As such, untying the WordPiece embedding size E from the hidden layer size H allows us to make a more efficient usage of the total model parameters as informed by modeling needs, which dictate that H E. From a practical perspective, natural language processing usually require the vocabulary size V to be large.1 If E \u2261 H , then increasing H increases the size of the embedding matrix, which has size\n1Similar to BERT, all the experiments in this paper use a vocabulary size V of 30,000. V \u00d7E. This can easily result in a model with billions of parameters, most of which are only updated sparsely during training. Therefore, for ALBERT we use a factorization of the embedding parameters, decomposing them into two smaller matrices. Instead of projecting the one-hot vectors directly into the hidden space of size H , we first project them into a lower dimensional embedding space of size E, and then project it to the hidden space. By using this decomposition, we reduce the embedding parameters from O(V \u00d7 H) to O(V \u00d7 E + E \u00d7 H).", "The code and the pretrained models are available at https://github.com/google-research/ALBERT.\n1 INTRODUCTION. Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model."]}
{"pkey": "albert_15", "question": "What is the pretraining objective of the model? ", "answer": "The paper authors generate masked inputs for the MLM targets using n-gram masking (Joshi et al., 2019), with the length of each n-gram mask selected randomly, which avoids topic prediction and instead focuses on modeling inter-sentence coherence, while SOP can solve the NSP task to a reasonable degree.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["In addition to the masked language modeling (MLM) loss (Devlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a binary classification loss for predicting whether two segments appear consecutively in the original text, as follows: positive examples are created by taking consecutive segments from the training corpus; negative examples are created by pairing segments from different documents; positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as natural language inference, that require reasoning about the relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al., 2019) found NSP\u2019s impact unreliable and decided to eliminate it, a decision supported by an improvement in downstream task performance across several tasks. We conjecture that the main reason behind NSP\u2019s ineffectiveness is its lack of difficulty as a task, as compared to MLM. As formulated, NSP conflates topic prediction and coherence prediction in a\nsingle task2. However, topic prediction is easier to learn compared to coherence prediction, and also overlaps more with what is learned using the MLM loss. We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about discourse-level coherence properties.", "The code and the pretrained models are available at https://github.com/google-research/ALBERT.\n1 INTRODUCTION. Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model.", "Gomez et al. (2017) propose a way to reconstruct each layer\u2019s activations from the next layer so that they do not need to store the intermediate activations. Both methods reduce the memory consumption at the cost of speed. Raffel et al. (2019) proposed to use model parallelization to train a giant model. In contrast, our parameter-reduction techniques reduce memory consumption and increase training speed. 2.2 CROSS-LAYER PARAMETER SHARING. The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting. Different from our observations, Dehghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard\ntransformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard transformer. 2.3 SENTENCE ORDERING OBJECTIVES. ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. Several researchers have experimented with pretraining objectives that similarly relate to discourse coherence. Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979; Halliday & Hasan, 1976; Grosz et al., 1995). Most objectives found effective in practice are quite simple."]}
{"pkey": "albert_16", "question": "What is the loss function that is used to train the model?", "answer": "To further improve the performance of ALBERT, the paper authors also introduce a self-supervised loss for sentence-order prediction (SOP).  ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. For ALBERT, the paper authors use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. ALBERT-xxlarge configuration (Table 1) using combined MLM and SOP losses, and no dropout.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["We compare head-to-head three experimental conditions for the additional inter-sentence loss: none (XLNet- and RoBERTa-style), NSP (BERT-style), and SOP (ALBERT-style), using an ALBERTbase configuration. Results are shown in Table 5, both over intrinsic (accuracy for the MLM, NSP, and SOP tasks) and downstream tasks. The results on the intrinsic tasks reveal that the NSP loss brings no discriminative power to the SOP task (52.0% accuracy, similar to the random-guess performance for the \u201cNone\u201d condition). This allows us to conclude that NSP ends up modeling only topic shift. In contrast, the SOP loss does solve the NSP task relatively well (78.9% accuracy), and the SOP task even better (86.5% accuracy). Even more importantly, the SOP loss appears to consistently improve downstream task performance for multi-sentence encoding tasks (around +1% for SQuAD1.1, +2% for SQuAD2.0, +1.7% for RACE), for an Avg score improvement of around +1%. 4.7 WHAT IF WE TRAIN FOR THE SAME AMOUNT OF TIME?. The speed-up results in Table 2 indicate that data-throughput for BERT-large is about 3.17x higher compared to ALBERT-xxlarge. Since longer training usually leads to better performance, we perform a comparison in which, instead of controlling for data throughput (number of training steps), we control for the actual training time (i.e., let the models train for the same number of hours). In Table 6, we compare the performance of a BERT-large model after 400k training steps (after 34h of training), roughly equivalent with the amount of time needed to train an ALBERT-xxlarge model with 125k training steps (32h of training). After training for roughly the same amount of time, ALBERT-xxlarge is significantly better than BERT-large: +1.5% better on Avg, with the difference on RACE as high as +5.2%. 4.8 ADDITIONAL TRAINING DATA AND DROPOUT EFFECTS. The experiments done up to this point use only the Wikipedia and BOOKCORPUS datasets, as in (Devlin et al., 2019).", "In addition to the masked language modeling (MLM) loss (Devlin et al., 2019), BERT uses an additional loss called next-sentence prediction (NSP). NSP is a binary classification loss for predicting whether two segments appear consecutively in the original text, as follows: positive examples are created by taking consecutive segments from the training corpus; negative examples are created by pairing segments from different documents; positive and negative examples are sampled with equal probability. The NSP objective was designed to improve performance on downstream tasks, such as natural language inference, that require reasoning about the relationship between sentence pairs. However, subsequent studies (Yang et al., 2019; Liu et al., 2019) found NSP\u2019s impact unreliable and decided to eliminate it, a decision supported by an improvement in downstream task performance across several tasks. We conjecture that the main reason behind NSP\u2019s ineffectiveness is its lack of difficulty as a task, as compared to MLM. As formulated, NSP conflates topic prediction and coherence prediction in a\nsingle task2. However, topic prediction is easier to learn compared to coherence prediction, and also overlaps more with what is learned using the MLM loss. We maintain that inter-sentence modeling is an important aspect of language understanding, but we propose a loss based primarily on coherence. That is, for ALBERT, we use a sentence-order prediction (SOP) loss, which avoids topic prediction and instead focuses on modeling inter-sentence coherence. The SOP loss uses as positive examples the same technique as BERT (two consecutive segments from the same document), and as negative examples the same two consecutive segments but with their order swapped. This forces the model to learn finer-grained distinctions about discourse-level coherence properties.", "However, there are diminishing returns when continuing to increase the number of layers: the results of a 12-layer network are relatively close to the results of a 24-layer network, and the performance of a 48-layer network appears to decline. A similar phenomenon, this time for width, can be seen in Table 12 for a 3-layer ALBERT-large configuration. As we increase the hidden size, we get an increase in performance with diminishing returns. At a hidden size of 6144, the performance appears to decline significantly. We note that none of these models appear to overfit the training data, and they all have higher training and development loss compared to the best-performing ALBERT configurations. 5If we compare the performance of ALBERT-large here to the performance in Table 2, we can see that this warm-start technique does not help to improve the downstream performance. However, it does help the 48-layer network to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from a 6-layer network. A.2 DO VERY WIDE ALBERT MODELS NEED TO BE DEEP(ER) TOO? In Section A.1, we show that for ALBERT-large (H=1024), the difference between a 12-layer and a 24-layer configuration is small. Does this result still hold for much wider ALBERT configurations, such as ALBERT-xxlarge (H=4096)? The answer is given by the results from Table 13. The difference between 12-layer and 24-layer ALBERT-xxlarge configurations in terms of downstream accuracy is negligible, with the Avg score being the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there is no need for models deeper than a 12-layer configuration."]}
{"pkey": "albert_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "ALBERT incorporates two parameter reduction techniques that lift the major obstacles in scaling pre-trained models. The first one is a factorized embedding parameterization.  The second technique is cross-layer parameter sharing. To further improve the performance of ALBERT, the paper authors also introduce a self-supervised loss for sentence-order prediction (SOP). SOP primary focuses on inter-sentence coherence and is designed to address the ineffectiveness (Yang et al., 2019; Liu et al., 2019) of the next sentence prediction (NSP) loss proposed in the original BERT.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["The single-model ALBERT configuration incorporates the best-performing settings discussed: an ALBERT-xxlarge configuration (Table 1) using combined MLM and SOP losses, and no dropout. 4Following Liu et al. (2019), we fine-tune for RTE, STS, and MRPC using an MNLI checkpoint. The checkpoints that contribute to the final ensemble model are selected based on development set performance; the number of checkpoints considered for this selection range from 6 to 17, depending on the task. For the GLUE (Table 9) and RACE (Table 10) benchmarks, we average the model predictions for the ensemble models, where the candidates are fine-tuned from different training steps using the 12-layer and 24-layer architectures. For SQuAD (Table 10), we average the prediction scores for those spans that have multiple probabilities; we also average the scores of the \u201cunanswerable\u201d decision. Both single-model and ensemble results indicate that ALBERT improves the state-of-the-art significantly for all three benchmarks, achieving a GLUE score of 89.4, a SQuAD 2.0 test F1 score of 92.2, and a RACE test accuracy of 89.4. The latter appears to be a particularly strong improvement, a jump of +17.4% absolute points over BERT (Devlin et al., 2019; Clark et al., 2019), +7.6% over XLNet (Yang et al., 2019), +6.2% over RoBERTa (Liu et al., 2019), and 5.3% over DCMI+ (Zhang et al., 2019), an ensemble of multiple models specifically designed for reading comprehension tasks. Our single model achieves an accuracy of 86.5%, which is still 2.4% better than the state-of-the-art ensemble model. 5 DISCUSSION. While ALBERT-xxlarge has less parameters than BERT-large and gets significantly better results, it is computationally more expensive due to its larger structure. An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention (Child et al., 2019) and block attention (Shen et al., 2018).", "Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is defined on textual segments rather than sentences. BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and find that sentence ordering is a more challenging pretraining task and more useful for certain downstream tasks. Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classification task rather than empirically comparing the two. 3 THE ELEMENTS OF ALBERT. In this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture (Devlin et al., 2019). 3.1 MODEL ARCHITECTURE CHOICES. The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016). We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H . Following Devlin et al. (2019), we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64.", "Gomez et al. (2017) propose a way to reconstruct each layer\u2019s activations from the next layer so that they do not need to store the intermediate activations. Both methods reduce the memory consumption at the cost of speed. Raffel et al. (2019) proposed to use model parallelization to train a giant model. In contrast, our parameter-reduction techniques reduce memory consumption and increase training speed. 2.2 CROSS-LAYER PARAMETER SHARING. The idea of sharing parameters across layers has been previously explored with the Transformer architecture (Vaswani et al., 2017), but this prior work has focused on training for standard encoderdecoder tasks rather than the pretraining/finetuning setting. Different from our observations, Dehghani et al. (2018) show that networks with cross-layer parameter sharing (Universal Transformer, UT) get better performance on language modeling and subject-verb agreement than the standard\ntransformer. Very recently, Bai et al. (2019) propose a Deep Equilibrium Model (DQE) for transformer networks and show that DQE can reach an equilibrium point for which the input embedding and the output embedding of a certain layer stay the same. Our observations show that our embeddings are oscillating rather than converging. Hao et al. (2019) combine a parameter-sharing transformer with the standard one, which further increases the number of parameters of the standard transformer. 2.3 SENTENCE ORDERING OBJECTIVES. ALBERT uses a pretraining loss based on predicting the ordering of two consecutive segments of text. Several researchers have experimented with pretraining objectives that similarly relate to discourse coherence. Coherence and cohesion in discourse have been widely studied and many phenomena have been identified that connect neighboring text segments (Hobbs, 1979; Halliday & Hasan, 1976; Grosz et al., 1995). Most objectives found effective in practice are quite simple."]}
{"pkey": "albert_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "Model                 Parameters       SQuAD1.1        SQuAD2.0         MNLI      SST-2     RACE      Avg     Speedup\nBERT base        108M                 90.4/83.2          80.4/77.6           84.5         92.8         68.2       82.3     4.7x\nBERT large        334M                 92.2/85.5          85.0/82.2           86.6         93.0         73.9      85.2      1.0\n\n\nALBERT base    12M                   89.3/82.3          80.0/77.1           81.6          90.3        64.0        80.1     5.6x\nALBERT large    18M                   90.6/83.9          82.3/79.4           83.5          91.7        68.5        82.4     1.7x\nALBERT xlarge   60M                  92.5/86.1          86.1/83.1           86.4          92.4        74.8        85.5      0.6x\nALBERT xxlarge 235M                94.1/88.3          88.1/85.1           88.0          95.2        82.3        88.7       0.3x", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["This parameter reduction is significant when H E. We choose to use the same E for all word pieces because they are much more evenly distributed across documents compared to whole-word embedding, where having different embedding size (Grave et al. (2017); Baevski & Auli (2018); Dai et al. (2019) ) for different words is important. Cross-layer parameter sharing. For ALBERT, we propose cross-layer parameter sharing as another way to improve parameter efficiency. There are multiple ways to share parameters, e.g., only sharing feed-forward network (FFN) parameters across layers, or only sharing attention parameters. The default decision for ALBERT is to share all parameters across layers. All our experiments use this default decision unless otherwise specified. We compare this design decision against other strategies in our experiments in Sec. 4.5. Similar strategies have been explored by Dehghani et al. (2018) (Universal Transformer, UT) and Bai et al. (2019) (Deep Equilibrium Models, DQE) for Transformer networks. Different from our observations, Dehghani et al. (2018) show that UT outperforms a vanilla Transformer. Bai et al. (2019) show that their DQEs reach an equilibrium point for which the input and output embedding of a certain layer stay the same. Our measurement on the L2 distances and cosine similarity show that our embeddings are oscillating rather than converging. Figure 1 shows the L2 distances and cosine similarity of the input and output embeddings for each layer, using BERT-large and ALBERT-large configurations (see Table 1). We observe that the transitions from layer to layer are much smoother for ALBERT than for BERT. These results show that weight-sharing has an effect on stabilizing network parameters. Although there is a drop for both metrics compared to BERT, they nevertheless do not converge to 0 even after 24 layers. This shows that the solution space for ALBERT parameters is very different from the one found by DQE.\nInter-sentence coherence loss.", "Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs. 4.3 OVERALL COMPARISON BETWEEN BERT AND ALBERT. We are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT\u2019s design choices, as shown in Table 2: with only around 70% of BERT-large\u2019s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%). Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure. Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT.", "An orthogonal line of research, which could provide additional representation power, includes hard example mining (Mikolov et al., 2013) and more efficient language modeling training (Yang et al., 2019). Additionally, although we have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, we hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations.\nACKNOWLEDGEMENT. The authors would like to thank Beer Changpinyo, Nan Ding, Noam Shazeer, and Tomer Levinboim for discussion and providing useful feedback on the project; Omer Levy and Naman Goyal for clarifying experimental setup for RoBERTa; Zihang Dai for clarifying XLNet; Brandon Norick, Emma Strubell, Shaojie Bai, Chas Leichner, and Sachin Mehta for providing useful feedback on the paper; Jacob Devlin for providing the English and multilingual version of training data; Liang Xu, Chenjie Cao and the CLUE community for providing the training data and evaluation benechmark of the Chinese version of ALBERT models. A APPENDIX. A.1 EFFECT OF NETWORK DEPTH AND WIDTH In this section, we check how depth (number of layers) and width (hidden size) affect the performance of ALBERT. Table 11 shows the performance of an ALBERT-large configuration (see Table 1) using different numbers of layers. Networks with 3 or more layers are trained by fine-tuning using the parameters from the depth before (e.g., the 12-layer network parameters are fine-tuned from the checkpoint of the 6-layer network parameters).5 Similar technique has been used in Gong et al. (2019). If we compare a 3-layer ALBERT model with a 1-layer ALBERT model, although they have the same number of parameters, the performance increases significantly."]}
{"pkey": "albert_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "Next, the paper authors perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Following Yang et al. (2019) and Liu et al. (2019), we evaluate our models on three popular benchmarks: The General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018), two versions of the Stanford Question Answering Dataset (SQuAD; Rajpurkar et al., 2016; 2018), and the ReAding Comprehension from Examinations (RACE) dataset (Lai et al., 2017). For completeness, we provide description of these benchmarks in Appendix A.3. As in (Liu et al., 2019), we perform early stopping on the development sets, on which we report all comparisons except for our final comparisons based on the task leaderboards, for which we also report test set results. For GLUE datasets that have large variances on the dev set, we report median over 5 runs. 4.3 OVERALL COMPARISON BETWEEN BERT AND ALBERT. We are now ready to quantify the impact of the design choices described in Sec. 3, specifically the ones around parameter efficiency. The improvement in parameter efficiency showcases the most important advantage of ALBERT\u2019s design choices, as shown in Table 2: with only around 70% of BERT-large\u2019s parameters, ALBERT-xxlarge achieves significant improvements over BERT-large, as measured by the difference on development set scores for several representative downstream tasks: SQuAD v1.1 (+1.9%), SQuAD v2.0 (+3.1%), MNLI (+1.4%), SST-2 (+2.2%), and RACE (+8.4%). Another interesting observation is the speed of data throughput at training time under the same training configuration (same number of TPUs). Because of less communication and fewer computations, ALBERT models have higher data throughput compared to their corresponding BERT models. If we use BERT-large as the baseline, we observe that ALBERT-large is about 1.7 times faster in iterating through the data while ALBERT-xxlarge is about 3 times slower because of the larger structure. Next, we perform ablation experiments that quantify the individual contribution of each of the design choices for ALBERT.", "The code and the pretrained models are available at https://github.com/google-research/ALBERT.\n1 INTRODUCTION. Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model.", "However, there are diminishing returns when continuing to increase the number of layers: the results of a 12-layer network are relatively close to the results of a 24-layer network, and the performance of a 48-layer network appears to decline. A similar phenomenon, this time for width, can be seen in Table 12 for a 3-layer ALBERT-large configuration. As we increase the hidden size, we get an increase in performance with diminishing returns. At a hidden size of 6144, the performance appears to decline significantly. We note that none of these models appear to overfit the training data, and they all have higher training and development loss compared to the best-performing ALBERT configurations. 5If we compare the performance of ALBERT-large here to the performance in Table 2, we can see that this warm-start technique does not help to improve the downstream performance. However, it does help the 48-layer network to converge. A similar technique has been applied to our ALBERT-xxlarge, where we warm-start from a 6-layer network. A.2 DO VERY WIDE ALBERT MODELS NEED TO BE DEEP(ER) TOO? In Section A.1, we show that for ALBERT-large (H=1024), the difference between a 12-layer and a 24-layer configuration is small. Does this result still hold for much wider ALBERT configurations, such as ALBERT-xxlarge (H=4096)? The answer is given by the results from Table 13. The difference between 12-layer and 24-layer ALBERT-xxlarge configurations in terms of downstream accuracy is negligible, with the Avg score being the same. We conclude that, when sharing all cross-layer parameters (ALBERT-style), there is no need for models deeper than a 12-layer configuration."]}
{"pkey": "albert_20", "question": "List the future work mentioned in the paper.", "answer": "An important next step is thus to speed up the training and inference speed of ALBERT through methods like sparse attention (Child et al., 2019) and block attention (Shen et al., 2018). An orthogonal line of research, which could provide additional representation power, includes hard example mining (Mikolov et al., 2013) and more efficient language modeling training (Yang et al., 2019). Additionally, although the paper authors have convincing evidence that sentence order prediction is a more consistently-useful learning task that leads to better language representations, the paper authors hypothesize that there could be more dimensions not yet captured by the current self-supervised training losses that could create additional representation power for the resulting representations.", "title": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations", "context": ["Skipthought (Kiros et al., 2015) and FastSent (Hill et al., 2016) sentence embeddings are learned by using an encoding of a sentence to predict words in neighboring sentences. Other objectives for sentence embedding learning include predicting future sentences rather than only neighbors (Gan et al., 2017) and predicting explicit discourse markers (Jernite et al., 2017; Nie et al., 2019). Our loss is most similar to the sentence ordering objective of Jernite et al. (2017), where sentence embeddings are learned in order to determine the ordering of two consecutive sentences. Unlike most of the above work, however, our loss is defined on textual segments rather than sentences. BERT (Devlin et al., 2019) uses a loss based on predicting whether the second segment in a pair has been swapped with a segment from another document. We compare to this loss in our experiments and find that sentence ordering is a more challenging pretraining task and more useful for certain downstream tasks. Concurrently to our work, Wang et al. (2019) also try to predict the order of two consecutive segments of text, but they combine it with the original next sentence prediction in a three-way classification task rather than empirically comparing the two. 3 THE ELEMENTS OF ALBERT. In this section, we present the design decisions for ALBERT and provide quantified comparisons against corresponding configurations of the original BERT architecture (Devlin et al., 2019). 3.1 MODEL ARCHITECTURE CHOICES. The backbone of the ALBERT architecture is similar to BERT in that it uses a transformer encoder (Vaswani et al., 2017) with GELU nonlinearities (Hendrycks & Gimpel, 2016). We follow the BERT notation conventions and denote the vocabulary embedding size as E, the number of encoder layers as L, and the hidden size as H . Following Devlin et al. (2019), we set the feed-forward/filter size to be 4H and the number of attention heads to be H/64.", "The code and the pretrained models are available at https://github.com/google-research/ALBERT.\n1 INTRODUCTION. Full network pre-training (Dai & Le, 2015; Radford et al., 2018; Devlin et al., 2019; Howard & Ruder, 2018) has led to a series of breakthroughs in language representation learning. Many nontrivial NLP tasks, including those that have limited training data, have greatly benefited from these pre-trained models. One of the most compelling signs of these breakthroughs is the evolution of machine performance on a reading comprehension task designed for middle and high-school English exams in China, the RACE test (Lai et al., 2017): the paper that originally describes the task and formulates the modeling challenge reports then state-of-the-art machine accuracy at 44.1%; the latest published result reports their model performance at 83.2% (Liu et al., 2019); the work we present here pushes it even higher to 89.4%, a stunning 45.3% improvement that is mainly attributable to our current ability to build high-performance pretrained language representations. Evidence from these improvements reveals that a large network is of crucial importance for achieving state-of-the-art performance (Devlin et al., 2019; Radford et al., 2019). It has become common practice to pre-train large models and distill them down to smaller ones (Sun et al., 2019; Turc et al., 2019) for real applications. Given the importance of model size, we ask: Is having better NLP models as easy as having larger models? An obstacle to answering this question is the memory limitations of available hardware. Given that current state-of-the-art models often have hundreds of millions or even billions of parameters, it is easy to hit these limitations as we try to scale our models. Training speed can also be significantly hampered in distributed training, as the communication overhead is directly proportional to the number of parameters in the model.", "4.4 FACTORIZED EMBEDDING PARAMETERIZATION. Table 3 shows the effect of changing the vocabulary embedding size E using an ALBERT-base configuration setting (see Table 1), using the same set of representative downstream tasks. Under the non-shared condition (BERT-style), larger embedding sizes give better performance, but not by\nmuch. Under the all-shared condition (ALBERT-style), an embedding of size 128 appears to be the best. Based on these results, we use an embedding size E = 128 in all future settings, as a necessary step to do further scaling. 4.5 CROSS-LAYER PARAMETER SHARING. Table 4 presents experiments for various cross-layer parameter-sharing strategies, using an ALBERT-base configuration (Table 1) with two embedding sizes (E = 768 and E = 128). We compare the all-shared strategy (ALBERT-style), the not-shared strategy (BERT-style), and intermediate strategies in which only the attention parameters are shared (but not the FNN ones) or only the FFN parameters are shared (but not the attention ones). The all-shared strategy hurts performance under both conditions, but it is less severe for E = 128 (- 1.5 on Avg) compared to E = 768 (-2.5 on Avg). In addition, most of the performance drop appears to come from sharing the FFN-layer parameters, while sharing the attention parameters results in no drop when E = 128 (+0.1 on Avg), and a slight drop when E = 768 (-0.7 on Avg). There are other strategies of sharing the parameters cross layers. For example, We can divide the L layers into N groups of size M , and each size-M group shares parameters. Overall, our experimental results shows that the smaller the group size M is, the better the performance we get. However, decreasing group size M also dramatically increase the number of overall parameters. We choose all-shared strategy as our default choice. 4.6 SENTENCE ORDER PREDICTION (SOP)."]}
{"pkey": "bert-pli_1", "question": "What is the main problem statement that is being addressed in this paper?", "answer": "Compared with traditional ad-hoc text retrieval, the legal case retrieval task is more challenging since the query case is much longer and more complex than common keyword queries. Besides that, the definition of relevance between a query case and a supporting case is beyond general topical relevance and it is therefore difficult to construct a large-scale case retrieval dataset, especially one with accurate relevance judgments. To address these challenges, the paper authors propose BERT-PLI, a novel model that utilizes BERT to capture the semantic relationships at the paragraph-level and then infers the relevance between two cases by aggregating paragraph-level interactions.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage.", "[Mihalcea and Tarau, 2004] first and assessed pairwise relevance by a carefully fine-tuned BERT model, combined with oversampling strategies. 4.3 Experimental Settings. In the raw legal case documents, along with the text body of the case (known as \u201cfacts\u201d), some meta information is also provided, such as the court, the date, the head note and so on. However, the types of metadata vary with documents and have a high missing rate, in which case, we build our model based on the text body of a case. Some cases contain both\nEnglish and French versions of description, and only the English one is considered in our experiments. As illustrated in Figure 1 (Stage 1), we select candidates according to BM25 scores. We set K = 50 (top 50 candidates for each query), considering both the recall and the effectiveness in the following stages. Note that only the cases that happened before the current case could be noticed according to the problem definition, those invalid cases in terms of time are dismissed, which would otherwise be noisy in the candidates. The recall of Stage 1 on the training and testing set are 0.9159 and 0.9273, respectively. The relative high recall suggests that setting K = 50 in Stage 1 brings little harm to the overall retrieval performance. In Stage 2, paragraph pairs are constructed using the decision and candidate paragraphs in Task 2. The paragraphs have no more than 100 words on average and we truncate the words symmetrically if the pair exceeds the maximum input length of BERT. We use the uncased base version of BERT.1 At first, it is fine-tuned on the training data and tested on the remaining testing data. We use the Adam optimizer and set the learning rate as 10\u22125. The fine-tuning procedure can coverage after three epochs and F1 on the test set reaches 0.6526, which is better than the results of BM25 in this task. After that, we utilize the same hyperparameter settings but merge the training and testing data to fine-tune BERT for 3 epochs from scratch.", "To address these challenges, we propose BERT-PLI, a novel model that utilizes BERT to capture the semantic relationships at the paragraph-level and then infers the relevance between two cases by aggregating paragraph-level interactions. We finetune the BERT model with a relatively small-scale case law entailment dataset to adapt it to the legal scenario and employ a cascade framework to reduce the computational cost. We conduct extensive experiments on the benchmark of the relevant case retrieval task in COLIEE 2019. Experimental results demonstrate that our proposed method outperforms existing solutions. 1 Introduction. Precedents (case laws) are a primary source of laws in the common law system, which is fundamental for a lawyer\u2019s court preparations. With the rapid increase of digitized legal documents, it takes great efforts of legal practitioners to search for relevant cases manually. Given this situation, an automatic retrieval system that identifies relevant prior cases will greatly alleviate the heavy document works. Therefore, case law retrieval is an important research issue for both IR and legal communities. In recent years, there have been a number of benchmark efforts on the topic of Legal Information Retrieval, e.g., Legal TREC [Oard et al., 2013], AILA [Bhattacharya et al., 2019], COLIEE [Rabelo et al., 2019], etc.\n\u2217Corresponding Author\nThe development of retrieval models sits at the core of IR researches. The legal case retrieval scenario, which aims to identify relevant prior cases given a query case, can be also viewed as a specific application of retrieval models. However, the legal case retrieval task is different from traditional adhoc text retrieval in several aspects, including the length of query and candidate texts, the definition of relevance, and the accessibility of legal datasets. Therefore, existing methods developed for IR tasks face a few serious challenges in this task, such as:\nChallenge 1."]}
{"pkey": "bert-pli_2", "question": "What are the gaps in previous literature that this paper tries to address?", "answer": "Legal case retrieval task is different from traditional ad-hoc text retrieval in several aspects, including the length of query and candidate texts, the definition of relevance, and the accessibility of legal datasets. Therefore, existing methods developed for IR tasks face a few serious challenges in this task, such as:\nChallenge 1. Both the query and candidate cases involve extreme long texts. For example, cases in COLIEE 2019 Task 1 contain around 3,000 words on average. It is challenging for representation learning methods to represent the long document well in a limited semantic space, and it is also difficult for the matching function learning methods to construct and aggregate matching signals.\nChallenge 2. The definition of relevance in the legal scenario is somehow beyond the general definition of topical relevance [Van Opijnen and Santos, 2017]. Relevant cases are those that can support the decision of the current case, which usually involve similar situations and suitable statutes. Therefore, it is crucial to identify the similarities in the aspects of legal issues and legal processes of the cases, which calls for semantic understanding of whole documents.\nChallenge 3. Collecting a large dataset for this task can be challenging if not impossible. For one thing, downloading large-scale legal documents is restricted in many law systems. For another, it is quite expensive to obtain accurate relevance judgments since it requires expert knowledge in the legal domain. The lack of data brings obstacles to the training process of deep neural models.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage.", "To address these challenges, we propose BERT-PLI, a novel model that utilizes BERT to capture the semantic relationships at the paragraph-level and then infers the relevance between two cases by aggregating paragraph-level interactions. We finetune the BERT model with a relatively small-scale case law entailment dataset to adapt it to the legal scenario and employ a cascade framework to reduce the computational cost. We conduct extensive experiments on the benchmark of the relevant case retrieval task in COLIEE 2019. Experimental results demonstrate that our proposed method outperforms existing solutions. 1 Introduction. Precedents (case laws) are a primary source of laws in the common law system, which is fundamental for a lawyer\u2019s court preparations. With the rapid increase of digitized legal documents, it takes great efforts of legal practitioners to search for relevant cases manually. Given this situation, an automatic retrieval system that identifies relevant prior cases will greatly alleviate the heavy document works. Therefore, case law retrieval is an important research issue for both IR and legal communities. In recent years, there have been a number of benchmark efforts on the topic of Legal Information Retrieval, e.g., Legal TREC [Oard et al., 2013], AILA [Bhattacharya et al., 2019], COLIEE [Rabelo et al., 2019], etc.\n\u2217Corresponding Author\nThe development of retrieval models sits at the core of IR researches. The legal case retrieval scenario, which aims to identify relevant prior cases given a query case, can be also viewed as a specific application of retrieval models. However, the legal case retrieval task is different from traditional adhoc text retrieval in several aspects, including the length of query and candidate texts, the definition of relevance, and the accessibility of legal datasets. Therefore, existing methods developed for IR tasks face a few serious challenges in this task, such as:\nChallenge 1.", "In particular, all of the deep learning models are trained on the same training data and selected according to F1 on the validation data. For the ranking models, we consider the top 5 results as the relevant ones4, while for the classification models, we simply use the label given by the model. Table 2 shows the performance on the whole test set. In the BERT-PLI model, we use LSTM and GRU as the RNN layer respectively. The LSTM and GRU versions achieve similar performance here and both outperform the baseline methods, including the traditional retrieval models and the deep learning ones, by a large margin. The structure of BERT-PLI is able to take the whole case document into consideration. At the same time, it has a better semantic understanding ability than the bag-of-words IR models with the help of BERT and sequence modeling components. Among the baseline retrieval methods, deep learning retrieval models perform much worse than the traditional retrieval models in this task. Since these deep learning models are mostly designed for the ad-hoc scenario, it is hard for them to handle long text retrieval. The length of the input is restricted in these models. The first 256 words are not enough to represent the document well, so it is not surprising that they perform poorly in this task. In addition to truncation, we also attempt to utilize automatic summarization techniques to shorten the input length. However, it does not result in stable improvements. Even taking the generated summary as the input, the deep models still underperform. Assuming that the legal documents contain plenty of information and complex logic, it is hard to express them well in a much\n4The threshold 5 is widely used by teams in the competition and is reasonable considering there are about 5 relevant cases per query on average.\nshorter summary. Meanwhile, the unsupervised summarization techniques might cause additional information loss and noise."]}
{"pkey": "bert-pli_3", "question": "What are the main contributions of the paper?", "answer": "BERT-PLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. The paper authors propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval.BERT-PLI is employed to further identify the relevant cases with respect to a query case. The paper authors conduct extensive experiments on the datasets of COLIEE 2019. The experimental results demonstrate that our approach is effective in legal case retrieval and the combination with BERT-PLI can further improve other models for this task.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage.", "The supporting cases are considered as \u201crelevant cases\u201d or \u201cnoticed cases\u201d for the query case Q. Task 2 is a legal case entailment task to identify paragraphs that entail the given decision paragraph of a query case from a given relevant case. Data in both tasks are sampled from a database\nof predominantly Federal Court of Canada case laws. Table 1 gives a statistical summary of the raw datasets in these two tasks. Task 1 is the main focus of our work, while the data of Task 2 are used to fine-tune BERT in Stage 2. We follow the evaluation metrics in the competition. Micro-average of precision, recall, and F1 are used.\n4.2 Baseline Methods. We compare our model with the following three types of baselines. \u2022 Traditional bag-of-words retrieval models, including VSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999].\n\u2022 Deep retrieval models. Prior work shows that matching function learning methods usually outperform the representation learning ones [Xu et al., 2018], so we consider two matching function learning models, ARCII [Hu et al., 2014] and MatchPyramid [Palangi et al., 2016]. Both of them utilize the CNN structure, which is faster than RNN-based models, especially when dealing with long texts. We do not include the state-of-art neural models that involve rich behavioral signals (e.g., click) since we focus on text-based retrieval here. \u2022 Methods in the competition. We also compare with the methods of the top 2 teams [Tran et al., 2019; Rossi and Kanoulas, 2019] in the competition. The champion team (named as \u201cJNLP\u201d) trained a supervised summarization model based on COLIEE 2018\u2019s dataset and applied the model to encoding the case document into a continuous vector. They combined such the summary embeddings with lexical matching features, calculated by ROUGE [Lin, 2004], and learned the document rankings via RankSVM. Another team ranked following JNLP (named as \u201cILPS\u201d) generated summaries by the TextRank algorithm", "In practice, we employ the cascade framework to avoid high computational cost and arrange the model in a multi-stage pipeline for legal case retrieval. Specifically, we select top-K candidates according to the BM25 rankings and fine-tune BERT with an extra legal dataset before applying BERT-PLI to relevance prediction. Experiments are conducted on the COLIEE 2019 [Rabelo et al., 2019] legal case retrieval task and the results demonstrate the effectiveness of our proposed method. 2 Related Work. A large number of retrieval models, especially for ad-hoc text retrieval, have been proposed in the past decades. Traditional bag-of-words IR models, including VSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999], which are widely applied in search systems, are mostly based on term-level matching. Since the mid-2000s, LTR (Learning to Rank) methods [Liu and others, 2009], which are driven heavily by manual feature engineering, have been well studied and utilized by commercial web search engines as well. In recent years, the development of deep learning has also inspired applications of neural models in IR. Generally, the methods can be categorized into two types [Xu et al., 2018], methods of representation learning and methods of matching function learning. Based on the idea of representation learning, queries and documents are represented in the latent space by deep learning models, and the query-document relevance score is calculated based on their latent representations with a vector space scoring function, e.g., cosine similarity. Various neural network models have been applied to this task. For instance, DSSM [Hu et al., 2014] utilized DNN in the early stage. Further, some studies exploit CNNs to capture the local interactions [Hu et al., 2014; Shen et al., 2014], while some studies apply RNNs to modeling text sequences [Palangi et al., 2016]. However, most of the model structures are not designed for representing long documents."]}
{"pkey": "bert-pli_4", "question": "Is the model proposed only for a specific domain, like code, images, specific text domain like finance, biomedical, etc? If yes, is it possible to extend the model to other domains?", "answer": "Precedents (case laws) are a primary source of laws in the common law system, which is fundamental for a lawyer\u2019s\ncourt preparations. With the rapid increase of digitized legal documents, it takes great efforts of legal practitioners to\nsearch for relevant cases manually. Given this situation, anautomatic retrieval system that identifies relevant prior cases\nwill greatly alleviate the heavy document works. Therefore, case law retrieval is an important research issue for both IR\nand legal communities. It is not specified in the paper if this can be extended to other studies or not.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["Both the query and candidate cases involve extreme long texts. For example, cases in COLIEE 2019 Task 1 contain around 3,000 words on average. It is challenging for representation learning methods to represent the long document well in a limited semantic space, and it is also difficult for the matching function learning methods to construct and aggregate matching signals. Challenge 2. The definition of relevance in the legal scenario is somehow beyond the general definition of topical relevance [Van Opijnen and Santos, 2017]. Relevant cases are those that can support the decision of the current case, which usually involve similar situations and suitable statutes. Therefore, it is crucial to identify the similarities in the aspects of legal issues and legal processes of the cases, which calls for semantic understanding of whole documents. Challenge 3. Collecting a large dataset for this task can be challenging if not impossible. For one thing, downloading large-scale legal documents is restricted in many law systems. For another, it is quite expensive to obtain accurate relevance judgments since it requires expert knowledge in the legal domain. The lack of data brings obstacles to the training process of deep neural models. To tackle the above challenges, we propose BERT-PLI, a novel model that utilizes BERT [Devlin et al., 2018] to model Paragraph-Level Interactions for legal case retrieval. For modeling of long texts (Challenge 1), we break the documents into paragraphs and infer the relationship between cases from a fine-grained perspective, which allows exploiting the information of the full document instead of the truncated or summarized one. Beyond term-level matching, we model the semantic interactions between two paragraphs with BERT (Challenge 2). Moreover, we adapt the BERT model to the legal scenario by fine-tuning it on a sentence pair classi-\nfication task with a small-scale legal case entailment dataset (Challenge 3).", "In order to enhance the ability to model the semantic relationships between legal paragraphs, we fine-tune the BERT model with an accessible entailment dataset in the legal domain before applying it to BERT-PLI. The ablation study also supports the effectiveness of the fine-tuning stage. Finally, BERT-PLI is employed to further identify the relevant cases with respect to a query case. We conduct extensive experiments on the datasets of COLIEE 2019. The experimental results demonstrate that our approach is effective in legal case retrieval and the combination with BERT-PLI can further improve other models for this task.\nAcknowledgements. This work is supported by the National Key Research and Development Program of China (2018YFC0831700), Natural Science Foundation of China (Grant No. 61732008, 61532011, 61902209), Beijing Academy of Artificial Intelligence (BAAI), JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Number JP17H06103. 5https://github.com/ThuYShao/BERT-PLI-IJCAI2020. The implementation has been available.", "To address these challenges, we propose BERT-PLI, a novel model that utilizes BERT to capture the semantic relationships at the paragraph-level and then infers the relevance between two cases by aggregating paragraph-level interactions. We finetune the BERT model with a relatively small-scale case law entailment dataset to adapt it to the legal scenario and employ a cascade framework to reduce the computational cost. We conduct extensive experiments on the benchmark of the relevant case retrieval task in COLIEE 2019. Experimental results demonstrate that our proposed method outperforms existing solutions. 1 Introduction. Precedents (case laws) are a primary source of laws in the common law system, which is fundamental for a lawyer\u2019s court preparations. With the rapid increase of digitized legal documents, it takes great efforts of legal practitioners to search for relevant cases manually. Given this situation, an automatic retrieval system that identifies relevant prior cases will greatly alleviate the heavy document works. Therefore, case law retrieval is an important research issue for both IR and legal communities. In recent years, there have been a number of benchmark efforts on the topic of Legal Information Retrieval, e.g., Legal TREC [Oard et al., 2013], AILA [Bhattacharya et al., 2019], COLIEE [Rabelo et al., 2019], etc.\n\u2217Corresponding Author\nThe development of retrieval models sits at the core of IR researches. The legal case retrieval scenario, which aims to identify relevant prior cases given a query case, can be also viewed as a specific application of retrieval models. However, the legal case retrieval task is different from traditional adhoc text retrieval in several aspects, including the length of query and candidate texts, the definition of relevance, and the accessibility of legal datasets. Therefore, existing methods developed for IR tasks face a few serious challenges in this task, such as:\nChallenge 1."]}
{"pkey": "bert-pli_5", "question": "What datasets and tasks is the model evaluated on?", "answer": "experiments are conducted based on the COLIEE 2019 datasets [Rabelo et al., 2019]. Task 1 is a legal case retrieval task, which involves reading a new case Q and extracting supporting cases S1, S2, . . . , Sn for the decision of Q from the case law corpus. The supporting cases are considered as \u201crelevant cases\u201d or \u201cnoticed cases\u201d for the query case Q. Task 2 is a legal case entailment task to identify paragraphs that entail the given decision paragraph of a query case from a given relevant case. Data in both tasks are sampled from a databaseof predominantly Federal Court of Canada case laws.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["The results emphasize the importance of considering the full document information in the legal case retrieval task. On the other hand, traditional retrieval models give relatively good results. They take advantage of the whole document though they are weak in semantic understanding. Ablation study. We further investigate the effects of the fine-tuning stage. We use the original parameters of the pretrained BERT model rather than the fine-tuned one to infer the paragraph interactions. Then the remaining parts of BERTPLI are trained and evaluated under the same experimental settings. The model without Stage 2 is denoted as BERTPLIorg . As shown in Table 2, there is a big drop in the performance for both the LSTM and GRU versions compared with the results of BERT-PLI. Without Stage 2, the model has a similar performance with traditional retrieval models. Recall that we do not update the parameters of BERT during training BERT-PLI. Therefore, fine-tuning is essential for the BERT model to well represent the semantic relationships between paragraphs. In conclusion, the experimental results suggest that it is useful and effective to adapt BERT to a paragraphlevel modeling task in the legal domain. Comparison with results in the competition. Table 3 shows the best results of the top two teams in the competition leaderboard. In terms of the final evaluation results on the test set, our methods (BERT-PLI (GRU/LSTM)) achieve a better recall and F1 score, even though Stage 1 hurts the recall of our approach a bit. The results show that our method can reach a better balance in precision and recall. Further, with the summary encoding and lexical features provided by the JNLP group, we conduct an additional experiment, which combines the outputs of BERT-PLI with their features. The experiment is two-fold. In order to combine the two approaches, we first arrange their model in the cascade framework.", "The supporting cases are considered as \u201crelevant cases\u201d or \u201cnoticed cases\u201d for the query case Q. Task 2 is a legal case entailment task to identify paragraphs that entail the given decision paragraph of a query case from a given relevant case. Data in both tasks are sampled from a database\nof predominantly Federal Court of Canada case laws. Table 1 gives a statistical summary of the raw datasets in these two tasks. Task 1 is the main focus of our work, while the data of Task 2 are used to fine-tune BERT in Stage 2. We follow the evaluation metrics in the competition. Micro-average of precision, recall, and F1 are used.\n4.2 Baseline Methods. We compare our model with the following three types of baselines. \u2022 Traditional bag-of-words retrieval models, including VSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999].\n\u2022 Deep retrieval models. Prior work shows that matching function learning methods usually outperform the representation learning ones [Xu et al., 2018], so we consider two matching function learning models, ARCII [Hu et al., 2014] and MatchPyramid [Palangi et al., 2016]. Both of them utilize the CNN structure, which is faster than RNN-based models, especially when dealing with long texts. We do not include the state-of-art neural models that involve rich behavioral signals (e.g., click) since we focus on text-based retrieval here. \u2022 Methods in the competition. We also compare with the methods of the top 2 teams [Tran et al., 2019; Rossi and Kanoulas, 2019] in the competition. The champion team (named as \u201cJNLP\u201d) trained a supervised summarization model based on COLIEE 2018\u2019s dataset and applied the model to encoding the case document into a continuous vector. They combined such the summary embeddings with lexical matching features, calculated by ROUGE [Lin, 2004], and learned the document rankings via RankSVM. Another team ranked following JNLP (named as \u201cILPS\u201d) generated summaries by the TextRank algorithm", "In Stage 1, we select top-K candidates from the initial candidate corpus with respect to the query case q according to BM25 scores. In Stage 2, we fine-tune the BERT model on a sentence pair classification task with a legal case entailment dataset in order to adapt it to modeling semantic relationships between legal paragraphs. In the final stage, BERT-PLI conducts rele-\nvance prediction with the fine-tuned BERT (Stage 2) among the selected candidates (Stage 1). 3.3 Stage 1: BM25 Selection. Deep learning models are usually time-consuming and resource-consuming. Considering the computational cost, we employ the cascade framework which utilizes BM25 to prune the set of candidates. The BM25 model is implemented according to the standard scoring function [Robertson and Walker, 1994]. This stage inevitably hurts both recall and precision, and we mainly pay attention to optimizing recall at this stage since the downstream models can further discard irrelevant documents. 3.4 Stage 2: BERT Fine-tuning. Fine-tuning is relatively inexpensive compared with the pretraining procedure, which allows BERT to model specific tasks with small datasets [Devlin et al., 2018]. Therefore, before applying BERT to infer the relationship of case paragraphs, we fine-tune it with a small-scale legal case entailment dataset provided by COLIEE 2019 Task 2 (Challenge 3). This task involves identifying the paragraphs that entail the decision paragraph of a query case from a given relevant case. Fine-tuning on this task enables BERT to infer the supportive relationships between paragraphs, which is useful for the legal case retrieval task. We fine-tune all parameters of BERT on a sentence pair classification task in an end-to-end fashion. The input is composed of the decision paragraph of a query case and a candidate paragraph in the relevant case. The text pair is separated by the [SEP] token and a [CLS] token is prepended to the text pair."]}
{"pkey": "bert-pli_6", "question": "Does the model show any bias, prejudice that is mentioned in the paper?", "answer": "Not Specified in the paper.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["In particular, all of the deep learning models are trained on the same training data and selected according to F1 on the validation data. For the ranking models, we consider the top 5 results as the relevant ones4, while for the classification models, we simply use the label given by the model. Table 2 shows the performance on the whole test set. In the BERT-PLI model, we use LSTM and GRU as the RNN layer respectively. The LSTM and GRU versions achieve similar performance here and both outperform the baseline methods, including the traditional retrieval models and the deep learning ones, by a large margin. The structure of BERT-PLI is able to take the whole case document into consideration. At the same time, it has a better semantic understanding ability than the bag-of-words IR models with the help of BERT and sequence modeling components. Among the baseline retrieval methods, deep learning retrieval models perform much worse than the traditional retrieval models in this task. Since these deep learning models are mostly designed for the ad-hoc scenario, it is hard for them to handle long text retrieval. The length of the input is restricted in these models. The first 256 words are not enough to represent the document well, so it is not surprising that they perform poorly in this task. In addition to truncation, we also attempt to utilize automatic summarization techniques to shorten the input length. However, it does not result in stable improvements. Even taking the generated summary as the input, the deep models still underperform. Assuming that the legal documents contain plenty of information and complex logic, it is hard to express them well in a much\n4The threshold 5 is widely used by teams in the competition and is reasonable considering there are about 5 relevant cases per query on average.\nshorter summary. Meanwhile, the unsupervised summarization techniques might cause additional information loss and noise.", "Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage.", "The results emphasize the importance of considering the full document information in the legal case retrieval task. On the other hand, traditional retrieval models give relatively good results. They take advantage of the whole document though they are weak in semantic understanding. Ablation study. We further investigate the effects of the fine-tuning stage. We use the original parameters of the pretrained BERT model rather than the fine-tuned one to infer the paragraph interactions. Then the remaining parts of BERTPLI are trained and evaluated under the same experimental settings. The model without Stage 2 is denoted as BERTPLIorg . As shown in Table 2, there is a big drop in the performance for both the LSTM and GRU versions compared with the results of BERT-PLI. Without Stage 2, the model has a similar performance with traditional retrieval models. Recall that we do not update the parameters of BERT during training BERT-PLI. Therefore, fine-tuning is essential for the BERT model to well represent the semantic relationships between paragraphs. In conclusion, the experimental results suggest that it is useful and effective to adapt BERT to a paragraphlevel modeling task in the legal domain. Comparison with results in the competition. Table 3 shows the best results of the top two teams in the competition leaderboard. In terms of the final evaluation results on the test set, our methods (BERT-PLI (GRU/LSTM)) achieve a better recall and F1 score, even though Stage 1 hurts the recall of our approach a bit. The results show that our method can reach a better balance in precision and recall. Further, with the summary encoding and lexical features provided by the JNLP group, we conduct an additional experiment, which combines the outputs of BERT-PLI with their features. The experiment is two-fold. In order to combine the two approaches, we first arrange their model in the cascade framework."]}
{"pkey": "bert-pli_7", "question": "List the limitations of the model discussed in the paper.", "answer": "The results emphasize the importance of considering the full document information in the legal case retrieval task. On the other hand, traditional retrieval models give relatively\ngood results. They take advantage of the whole document though they are weak in semantic understanding.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["In Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, we simply truncate the texts. We set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT hidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. We train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, we use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126. As for the baseline methods, we use a bigram language model with linear smoothing in LMIR while VSM and BM25 are calculated based on the standard scoring functions. ARCII and MatchPyramid are implemented by MatchZoo [Guo et al., 2019]. We train ARC-II and MatchPyramid in a pairwise way since pairwise training usually outperforms pointwise training. The input length is also restricted due to memory and time limit. We truncate both the query and candidate document to 256 words, which is consistent with the restrictions of BERT. Meanwhile, we use TextRank2 to generate a summary with a 256-words length limit for each case. JNLP group also provides the summary encoding and the lexical features, and we further conduct experiments based on the provided features using RankSVM.3\n1https://github.com/google-research/bert 2https://radimrehurek.com/gensim/ 3https://www.cs.cornell.edu/people/tj/svm light/svm rank.html\n4.4 Results and Analysis. All of the models employ the cascade framework except BM25. In other words, these models conduct further ranking or classification based on the dataset D1 selected in Stage 1.", "Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage.", "Both the query and candidate cases involve extreme long texts. For example, cases in COLIEE 2019 Task 1 contain around 3,000 words on average. It is challenging for representation learning methods to represent the long document well in a limited semantic space, and it is also difficult for the matching function learning methods to construct and aggregate matching signals. Challenge 2. The definition of relevance in the legal scenario is somehow beyond the general definition of topical relevance [Van Opijnen and Santos, 2017]. Relevant cases are those that can support the decision of the current case, which usually involve similar situations and suitable statutes. Therefore, it is crucial to identify the similarities in the aspects of legal issues and legal processes of the cases, which calls for semantic understanding of whole documents. Challenge 3. Collecting a large dataset for this task can be challenging if not impossible. For one thing, downloading large-scale legal documents is restricted in many law systems. For another, it is quite expensive to obtain accurate relevance judgments since it requires expert knowledge in the legal domain. The lack of data brings obstacles to the training process of deep neural models. To tackle the above challenges, we propose BERT-PLI, a novel model that utilizes BERT [Devlin et al., 2018] to model Paragraph-Level Interactions for legal case retrieval. For modeling of long texts (Challenge 1), we break the documents into paragraphs and infer the relationship between cases from a fine-grained perspective, which allows exploiting the information of the full document instead of the truncated or summarized one. Beyond term-level matching, we model the semantic interactions between two paragraphs with BERT (Challenge 2). Moreover, we adapt the BERT model to the legal scenario by fine-tuning it on a sentence pair classi-\nfication task with a small-scale legal case entailment dataset (Challenge 3)."]}
{"pkey": "bert-pli_8", "question": "List the datasets on which the model was trained alongwith a brief summary and the size of each dataset.", "answer": "Experiments are conducted based on the COLIEE 2019 datasets [Rabelo et al., 2019].Task 1 is a legal case retrieval task, which involves reading a new case Q and extracting supporting cases S1, S2, . . . , Sn for the decision of Q from the case law corpus. The supporting cases are considered as \u201crelevant cases\u201d or \u201cnoticed cases\u201d for the query case Q. Task 2 is a legal case entailment task to identify paragraphs that entail the given decision paragraph of a query case from a given relevant case. Data in both tasks are sampled from a databaseof predominantly Federal Court of Canada case laws.Table 1 gives a statistical summary of the raw datasets in these two tasks. Task 1 is the main focus of our work, while the data of Task 2 are used to fine-tune BERT in Stage 2.       \nTask 1 Train Test\n# query case 285 61\n# candidate cases / query 200 200\n# noticed cases / query 5.21 5.41\nTask 2 Train Test\n# query case 181 44\n# candidate paragraphs / query 32.12 32.91\n# entailing paragraphs / query 1.12 1.02\nTable 1: Summary of datasets in COLIEE 2019 task 1 and task 2.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["In Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, we simply truncate the texts. We set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT hidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. We train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, we use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126. As for the baseline methods, we use a bigram language model with linear smoothing in LMIR while VSM and BM25 are calculated based on the standard scoring functions. ARCII and MatchPyramid are implemented by MatchZoo [Guo et al., 2019]. We train ARC-II and MatchPyramid in a pairwise way since pairwise training usually outperforms pointwise training. The input length is also restricted due to memory and time limit. We truncate both the query and candidate document to 256 words, which is consistent with the restrictions of BERT. Meanwhile, we use TextRank2 to generate a summary with a 256-words length limit for each case. JNLP group also provides the summary encoding and the lexical features, and we further conduct experiments based on the provided features using RankSVM.3\n1https://github.com/google-research/bert 2https://radimrehurek.com/gensim/ 3https://www.cs.cornell.edu/people/tj/svm light/svm rank.html\n4.4 Results and Analysis. All of the models employ the cascade framework except BM25. In other words, these models conduct further ranking or classification based on the dataset D1 selected in Stage 1.", "The supporting cases are considered as \u201crelevant cases\u201d or \u201cnoticed cases\u201d for the query case Q. Task 2 is a legal case entailment task to identify paragraphs that entail the given decision paragraph of a query case from a given relevant case. Data in both tasks are sampled from a database\nof predominantly Federal Court of Canada case laws. Table 1 gives a statistical summary of the raw datasets in these two tasks. Task 1 is the main focus of our work, while the data of Task 2 are used to fine-tune BERT in Stage 2. We follow the evaluation metrics in the competition. Micro-average of precision, recall, and F1 are used.\n4.2 Baseline Methods. We compare our model with the following three types of baselines. \u2022 Traditional bag-of-words retrieval models, including VSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999].\n\u2022 Deep retrieval models. Prior work shows that matching function learning methods usually outperform the representation learning ones [Xu et al., 2018], so we consider two matching function learning models, ARCII [Hu et al., 2014] and MatchPyramid [Palangi et al., 2016]. Both of them utilize the CNN structure, which is faster than RNN-based models, especially when dealing with long texts. We do not include the state-of-art neural models that involve rich behavioral signals (e.g., click) since we focus on text-based retrieval here. \u2022 Methods in the competition. We also compare with the methods of the top 2 teams [Tran et al., 2019; Rossi and Kanoulas, 2019] in the competition. The champion team (named as \u201cJNLP\u201d) trained a supervised summarization model based on COLIEE 2018\u2019s dataset and applied the model to encoding the case document into a continuous vector. They combined such the summary embeddings with lexical matching features, calculated by ROUGE [Lin, 2004], and learned the document rankings via RankSVM. Another team ranked following JNLP (named as \u201cILPS\u201d) generated summaries by the TextRank algorithm", "Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage."]}
{"pkey": "bert-pli_9", "question": "List the tokenizer used alongwith the size of the vocabulary.", "answer": "To tackle the challenge brought by long and complex documents, the paper authors first break a document into paragraphs (Challenge 1) and model the interactions between paragraphs in the semantic level (Challenge 2). In Figure 1 (Stage 3), the query q and one of the candidate document dk are represented by paragraphs, denoted as q = (pq1, pq2, . . . , pqN ) and dk = (pk1, pk2, . . . , pkM ), where N and M denote the total numbers of paragraphs in q and dk, respectively. For each paragraph in q and dk, the paper authors construct a paragraph pair (pqi, pkj ), where 1 \u2264 i \u2264 N and 1 \u2264 j \u2264 M , as the input of BERT, along with the reserved tokens (i.e. [CLS] and [SEP]). The final hidden state vector of the token [CLS] is viewed as the aggregate representation of the input paragraph pair. In that way, the paper authors can get an interaction map of paragraphs, in which each component Cij , Cij \u2208 RHB , models the semantic relationship between the query paragraph pqi and the candidate paragraph pkj . Not Specified in the paper exactly.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["As for the output, we feed the final hidden state vector corresponding to the first input token ([CLS]) into a classification layer. In this task, we use a fully-connected layer to do\nbinary classification, optimizing a cross-entropy loss, written as \u2212(y log(y\u0302) + (1\u2212 y) log(1\u2212 y\u0302)). 3.5 Stage 3: BERT-PLI. To tackle the challenge brought by long and complex documents, we first break a document into paragraphs (Challenge 1) and model the interactions between paragraphs in the semantic level (Challenge 2). In Figure 1 (Stage 3), the query q and one of the candidate document dk are represented by paragraphs, denoted as q = (pq1, pq2, . . . , pqN ) and dk = (pk1, pk2, . . . , pkM ), where N and M denote the total numbers of paragraphs in q and dk, respectively. For each paragraph in q and dk, we construct a paragraph pair (pqi, pkj), where 1 \u2264 i \u2264 N and 1 \u2264 j \u2264 M , as the input of BERT, along with the reserved tokens (i.e. [CLS] and [SEP]). The final hidden state vector of the token [CLS] is viewed as the aggregate representation of the input paragraph pair. In that way, we can get an interaction map of paragraphs, in which each component Cij , Cij \u2208 RHB , models the semantic relationship between the query paragraph pqi and the candidate paragraph pkj . For each paragraph of the query, we capture the strongest matching signals with the candidate document using the maxpooling strategy, and hence get a sequence of vectors, denoted as p\u2032qk = [p \u2032 qk1,p \u2032 qk2, . . . ,p \u2032 qkN ]. Each component p \u2032 qki aggregates the interactions of all of paragraphs in the candidate document corresponding to the i-th query paragraph as follows: p\u2032qki = MaxPool(Ci1,Ci2, . . . ,CiM ), p \u2032 qki \u2208 RHB . (1)\nWe use an RNN structure to further encode the representation sequence. Assuming that the legal document always follows a certain reasoning order, we consider the forward\nRNN in this model. In the forward pass, RNN generates a sequence of hidden states:\nhqk = [hqk1,hqk2, . . .", "In Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, we simply truncate the texts. We set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT hidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. We train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, we use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126. As for the baseline methods, we use a bigram language model with linear smoothing in LMIR while VSM and BM25 are calculated based on the standard scoring functions. ARCII and MatchPyramid are implemented by MatchZoo [Guo et al., 2019]. We train ARC-II and MatchPyramid in a pairwise way since pairwise training usually outperforms pointwise training. The input length is also restricted due to memory and time limit. We truncate both the query and candidate document to 256 words, which is consistent with the restrictions of BERT. Meanwhile, we use TextRank2 to generate a summary with a 256-words length limit for each case. JNLP group also provides the summary encoding and the lexical features, and we further conduct experiments based on the provided features using RankSVM.3\n1https://github.com/google-research/bert 2https://radimrehurek.com/gensim/ 3https://www.cs.cornell.edu/people/tj/svm light/svm rank.html\n4.4 Results and Analysis. All of the models employ the cascade framework except BM25. In other words, these models conduct further ranking or classification based on the dataset D1 selected in Stage 1.", "In Stage 1, we select top-K candidates from the initial candidate corpus with respect to the query case q according to BM25 scores. In Stage 2, we fine-tune the BERT model on a sentence pair classification task with a legal case entailment dataset in order to adapt it to modeling semantic relationships between legal paragraphs. In the final stage, BERT-PLI conducts rele-\nvance prediction with the fine-tuned BERT (Stage 2) among the selected candidates (Stage 1). 3.3 Stage 1: BM25 Selection. Deep learning models are usually time-consuming and resource-consuming. Considering the computational cost, we employ the cascade framework which utilizes BM25 to prune the set of candidates. The BM25 model is implemented according to the standard scoring function [Robertson and Walker, 1994]. This stage inevitably hurts both recall and precision, and we mainly pay attention to optimizing recall at this stage since the downstream models can further discard irrelevant documents. 3.4 Stage 2: BERT Fine-tuning. Fine-tuning is relatively inexpensive compared with the pretraining procedure, which allows BERT to model specific tasks with small datasets [Devlin et al., 2018]. Therefore, before applying BERT to infer the relationship of case paragraphs, we fine-tune it with a small-scale legal case entailment dataset provided by COLIEE 2019 Task 2 (Challenge 3). This task involves identifying the paragraphs that entail the decision paragraph of a query case from a given relevant case. Fine-tuning on this task enables BERT to infer the supportive relationships between paragraphs, which is useful for the legal case retrieval task. We fine-tune all parameters of BERT on a sentence pair classification task in an end-to-end fashion. The input is composed of the decision paragraph of a query case and a candidate paragraph in the relevant case. The text pair is separated by the [SEP] token and a [CLS] token is prepended to the text pair."]}
{"pkey": "bert-pli_10", "question": "List the preprocessing techinques used on the dataset.", "answer": "- the paper authors employ the cascade framework which utilizes BM25 to prune the set of candidates. The BM25 model is implemented according to the standard scoring function. \n- The text pair is separated by the [SEP] token and a [CLS] token is prepended to the text pair. As for the output, the paper authors feed the final hidden state vector corresponding to the first input token ([CLS]) into a classification layer.  To tackle the challenge brought by long and complex documents, the paper authors first break a document into paragraphs (Challenge1) and model the interactions between paragraphs in the semantic level (Challenge 2).\n- For each paragraph in q and dk, the paper authors construct a paragraph pair (pqi, pkj ), where 1 \u2264 i \u2264 N and 1 \u2264 j \u2264 M , as the input of BERT, along with the reserved tokens (i.e. [CLS] and [SEP]). The final hidden state vector of the token [CLS] is viewed as the aggregate representation of the input paragraph pair.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["In Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, we simply truncate the texts. We set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT hidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. We train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, we use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126. As for the baseline methods, we use a bigram language model with linear smoothing in LMIR while VSM and BM25 are calculated based on the standard scoring functions. ARCII and MatchPyramid are implemented by MatchZoo [Guo et al., 2019]. We train ARC-II and MatchPyramid in a pairwise way since pairwise training usually outperforms pointwise training. The input length is also restricted due to memory and time limit. We truncate both the query and candidate document to 256 words, which is consistent with the restrictions of BERT. Meanwhile, we use TextRank2 to generate a summary with a 256-words length limit for each case. JNLP group also provides the summary encoding and the lexical features, and we further conduct experiments based on the provided features using RankSVM.3\n1https://github.com/google-research/bert 2https://radimrehurek.com/gensim/ 3https://www.cs.cornell.edu/people/tj/svm light/svm rank.html\n4.4 Results and Analysis. All of the models employ the cascade framework except BM25. In other words, these models conduct further ranking or classification based on the dataset D1 selected in Stage 1.", "In Stage 1, we select top-K candidates from the initial candidate corpus with respect to the query case q according to BM25 scores. In Stage 2, we fine-tune the BERT model on a sentence pair classification task with a legal case entailment dataset in order to adapt it to modeling semantic relationships between legal paragraphs. In the final stage, BERT-PLI conducts rele-\nvance prediction with the fine-tuned BERT (Stage 2) among the selected candidates (Stage 1). 3.3 Stage 1: BM25 Selection. Deep learning models are usually time-consuming and resource-consuming. Considering the computational cost, we employ the cascade framework which utilizes BM25 to prune the set of candidates. The BM25 model is implemented according to the standard scoring function [Robertson and Walker, 1994]. This stage inevitably hurts both recall and precision, and we mainly pay attention to optimizing recall at this stage since the downstream models can further discard irrelevant documents. 3.4 Stage 2: BERT Fine-tuning. Fine-tuning is relatively inexpensive compared with the pretraining procedure, which allows BERT to model specific tasks with small datasets [Devlin et al., 2018]. Therefore, before applying BERT to infer the relationship of case paragraphs, we fine-tune it with a small-scale legal case entailment dataset provided by COLIEE 2019 Task 2 (Challenge 3). This task involves identifying the paragraphs that entail the decision paragraph of a query case from a given relevant case. Fine-tuning on this task enables BERT to infer the supportive relationships between paragraphs, which is useful for the legal case retrieval task. We fine-tune all parameters of BERT on a sentence pair classification task in an end-to-end fashion. The input is composed of the decision paragraph of a query case and a candidate paragraph in the relevant case. The text pair is separated by the [SEP] token and a [CLS] token is prepended to the text pair.", "The supporting cases are considered as \u201crelevant cases\u201d or \u201cnoticed cases\u201d for the query case Q. Task 2 is a legal case entailment task to identify paragraphs that entail the given decision paragraph of a query case from a given relevant case. Data in both tasks are sampled from a database\nof predominantly Federal Court of Canada case laws. Table 1 gives a statistical summary of the raw datasets in these two tasks. Task 1 is the main focus of our work, while the data of Task 2 are used to fine-tune BERT in Stage 2. We follow the evaluation metrics in the competition. Micro-average of precision, recall, and F1 are used.\n4.2 Baseline Methods. We compare our model with the following three types of baselines. \u2022 Traditional bag-of-words retrieval models, including VSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999].\n\u2022 Deep retrieval models. Prior work shows that matching function learning methods usually outperform the representation learning ones [Xu et al., 2018], so we consider two matching function learning models, ARCII [Hu et al., 2014] and MatchPyramid [Palangi et al., 2016]. Both of them utilize the CNN structure, which is faster than RNN-based models, especially when dealing with long texts. We do not include the state-of-art neural models that involve rich behavioral signals (e.g., click) since we focus on text-based retrieval here. \u2022 Methods in the competition. We also compare with the methods of the top 2 teams [Tran et al., 2019; Rossi and Kanoulas, 2019] in the competition. The champion team (named as \u201cJNLP\u201d) trained a supervised summarization model based on COLIEE 2018\u2019s dataset and applied the model to encoding the case document into a continuous vector. They combined such the summary embeddings with lexical matching features, calculated by ROUGE [Lin, 2004], and learned the document rankings via RankSVM. Another team ranked following JNLP (named as \u201cILPS\u201d) generated summaries by the TextRank algorithm"]}
{"pkey": "bert-pli_11", "question": "Describe the architecture details (whether it is encoder-decoder, encoder only or decoder only framework, number of layers, number of heads, embedding dimension, total parameters). In case multiple models of varying sizes are trained, list the details for all configurations.", "answer": "The paper authors use the uncased base version of BERT.1 At first, it is fine-tuned on the training data and tested on the remaining testing data. In BERT-PLI,HB = 768, which is determined by the size of the BERT hidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["[Mihalcea and Tarau, 2004] first and assessed pairwise relevance by a carefully fine-tuned BERT model, combined with oversampling strategies. 4.3 Experimental Settings. In the raw legal case documents, along with the text body of the case (known as \u201cfacts\u201d), some meta information is also provided, such as the court, the date, the head note and so on. However, the types of metadata vary with documents and have a high missing rate, in which case, we build our model based on the text body of a case. Some cases contain both\nEnglish and French versions of description, and only the English one is considered in our experiments. As illustrated in Figure 1 (Stage 1), we select candidates according to BM25 scores. We set K = 50 (top 50 candidates for each query), considering both the recall and the effectiveness in the following stages. Note that only the cases that happened before the current case could be noticed according to the problem definition, those invalid cases in terms of time are dismissed, which would otherwise be noisy in the candidates. The recall of Stage 1 on the training and testing set are 0.9159 and 0.9273, respectively. The relative high recall suggests that setting K = 50 in Stage 1 brings little harm to the overall retrieval performance. In Stage 2, paragraph pairs are constructed using the decision and candidate paragraphs in Task 2. The paragraphs have no more than 100 words on average and we truncate the words symmetrically if the pair exceeds the maximum input length of BERT. We use the uncased base version of BERT.1 At first, it is fine-tuned on the training data and tested on the remaining testing data. We use the Adam optimizer and set the learning rate as 10\u22125. The fine-tuning procedure can coverage after three epochs and F1 on the test set reaches 0.6526, which is better than the results of BM25 in this task. After that, we utilize the same hyperparameter settings but merge the training and testing data to fine-tune BERT for 3 epochs from scratch.", "In Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, we simply truncate the texts. We set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT hidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. We train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, we use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126. As for the baseline methods, we use a bigram language model with linear smoothing in LMIR while VSM and BM25 are calculated based on the standard scoring functions. ARCII and MatchPyramid are implemented by MatchZoo [Guo et al., 2019]. We train ARC-II and MatchPyramid in a pairwise way since pairwise training usually outperforms pointwise training. The input length is also restricted due to memory and time limit. We truncate both the query and candidate document to 256 words, which is consistent with the restrictions of BERT. Meanwhile, we use TextRank2 to generate a summary with a 256-words length limit for each case. JNLP group also provides the summary encoding and the lexical features, and we further conduct experiments based on the provided features using RankSVM.3\n1https://github.com/google-research/bert 2https://radimrehurek.com/gensim/ 3https://www.cs.cornell.edu/people/tj/svm light/svm rank.html\n4.4 Results and Analysis. All of the models employ the cascade framework except BM25. In other words, these models conduct further ranking or classification based on the dataset D1 selected in Stage 1.", "As for the output, we feed the final hidden state vector corresponding to the first input token ([CLS]) into a classification layer. In this task, we use a fully-connected layer to do\nbinary classification, optimizing a cross-entropy loss, written as \u2212(y log(y\u0302) + (1\u2212 y) log(1\u2212 y\u0302)). 3.5 Stage 3: BERT-PLI. To tackle the challenge brought by long and complex documents, we first break a document into paragraphs (Challenge 1) and model the interactions between paragraphs in the semantic level (Challenge 2). In Figure 1 (Stage 3), the query q and one of the candidate document dk are represented by paragraphs, denoted as q = (pq1, pq2, . . . , pqN ) and dk = (pk1, pk2, . . . , pkM ), where N and M denote the total numbers of paragraphs in q and dk, respectively. For each paragraph in q and dk, we construct a paragraph pair (pqi, pkj), where 1 \u2264 i \u2264 N and 1 \u2264 j \u2264 M , as the input of BERT, along with the reserved tokens (i.e. [CLS] and [SEP]). The final hidden state vector of the token [CLS] is viewed as the aggregate representation of the input paragraph pair. In that way, we can get an interaction map of paragraphs, in which each component Cij , Cij \u2208 RHB , models the semantic relationship between the query paragraph pqi and the candidate paragraph pkj . For each paragraph of the query, we capture the strongest matching signals with the candidate document using the maxpooling strategy, and hence get a sequence of vectors, denoted as p\u2032qk = [p \u2032 qk1,p \u2032 qk2, . . . ,p \u2032 qkN ]. Each component p \u2032 qki aggregates the interactions of all of paragraphs in the candidate document corresponding to the i-th query paragraph as follows: p\u2032qki = MaxPool(Ci1,Ci2, . . . ,CiM ), p \u2032 qki \u2208 RHB . (1)\nWe use an RNN structure to further encode the representation sequence. Assuming that the legal document always follows a certain reasoning order, we consider the forward\nRNN in this model. In the forward pass, RNN generates a sequence of hidden states:\nhqk = [hqk1,hqk2, . . ."]}
{"pkey": "bert-pli_12", "question": "Describe the training setup (e.g. learning rate, steps, epochs, optimizer, etc.)", "answer": "The paper authors use the Adam optimizer and set the learning rate as 10\u22125. The fine-tuning procedure can coverage after three epochs and F 1 on the test set reaches 0.6526,which is better than the results of BM25 in this task. After that, the paper authors utilize the same hyperparameter settings but merge the training and testing data to fine-tune BERT for 3 epochs from scratch.                                                                                                                                       In Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, the paper authors simply truncate the\ntexts. The paper authors set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT\nhidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. The paper authors train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, the paper authors use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["[Mihalcea and Tarau, 2004] first and assessed pairwise relevance by a carefully fine-tuned BERT model, combined with oversampling strategies. 4.3 Experimental Settings. In the raw legal case documents, along with the text body of the case (known as \u201cfacts\u201d), some meta information is also provided, such as the court, the date, the head note and so on. However, the types of metadata vary with documents and have a high missing rate, in which case, we build our model based on the text body of a case. Some cases contain both\nEnglish and French versions of description, and only the English one is considered in our experiments. As illustrated in Figure 1 (Stage 1), we select candidates according to BM25 scores. We set K = 50 (top 50 candidates for each query), considering both the recall and the effectiveness in the following stages. Note that only the cases that happened before the current case could be noticed according to the problem definition, those invalid cases in terms of time are dismissed, which would otherwise be noisy in the candidates. The recall of Stage 1 on the training and testing set are 0.9159 and 0.9273, respectively. The relative high recall suggests that setting K = 50 in Stage 1 brings little harm to the overall retrieval performance. In Stage 2, paragraph pairs are constructed using the decision and candidate paragraphs in Task 2. The paragraphs have no more than 100 words on average and we truncate the words symmetrically if the pair exceeds the maximum input length of BERT. We use the uncased base version of BERT.1 At first, it is fine-tuned on the training data and tested on the remaining testing data. We use the Adam optimizer and set the learning rate as 10\u22125. The fine-tuning procedure can coverage after three epochs and F1 on the test set reaches 0.6526, which is better than the results of BM25 in this task. After that, we utilize the same hyperparameter settings but merge the training and testing data to fine-tune BERT for 3 epochs from scratch.", "In Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, we simply truncate the texts. We set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT hidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. We train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, we use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126. As for the baseline methods, we use a bigram language model with linear smoothing in LMIR while VSM and BM25 are calculated based on the standard scoring functions. ARCII and MatchPyramid are implemented by MatchZoo [Guo et al., 2019]. We train ARC-II and MatchPyramid in a pairwise way since pairwise training usually outperforms pointwise training. The input length is also restricted due to memory and time limit. We truncate both the query and candidate document to 256 words, which is consistent with the restrictions of BERT. Meanwhile, we use TextRank2 to generate a summary with a 256-words length limit for each case. JNLP group also provides the summary encoding and the lexical features, and we further conduct experiments based on the provided features using RankSVM.3\n1https://github.com/google-research/bert 2https://radimrehurek.com/gensim/ 3https://www.cs.cornell.edu/people/tj/svm light/svm rank.html\n4.4 Results and Analysis. All of the models employ the cascade framework except BM25. In other words, these models conduct further ranking or classification based on the dataset D1 selected in Stage 1.", "A variety of approaches as well as expert knowledge are involved in this task [Bench-Capon et al., 2012], e.g., logical analysis, lexical matching, distributed representation, etc. For instance, decomposition of legal issues [Zeng et al., 2005], ontological frameworks [Saravanan et al., 2009], and link analysis [Monroy et al., 2013] have been explored. Generally speaking, methods can be grouped into two broad categories: those based on manual knowledge engineering (KE) and those based on natural language processing (NLP) [Maxwell and Schafer, 2008]. The competitions held recently, such as COLIEE, are mostly aimed at exploring the application of NLP-based methods and providing benchmarks for the legal case retrieval task. In COLIEE 2019, both traditional retrieval models and neural models are explored in the legal case retrieval task. Specifically, [Tran et al., 2019] combined distributed representation with lexical matching features via LTR algorithms while [Rossi and Kanoulas, 2019] utilized BERT with the help of automatic summarization algorithms. 3 Method. 3.1 Task Description. The legal case retrieval task involves finding prior cases that should be \u201cnoticed\u201d concerning a given query case in the set of candidate cases [Rabelo et al., 2019]. \u201cNoticed case\u201d is a legal technical term denoting that a precedent is relevant to a query case, in other words, it supports the decision of a query case. Formally, given a query case q, and a set of candidate cases D = {d1, d2, . . . , dn}, the task is to identify the supporting cases D\u2217 = {d\u2217i | d\u2217i \u2208 D \u2227 noticed(d\u2217i , q)}, where noticed(d\u2217i , q) denotes that d \u2217 i should be noticed given the query case q. Both the query and the candidates are legal documents containing long texts, which consist of the facts in a case.\n3.2 Architecture Overview. In general, we deal with the legal case retrieval task within a multi-stage pipeline inspired by the cascade framework. As illustrated in Figure 1, it consists of three stages."]}
{"pkey": "bert-pli_13", "question": "Describe the computational resources used to train the model.", "answer": "Not Specified in the paper.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["In Stage 1, we select top-K candidates from the initial candidate corpus with respect to the query case q according to BM25 scores. In Stage 2, we fine-tune the BERT model on a sentence pair classification task with a legal case entailment dataset in order to adapt it to modeling semantic relationships between legal paragraphs. In the final stage, BERT-PLI conducts rele-\nvance prediction with the fine-tuned BERT (Stage 2) among the selected candidates (Stage 1). 3.3 Stage 1: BM25 Selection. Deep learning models are usually time-consuming and resource-consuming. Considering the computational cost, we employ the cascade framework which utilizes BM25 to prune the set of candidates. The BM25 model is implemented according to the standard scoring function [Robertson and Walker, 1994]. This stage inevitably hurts both recall and precision, and we mainly pay attention to optimizing recall at this stage since the downstream models can further discard irrelevant documents. 3.4 Stage 2: BERT Fine-tuning. Fine-tuning is relatively inexpensive compared with the pretraining procedure, which allows BERT to model specific tasks with small datasets [Devlin et al., 2018]. Therefore, before applying BERT to infer the relationship of case paragraphs, we fine-tune it with a small-scale legal case entailment dataset provided by COLIEE 2019 Task 2 (Challenge 3). This task involves identifying the paragraphs that entail the decision paragraph of a query case from a given relevant case. Fine-tuning on this task enables BERT to infer the supportive relationships between paragraphs, which is useful for the legal case retrieval task. We fine-tune all parameters of BERT on a sentence pair classification task in an end-to-end fashion. The input is composed of the decision paragraph of a query case and a candidate paragraph in the relevant case. The text pair is separated by the [SEP] token and a [CLS] token is prepended to the text pair.", "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval. Legal case retrieval is a specialized IR task that involves retrieving supporting cases given a query case. Compared with traditional ad-hoc text retrieval, the legal case retrieval task is more challenging since the query case is much longer and more complex than common keyword queries. Besides that, the definition of relevance between a query case and a supporting case is beyond general topical relevance and it is therefore difficult to construct a large-scale case retrieval dataset, especially one with accurate relevance judgments. To address these challenges, we propose BERT-PLI, a novel model that utilizes BERT to capture the semantic relationships at the paragraph-level and then infers the relevance between two cases by aggregating paragraph-level interactions. We finetune the BERT model with a relatively small-scale case law entailment dataset to adapt it to the legal scenario and employ a cascade framework to reduce the computational cost. We conduct extensive experiments on the benchmark of the relevant case retrieval task in COLIEE 2019. Experimental results demonstrate that our proposed method outperforms existing solutions. BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval. Legal case retrieval is a specialized IR task that involves retrieving supporting cases given a query case. Compared with traditional ad-hoc text retrieval, the legal case retrieval task is more challenging since the query case is much longer and more complex than common keyword queries. Besides that, the definition of relevance between a query case and a supporting case is beyond general topical relevance and it is therefore difficult to construct a large-scale case retrieval dataset, especially one with accurate relevance judgments.", "In practice, we employ the cascade framework to avoid high computational cost and arrange the model in a multi-stage pipeline for legal case retrieval. Specifically, we select top-K candidates according to the BM25 rankings and fine-tune BERT with an extra legal dataset before applying BERT-PLI to relevance prediction. Experiments are conducted on the COLIEE 2019 [Rabelo et al., 2019] legal case retrieval task and the results demonstrate the effectiveness of our proposed method. 2 Related Work. A large number of retrieval models, especially for ad-hoc text retrieval, have been proposed in the past decades. Traditional bag-of-words IR models, including VSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999], which are widely applied in search systems, are mostly based on term-level matching. Since the mid-2000s, LTR (Learning to Rank) methods [Liu and others, 2009], which are driven heavily by manual feature engineering, have been well studied and utilized by commercial web search engines as well. In recent years, the development of deep learning has also inspired applications of neural models in IR. Generally, the methods can be categorized into two types [Xu et al., 2018], methods of representation learning and methods of matching function learning. Based on the idea of representation learning, queries and documents are represented in the latent space by deep learning models, and the query-document relevance score is calculated based on their latent representations with a vector space scoring function, e.g., cosine similarity. Various neural network models have been applied to this task. For instance, DSSM [Hu et al., 2014] utilized DNN in the early stage. Further, some studies exploit CNNs to capture the local interactions [Hu et al., 2014; Shen et al., 2014], while some studies apply RNNs to modeling text sequences [Palangi et al., 2016]. However, most of the model structures are not designed for representing long documents."]}
{"pkey": "bert-pli_14", "question": "Are all details necessary to reproduce the paper are provided in the paper?", "answer": "https://github.com/ThuYShao/BERT-PLI-IJCAI2020. The implementation has been available.                                                                                                                                        1https://github.com/google-research/bert\n2https://radimrehurek.com/gensim/\n3https://www.cs.cornell.edu/people/tj/svm light/svm rank.html", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage.", "In Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, we simply truncate the texts. We set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT hidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. We train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, we use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126. As for the baseline methods, we use a bigram language model with linear smoothing in LMIR while VSM and BM25 are calculated based on the standard scoring functions. ARCII and MatchPyramid are implemented by MatchZoo [Guo et al., 2019]. We train ARC-II and MatchPyramid in a pairwise way since pairwise training usually outperforms pointwise training. The input length is also restricted due to memory and time limit. We truncate both the query and candidate document to 256 words, which is consistent with the restrictions of BERT. Meanwhile, we use TextRank2 to generate a summary with a 256-words length limit for each case. JNLP group also provides the summary encoding and the lexical features, and we further conduct experiments based on the provided features using RankSVM.3\n1https://github.com/google-research/bert 2https://radimrehurek.com/gensim/ 3https://www.cs.cornell.edu/people/tj/svm light/svm rank.html\n4.4 Results and Analysis. All of the models employ the cascade framework except BM25. In other words, these models conduct further ranking or classification based on the dataset D1 selected in Stage 1.", "In Stage 1, we select top-K candidates from the initial candidate corpus with respect to the query case q according to BM25 scores. In Stage 2, we fine-tune the BERT model on a sentence pair classification task with a legal case entailment dataset in order to adapt it to modeling semantic relationships between legal paragraphs. In the final stage, BERT-PLI conducts rele-\nvance prediction with the fine-tuned BERT (Stage 2) among the selected candidates (Stage 1). 3.3 Stage 1: BM25 Selection. Deep learning models are usually time-consuming and resource-consuming. Considering the computational cost, we employ the cascade framework which utilizes BM25 to prune the set of candidates. The BM25 model is implemented according to the standard scoring function [Robertson and Walker, 1994]. This stage inevitably hurts both recall and precision, and we mainly pay attention to optimizing recall at this stage since the downstream models can further discard irrelevant documents. 3.4 Stage 2: BERT Fine-tuning. Fine-tuning is relatively inexpensive compared with the pretraining procedure, which allows BERT to model specific tasks with small datasets [Devlin et al., 2018]. Therefore, before applying BERT to infer the relationship of case paragraphs, we fine-tune it with a small-scale legal case entailment dataset provided by COLIEE 2019 Task 2 (Challenge 3). This task involves identifying the paragraphs that entail the decision paragraph of a query case from a given relevant case. Fine-tuning on this task enables BERT to infer the supportive relationships between paragraphs, which is useful for the legal case retrieval task. We fine-tune all parameters of BERT on a sentence pair classification task in an end-to-end fashion. The input is composed of the decision paragraph of a query case and a candidate paragraph in the relevant case. The text pair is separated by the [SEP] token and a [CLS] token is prepended to the text pair."]}
{"pkey": "bert-pli_15", "question": "What is the pretraining objective of the model? ", "answer": "Fine-tuning is relatively inexpensive compared with the pretraining procedure, which allows BERT to model specific tasks with small datasets [Devlin et al., 2018]. Therefore, before applying BERT to infer the relationship of case paragraphs, the paper authors fine-tune it with a small-scale legal case entailment dataset provided by COLIEE 2019 Task 2 (Challenge3). This task involves identifying the paragraphs that entail the decision paragraph of a query case from a given relevant case. Fine-tuning on this task enables BERT to infer the supportive relationships between paragraphs, which is useful for the legal case retrieval task.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["The results emphasize the importance of considering the full document information in the legal case retrieval task. On the other hand, traditional retrieval models give relatively good results. They take advantage of the whole document though they are weak in semantic understanding. Ablation study. We further investigate the effects of the fine-tuning stage. We use the original parameters of the pretrained BERT model rather than the fine-tuned one to infer the paragraph interactions. Then the remaining parts of BERTPLI are trained and evaluated under the same experimental settings. The model without Stage 2 is denoted as BERTPLIorg . As shown in Table 2, there is a big drop in the performance for both the LSTM and GRU versions compared with the results of BERT-PLI. Without Stage 2, the model has a similar performance with traditional retrieval models. Recall that we do not update the parameters of BERT during training BERT-PLI. Therefore, fine-tuning is essential for the BERT model to well represent the semantic relationships between paragraphs. In conclusion, the experimental results suggest that it is useful and effective to adapt BERT to a paragraphlevel modeling task in the legal domain. Comparison with results in the competition. Table 3 shows the best results of the top two teams in the competition leaderboard. In terms of the final evaluation results on the test set, our methods (BERT-PLI (GRU/LSTM)) achieve a better recall and F1 score, even though Stage 1 hurts the recall of our approach a bit. The results show that our method can reach a better balance in precision and recall. Further, with the summary encoding and lexical features provided by the JNLP group, we conduct an additional experiment, which combines the outputs of BERT-PLI with their features. The experiment is two-fold. In order to combine the two approaches, we first arrange their model in the cascade framework.", "In Stage 1, we select top-K candidates from the initial candidate corpus with respect to the query case q according to BM25 scores. In Stage 2, we fine-tune the BERT model on a sentence pair classification task with a legal case entailment dataset in order to adapt it to modeling semantic relationships between legal paragraphs. In the final stage, BERT-PLI conducts rele-\nvance prediction with the fine-tuned BERT (Stage 2) among the selected candidates (Stage 1). 3.3 Stage 1: BM25 Selection. Deep learning models are usually time-consuming and resource-consuming. Considering the computational cost, we employ the cascade framework which utilizes BM25 to prune the set of candidates. The BM25 model is implemented according to the standard scoring function [Robertson and Walker, 1994]. This stage inevitably hurts both recall and precision, and we mainly pay attention to optimizing recall at this stage since the downstream models can further discard irrelevant documents. 3.4 Stage 2: BERT Fine-tuning. Fine-tuning is relatively inexpensive compared with the pretraining procedure, which allows BERT to model specific tasks with small datasets [Devlin et al., 2018]. Therefore, before applying BERT to infer the relationship of case paragraphs, we fine-tune it with a small-scale legal case entailment dataset provided by COLIEE 2019 Task 2 (Challenge 3). This task involves identifying the paragraphs that entail the decision paragraph of a query case from a given relevant case. Fine-tuning on this task enables BERT to infer the supportive relationships between paragraphs, which is useful for the legal case retrieval task. We fine-tune all parameters of BERT on a sentence pair classification task in an end-to-end fashion. The input is composed of the decision paragraph of a query case and a candidate paragraph in the relevant case. The text pair is separated by the [SEP] token and a [CLS] token is prepended to the text pair.", "In particular, all of the deep learning models are trained on the same training data and selected according to F1 on the validation data. For the ranking models, we consider the top 5 results as the relevant ones4, while for the classification models, we simply use the label given by the model. Table 2 shows the performance on the whole test set. In the BERT-PLI model, we use LSTM and GRU as the RNN layer respectively. The LSTM and GRU versions achieve similar performance here and both outperform the baseline methods, including the traditional retrieval models and the deep learning ones, by a large margin. The structure of BERT-PLI is able to take the whole case document into consideration. At the same time, it has a better semantic understanding ability than the bag-of-words IR models with the help of BERT and sequence modeling components. Among the baseline retrieval methods, deep learning retrieval models perform much worse than the traditional retrieval models in this task. Since these deep learning models are mostly designed for the ad-hoc scenario, it is hard for them to handle long text retrieval. The length of the input is restricted in these models. The first 256 words are not enough to represent the document well, so it is not surprising that they perform poorly in this task. In addition to truncation, we also attempt to utilize automatic summarization techniques to shorten the input length. However, it does not result in stable improvements. Even taking the generated summary as the input, the deep models still underperform. Assuming that the legal documents contain plenty of information and complex logic, it is hard to express them well in a much\n4The threshold 5 is widely used by teams in the competition and is reasonable considering there are about 5 relevant cases per query on average.\nshorter summary. Meanwhile, the unsupervised summarization techniques might cause additional information loss and noise."]}
{"pkey": "bert-pli_16", "question": "What is the loss function that is used to train the model?", "answer": "In this task, the paper authors use a fully-connected layer to do binary classification, optimizing a cross-entropy loss, written as \u2212(y log(\u02c6y) + (1 \u2212 y) log(1 \u2212 \u02c6y)).", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["In particular, all of the deep learning models are trained on the same training data and selected according to F1 on the validation data. For the ranking models, we consider the top 5 results as the relevant ones4, while for the classification models, we simply use the label given by the model. Table 2 shows the performance on the whole test set. In the BERT-PLI model, we use LSTM and GRU as the RNN layer respectively. The LSTM and GRU versions achieve similar performance here and both outperform the baseline methods, including the traditional retrieval models and the deep learning ones, by a large margin. The structure of BERT-PLI is able to take the whole case document into consideration. At the same time, it has a better semantic understanding ability than the bag-of-words IR models with the help of BERT and sequence modeling components. Among the baseline retrieval methods, deep learning retrieval models perform much worse than the traditional retrieval models in this task. Since these deep learning models are mostly designed for the ad-hoc scenario, it is hard for them to handle long text retrieval. The length of the input is restricted in these models. The first 256 words are not enough to represent the document well, so it is not surprising that they perform poorly in this task. In addition to truncation, we also attempt to utilize automatic summarization techniques to shorten the input length. However, it does not result in stable improvements. Even taking the generated summary as the input, the deep models still underperform. Assuming that the legal documents contain plenty of information and complex logic, it is hard to express them well in a much\n4The threshold 5 is widely used by teams in the competition and is reasonable considering there are about 5 relevant cases per query on average.\nshorter summary. Meanwhile, the unsupervised summarization techniques might cause additional information loss and noise.", "The results emphasize the importance of considering the full document information in the legal case retrieval task. On the other hand, traditional retrieval models give relatively good results. They take advantage of the whole document though they are weak in semantic understanding. Ablation study. We further investigate the effects of the fine-tuning stage. We use the original parameters of the pretrained BERT model rather than the fine-tuned one to infer the paragraph interactions. Then the remaining parts of BERTPLI are trained and evaluated under the same experimental settings. The model without Stage 2 is denoted as BERTPLIorg . As shown in Table 2, there is a big drop in the performance for both the LSTM and GRU versions compared with the results of BERT-PLI. Without Stage 2, the model has a similar performance with traditional retrieval models. Recall that we do not update the parameters of BERT during training BERT-PLI. Therefore, fine-tuning is essential for the BERT model to well represent the semantic relationships between paragraphs. In conclusion, the experimental results suggest that it is useful and effective to adapt BERT to a paragraphlevel modeling task in the legal domain. Comparison with results in the competition. Table 3 shows the best results of the top two teams in the competition leaderboard. In terms of the final evaluation results on the test set, our methods (BERT-PLI (GRU/LSTM)) achieve a better recall and F1 score, even though Stage 1 hurts the recall of our approach a bit. The results show that our method can reach a better balance in precision and recall. Further, with the summary encoding and lexical features provided by the JNLP group, we conduct an additional experiment, which combines the outputs of BERT-PLI with their features. The experiment is two-fold. In order to combine the two approaches, we first arrange their model in the cascade framework.", ",hqkN ], hqki \u2208 RHR , (2)\nwhere hqki = RNN(hqk(i\u22121),p\u2032qki) is generated by LSTM or GRU in practice. The attention mechanism is also employed to infer the importance of each position. The attention weight of each position is measured by:\n\u03b1qki = exp(hqki \u00b7 uqk)\u2211 i\u2032 exp(hqki\u2032 \u00b7 uqk) , (3)\nwhere hqki is the i-th hidden state given by the forward RNN and uqk is generated as follows: uqk = Wu \u00b7MaxPool(hqk) + bu, (4)\nwhere Wu \u2208 RHR\u00d7HR , bu \u2208 RHR . We can then get the document-level representation via attentive aggregation:\ndqk = \u2211 i \u03b1qkihqki. (5)\nFinally, the representation dqk is passed through a fullyconnected layer followed by a softmax function to make prediction as follows: y\u0302qk = softmax(Wp \u00b7 dqk + bp), (6)\nwhere Wp \u2208 R|R|\u00d7HR , bp \u2208 R|R|, and R denotes the set of relevance labels, e.g., R = {0, 1} and |R| = 2. During the training procedure, we optimize the following cross-entropy loss:\nLqk (y\u0302qk,yqk) = \u2212 |R|\u2211 r=1 yqkr log (y\u0302qkr) , (7) In the practice of the legal case retrieval task, BERT-PLI is combined with the two stages mentioned above. To be specific, the candidate documents at the input of BERT-PLI are those selected by BM25 in Stage 1. After Stage 2, we consider that the BERT is able to well represent the paragraphlevel semantic relationships in legal case documents. Hence, we utilize the fine-tuned parameters directly without updating them when training BERT-PLI. The multi-stage operations make the training process easy and affordable. As for testing, we return the ones that are predicted as relevant cases by BERT-PLI corresponding to a given query case. 4 Experiments. 4.1 Datasets and Evaluation Metrics. Our experiments are conducted based on the COLIEE 2019 datasets [Rabelo et al., 2019]. Task 1 is a legal case retrieval task, which involves reading a new caseQ and extracting supporting cases S1, S2, . . . , Sn for the decision of Q from the case law corpus."]}
{"pkey": "bert-pli_17", "question": "Consider the transformer model as base architecture for encoder-decoder (ED) models. Similarly consider BERT and GPT as base for encoder-only (E) and decoder-only (D) architectures. How is the architecture of this paper different from the base architectures like transformer or bert or gpt (depending on ED, E, or D only model respectively)?", "answer": "In general, the paper authors deal with the legal case retrieval task within a multi-stage pipeline inspired by the cascade framework. As illustrated in Figure 1, it consists of three stages. In Stage 1, the paper authors select top-K candidates from the initial candidate corpus with respect to the query case q according to BM25 scores. In Stage 2, the paper authors fine-tune the BERT model on a sentence pair classification task with a legal case entailment dataset in order to adapt it to modeling semantic relationships between legal paragraphs. In the final stage, BERT-PLI conducts relevance prediction with the fine-tuned BERT (Stage 2) among the selected candidates (Stage 1).", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["A variety of approaches as well as expert knowledge are involved in this task [Bench-Capon et al., 2012], e.g., logical analysis, lexical matching, distributed representation, etc. For instance, decomposition of legal issues [Zeng et al., 2005], ontological frameworks [Saravanan et al., 2009], and link analysis [Monroy et al., 2013] have been explored. Generally speaking, methods can be grouped into two broad categories: those based on manual knowledge engineering (KE) and those based on natural language processing (NLP) [Maxwell and Schafer, 2008]. The competitions held recently, such as COLIEE, are mostly aimed at exploring the application of NLP-based methods and providing benchmarks for the legal case retrieval task. In COLIEE 2019, both traditional retrieval models and neural models are explored in the legal case retrieval task. Specifically, [Tran et al., 2019] combined distributed representation with lexical matching features via LTR algorithms while [Rossi and Kanoulas, 2019] utilized BERT with the help of automatic summarization algorithms. 3 Method. 3.1 Task Description. The legal case retrieval task involves finding prior cases that should be \u201cnoticed\u201d concerning a given query case in the set of candidate cases [Rabelo et al., 2019]. \u201cNoticed case\u201d is a legal technical term denoting that a precedent is relevant to a query case, in other words, it supports the decision of a query case. Formally, given a query case q, and a set of candidate cases D = {d1, d2, . . . , dn}, the task is to identify the supporting cases D\u2217 = {d\u2217i | d\u2217i \u2208 D \u2227 noticed(d\u2217i , q)}, where noticed(d\u2217i , q) denotes that d \u2217 i should be noticed given the query case q. Both the query and the candidates are legal documents containing long texts, which consist of the facts in a case.\n3.2 Architecture Overview. In general, we deal with the legal case retrieval task within a multi-stage pipeline inspired by the cascade framework. As illustrated in Figure 1, it consists of three stages.", "Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage.", "[Mihalcea and Tarau, 2004] first and assessed pairwise relevance by a carefully fine-tuned BERT model, combined with oversampling strategies. 4.3 Experimental Settings. In the raw legal case documents, along with the text body of the case (known as \u201cfacts\u201d), some meta information is also provided, such as the court, the date, the head note and so on. However, the types of metadata vary with documents and have a high missing rate, in which case, we build our model based on the text body of a case. Some cases contain both\nEnglish and French versions of description, and only the English one is considered in our experiments. As illustrated in Figure 1 (Stage 1), we select candidates according to BM25 scores. We set K = 50 (top 50 candidates for each query), considering both the recall and the effectiveness in the following stages. Note that only the cases that happened before the current case could be noticed according to the problem definition, those invalid cases in terms of time are dismissed, which would otherwise be noisy in the candidates. The recall of Stage 1 on the training and testing set are 0.9159 and 0.9273, respectively. The relative high recall suggests that setting K = 50 in Stage 1 brings little harm to the overall retrieval performance. In Stage 2, paragraph pairs are constructed using the decision and candidate paragraphs in Task 2. The paragraphs have no more than 100 words on average and we truncate the words symmetrically if the pair exceeds the maximum input length of BERT. We use the uncased base version of BERT.1 At first, it is fine-tuned on the training data and tested on the remaining testing data. We use the Adam optimizer and set the learning rate as 10\u22125. The fine-tuning procedure can coverage after three epochs and F1 on the test set reaches 0.6526, which is better than the results of BM25 in this task. After that, we utilize the same hyperparameter settings but merge the training and testing data to fine-tune BERT for 3 epochs from scratch."]}
{"pkey": "bert-pli_18", "question": "What experiments are conducted in the paper? Provide a brief summary of each experiment by commenting on the task description, input, expected output, evaluation metric.", "answer": "The paper authors compare our model with the following three types of baselines.                                                          \n\u2022 Traditional bag-of-words retrieval models, includingVSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999].\n\u2022 Deep retrieval models. Prior work shows that matching function learning methods usually outperform the representation learning ones [Xu et al., 2018], so the paper authors consider two matching function learning models, ARCII [Hu et al., 2014] and MatchPyramid [Palangi et al., 2016]. Both of them utilize the CNN structure, which is faster than RNN-based models, especially when dealing with long texts. The paper authors do not include the state-of-art neural models that involve rich behavioral signals (e.g., click) since the paper authors focus on text-based retrieval here.\n\u2022 Methods in the competition. The paper authors also compare with the methods of the top 2 teams [Tran et al., 2019; Rossi and Kanoulas, 2019] in the competition. The champion team (named as \u201cJNLP\u201d) trained a supervised summarization model based on COLIEE 2018\u2019s dataset\nand applied the model to encoding the case document into a continuous vector. They combined such the summary embeddings with lexical matching features, calculated by ROUGE [Lin, 2004], and learned the document rankings via RankSVM. Another team ranked following JNLP (named as \u201cILPS\u201d) generated summaries by the TextRank algorithm [Mihalcea and Tarau, 2004] first and assessed pairwise relevance by a carefully fine-tuned\nBERT model, combined with oversampling strategies.                                                                                                                                    \nIn Stage 2, paragraph pairs are constructed using the decision and candidate paragraphs in Task 2. The paragraphs have no more than 100 words on average and the paper authors truncate the words symmetrically if the pair exceeds the maximum input length of BERT. The paper authors use the uncased base version of BERT.1 At first, it is fine-tuned on the training data and tested on the remaining testing data. The paper authors use the Adam optimizer and set the learning rate as 10\u22125. The fine-tuning procedure can coverage after three epochs and F 1 on the test set reaches 0.6526, which is better than the results of BM25 in this task. After that, the paper authors utilize the same hyperparameter settings but merge the training and testing data to fine-tune BERT for 3 epochs from scratch.\nIn Stage 3, the paragraph segmentation given by the original documents is adopted. Similarly, if the paragraph pair exceeds the maximum input length, the paper authors simply truncate the\ntexts. The paper authors set N = 54 and M = 40, which can cover 3/4 of paragraphs in most query and candidate cases. In BERT-PLI, HB = 768, which is determined by the size of the BERT\nhidden vector. As for RNN, HR is set as 256 and only one hidden layer is used for both LSTM and GRU. The training set is split into two parts. 20% queries from the training set as well as all of their candidates are treated as the validation set. The paper authors train the model on the training data left for no more than 60 epochs and select the best model in the training process according to the F1 measure on the validation set. During the training process, the paper authors use the Adam optimizer and set the start learning rate as 10\u22124 with a weight decay of 10\u22126.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["The results emphasize the importance of considering the full document information in the legal case retrieval task. On the other hand, traditional retrieval models give relatively good results. They take advantage of the whole document though they are weak in semantic understanding. Ablation study. We further investigate the effects of the fine-tuning stage. We use the original parameters of the pretrained BERT model rather than the fine-tuned one to infer the paragraph interactions. Then the remaining parts of BERTPLI are trained and evaluated under the same experimental settings. The model without Stage 2 is denoted as BERTPLIorg . As shown in Table 2, there is a big drop in the performance for both the LSTM and GRU versions compared with the results of BERT-PLI. Without Stage 2, the model has a similar performance with traditional retrieval models. Recall that we do not update the parameters of BERT during training BERT-PLI. Therefore, fine-tuning is essential for the BERT model to well represent the semantic relationships between paragraphs. In conclusion, the experimental results suggest that it is useful and effective to adapt BERT to a paragraphlevel modeling task in the legal domain. Comparison with results in the competition. Table 3 shows the best results of the top two teams in the competition leaderboard. In terms of the final evaluation results on the test set, our methods (BERT-PLI (GRU/LSTM)) achieve a better recall and F1 score, even though Stage 1 hurts the recall of our approach a bit. The results show that our method can reach a better balance in precision and recall. Further, with the summary encoding and lexical features provided by the JNLP group, we conduct an additional experiment, which combines the outputs of BERT-PLI with their features. The experiment is two-fold. In order to combine the two approaches, we first arrange their model in the cascade framework.", ",hqkN ], hqki \u2208 RHR , (2)\nwhere hqki = RNN(hqk(i\u22121),p\u2032qki) is generated by LSTM or GRU in practice. The attention mechanism is also employed to infer the importance of each position. The attention weight of each position is measured by:\n\u03b1qki = exp(hqki \u00b7 uqk)\u2211 i\u2032 exp(hqki\u2032 \u00b7 uqk) , (3)\nwhere hqki is the i-th hidden state given by the forward RNN and uqk is generated as follows: uqk = Wu \u00b7MaxPool(hqk) + bu, (4)\nwhere Wu \u2208 RHR\u00d7HR , bu \u2208 RHR . We can then get the document-level representation via attentive aggregation:\ndqk = \u2211 i \u03b1qkihqki. (5)\nFinally, the representation dqk is passed through a fullyconnected layer followed by a softmax function to make prediction as follows: y\u0302qk = softmax(Wp \u00b7 dqk + bp), (6)\nwhere Wp \u2208 R|R|\u00d7HR , bp \u2208 R|R|, and R denotes the set of relevance labels, e.g., R = {0, 1} and |R| = 2. During the training procedure, we optimize the following cross-entropy loss:\nLqk (y\u0302qk,yqk) = \u2212 |R|\u2211 r=1 yqkr log (y\u0302qkr) , (7) In the practice of the legal case retrieval task, BERT-PLI is combined with the two stages mentioned above. To be specific, the candidate documents at the input of BERT-PLI are those selected by BM25 in Stage 1. After Stage 2, we consider that the BERT is able to well represent the paragraphlevel semantic relationships in legal case documents. Hence, we utilize the fine-tuned parameters directly without updating them when training BERT-PLI. The multi-stage operations make the training process easy and affordable. As for testing, we return the ones that are predicted as relevant cases by BERT-PLI corresponding to a given query case. 4 Experiments. 4.1 Datasets and Evaluation Metrics. Our experiments are conducted based on the COLIEE 2019 datasets [Rabelo et al., 2019]. Task 1 is a legal case retrieval task, which involves reading a new caseQ and extracting supporting cases S1, S2, . . . , Sn for the decision of Q from the case law corpus.", "The supporting cases are considered as \u201crelevant cases\u201d or \u201cnoticed cases\u201d for the query case Q. Task 2 is a legal case entailment task to identify paragraphs that entail the given decision paragraph of a query case from a given relevant case. Data in both tasks are sampled from a database\nof predominantly Federal Court of Canada case laws. Table 1 gives a statistical summary of the raw datasets in these two tasks. Task 1 is the main focus of our work, while the data of Task 2 are used to fine-tune BERT in Stage 2. We follow the evaluation metrics in the competition. Micro-average of precision, recall, and F1 are used.\n4.2 Baseline Methods. We compare our model with the following three types of baselines. \u2022 Traditional bag-of-words retrieval models, including VSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999].\n\u2022 Deep retrieval models. Prior work shows that matching function learning methods usually outperform the representation learning ones [Xu et al., 2018], so we consider two matching function learning models, ARCII [Hu et al., 2014] and MatchPyramid [Palangi et al., 2016]. Both of them utilize the CNN structure, which is faster than RNN-based models, especially when dealing with long texts. We do not include the state-of-art neural models that involve rich behavioral signals (e.g., click) since we focus on text-based retrieval here. \u2022 Methods in the competition. We also compare with the methods of the top 2 teams [Tran et al., 2019; Rossi and Kanoulas, 2019] in the competition. The champion team (named as \u201cJNLP\u201d) trained a supervised summarization model based on COLIEE 2018\u2019s dataset and applied the model to encoding the case document into a continuous vector. They combined such the summary embeddings with lexical matching features, calculated by ROUGE [Lin, 2004], and learned the document rankings via RankSVM. Another team ranked following JNLP (named as \u201cILPS\u201d) generated summaries by the TextRank algorithm"]}
{"pkey": "bert-pli_19", "question": "Are ablation studies conducted in the paper? If yes, which parameters are included in the ablation study?", "answer": "Ablation study. The paper authors further investigate the effects of the fine-tuning stage. The paper authors use the original parameters of the pre-trained BERT model rather than the fine-tuned one to infer the paragraph interactions. Then the remaining parts of BERT-PLI are trained and evaluated under the same experimental settings. The model without Stage 2 is denoted as BERT-PLI org . As shown in Table 2, there is a big drop in the performance for both the LSTM and GRU versions compared with the results of BERT-PLI. Without Stage 2, the model has a similar performance with traditional retrieval models. Recall that the paper authors do not update the parameters of BERT during training BERT-PLI. Therefore, fine-tuning is essential for the BERT model to well represent the semantic relationships between paragraphs. In conclusion, the experimental results suggest that it is useful and effective to adapt BERT to a paragraph level modeling task in the legal domain.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["The results emphasize the importance of considering the full document information in the legal case retrieval task. On the other hand, traditional retrieval models give relatively good results. They take advantage of the whole document though they are weak in semantic understanding. Ablation study. We further investigate the effects of the fine-tuning stage. We use the original parameters of the pretrained BERT model rather than the fine-tuned one to infer the paragraph interactions. Then the remaining parts of BERTPLI are trained and evaluated under the same experimental settings. The model without Stage 2 is denoted as BERTPLIorg . As shown in Table 2, there is a big drop in the performance for both the LSTM and GRU versions compared with the results of BERT-PLI. Without Stage 2, the model has a similar performance with traditional retrieval models. Recall that we do not update the parameters of BERT during training BERT-PLI. Therefore, fine-tuning is essential for the BERT model to well represent the semantic relationships between paragraphs. In conclusion, the experimental results suggest that it is useful and effective to adapt BERT to a paragraphlevel modeling task in the legal domain. Comparison with results in the competition. Table 3 shows the best results of the top two teams in the competition leaderboard. In terms of the final evaluation results on the test set, our methods (BERT-PLI (GRU/LSTM)) achieve a better recall and F1 score, even though Stage 1 hurts the recall of our approach a bit. The results show that our method can reach a better balance in precision and recall. Further, with the summary encoding and lexical features provided by the JNLP group, we conduct an additional experiment, which combines the outputs of BERT-PLI with their features. The experiment is two-fold. In order to combine the two approaches, we first arrange their model in the cascade framework.", "In order to enhance the ability to model the semantic relationships between legal paragraphs, we fine-tune the BERT model with an accessible entailment dataset in the legal domain before applying it to BERT-PLI. The ablation study also supports the effectiveness of the fine-tuning stage. Finally, BERT-PLI is employed to further identify the relevant cases with respect to a query case. We conduct extensive experiments on the datasets of COLIEE 2019. The experimental results demonstrate that our approach is effective in legal case retrieval and the combination with BERT-PLI can further improve other models for this task.\nAcknowledgements. This work is supported by the National Key Research and Development Program of China (2018YFC0831700), Natural Science Foundation of China (Grant No. 61732008, 61532011, 61902209), Beijing Academy of Artificial Intelligence (BAAI), JST CREST Grant Number JPMJCR1513 and JSPS KAKENHI Grant Number JP17H06103. 5https://github.com/ThuYShao/BERT-PLI-IJCAI2020. The implementation has been available.", "In practice, we employ the cascade framework to avoid high computational cost and arrange the model in a multi-stage pipeline for legal case retrieval. Specifically, we select top-K candidates according to the BM25 rankings and fine-tune BERT with an extra legal dataset before applying BERT-PLI to relevance prediction. Experiments are conducted on the COLIEE 2019 [Rabelo et al., 2019] legal case retrieval task and the results demonstrate the effectiveness of our proposed method. 2 Related Work. A large number of retrieval models, especially for ad-hoc text retrieval, have been proposed in the past decades. Traditional bag-of-words IR models, including VSM [Salton and Buckley, 1988], BM25 [Robertson and Walker, 1994], and LMIR [Song and Croft, 1999], which are widely applied in search systems, are mostly based on term-level matching. Since the mid-2000s, LTR (Learning to Rank) methods [Liu and others, 2009], which are driven heavily by manual feature engineering, have been well studied and utilized by commercial web search engines as well. In recent years, the development of deep learning has also inspired applications of neural models in IR. Generally, the methods can be categorized into two types [Xu et al., 2018], methods of representation learning and methods of matching function learning. Based on the idea of representation learning, queries and documents are represented in the latent space by deep learning models, and the query-document relevance score is calculated based on their latent representations with a vector space scoring function, e.g., cosine similarity. Various neural network models have been applied to this task. For instance, DSSM [Hu et al., 2014] utilized DNN in the early stage. Further, some studies exploit CNNs to capture the local interactions [Hu et al., 2014; Shen et al., 2014], while some studies apply RNNs to modeling text sequences [Palangi et al., 2016]. However, most of the model structures are not designed for representing long documents."]}
{"pkey": "bert-pli_20", "question": "List the future work mentioned in the paper.", "answer": "No future directions for extending the work are specified in paper.", "title": "BERT-PLI: Modeling Paragraph-Level Interactions for Legal Case Retrieval", "context": ["Similarly, we conduct the operations of Stage 1 to select candidate cases and get the dataset D1, which is the same as the one for BERT-PLI. Then, a RankSVM model is trained and the top 5 documents are used as the noticed cases. Since their model combines two types of features, the representation-based features (denoted as \u201cEM\u201d) and the term-matching features (denoted as \u201cROUGE\u201d), we conduct experiments on each type of features separately. As shown in Table 4, the performance of all kinds of features goes down, which might result from that we do not use the same training set (we only use D1 instead of the whole corpus to train the RankSVM). We further append the probabilities of relevance predicted by BERT-PLI to their features and then apply the RankSVM algorithm. The\nGRU and LSTM versions achieve similar performance but GRU gives a slightly better result in the prior experiments (Table 2), so we use the probabilities given by BERT-PLI (GRU) as the features. The results are given in the second part of Table 4, which show that combination with our model can lead to improvements in all types of their features. Compared among different types of features, our model can improve the representation-based features by a larger amount. We assume that the embedding features are generated by a summarization model while our method considers the whole document, and these two aspects might be complementary. 5 Conclusions. In this paper, we propose to address the problem of legal case retrieval. To tackle the challenge raised by the long and complex legal documents, we introduce a novel model, BERTPLI5, which models the paragraph-level interactions of case documents via BERT and then aggregate these interactions to infer the document relevance via a sequential modeling mechanism. We propose to arrange BERT-PLI in a multi-stage pipeline in the practice of legal case retrieval. To be specific, we prune the candidate set according to BM25 rankings in the first stage.", ",hqkN ], hqki \u2208 RHR , (2)\nwhere hqki = RNN(hqk(i\u22121),p\u2032qki) is generated by LSTM or GRU in practice. The attention mechanism is also employed to infer the importance of each position. The attention weight of each position is measured by:\n\u03b1qki = exp(hqki \u00b7 uqk)\u2211 i\u2032 exp(hqki\u2032 \u00b7 uqk) , (3)\nwhere hqki is the i-th hidden state given by the forward RNN and uqk is generated as follows: uqk = Wu \u00b7MaxPool(hqk) + bu, (4)\nwhere Wu \u2208 RHR\u00d7HR , bu \u2208 RHR . We can then get the document-level representation via attentive aggregation:\ndqk = \u2211 i \u03b1qkihqki. (5)\nFinally, the representation dqk is passed through a fullyconnected layer followed by a softmax function to make prediction as follows: y\u0302qk = softmax(Wp \u00b7 dqk + bp), (6)\nwhere Wp \u2208 R|R|\u00d7HR , bp \u2208 R|R|, and R denotes the set of relevance labels, e.g., R = {0, 1} and |R| = 2. During the training procedure, we optimize the following cross-entropy loss:\nLqk (y\u0302qk,yqk) = \u2212 |R|\u2211 r=1 yqkr log (y\u0302qkr) , (7) In the practice of the legal case retrieval task, BERT-PLI is combined with the two stages mentioned above. To be specific, the candidate documents at the input of BERT-PLI are those selected by BM25 in Stage 1. After Stage 2, we consider that the BERT is able to well represent the paragraphlevel semantic relationships in legal case documents. Hence, we utilize the fine-tuned parameters directly without updating them when training BERT-PLI. The multi-stage operations make the training process easy and affordable. As for testing, we return the ones that are predicted as relevant cases by BERT-PLI corresponding to a given query case. 4 Experiments. 4.1 Datasets and Evaluation Metrics. Our experiments are conducted based on the COLIEE 2019 datasets [Rabelo et al., 2019]. Task 1 is a legal case retrieval task, which involves reading a new caseQ and extracting supporting cases S1, S2, . . . , Sn for the decision of Q from the case law corpus.", "In particular, it is difficult for CNN models to represent the complex global semantic information while RNN models tend to forget important signals when dealing with a long sequence. On the other hand, matching function learning methods first construct a matching matrix or capture local interactions based on the word-level matching, and then use neural networks to discover the high-level matching patterns and aggregate the final relevance score. Well-known models include ARC-II [Hu et al., 2014], MatchPyramid [Pang et al., 2016], and Match-SRNN [Wan et al., 2016]. Although these models work well for ad-hoc text retrieval, where the query is quite short, their performances are restricted in the scenario of legal case retrieval due to its quadratic time and memory complexity in constructing the whole interaction matrix. Since BERT [Devlin et al., 2018] has reached stateof-art performance in 11 NLP tasks, pre-trained language models have drawn great attention in many fields related to NLP. Recently, some works have shed light on the application of BERT to ad-hoc retrieval [Dai and Callan, 2019; Yilmaz et al., 2019] by modeling evidence in query-sentence\nor query-passage pairs. As for legal case retrieval, it is still worth investigating how to utilize BERT to model the relationship between long case documents. Finding relevant materials is fundamental in the legal field. In countries following the common law system, prior cases are one of the primary sources of law. Thus, legal case retrieval is an important research topic. Meanwhile, legal case retrieval is challenging because it is different from ad-hoc retrieval in various aspects, including the definition of relevance in law [Van Opijnen and Santos, 2017] and the characteristics of legal documents, such as the document length, professional legal expressions, and the logical structures behind natural languages [Turtle, 1995]."]}
